{
  "dataset_revision": "bd27d0058bea2ad52470d9072a3b5da6b97c1ac3",
  "task_name": "VaccinChatNLClassification",
  "mteb_version": "2.1.8",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.423932,
            "f1": 0.486998,
            "f1_weighted": 0.420396,
            "precision": 0.480344,
            "precision_weighted": 0.510189,
            "recall": 0.603296,
            "recall_weighted": 0.423932,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.421368,
            "f1": 0.473994,
            "f1_weighted": 0.416173,
            "precision": 0.468427,
            "precision_weighted": 0.519484,
            "recall": 0.595376,
            "recall_weighted": 0.421368,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.417949,
            "f1": 0.474568,
            "f1_weighted": 0.404382,
            "precision": 0.456509,
            "precision_weighted": 0.48731,
            "recall": 0.605208,
            "recall_weighted": 0.417949,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.445299,
            "f1": 0.480688,
            "f1_weighted": 0.44143,
            "precision": 0.466255,
            "precision_weighted": 0.537719,
            "recall": 0.600651,
            "recall_weighted": 0.445299,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.439316,
            "f1": 0.495169,
            "f1_weighted": 0.430843,
            "precision": 0.473618,
            "precision_weighted": 0.50739,
            "recall": 0.603343,
            "recall_weighted": 0.439316,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.442735,
            "f1": 0.492909,
            "f1_weighted": 0.442738,
            "precision": 0.482255,
            "precision_weighted": 0.536645,
            "recall": 0.603685,
            "recall_weighted": 0.442735,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.439316,
            "f1": 0.497982,
            "f1_weighted": 0.424703,
            "precision": 0.488567,
            "precision_weighted": 0.5111,
            "recall": 0.614741,
            "recall_weighted": 0.439316,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.423077,
            "f1": 0.461852,
            "f1_weighted": 0.416306,
            "precision": 0.450871,
            "precision_weighted": 0.511199,
            "recall": 0.58146,
            "recall_weighted": 0.423077,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.437607,
            "f1": 0.491327,
            "f1_weighted": 0.426565,
            "precision": 0.480854,
            "precision_weighted": 0.530545,
            "recall": 0.612063,
            "recall_weighted": 0.437607,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.42906,
            "f1": 0.461778,
            "f1_weighted": 0.420537,
            "precision": 0.454377,
            "precision_weighted": 0.516648,
            "recall": 0.578696,
            "recall_weighted": 0.42906,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.431966,
        "f1": 0.481726,
        "f1_weighted": 0.424407,
        "precision": 0.470208,
        "precision_weighted": 0.516823,
        "recall": 0.599852,
        "recall_weighted": 0.431966,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.481726,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 76.37751293182373,
  "kg_co2_emissions": null
}