{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.77646,
        "f1": 0.658383,
        "f1_weighted": 0.807053,
        "ap": 0.282868,
        "ap_weighted": 0.282868,
        "scores_per_experiment": [
          {
            "accuracy": 0.620275,
            "f1": 0.526504,
            "f1_weighted": 0.684358,
            "ap": 0.180321,
            "ap_weighted": 0.180321
          },
          {
            "accuracy": 0.837629,
            "f1": 0.708333,
            "f1_weighted": 0.853812,
            "ap": 0.321384,
            "ap_weighted": 0.321384
          },
          {
            "accuracy": 0.829897,
            "f1": 0.714514,
            "f1_weighted": 0.850479,
            "ap": 0.342951,
            "ap_weighted": 0.342951
          },
          {
            "accuracy": 0.823024,
            "f1": 0.701884,
            "f1_weighted": 0.844248,
            "ap": 0.322104,
            "ap_weighted": 0.322104
          },
          {
            "accuracy": 0.821306,
            "f1": 0.658266,
            "f1_weighted": 0.835095,
            "ap": 0.245005,
            "ap_weighted": 0.245005
          },
          {
            "accuracy": 0.787801,
            "f1": 0.672324,
            "f1_weighted": 0.818049,
            "ap": 0.295809,
            "ap_weighted": 0.295809
          },
          {
            "accuracy": 0.813574,
            "f1": 0.683032,
            "f1_weighted": 0.835418,
            "ap": 0.292261,
            "ap_weighted": 0.292261
          },
          {
            "accuracy": 0.641753,
            "f1": 0.558686,
            "f1_weighted": 0.702119,
            "ap": 0.216957,
            "ap_weighted": 0.216957
          },
          {
            "accuracy": 0.79811,
            "f1": 0.675774,
            "f1_weighted": 0.824973,
            "ap": 0.29231,
            "ap_weighted": 0.29231
          },
          {
            "accuracy": 0.791237,
            "f1": 0.684517,
            "f1_weighted": 0.821976,
            "ap": 0.319578,
            "ap_weighted": 0.319578
          }
        ],
        "main_score": 0.77646,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 39.321532011032104,
  "kg_co2_emissions": 0.0012724958577299556
}