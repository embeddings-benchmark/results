{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 10295.686977386475,
  "kg_co2_emissions": 0.7890842590118139,
  "mteb_version": "1.18.0",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.05695113251293027,
        "map": 0.07542884361949234,
        "mrr": 0.05695113251293027,
        "nAUC_map_diff1": -0.1847649646140814,
        "nAUC_map_max": -0.183580567119166,
        "nAUC_map_std": 0.23225228542740708,
        "nAUC_mrr_diff1": -0.1649308979280266,
        "nAUC_mrr_max": -0.18335756955597432,
        "nAUC_mrr_std": 0.17025885903304264
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08547123419478993,
        "map": 0.10606927397575343,
        "mrr": 0.08547123419478993,
        "nAUC_map_diff1": 0.029054300850785593,
        "nAUC_map_max": -0.03562839253398648,
        "nAUC_map_std": -0.040648938278785254,
        "nAUC_mrr_diff1": 0.014458532417764645,
        "nAUC_mrr_max": -0.04243957338889384,
        "nAUC_mrr_std": -0.04730747065329754
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08510856127337352,
        "map": 0.10343605768083611,
        "mrr": 0.08510856127337352,
        "nAUC_map_diff1": 0.014423199637121931,
        "nAUC_map_max": 0.06238720444734144,
        "nAUC_map_std": 0.1702827155911208,
        "nAUC_mrr_diff1": 0.0073179077118746055,
        "nAUC_mrr_max": 0.07093113995985931,
        "nAUC_mrr_std": 0.16642391379256322
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08617919610275217,
        "map": 0.10502055362788233,
        "mrr": 0.08617919610275217,
        "nAUC_map_diff1": 0.007155537280987096,
        "nAUC_map_max": -0.041102829506652505,
        "nAUC_map_std": 0.1651908320109141,
        "nAUC_mrr_diff1": 0.008690472644560772,
        "nAUC_mrr_max": -0.0397374596001046,
        "nAUC_mrr_std": 0.14414523323080158
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.06362055212246052,
        "map": 0.0813251604023409,
        "mrr": 0.06362055212246052,
        "nAUC_map_diff1": 0.11269885044316862,
        "nAUC_map_max": 0.09246331875423924,
        "nAUC_map_std": 0.20705027284344216,
        "nAUC_mrr_diff1": 0.1155487421560502,
        "nAUC_mrr_max": 0.09525601396302685,
        "nAUC_mrr_std": 0.18450825847337915
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.11602582872759662,
        "map": 0.1346786163968676,
        "mrr": 0.11602582872759662,
        "nAUC_map_diff1": 0.1135407251601269,
        "nAUC_map_max": -0.11975549045017533,
        "nAUC_map_std": 0.02936072478758296,
        "nAUC_mrr_diff1": 0.11703149834649493,
        "nAUC_mrr_max": -0.11599918499358992,
        "nAUC_mrr_std": 0.02551087175285815
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}