{
  "dataset_revision": "ef5c383d1b87eb8feccde3dfb7f95e42b1b050dd",
  "evaluation_time": 35.2419593334198,
  "kg_co2_emissions": 0.001966533955422057,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "cosine_pearson": 0.7392813323602938,
        "cosine_spearman": 0.7393638466946258,
        "euclidean_pearson": 0.715322348006625,
        "euclidean_spearman": 0.7272027997209788,
        "hf_subset": "afr",
        "languages": [
          "afr-Latn"
        ],
        "main_score": 0.7393638466946258,
        "manhattan_pearson": 0.7170747782512598,
        "manhattan_spearman": 0.7282908016948357,
        "pearson": 0.7392813323602938,
        "spearman": 0.7393638466946258
      },
      {
        "cosine_pearson": 0.21675007776209493,
        "cosine_spearman": 0.20696430865382204,
        "euclidean_pearson": 0.19243310125652488,
        "euclidean_spearman": 0.18625106613695747,
        "hf_subset": "amh",
        "languages": [
          "amh-Ethi"
        ],
        "main_score": 0.20696430865382204,
        "manhattan_pearson": 0.19211194202062018,
        "manhattan_spearman": 0.18635441535767303,
        "pearson": 0.21675007776209493,
        "spearman": 0.20696430865382204
      },
      {
        "cosine_pearson": 0.1373319126650847,
        "cosine_spearman": 0.12541798515049798,
        "euclidean_pearson": 0.11486055414365173,
        "euclidean_spearman": 0.09389953536646038,
        "hf_subset": "arb",
        "languages": [
          "arb-Arab"
        ],
        "main_score": 0.12541798515049798,
        "manhattan_pearson": 0.11692321117016549,
        "manhattan_spearman": 0.09569903469083875,
        "pearson": 0.1373319126650847,
        "spearman": 0.12541798515049798
      },
      {
        "cosine_pearson": 0.38162871608793136,
        "cosine_spearman": 0.35541955990737023,
        "euclidean_pearson": 0.3469932546251433,
        "euclidean_spearman": 0.31147347329850406,
        "hf_subset": "arq",
        "languages": [
          "arq-Arab"
        ],
        "main_score": 0.35541955990737023,
        "manhattan_pearson": 0.3490670195455891,
        "manhattan_spearman": 0.3122067132801464,
        "pearson": 0.38162871608793136,
        "spearman": 0.35541955990737023
      },
      {
        "cosine_pearson": 0.039220137612640274,
        "cosine_spearman": 0.005382931172117553,
        "euclidean_pearson": -0.011333643355402974,
        "euclidean_spearman": -0.04226375506548513,
        "hf_subset": "ary",
        "languages": [
          "ary-Arab"
        ],
        "main_score": 0.005382931172117553,
        "manhattan_pearson": -0.010943481298567191,
        "manhattan_spearman": -0.0417167265749324,
        "pearson": 0.039220137612640274,
        "spearman": 0.005382931172117553
      },
      {
        "cosine_pearson": 0.8202791889164663,
        "cosine_spearman": 0.8070190114829279,
        "euclidean_pearson": 0.8020853322911536,
        "euclidean_spearman": 0.7811070411576991,
        "hf_subset": "eng",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8070190114829279,
        "manhattan_pearson": 0.8012868597881977,
        "manhattan_spearman": 0.7801470224846715,
        "pearson": 0.8202791889164663,
        "spearman": 0.8070190114829279
      },
      {
        "cosine_pearson": 0.31764693557142165,
        "cosine_spearman": 0.2919001748213383,
        "euclidean_pearson": 0.33239778147343146,
        "euclidean_spearman": 0.29356178038931335,
        "hf_subset": "hau",
        "languages": [
          "hau-Latn"
        ],
        "main_score": 0.2919001748213383,
        "manhattan_pearson": 0.3318445171754154,
        "manhattan_spearman": 0.29251596386636913,
        "pearson": 0.31764693557142165,
        "spearman": 0.2919001748213383
      },
      {
        "cosine_pearson": 0.42931173052886545,
        "cosine_spearman": 0.4201892320839924,
        "euclidean_pearson": 0.44683636548242023,
        "euclidean_spearman": 0.41824282725803064,
        "hf_subset": "hin",
        "languages": [
          "hin-Deva"
        ],
        "main_score": 0.4201892320839924,
        "manhattan_pearson": 0.44566526343981494,
        "manhattan_spearman": 0.41691403878053257,
        "pearson": 0.42931173052886545,
        "spearman": 0.4201892320839924
      },
      {
        "cosine_pearson": 0.3813935894352337,
        "cosine_spearman": 0.4006530418753548,
        "euclidean_pearson": 0.41118524093807723,
        "euclidean_spearman": 0.3851208092279266,
        "hf_subset": "ind",
        "languages": [
          "ind-Latn"
        ],
        "main_score": 0.4006530418753548,
        "manhattan_pearson": 0.4115765519491969,
        "manhattan_spearman": 0.38771166833425713,
        "pearson": 0.3813935894352337,
        "spearman": 0.4006530418753548
      },
      {
        "cosine_pearson": 0.434964254084493,
        "cosine_spearman": 0.4084852027472707,
        "euclidean_pearson": 0.44764447605338215,
        "euclidean_spearman": 0.42448325755570215,
        "hf_subset": "kin",
        "languages": [
          "kin-Latn"
        ],
        "main_score": 0.4084852027472707,
        "manhattan_pearson": 0.4490633378536058,
        "manhattan_spearman": 0.42863602352377866,
        "pearson": 0.434964254084493,
        "spearman": 0.4084852027472707
      },
      {
        "cosine_pearson": 0.4349680680534308,
        "cosine_spearman": 0.4357650822440733,
        "euclidean_pearson": 0.46617228649548403,
        "euclidean_spearman": 0.4413986068635849,
        "hf_subset": "mar",
        "languages": [
          "mar-Deva"
        ],
        "main_score": 0.4357650822440733,
        "manhattan_pearson": 0.4629279559260908,
        "manhattan_spearman": 0.43769002480973057,
        "pearson": 0.4349680680534308,
        "spearman": 0.4357650822440733
      },
      {
        "cosine_pearson": 0.21397526075384804,
        "cosine_spearman": 0.2773922239835666,
        "euclidean_pearson": 0.26412961606870217,
        "euclidean_spearman": 0.27452324192242106,
        "hf_subset": "tel",
        "languages": [
          "tel-Telu"
        ],
        "main_score": 0.2773922239835666,
        "manhattan_pearson": 0.2639825516848789,
        "manhattan_spearman": 0.2746665073368179,
        "pearson": 0.21397526075384804,
        "spearman": 0.2773922239835666
      }
    ]
  },
  "task_name": "SemRel24STS"
}