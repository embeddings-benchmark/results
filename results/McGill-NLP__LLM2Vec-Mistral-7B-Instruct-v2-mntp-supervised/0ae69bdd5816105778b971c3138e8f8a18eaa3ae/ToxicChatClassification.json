{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.819845,
        "f1": 0.701189,
        "f1_weighted": 0.841775,
        "ap": 0.327048,
        "ap_weighted": 0.327048,
        "scores_per_experiment": [
          {
            "accuracy": 0.744845,
            "f1": 0.624481,
            "f1_weighted": 0.783749,
            "ap": 0.24244,
            "ap_weighted": 0.24244
          },
          {
            "accuracy": 0.880584,
            "f1": 0.733305,
            "f1_weighted": 0.881776,
            "ap": 0.343208,
            "ap_weighted": 0.343208
          },
          {
            "accuracy": 0.844502,
            "f1": 0.72735,
            "f1_weighted": 0.861238,
            "ap": 0.355935,
            "ap_weighted": 0.355935
          },
          {
            "accuracy": 0.841924,
            "f1": 0.731737,
            "f1_weighted": 0.860535,
            "ap": 0.369621,
            "ap_weighted": 0.369621
          },
          {
            "accuracy": 0.7689,
            "f1": 0.657919,
            "f1_weighted": 0.803885,
            "ap": 0.285391,
            "ap_weighted": 0.285391
          },
          {
            "accuracy": 0.841065,
            "f1": 0.715673,
            "f1_weighted": 0.857125,
            "ap": 0.333531,
            "ap_weighted": 0.333531
          },
          {
            "accuracy": 0.812715,
            "f1": 0.696649,
            "f1_weighted": 0.837218,
            "ap": 0.321837,
            "ap_weighted": 0.321837
          },
          {
            "accuracy": 0.780928,
            "f1": 0.672866,
            "f1_weighted": 0.813718,
            "ap": 0.304945,
            "ap_weighted": 0.304945
          },
          {
            "accuracy": 0.83677,
            "f1": 0.714357,
            "f1_weighted": 0.854441,
            "ap": 0.334985,
            "ap_weighted": 0.334985
          },
          {
            "accuracy": 0.84622,
            "f1": 0.737552,
            "f1_weighted": 0.864065,
            "ap": 0.37859,
            "ap_weighted": 0.37859
          }
        ],
        "main_score": 0.819845,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 16.576769590377808,
  "kg_co2_emissions": 0.0008983668597610844
}