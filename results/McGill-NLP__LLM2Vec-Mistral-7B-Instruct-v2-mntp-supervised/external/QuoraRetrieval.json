{
    "dataset_revision": "None",
    "task_name": "QuoraRetrieval",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "default",
                "languages": [
                    "eng-Latn"
                ],
                "map_at_1": 0.69577,
                "map_at_10": 0.8394400000000001,
                "map_at_100": 0.84604,
                "map_at_1000": 0.84618,
                "map_at_3": 0.80936,
                "map_at_5": 0.82812,
                "mrr_at_1": 0.804,
                "mrr_at_10": 0.86734,
                "mrr_at_100": 0.86851,
                "mrr_at_1000": 0.8685199999999998,
                "mrr_at_3": 0.8575500000000001,
                "mrr_at_5": 0.8639600000000001,
                "ndcg_at_1": 0.8043,
                "ndcg_at_10": 0.8775,
                "ndcg_at_100": 0.88999,
                "ndcg_at_1000": 0.8909199999999999,
                "ndcg_at_3": 0.8488,
                "ndcg_at_5": 0.8641599999999999,
                "precision_at_1": 0.8043,
                "precision_at_10": 0.13453,
                "precision_at_100": 0.01539,
                "precision_at_1000": 0.00157,
                "precision_at_3": 0.37403,
                "precision_at_5": 0.24648,
                "recall_at_1": 0.69577,
                "recall_at_10": 0.95233,
                "recall_at_100": 0.99531,
                "recall_at_1000": 0.99984,
                "recall_at_3": 0.86867,
                "recall_at_5": 0.91254,
                "main_score": 0.8775
            }
        ]
    }
}