{
  "dataset_revision": "9af5657575a669dc18c7f897a67287ff7d1a0c65",
  "task_name": "OpenTenderClassification",
  "mteb_version": "2.1.17",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.199108,
            "f1": 0.182152,
            "f1_weighted": 0.182198,
            "precision": 0.182936,
            "precision_weighted": 0.18297,
            "recall": 0.199041,
            "recall_weighted": 0.199108,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.195095,
            "f1": 0.18487,
            "f1_weighted": 0.18478,
            "precision": 0.206834,
            "precision_weighted": 0.206785,
            "recall": 0.195208,
            "recall_weighted": 0.195095,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.19777,
            "f1": 0.17955,
            "f1_weighted": 0.179475,
            "precision": 0.191889,
            "precision_weighted": 0.191838,
            "recall": 0.197838,
            "recall_weighted": 0.19777,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.210925,
            "f1": 0.193208,
            "f1_weighted": 0.193253,
            "precision": 0.202143,
            "precision_weighted": 0.202173,
            "recall": 0.210894,
            "recall_weighted": 0.210925,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.198885,
            "f1": 0.177647,
            "f1_weighted": 0.17769,
            "precision": 0.19414,
            "precision_weighted": 0.194129,
            "recall": 0.198802,
            "recall_weighted": 0.198885,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.206912,
            "f1": 0.190892,
            "f1_weighted": 0.190794,
            "precision": 0.205479,
            "precision_weighted": 0.205439,
            "recall": 0.207022,
            "recall_weighted": 0.206912,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.20602,
            "f1": 0.187855,
            "f1_weighted": 0.187825,
            "precision": 0.204121,
            "precision_weighted": 0.204096,
            "recall": 0.205995,
            "recall_weighted": 0.20602,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.206243,
            "f1": 0.188204,
            "f1_weighted": 0.188167,
            "precision": 0.206181,
            "precision_weighted": 0.206135,
            "recall": 0.206295,
            "recall_weighted": 0.206243,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.198885,
            "f1": 0.184179,
            "f1_weighted": 0.184164,
            "precision": 0.186608,
            "precision_weighted": 0.186652,
            "recall": 0.198988,
            "recall_weighted": 0.198885,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.200892,
            "f1": 0.185554,
            "f1_weighted": 0.185507,
            "precision": 0.199817,
            "precision_weighted": 0.1998,
            "recall": 0.200934,
            "recall_weighted": 0.200892,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.202074,
        "f1": 0.185411,
        "f1_weighted": 0.185385,
        "precision": 0.198015,
        "precision_weighted": 0.198002,
        "recall": 0.202102,
        "recall_weighted": 0.202074,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.185411,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 218.35234570503235,
  "kg_co2_emissions": null
}