{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.239795,
        "f1": 0.236486,
        "f1_weighted": 0.236484,
        "scores_per_experiment": [
          {
            "accuracy": 0.235352,
            "f1": 0.231082,
            "f1_weighted": 0.231082
          },
          {
            "accuracy": 0.204102,
            "f1": 0.200923,
            "f1_weighted": 0.200957
          },
          {
            "accuracy": 0.274414,
            "f1": 0.266819,
            "f1_weighted": 0.266782
          },
          {
            "accuracy": 0.276855,
            "f1": 0.277353,
            "f1_weighted": 0.277345
          },
          {
            "accuracy": 0.229004,
            "f1": 0.22728,
            "f1_weighted": 0.227256
          },
          {
            "accuracy": 0.234375,
            "f1": 0.226569,
            "f1_weighted": 0.226576
          },
          {
            "accuracy": 0.227539,
            "f1": 0.227771,
            "f1_weighted": 0.227777
          },
          {
            "accuracy": 0.242188,
            "f1": 0.239265,
            "f1_weighted": 0.239273
          },
          {
            "accuracy": 0.243164,
            "f1": 0.23964,
            "f1_weighted": 0.239634
          },
          {
            "accuracy": 0.230957,
            "f1": 0.228161,
            "f1_weighted": 0.228164
          }
        ],
        "main_score": 0.239795,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.233936,
        "f1": 0.231374,
        "f1_weighted": 0.231375,
        "scores_per_experiment": [
          {
            "accuracy": 0.21875,
            "f1": 0.212026,
            "f1_weighted": 0.212051
          },
          {
            "accuracy": 0.211914,
            "f1": 0.20983,
            "f1_weighted": 0.209844
          },
          {
            "accuracy": 0.259766,
            "f1": 0.251862,
            "f1_weighted": 0.251827
          },
          {
            "accuracy": 0.269531,
            "f1": 0.270432,
            "f1_weighted": 0.270443
          },
          {
            "accuracy": 0.208984,
            "f1": 0.206964,
            "f1_weighted": 0.206949
          },
          {
            "accuracy": 0.219727,
            "f1": 0.217825,
            "f1_weighted": 0.217826
          },
          {
            "accuracy": 0.212891,
            "f1": 0.21303,
            "f1_weighted": 0.213019
          },
          {
            "accuracy": 0.251465,
            "f1": 0.250166,
            "f1_weighted": 0.250176
          },
          {
            "accuracy": 0.242188,
            "f1": 0.238988,
            "f1_weighted": 0.23897
          },
          {
            "accuracy": 0.244141,
            "f1": 0.242622,
            "f1_weighted": 0.242647
          }
        ],
        "main_score": 0.233936,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 29.73231315612793,
  "kg_co2_emissions": 0.0013005486403699592
}