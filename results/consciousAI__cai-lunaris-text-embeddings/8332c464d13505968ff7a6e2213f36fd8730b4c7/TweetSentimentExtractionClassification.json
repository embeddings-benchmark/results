{
  "dataset_revision": "d604517c81ca91fe16a244d1248fc021f9ecee7a",
  "task_name": "TweetSentimentExtractionClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.468449,
        "f1": 0.46993,
        "f1_weighted": 0.465898,
        "scores_per_experiment": [
          {
            "accuracy": 0.471703,
            "f1": 0.47382,
            "f1_weighted": 0.469994
          },
          {
            "accuracy": 0.469157,
            "f1": 0.469835,
            "f1_weighted": 0.465702
          },
          {
            "accuracy": 0.479061,
            "f1": 0.480357,
            "f1_weighted": 0.474792
          },
          {
            "accuracy": 0.470289,
            "f1": 0.472144,
            "f1_weighted": 0.46728
          },
          {
            "accuracy": 0.490662,
            "f1": 0.491935,
            "f1_weighted": 0.488692
          },
          {
            "accuracy": 0.477929,
            "f1": 0.481639,
            "f1_weighted": 0.477808
          },
          {
            "accuracy": 0.479626,
            "f1": 0.478632,
            "f1_weighted": 0.469681
          },
          {
            "accuracy": 0.468025,
            "f1": 0.468751,
            "f1_weighted": 0.467744
          },
          {
            "accuracy": 0.440011,
            "f1": 0.442789,
            "f1_weighted": 0.440885
          },
          {
            "accuracy": 0.438031,
            "f1": 0.439399,
            "f1_weighted": 0.436403
          }
        ],
        "main_score": 0.468449,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 10.080860376358032,
  "kg_co2_emissions": 0.00043520737981575454
}