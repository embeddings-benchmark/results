{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.616237,
        "f1": 0.5169,
        "f1_weighted": 0.674374,
        "ap": 0.175791,
        "ap_weighted": 0.175791,
        "scores_per_experiment": [
          {
            "accuracy": 0.382302,
            "f1": 0.351163,
            "f1_weighted": 0.457647,
            "ap": 0.12483,
            "ap_weighted": 0.12483
          },
          {
            "accuracy": 0.75,
            "f1": 0.610253,
            "f1_weighted": 0.785087,
            "ap": 0.215065,
            "ap_weighted": 0.215065
          },
          {
            "accuracy": 0.707045,
            "f1": 0.576066,
            "f1_weighted": 0.752594,
            "ap": 0.192796,
            "ap_weighted": 0.192796
          },
          {
            "accuracy": 0.539519,
            "f1": 0.477859,
            "f1_weighted": 0.612277,
            "ap": 0.171527,
            "ap_weighted": 0.171527
          },
          {
            "accuracy": 0.661512,
            "f1": 0.539031,
            "f1_weighted": 0.717037,
            "ap": 0.170794,
            "ap_weighted": 0.170794
          },
          {
            "accuracy": 0.612543,
            "f1": 0.518236,
            "f1_weighted": 0.677917,
            "ap": 0.173838,
            "ap_weighted": 0.173838
          },
          {
            "accuracy": 0.695876,
            "f1": 0.539711,
            "f1_weighted": 0.740561,
            "ap": 0.15648,
            "ap_weighted": 0.15648
          },
          {
            "accuracy": 0.597938,
            "f1": 0.516383,
            "f1_weighted": 0.665161,
            "ap": 0.182485,
            "ap_weighted": 0.182485
          },
          {
            "accuracy": 0.702749,
            "f1": 0.577367,
            "f1_weighted": 0.749817,
            "ap": 0.197362,
            "ap_weighted": 0.197362
          },
          {
            "accuracy": 0.512887,
            "f1": 0.462931,
            "f1_weighted": 0.585638,
            "ap": 0.172731,
            "ap_weighted": 0.172731
          }
        ],
        "main_score": 0.616237,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 10.259185791015625,
  "kg_co2_emissions": 0.0004255188387458389
}