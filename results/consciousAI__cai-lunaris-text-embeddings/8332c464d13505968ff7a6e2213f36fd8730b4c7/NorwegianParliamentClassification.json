{
  "dataset_revision": "f7393532774c66312378d30b197610b43d751972",
  "task_name": "NorwegianParliamentClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.526583,
        "f1": 0.524426,
        "f1_weighted": 0.524426,
        "ap": 0.514509,
        "ap_weighted": 0.514509,
        "scores_per_experiment": [
          {
            "accuracy": 0.551667,
            "f1": 0.549131,
            "f1_weighted": 0.549131,
            "ap": 0.528974,
            "ap_weighted": 0.528974
          },
          {
            "accuracy": 0.485833,
            "f1": 0.48579,
            "f1_weighted": 0.48579,
            "ap": 0.493121,
            "ap_weighted": 0.493121
          },
          {
            "accuracy": 0.545833,
            "f1": 0.545825,
            "f1_weighted": 0.545825,
            "ap": 0.525,
            "ap_weighted": 0.525
          },
          {
            "accuracy": 0.514167,
            "f1": 0.507751,
            "f1_weighted": 0.507751,
            "ap": 0.507247,
            "ap_weighted": 0.507247
          },
          {
            "accuracy": 0.538333,
            "f1": 0.536167,
            "f1_weighted": 0.536167,
            "ap": 0.520459,
            "ap_weighted": 0.520459
          },
          {
            "accuracy": 0.488333,
            "f1": 0.487872,
            "f1_weighted": 0.487872,
            "ap": 0.494311,
            "ap_weighted": 0.494311
          },
          {
            "accuracy": 0.535833,
            "f1": 0.533029,
            "f1_weighted": 0.533029,
            "ap": 0.519028,
            "ap_weighted": 0.519028
          },
          {
            "accuracy": 0.536667,
            "f1": 0.534493,
            "f1_weighted": 0.534493,
            "ap": 0.519516,
            "ap_weighted": 0.519516
          },
          {
            "accuracy": 0.5225,
            "f1": 0.518673,
            "f1_weighted": 0.518673,
            "ap": 0.51168,
            "ap_weighted": 0.51168
          },
          {
            "accuracy": 0.546667,
            "f1": 0.54553,
            "f1_weighted": 0.54553,
            "ap": 0.525753,
            "ap_weighted": 0.525753
          }
        ],
        "main_score": 0.526583,
        "hf_subset": "default",
        "languages": [
          "nob-Latn"
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.541667,
        "f1": 0.539372,
        "f1_weighted": 0.539372,
        "ap": 0.523202,
        "ap_weighted": 0.523202,
        "scores_per_experiment": [
          {
            "accuracy": 0.5575,
            "f1": 0.556277,
            "f1_weighted": 0.556277,
            "ap": 0.532444,
            "ap_weighted": 0.532444
          },
          {
            "accuracy": 0.498333,
            "f1": 0.498244,
            "f1_weighted": 0.498244,
            "ap": 0.49917,
            "ap_weighted": 0.49917
          },
          {
            "accuracy": 0.561667,
            "f1": 0.561589,
            "f1_weighted": 0.561589,
            "ap": 0.534537,
            "ap_weighted": 0.534537
          },
          {
            "accuracy": 0.5325,
            "f1": 0.524775,
            "f1_weighted": 0.524775,
            "ap": 0.517092,
            "ap_weighted": 0.517092
          },
          {
            "accuracy": 0.5825,
            "f1": 0.582319,
            "f1_weighted": 0.582319,
            "ap": 0.547784,
            "ap_weighted": 0.547784
          },
          {
            "accuracy": 0.520833,
            "f1": 0.519825,
            "f1_weighted": 0.519825,
            "ap": 0.510894,
            "ap_weighted": 0.510894
          },
          {
            "accuracy": 0.515,
            "f1": 0.510265,
            "f1_weighted": 0.510265,
            "ap": 0.507688,
            "ap_weighted": 0.507688
          },
          {
            "accuracy": 0.525,
            "f1": 0.523888,
            "f1_weighted": 0.523888,
            "ap": 0.51307,
            "ap_weighted": 0.51307
          },
          {
            "accuracy": 0.553333,
            "f1": 0.547347,
            "f1_weighted": 0.547347,
            "ap": 0.528979,
            "ap_weighted": 0.528979
          },
          {
            "accuracy": 0.57,
            "f1": 0.569191,
            "f1_weighted": 0.569191,
            "ap": 0.540365,
            "ap_weighted": 0.540365
          }
        ],
        "main_score": 0.541667,
        "hf_subset": "default",
        "languages": [
          "nob-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 30.850793838500977,
  "kg_co2_emissions": 0.0020955901932679533
}