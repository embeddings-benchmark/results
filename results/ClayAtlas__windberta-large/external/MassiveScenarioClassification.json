{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 0.3541022192333557,
                "f1": 0.3204377399869686,
                "main_score": 0.3541022192333557
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 0.09045057162071285,
                "f1": 0.05989702120693128,
                "main_score": 0.09045057162071285
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 0.1491593813046402,
                "f1": 0.10836622322362433,
                "main_score": 0.1491593813046402
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 0.319670477471419,
                "f1": 0.29560759362481415,
                "main_score": 0.319670477471419
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 0.0914593140551446,
                "f1": 0.04755013470168768,
                "main_score": 0.0914593140551446
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 0.37451244115669136,
                "f1": 0.32721496788933135,
                "main_score": 0.37451244115669136
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 0.42652992602555473,
                "f1": 0.3854490018628139,
                "main_score": 0.42652992602555473
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 0.38325487558843313,
                "f1": 0.3401573345578629,
                "main_score": 0.38325487558843313
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 0.24451916610625418,
                "f1": 0.2126845806978251,
                "main_score": 0.24451916610625418
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 0.6342299932750504,
                "f1": 0.6031829616506453,
                "main_score": 0.6342299932750504
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 0.37730329522528583,
                "f1": 0.3447682087542897,
                "main_score": 0.37730329522528583
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 0.11842636180228647,
                "f1": 0.09103915091369244,
                "main_score": 0.11842636180228647
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 0.3448554135843981,
                "f1": 0.30544187613857654,
                "main_score": 0.3448554135843981
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 0.40917955615332885,
                "f1": 0.38272473597242007,
                "main_score": 0.40917955615332885
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 0.07642905178211164,
                "f1": 0.035932317435422974,
                "main_score": 0.07642905178211164
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 0.0863819771351715,
                "f1": 0.05367937513295658,
                "main_score": 0.0863819771351715
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 0.3724949562878279,
                "f1": 0.3356542557269889,
                "main_score": 0.3724949562878279
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 0.10907868190988568,
                "f1": 0.06164592664350547,
                "main_score": 0.10907868190988568
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 0.36109616677874923,
                "f1": 0.3360549268558145,
                "main_score": 0.36109616677874923
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 0.37804303967720243,
                "f1": 0.33702687457766195,
                "main_score": 0.37804303967720243
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 0.41684599865501004,
                "f1": 0.3782767863181736,
                "main_score": 0.41684599865501004
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 0.48379287155346323,
                "f1": 0.47429165307716514,
                "main_score": 0.48379287155346323
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 0.3519502353732348,
                "f1": 0.32346531581560084,
                "main_score": 0.3519502353732348
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 0.09902488231338263,
                "f1": 0.06949726319598101,
                "main_score": 0.09902488231338263
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 0.12750504371217217,
                "f1": 0.06484683676180122,
                "main_score": 0.12750504371217217
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 0.10309347679892403,
                "f1": 0.06303469804836742,
                "main_score": 0.10309347679892403
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 0.14515803631472762,
                "f1": 0.11448228509062176,
                "main_score": 0.14515803631472762
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 0.33076664425016816,
                "f1": 0.2962119483929902,
                "main_score": 0.33076664425016816
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 0.07441156691324816,
                "f1": 0.03950085352094378,
                "main_score": 0.07441156691324816
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 0.17982515131136517,
                "f1": 0.1621370873212602,
                "main_score": 0.17982515131136517
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 0.3792871553463349,
                "f1": 0.3345105376049966,
                "main_score": 0.3792871553463349
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 0.1172831203765972,
                "f1": 0.0844884960923806,
                "main_score": 0.1172831203765972
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 0.3802286482851379,
                "f1": 0.3467204151455546,
                "main_score": 0.3802286482851379
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 0.40373234700739746,
                "f1": 0.35654577119002684,
                "main_score": 0.40373234700739746
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 0.36344989912575654,
                "f1": 0.3279620374350878,
                "main_score": 0.36344989912575654
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 0.4182918628110289,
                "f1": 0.3867744252721392,
                "main_score": 0.4182918628110289
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 0.40625420309347665,
                "f1": 0.3698261635854706,
                "main_score": 0.40625420309347665
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 0.1895763281775387,
                "f1": 0.1630423107207311,
                "main_score": 0.1895763281775387
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 0.3529926025554808,
                "f1": 0.3176557408676722,
                "main_score": 0.3529926025554808
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 0.4195696032279759,
                "f1": 0.38574130779247523,
                "main_score": 0.4195696032279759
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 0.37192333557498325,
                "f1": 0.33812823010180165,
                "main_score": 0.37192333557498325
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 0.38876933422999327,
                "f1": 0.36374284150293923,
                "main_score": 0.38876933422999327
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 0.08513786146603901,
                "f1": 0.0377604514028691,
                "main_score": 0.08513786146603901
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 0.07347007397444519,
                "f1": 0.04316679614648472,
                "main_score": 0.07347007397444519
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 0.10104236718224613,
                "f1": 0.07587154404252398,
                "main_score": 0.10104236718224613
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 0.3590786819098857,
                "f1": 0.3242532534803655,
                "main_score": 0.3590786819098857
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 0.3207800941492939,
                "f1": 0.3107274116255098,
                "main_score": 0.3207800941492939
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 0.10373234700739746,
                "f1": 0.07124408430261137,
                "main_score": 0.10373234700739746
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 0.3391055817081373,
                "f1": 0.31824961224055043,
                "main_score": 0.3391055817081373
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 0.7494283792871552,
                "f1": 0.7429520816825985,
                "main_score": 0.7494283792871552
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 0.7099529253530599,
                "f1": 0.7048943927686703,
                "main_score": 0.7099529253530599
            }
        ]
    }
}