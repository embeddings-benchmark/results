{
  "dataset_revision": "b7c64bd89eb87f8ded463478346f76731f07bf8b",
  "task_name": "JDReview",
  "mteb_version": "2.3.0",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.906191,
            "f1": 0.858702,
            "f1_weighted": 0.910187,
            "precision": 0.833294,
            "precision_weighted": 0.919586,
            "recall": 0.895615,
            "recall_weighted": 0.906191,
            "ap": 0.63415,
            "ap_weighted": 0.63415
          },
          {
            "accuracy": 0.88743,
            "f1": 0.835737,
            "f1_weighted": 0.893653,
            "precision": 0.807147,
            "precision_weighted": 0.909408,
            "recall": 0.884094,
            "recall_weighted": 0.88743,
            "ap": 0.588844,
            "ap_weighted": 0.588844
          },
          {
            "accuracy": 0.88743,
            "f1": 0.836737,
            "f1_weighted": 0.893916,
            "precision": 0.807279,
            "precision_weighted": 0.910949,
            "recall": 0.887993,
            "recall_weighted": 0.88743,
            "ap": 0.591603,
            "ap_weighted": 0.591603
          },
          {
            "accuracy": 0.893058,
            "f1": 0.842492,
            "f1_weighted": 0.898584,
            "precision": 0.814583,
            "precision_weighted": 0.912312,
            "recall": 0.887551,
            "recall_weighted": 0.893058,
            "ap": 0.601715,
            "ap_weighted": 0.601715
          },
          {
            "accuracy": 0.891182,
            "f1": 0.841213,
            "f1_weighted": 0.897198,
            "precision": 0.812107,
            "precision_weighted": 0.91283,
            "recall": 0.890297,
            "recall_weighted": 0.891182,
            "ap": 0.600062,
            "ap_weighted": 0.600062
          },
          {
            "accuracy": 0.870544,
            "f1": 0.818199,
            "f1_weighted": 0.879512,
            "precision": 0.787689,
            "precision_weighted": 0.904805,
            "recall": 0.881523,
            "recall_weighted": 0.870544,
            "ap": 0.559371,
            "ap_weighted": 0.559371
          },
          {
            "accuracy": 0.902439,
            "f1": 0.85305,
            "f1_weighted": 0.906595,
            "precision": 0.828069,
            "precision_weighted": 0.916104,
            "recall": 0.889413,
            "recall_weighted": 0.902439,
            "ap": 0.622047,
            "ap_weighted": 0.622047
          },
          {
            "accuracy": 0.889306,
            "f1": 0.838969,
            "f1_weighted": 0.895556,
            "precision": 0.809676,
            "precision_weighted": 0.911883,
            "recall": 0.889145,
            "recall_weighted": 0.889306,
            "ap": 0.595801,
            "ap_weighted": 0.595801
          },
          {
            "accuracy": 0.889306,
            "f1": 0.837976,
            "f1_weighted": 0.895294,
            "precision": 0.809589,
            "precision_weighted": 0.910363,
            "recall": 0.885246,
            "recall_weighted": 0.889306,
            "ap": 0.59307,
            "ap_weighted": 0.59307
          },
          {
            "accuracy": 0.878049,
            "f1": 0.826765,
            "f1_weighted": 0.886007,
            "precision": 0.796174,
            "precision_weighted": 0.908125,
            "recall": 0.886131,
            "recall_weighted": 0.878049,
            "ap": 0.574387,
            "ap_weighted": 0.574387
          }
        ],
        "accuracy": 0.889493,
        "f1": 0.838984,
        "f1_weighted": 0.89565,
        "precision": 0.810561,
        "precision_weighted": 0.911637,
        "recall": 0.887701,
        "recall_weighted": 0.889493,
        "ap": 0.596105,
        "ap_weighted": 0.596105,
        "main_score": 0.889493,
        "hf_subset": "default",
        "languages": [
          "cmn-Hans"
        ]
      }
    ]
  },
  "evaluation_time": 192.60886001586914,
  "kg_co2_emissions": null
}