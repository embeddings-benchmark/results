{
  "dataset_revision": "20b0e6081892e78179356fada741b7afa381443d",
  "task_name": "AngryTweetsClassification",
  "mteb_version": "2.3.11",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.556829,
            "f1": 0.549359,
            "f1_weighted": 0.553267,
            "precision": 0.548627,
            "precision_weighted": 0.557469,
            "recall": 0.558784,
            "recall_weighted": 0.556829,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.596944,
            "f1": 0.58964,
            "f1_weighted": 0.594434,
            "precision": 0.588671,
            "precision_weighted": 0.594107,
            "recall": 0.592757,
            "recall_weighted": 0.596944,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.540592,
            "f1": 0.520125,
            "f1_weighted": 0.524457,
            "precision": 0.523164,
            "precision_weighted": 0.526902,
            "recall": 0.534516,
            "recall_weighted": 0.540592,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.561605,
            "f1": 0.558699,
            "f1_weighted": 0.561023,
            "precision": 0.557176,
            "precision_weighted": 0.562454,
            "recall": 0.562516,
            "recall_weighted": 0.561605,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.571156,
            "f1": 0.562063,
            "f1_weighted": 0.570862,
            "precision": 0.561659,
            "precision_weighted": 0.570622,
            "recall": 0.562521,
            "recall_weighted": 0.571156,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.600764,
            "f1": 0.583704,
            "f1_weighted": 0.592021,
            "precision": 0.593992,
            "precision_weighted": 0.595695,
            "recall": 0.5854,
            "recall_weighted": 0.600764,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.532951,
            "f1": 0.529422,
            "f1_weighted": 0.536077,
            "precision": 0.534165,
            "precision_weighted": 0.546914,
            "recall": 0.533228,
            "recall_weighted": 0.532951,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.535817,
            "f1": 0.524494,
            "f1_weighted": 0.532562,
            "precision": 0.537306,
            "precision_weighted": 0.538048,
            "recall": 0.521886,
            "recall_weighted": 0.535817,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.567335,
            "f1": 0.561799,
            "f1_weighted": 0.56999,
            "precision": 0.565833,
            "precision_weighted": 0.575513,
            "recall": 0.560373,
            "recall_weighted": 0.567335,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.600764,
            "f1": 0.584697,
            "f1_weighted": 0.590999,
            "precision": 0.594498,
            "precision_weighted": 0.595412,
            "recall": 0.588123,
            "recall_weighted": 0.600764,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.566476,
        "f1": 0.5564,
        "f1_weighted": 0.562569,
        "precision": 0.560509,
        "precision_weighted": 0.566314,
        "recall": 0.560011,
        "recall_weighted": 0.566476,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.566476,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 76.07680630683899,
  "kg_co2_emissions": null
}