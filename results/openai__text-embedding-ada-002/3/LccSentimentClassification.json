{
  "dataset_revision": "de7ba3406ee55ea2cc52a0a41408fa6aede6d3c6",
  "task_name": "LccSentimentClassification",
  "mteb_version": "2.3.11",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.646667,
            "f1": 0.646082,
            "f1_weighted": 0.648328,
            "precision": 0.675968,
            "precision_weighted": 0.779984,
            "recall": 0.735124,
            "recall_weighted": 0.646667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.566667,
            "f1": 0.572714,
            "f1_weighted": 0.560872,
            "precision": 0.567397,
            "precision_weighted": 0.616421,
            "recall": 0.643968,
            "recall_weighted": 0.566667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.633333,
            "f1": 0.633891,
            "f1_weighted": 0.636805,
            "precision": 0.648407,
            "precision_weighted": 0.715863,
            "recall": 0.707795,
            "recall_weighted": 0.633333,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.58,
            "f1": 0.584285,
            "f1_weighted": 0.568813,
            "precision": 0.628892,
            "precision_weighted": 0.727234,
            "recall": 0.690432,
            "recall_weighted": 0.58,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.62,
            "f1": 0.614895,
            "f1_weighted": 0.623321,
            "precision": 0.616402,
            "precision_weighted": 0.685855,
            "recall": 0.679454,
            "recall_weighted": 0.62,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.606667,
            "f1": 0.59522,
            "f1_weighted": 0.61069,
            "precision": 0.585511,
            "precision_weighted": 0.63274,
            "recall": 0.627448,
            "recall_weighted": 0.606667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.593333,
            "f1": 0.584524,
            "f1_weighted": 0.603643,
            "precision": 0.588767,
            "precision_weighted": 0.657466,
            "recall": 0.628477,
            "recall_weighted": 0.593333,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.68,
            "f1": 0.664884,
            "f1_weighted": 0.683423,
            "precision": 0.660212,
            "precision_weighted": 0.69597,
            "recall": 0.678502,
            "recall_weighted": 0.68,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.633333,
            "f1": 0.637479,
            "f1_weighted": 0.645724,
            "precision": 0.673575,
            "precision_weighted": 0.764443,
            "recall": 0.710141,
            "recall_weighted": 0.633333,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.606667,
            "f1": 0.594043,
            "f1_weighted": 0.609646,
            "precision": 0.594771,
            "precision_weighted": 0.644183,
            "recall": 0.637521,
            "recall_weighted": 0.606667,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.616667,
        "f1": 0.612801,
        "f1_weighted": 0.619127,
        "precision": 0.62399,
        "precision_weighted": 0.692016,
        "recall": 0.673886,
        "recall_weighted": 0.616667,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.616667,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 51.30151700973511,
  "kg_co2_emissions": null
}