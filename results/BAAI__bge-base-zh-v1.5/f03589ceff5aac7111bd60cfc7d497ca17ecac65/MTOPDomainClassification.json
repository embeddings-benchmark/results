{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "task_name": "MTOPDomainClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.634876,
        "f1": 0.611762,
        "f1_weighted": 0.633182,
        "scores_per_experiment": [
          {
            "accuracy": 0.632507,
            "f1": 0.610922,
            "f1_weighted": 0.631848
          },
          {
            "accuracy": 0.628099,
            "f1": 0.612205,
            "f1_weighted": 0.625088
          },
          {
            "accuracy": 0.622039,
            "f1": 0.60382,
            "f1_weighted": 0.623212
          },
          {
            "accuracy": 0.644628,
            "f1": 0.622673,
            "f1_weighted": 0.644792
          },
          {
            "accuracy": 0.65124,
            "f1": 0.632336,
            "f1_weighted": 0.648973
          },
          {
            "accuracy": 0.601653,
            "f1": 0.564876,
            "f1_weighted": 0.596071
          },
          {
            "accuracy": 0.651791,
            "f1": 0.623549,
            "f1_weighted": 0.649305
          },
          {
            "accuracy": 0.639118,
            "f1": 0.621294,
            "f1_weighted": 0.642647
          },
          {
            "accuracy": 0.647934,
            "f1": 0.621881,
            "f1_weighted": 0.645269
          },
          {
            "accuracy": 0.629752,
            "f1": 0.604069,
            "f1_weighted": 0.62461
          }
        ],
        "main_score": 0.634876,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.645562,
        "f1": 0.616853,
        "f1_weighted": 0.644504,
        "scores_per_experiment": [
          {
            "accuracy": 0.651169,
            "f1": 0.627795,
            "f1_weighted": 0.650126
          },
          {
            "accuracy": 0.621302,
            "f1": 0.596932,
            "f1_weighted": 0.619025
          },
          {
            "accuracy": 0.64497,
            "f1": 0.617476,
            "f1_weighted": 0.647142
          },
          {
            "accuracy": 0.661313,
            "f1": 0.634705,
            "f1_weighted": 0.663014
          },
          {
            "accuracy": 0.664976,
            "f1": 0.63917,
            "f1_weighted": 0.663963
          },
          {
            "accuracy": 0.615103,
            "f1": 0.578897,
            "f1_weighted": 0.612193
          },
          {
            "accuracy": 0.668921,
            "f1": 0.637039,
            "f1_weighted": 0.666456
          },
          {
            "accuracy": 0.639053,
            "f1": 0.615818,
            "f1_weighted": 0.643902
          },
          {
            "accuracy": 0.656241,
            "f1": 0.618929,
            "f1_weighted": 0.652024
          },
          {
            "accuracy": 0.632573,
            "f1": 0.601773,
            "f1_weighted": 0.627195
          }
        ],
        "main_score": 0.645562,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 10.394733905792236,
  "kg_co2_emissions": null
}