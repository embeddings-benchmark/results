{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 0.5086415601882985,
                "f1": 0.4941696672602645,
                "main_score": 0.5086415601882985
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 0.4118359112306658,
                "f1": 0.4004563865770774,
                "main_score": 0.4118359112306658
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 0.5008069939475455,
                "f1": 0.5072480016584613,
                "main_score": 0.5008069939475455
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 0.512878278412912,
                "f1": 0.5072873776739851,
                "main_score": 0.512878278412912
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 0.4653328850033624,
                "f1": 0.45933178666396673,
                "main_score": 0.4653328850033624
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 0.34347679892400806,
                "f1": 0.3194158114128083,
                "main_score": 0.34347679892400806
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 0.6307330195023537,
                "f1": 0.6222872894011106,
                "main_score": 0.6307330195023537
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 0.5639878950907867,
                "f1": 0.5480778341609032,
                "main_score": 0.5639878950907867
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 0.6179892400806993,
                "f1": 0.6069430756982446,
                "main_score": 0.6179892400806993
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 0.6696368527236046,
                "f1": 0.665893927997656,
                "main_score": 0.6696368527236046
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 0.6221250840618695,
                "f1": 0.6234717779412893,
                "main_score": 0.6221250840618695
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 0.6243779421654339,
                "f1": 0.613077013120856,
                "main_score": 0.6243779421654339
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 0.6109952925353059,
                "f1": 0.6031390792738691,
                "main_score": 0.6109952925353059
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 0.6338601210490922,
                "f1": 0.6305968938353488,
                "main_score": 0.6338601210490922
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 0.5628782784129119,
                "f1": 0.5592927644838597,
                "main_score": 0.5628782784129119
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 0.6062878278412912,
                "f1": 0.6025299253652635,
                "main_score": 0.6062878278412912
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 0.6328850033624748,
                "f1": 0.6277053246337031,
                "main_score": 0.6328850033624748
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 0.5487558843308675,
                "f1": 0.5430717357279135,
                "main_score": 0.5487558843308675
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 0.6199394754539341,
                "f1": 0.6173085530883037,
                "main_score": 0.6199394754539341
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 0.38581035642232686,
                "f1": 0.3696287269695893,
                "main_score": 0.38581035642232686
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 0.6235036987222597,
                "f1": 0.6180732732482397,
                "main_score": 0.6235036987222597
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 0.6517148621385338,
                "f1": 0.652962014465675,
                "main_score": 0.6517148621385338
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 0.36126429051782116,
                "f1": 0.35334393048479484,
                "main_score": 0.36126429051782116
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 0.5026899798251513,
                "f1": 0.49041065960139435,
                "main_score": 0.5026899798251513
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 0.44243443174176195,
                "f1": 0.4242177854872125,
                "main_score": 0.44243443174176195
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 0.4737054472091459,
                "f1": 0.46589722581465326,
                "main_score": 0.4737054472091459
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 0.5889038332212508,
                "f1": 0.5775360792199039,
                "main_score": 0.5889038332212508
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 0.5650638870208475,
                "f1": 0.560485860423295,
                "main_score": 0.5650638870208475
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 0.5006388702084734,
                "f1": 0.5010936464182458,
                "main_score": 0.5006388702084734
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 0.5505379959650303,
                "f1": 0.5449066570566669,
                "main_score": 0.5505379959650303
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 0.5977135171486213,
                "f1": 0.582808650158803,
                "main_score": 0.5977135171486213
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 0.5571620712844654,
                "f1": 0.538630348824753,
                "main_score": 0.5571620712844654
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 0.6026227303295225,
                "f1": 0.5986604657147016,
                "main_score": 0.6026227303295225
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 0.633759246805649,
                "f1": 0.6245257339288532,
                "main_score": 0.633759246805649
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 0.6255211835911231,
                "f1": 0.6135444960577676,
                "main_score": 0.6255211835911231
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 0.6240753194351043,
                "f1": 0.6198779889528889,
                "main_score": 0.6240753194351043
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 0.6068258238063214,
                "f1": 0.605997397897657,
                "main_score": 0.6068258238063214
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 0.6231002017484868,
                "f1": 0.6241231226850366,
                "main_score": 0.6231002017484868
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 0.6142905178211163,
                "f1": 0.6160095590401424,
                "main_score": 0.6142905178211163
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 0.6222932078009416,
                "f1": 0.6102251426747547,
                "main_score": 0.6222932078009416
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 0.6442501681237391,
                "f1": 0.6346149443060524,
                "main_score": 0.6442501681237391
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 0.3851714862138534,
                "f1": 0.37124667229863617,
                "main_score": 0.3851714862138534
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 0.4699731002017485,
                "f1": 0.45859147049984833,
                "main_score": 0.4699731002017485
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 0.5101882985877605,
                "f1": 0.4901040173136056,
                "main_score": 0.5101882985877605
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 0.6323470073974445,
                "f1": 0.6273229459521474,
                "main_score": 0.6323470073974445
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 0.38722259583053126,
                "f1": 0.36603231928120905,
                "main_score": 0.38722259583053126
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 0.6448554135843981,
                "f1": 0.6397380562022752,
                "main_score": 0.6448554135843981
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 0.567955615332885,
                "f1": 0.5595308241204802,
                "main_score": 0.567955615332885
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 0.5706455951580363,
                "f1": 0.5695570494066693,
                "main_score": 0.5706455951580363
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 0.658338937457969,
                "f1": 0.656778746906008,
                "main_score": 0.658338937457969
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 0.6336919973100203,
                "f1": 0.6352765011605994,
                "main_score": 0.6336919973100203
            }
        ]
    }
}