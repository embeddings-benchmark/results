{
    "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
    "task_name": "TwitterSemEval2015",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "default",
                "languages": [
                    "eng-Latn"
                ],
                "cos_sim_accuracy": 0.8509864695714371,
                "cos_sim_ap": 0.7033704198164713,
                "cos_sim_f1": 0.6622893954410307,
                "cos_sim_precision": 0.6242410088743577,
                "cos_sim_recall": 0.7052770448548813,
                "dot_accuracy": 0.7911426357513263,
                "dot_ap": 0.4915484584572233,
                "dot_f1": 0.5112580243364951,
                "dot_precision": 0.4013840830449827,
                "dot_recall": 0.703957783641161,
                "euclidean_accuracy": 0.8515825236931513,
                "euclidean_ap": 0.7051017350854076,
                "euclidean_f1": 0.6645416294785158,
                "euclidean_precision": 0.6429805082654824,
                "euclidean_recall": 0.687598944591029,
                "manhattan_accuracy": 0.8514037074566371,
                "manhattan_ap": 0.7047587863399994,
                "manhattan_f1": 0.6645768025078369,
                "manhattan_precision": 0.6332138590203107,
                "manhattan_recall": 0.6992084432717678,
                "max_accuracy": 0.8515825236931513,
                "max_ap": 0.7051017350854076,
                "max_f1": 0.6645768025078369,
                "main_score": 0.7051017350854076
            }
        ]
    }
}