{
  "dataset_revision": "12ca3b695563788fead87a982ad1a068284413f4",
  "evaluation_time": 1.4480478763580322,
  "kg_co2_emissions": 6.913673991702043e-05,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.68408203125,
          "accuracy_threshold": 0.30736058950424194,
          "ap": 0.7844179765721532,
          "f1": 0.7569644572526417,
          "f1_threshold": 0.13572221994400024,
          "precision": 0.6214511041009464,
          "recall": 0.9680589680589681
        },
        "dot": {
          "accuracy": 0.67529296875,
          "accuracy_threshold": 2.589522361755371,
          "ap": 0.7761183940183691,
          "f1": 0.7569352708058124,
          "f1_threshold": 1.5228335857391357,
          "precision": 0.6342003320420586,
          "recall": 0.9385749385749386
        },
        "euclidean": {
          "accuracy": 0.6826171875,
          "accuracy_threshold": 3.5687978267669678,
          "ap": 0.7839995511375163,
          "f1": 0.7654234925060995,
          "f1_threshold": 3.7421929836273193,
          "precision": 0.6662621359223301,
          "recall": 0.8992628992628993
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.68408203125,
        "manhattan": {
          "accuracy": 0.68359375,
          "accuracy_threshold": 76.82199096679688,
          "ap": 0.7909797314732623,
          "f1": 0.7656946826758146,
          "f1_threshold": 81.29424285888672,
          "precision": 0.6587957497048406,
          "recall": 0.914004914004914
        },
        "max": {
          "accuracy": 0.68408203125,
          "ap": 0.7909797314732623,
          "f1": 0.7656946826758146
        },
        "similarity": {
          "accuracy": 0.68408203125,
          "accuracy_threshold": 0.30736052989959717,
          "ap": 0.7844179765721532,
          "f1": 0.7569644572526417,
          "f1_threshold": 0.13572201132774353,
          "precision": 0.6214511041009464,
          "recall": 0.9680589680589681
        }
      }
    ]
  },
  "task_name": "LegalBenchPC"
}