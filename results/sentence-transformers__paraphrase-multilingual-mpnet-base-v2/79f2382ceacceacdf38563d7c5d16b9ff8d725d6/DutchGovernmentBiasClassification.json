{
  "dataset_revision": "bf5e20ee2d3ce2e24e4de50f5dd8573e0e0e2fec",
  "task_name": "DutchGovernmentBiasClassification",
  "mteb_version": "2.1.0",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.664894,
            "f1": 0.619397,
            "f1_weighted": 0.682043,
            "precision": 0.619145,
            "precision_weighted": 0.720244,
            "recall": 0.646911,
            "recall_weighted": 0.664894,
            "ap": 0.350178,
            "ap_weighted": 0.350178
          },
          {
            "accuracy": 0.634309,
            "f1": 0.587633,
            "f1_weighted": 0.65368,
            "precision": 0.590986,
            "precision_weighted": 0.69578,
            "recall": 0.613093,
            "recall_weighted": 0.634309,
            "ap": 0.323877,
            "ap_weighted": 0.323877
          },
          {
            "accuracy": 0.659574,
            "f1": 0.62168,
            "f1_weighted": 0.678681,
            "precision": 0.625309,
            "precision_weighted": 0.729267,
            "recall": 0.658042,
            "recall_weighted": 0.659574,
            "ap": 0.356899,
            "ap_weighted": 0.356899
          },
          {
            "accuracy": 0.625,
            "f1": 0.60112,
            "f1_weighted": 0.647583,
            "precision": 0.621856,
            "precision_weighted": 0.734403,
            "recall": 0.657539,
            "recall_weighted": 0.625,
            "ap": 0.351598,
            "ap_weighted": 0.351598
          },
          {
            "accuracy": 0.62234,
            "f1": 0.589734,
            "f1_weighted": 0.644796,
            "precision": 0.602237,
            "precision_weighted": 0.711284,
            "recall": 0.631179,
            "recall_weighted": 0.62234,
            "ap": 0.334226,
            "ap_weighted": 0.334226
          },
          {
            "accuracy": 0.62633,
            "f1": 0.60743,
            "f1_weighted": 0.648436,
            "precision": 0.63538,
            "precision_weighted": 0.751098,
            "recall": 0.674811,
            "recall_weighted": 0.62633,
            "ap": 0.363196,
            "ap_weighted": 0.363196
          },
          {
            "accuracy": 0.675532,
            "f1": 0.629128,
            "f1_weighted": 0.691581,
            "precision": 0.627269,
            "precision_weighted": 0.726568,
            "recall": 0.655755,
            "recall_weighted": 0.675532,
            "ap": 0.358226,
            "ap_weighted": 0.358226
          },
          {
            "accuracy": 0.581117,
            "f1": 0.564942,
            "f1_weighted": 0.604878,
            "precision": 0.603487,
            "precision_weighted": 0.721261,
            "recall": 0.632721,
            "recall_weighted": 0.581117,
            "ap": 0.331729,
            "ap_weighted": 0.331729
          },
          {
            "accuracy": 0.675532,
            "f1": 0.615087,
            "f1_weighted": 0.687702,
            "precision": 0.611351,
            "precision_weighted": 0.707892,
            "recall": 0.629561,
            "recall_weighted": 0.675532,
            "ap": 0.340101,
            "ap_weighted": 0.340101
          },
          {
            "accuracy": 0.62766,
            "f1": 0.589967,
            "f1_weighted": 0.649151,
            "precision": 0.598309,
            "precision_weighted": 0.705528,
            "recall": 0.62496,
            "recall_weighted": 0.62766,
            "ap": 0.330784,
            "ap_weighted": 0.330784
          }
        ],
        "accuracy": 0.639229,
        "f1": 0.602612,
        "f1_weighted": 0.658853,
        "precision": 0.613533,
        "precision_weighted": 0.720333,
        "recall": 0.642457,
        "recall_weighted": 0.639229,
        "ap": 0.344081,
        "ap_weighted": 0.344081,
        "main_score": 0.602612,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 4.875375509262085,
  "kg_co2_emissions": null
}