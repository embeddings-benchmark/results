{
  "dataset_revision": "bd27d0058bea2ad52470d9072a3b5da6b97c1ac3",
  "task_name": "VaccinChatNLClassification",
  "mteb_version": "2.1.0",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.528205,
            "f1": 0.53867,
            "f1_weighted": 0.522355,
            "precision": 0.530693,
            "precision_weighted": 0.596393,
            "recall": 0.663489,
            "recall_weighted": 0.528205,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.518803,
            "f1": 0.558095,
            "f1_weighted": 0.503909,
            "precision": 0.545912,
            "precision_weighted": 0.61386,
            "recall": 0.682067,
            "recall_weighted": 0.518803,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.517949,
            "f1": 0.55277,
            "f1_weighted": 0.507368,
            "precision": 0.540887,
            "precision_weighted": 0.587786,
            "recall": 0.674822,
            "recall_weighted": 0.517949,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.517949,
            "f1": 0.54063,
            "f1_weighted": 0.509424,
            "precision": 0.533911,
            "precision_weighted": 0.597983,
            "recall": 0.660418,
            "recall_weighted": 0.517949,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.523932,
            "f1": 0.543978,
            "f1_weighted": 0.518726,
            "precision": 0.529991,
            "precision_weighted": 0.617016,
            "recall": 0.659882,
            "recall_weighted": 0.523932,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.526496,
            "f1": 0.565526,
            "f1_weighted": 0.514032,
            "precision": 0.541807,
            "precision_weighted": 0.596552,
            "recall": 0.696616,
            "recall_weighted": 0.526496,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.523932,
            "f1": 0.555371,
            "f1_weighted": 0.506809,
            "precision": 0.547779,
            "precision_weighted": 0.619545,
            "recall": 0.680551,
            "recall_weighted": 0.523932,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.519658,
            "f1": 0.544087,
            "f1_weighted": 0.511593,
            "precision": 0.527198,
            "precision_weighted": 0.592668,
            "recall": 0.663362,
            "recall_weighted": 0.519658,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.51453,
            "f1": 0.538351,
            "f1_weighted": 0.500902,
            "precision": 0.523999,
            "precision_weighted": 0.574787,
            "recall": 0.655164,
            "recall_weighted": 0.51453,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.524786,
            "f1": 0.548489,
            "f1_weighted": 0.51419,
            "precision": 0.539223,
            "precision_weighted": 0.603342,
            "recall": 0.667524,
            "recall_weighted": 0.524786,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.521624,
        "f1": 0.548597,
        "f1_weighted": 0.510931,
        "precision": 0.53614,
        "precision_weighted": 0.599993,
        "recall": 0.67039,
        "recall_weighted": 0.521624,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.548597,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 28.37408471107483,
  "kg_co2_emissions": null
}