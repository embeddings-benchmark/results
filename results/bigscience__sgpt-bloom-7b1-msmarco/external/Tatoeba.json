{
    "dataset_revision": "ed9e4a974f867fd9736efcf222fc3a26487387a5",
    "task_name": "Tatoeba",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "sqi-eng",
                "languages": [
                    "sqi-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.127,
                "f1": 0.10384182044950326,
                "precision": 0.09805277385275313,
                "recall": 0.127,
                "main_score": 0.10384182044950326
            },
            {
                "hf_subset": "fry-eng",
                "languages": [
                    "fry-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.3063583815028902,
                "f1": 0.24623726947426372,
                "precision": 0.22987809919828014,
                "recall": 0.3063583815028902,
                "main_score": 0.24623726947426372
            },
            {
                "hf_subset": "kur-eng",
                "languages": [
                    "kur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.10487804878048781,
                "f1": 0.08255945048627975,
                "precision": 0.07649047253615002,
                "recall": 0.10487804878048781,
                "main_score": 0.08255945048627975
            },
            {
                "hf_subset": "tur-eng",
                "languages": [
                    "tur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.085,
                "f1": 0.06154428783776609,
                "precision": 0.05680727638128585,
                "recall": 0.085,
                "main_score": 0.06154428783776609
            },
            {
                "hf_subset": "deu-eng",
                "languages": [
                    "deu-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.73,
                "f1": 0.7010046605876393,
                "precision": 0.690018253968254,
                "recall": 0.73,
                "main_score": 0.7010046605876393
            },
            {
                "hf_subset": "nld-eng",
                "languages": [
                    "nld-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.327,
                "f1": 0.297428583868239,
                "precision": 0.2881671359506905,
                "recall": 0.327,
                "main_score": 0.297428583868239
            },
            {
                "hf_subset": "ron-eng",
                "languages": [
                    "ron-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.315,
                "f1": 0.27228675552174003,
                "precision": 0.2595006229984775,
                "recall": 0.315,
                "main_score": 0.27228675552174003
            },
            {
                "hf_subset": "ang-eng",
                "languages": [
                    "ang-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.3582089552238806,
                "f1": 0.2875836980510979,
                "precision": 0.2697164361343466,
                "recall": 0.3582089552238806,
                "main_score": 0.2875836980510979
            },
            {
                "hf_subset": "ido-eng",
                "languages": [
                    "ido-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.498,
                "f1": 0.4390923740145178,
                "precision": 0.41944763440988936,
                "recall": 0.498,
                "main_score": 0.4390923740145178
            },
            {
                "hf_subset": "jav-eng",
                "languages": [
                    "jav-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.18536585365853658,
                "f1": 0.1502018257024675,
                "precision": 0.14231108073213336,
                "recall": 0.18536585365853658,
                "main_score": 0.1502018257024675
            },
            {
                "hf_subset": "isl-eng",
                "languages": [
                    "isl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.087,
                "f1": 0.06293478490288536,
                "precision": 0.05685926293425392,
                "recall": 0.087,
                "main_score": 0.06293478490288536
            },
            {
                "hf_subset": "slv-eng",
                "languages": [
                    "slv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.12879708383961117,
                "f1": 0.10136118341751114,
                "precision": 0.09571444036679436,
                "recall": 0.12879708383961117,
                "main_score": 0.10136118341751114
            },
            {
                "hf_subset": "cym-eng",
                "languages": [
                    "cym-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.09217391304347826,
                "f1": 0.06965003297761793,
                "precision": 0.06476093529199119,
                "recall": 0.09217391304347826,
                "main_score": 0.06965003297761793
            },
            {
                "hf_subset": "kaz-eng",
                "languages": [
                    "kaz-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.043478260869565216,
                "f1": 0.0331869717076774,
                "precision": 0.03198658632552104,
                "recall": 0.043478260869565216,
                "main_score": 0.0331869717076774
            },
            {
                "hf_subset": "est-eng",
                "languages": [
                    "est-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.069,
                "f1": 0.04760708297894056,
                "precision": 0.0428409511756074,
                "recall": 0.069,
                "main_score": 0.04760708297894056
            },
            {
                "hf_subset": "heb-eng",
                "languages": [
                    "heb-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.022,
                "f1": 0.016862703878117107,
                "precision": 0.016048118233915602,
                "recall": 0.022,
                "main_score": 0.016862703878117107
            },
            {
                "hf_subset": "gla-eng",
                "languages": [
                    "gla-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.030156815440289506,
                "f1": 0.020913257250659133,
                "precision": 0.01907277548646165,
                "recall": 0.030156815440289506,
                "main_score": 0.020913257250659133
            },
            {
                "hf_subset": "mar-eng",
                "languages": [
                    "mar-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.49,
                "f1": 0.455254456536713,
                "precision": 0.44134609250398726,
                "recall": 0.49,
                "main_score": 0.455254456536713
            },
            {
                "hf_subset": "lat-eng",
                "languages": [
                    "lat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.335,
                "f1": 0.28759893973182565,
                "precision": 0.27401259116024834,
                "recall": 0.335,
                "main_score": 0.28759893973182565
            },
            {
                "hf_subset": "bel-eng",
                "languages": [
                    "bel-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.102,
                "f1": 0.08030039981676275,
                "precision": 0.07548748077210127,
                "recall": 0.102,
                "main_score": 0.08030039981676275
            },
            {
                "hf_subset": "pms-eng",
                "languages": [
                    "pms-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.38095238095238093,
                "f1": 0.31944999250262407,
                "precision": 0.3004452690166976,
                "recall": 0.38095238095238093,
                "main_score": 0.31944999250262407
            },
            {
                "hf_subset": "gle-eng",
                "languages": [
                    "gle-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.048,
                "f1": 0.03263896078670807,
                "precision": 0.030495382950729642,
                "recall": 0.048,
                "main_score": 0.03263896078670807
            },
            {
                "hf_subset": "pes-eng",
                "languages": [
                    "pes-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.158,
                "f1": 0.12131087470371275,
                "precision": 0.11141304011547815,
                "recall": 0.158,
                "main_score": 0.12131087470371275
            },
            {
                "hf_subset": "nob-eng",
                "languages": [
                    "nob-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.233,
                "f1": 0.21073044636921384,
                "precision": 0.20374220568287285,
                "recall": 0.233,
                "main_score": 0.21073044636921384
            },
            {
                "hf_subset": "bul-eng",
                "languages": [
                    "bul-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.249,
                "f1": 0.20091060685364986,
                "precision": 0.18899700591081225,
                "recall": 0.249,
                "main_score": 0.20091060685364986
            },
            {
                "hf_subset": "cbk-eng",
                "languages": [
                    "cbk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.701,
                "f1": 0.6462940836940835,
                "precision": 0.6246559523809524,
                "recall": 0.701,
                "main_score": 0.6462940836940835
            },
            {
                "hf_subset": "hun-eng",
                "languages": [
                    "hun-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.072,
                "f1": 0.050661346057611495,
                "precision": 0.04625224463391809,
                "recall": 0.072,
                "main_score": 0.050661346057611495
            },
            {
                "hf_subset": "uig-eng",
                "languages": [
                    "uig-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.018,
                "f1": 0.012716249514772895,
                "precision": 0.012107445914723798,
                "recall": 0.018,
                "main_score": 0.012716249514772895
            },
            {
                "hf_subset": "rus-eng",
                "languages": [
                    "rus-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.655,
                "f1": 0.5984399711399712,
                "precision": 0.5786349567099567,
                "recall": 0.655,
                "main_score": 0.5984399711399712
            },
            {
                "hf_subset": "spa-eng",
                "languages": [
                    "spa-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9570000000000001,
                "f1": 0.9448333333333333,
                "precision": 0.939,
                "recall": 0.9570000000000001,
                "main_score": 0.9448333333333333
            },
            {
                "hf_subset": "hye-eng",
                "languages": [
                    "hye-Armn",
                    "eng-Latn"
                ],
                "accuracy": 0.008086253369272238,
                "f1": 0.004962046191492002,
                "precision": 0.004727243857855439,
                "recall": 0.008086253369272238,
                "main_score": 0.004962046191492002
            },
            {
                "hf_subset": "tel-eng",
                "languages": [
                    "tel-Telu",
                    "eng-Latn"
                ],
                "accuracy": 0.6923076923076923,
                "f1": 0.646227941099736,
                "precision": 0.6303795877325289,
                "recall": 0.6923076923076923,
                "main_score": 0.646227941099736
            },
            {
                "hf_subset": "afr-eng",
                "languages": [
                    "afr-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.206,
                "f1": 0.1662410040660465,
                "precision": 0.1559835243796707,
                "recall": 0.206,
                "main_score": 0.1662410040660465
            },
            {
                "hf_subset": "mon-eng",
                "languages": [
                    "mon-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.04318181818181818,
                "f1": 0.028467211925356607,
                "precision": 0.026787861417537147,
                "recall": 0.04318181818181818,
                "main_score": 0.028467211925356607
            },
            {
                "hf_subset": "arz-eng",
                "languages": [
                    "arz-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.7484276729559748,
                "f1": 0.706638714185884,
                "precision": 0.6886792452830188,
                "recall": 0.7484276729559748,
                "main_score": 0.706638714185884
            },
            {
                "hf_subset": "hrv-eng",
                "languages": [
                    "hrv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.159,
                "f1": 0.12793698974586706,
                "precision": 0.12088118017657737,
                "recall": 0.159,
                "main_score": 0.12793698974586706
            },
            {
                "hf_subset": "nov-eng",
                "languages": [
                    "nov-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.5992217898832685,
                "f1": 0.5223086900129701,
                "precision": 0.49258538694336357,
                "recall": 0.5992217898832685,
                "main_score": 0.5223086900129701
            },
            {
                "hf_subset": "gsw-eng",
                "languages": [
                    "gsw-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.27350427350427353,
                "f1": 0.21033781033781032,
                "precision": 0.19337955491801645,
                "recall": 0.27350427350427353,
                "main_score": 0.21033781033781032
            },
            {
                "hf_subset": "nds-eng",
                "languages": [
                    "nds-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.293,
                "f1": 0.2391597452425777,
                "precision": 0.22366965983649417,
                "recall": 0.293,
                "main_score": 0.2391597452425777
            },
            {
                "hf_subset": "ukr-eng",
                "languages": [
                    "ukr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.273,
                "f1": 0.22059393517688886,
                "precision": 0.20503235534170888,
                "recall": 0.273,
                "main_score": 0.22059393517688886
            },
            {
                "hf_subset": "uzb-eng",
                "languages": [
                    "uzb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.08177570093457943,
                "f1": 0.04714367017906037,
                "precision": 0.04163882933965758,
                "recall": 0.08177570093457943,
                "main_score": 0.04714367017906037
            },
            {
                "hf_subset": "lit-eng",
                "languages": [
                    "lit-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.05800000000000001,
                "f1": 0.04485935743229383,
                "precision": 0.042478144656140436,
                "recall": 0.05800000000000001,
                "main_score": 0.04485935743229383
            },
            {
                "hf_subset": "ina-eng",
                "languages": [
                    "ina-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.784,
                "f1": 0.7367166666666667,
                "precision": 0.7183285714285713,
                "recall": 0.784,
                "main_score": 0.7367166666666667
            },
            {
                "hf_subset": "lfn-eng",
                "languages": [
                    "lfn-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.503,
                "f1": 0.44852215458833106,
                "precision": 0.43049130262439095,
                "recall": 0.503,
                "main_score": 0.44852215458833106
            },
            {
                "hf_subset": "zsm-eng",
                "languages": [
                    "zsm-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.835,
                "f1": 0.7995151515151515,
                "precision": 0.7853611111111111,
                "recall": 0.835,
                "main_score": 0.7995151515151515
            },
            {
                "hf_subset": "ita-eng",
                "languages": [
                    "ita-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.699,
                "f1": 0.6503756269256269,
                "precision": 0.6323351953601953,
                "recall": 0.699,
                "main_score": 0.6503756269256269
            },
            {
                "hf_subset": "cmn-eng",
                "languages": [
                    "cmn-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.932,
                "f1": 0.9144666666666665,
                "precision": 0.9063333333333332,
                "recall": 0.932,
                "main_score": 0.9144666666666665
            },
            {
                "hf_subset": "lvs-eng",
                "languages": [
                    "lvs-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.083,
                "f1": 0.06553388144729963,
                "precision": 0.06313497782829976,
                "recall": 0.083,
                "main_score": 0.06553388144729963
            },
            {
                "hf_subset": "glg-eng",
                "languages": [
                    "glg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.836,
                "f1": 0.7986243107769424,
                "precision": 0.7832555555555555,
                "recall": 0.836,
                "main_score": 0.7986243107769424
            },
            {
                "hf_subset": "ceb-eng",
                "languages": [
                    "ceb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.09166666666666666,
                "f1": 0.06637753604420271,
                "precision": 0.0610568253585495,
                "recall": 0.09166666666666666,
                "main_score": 0.06637753604420271
            },
            {
                "hf_subset": "bre-eng",
                "languages": [
                    "bre-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.074,
                "f1": 0.04672948361232217,
                "precision": 0.041038445202926585,
                "recall": 0.074,
                "main_score": 0.04672948361232217
            },
            {
                "hf_subset": "ben-eng",
                "languages": [
                    "ben-Beng",
                    "eng-Latn"
                ],
                "accuracy": 0.8030000000000002,
                "f1": 0.7597666666666667,
                "precision": 0.7415999999999999,
                "recall": 0.8030000000000002,
                "main_score": 0.7597666666666667
            },
            {
                "hf_subset": "swg-eng",
                "languages": [
                    "swg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.23214285714285715,
                "f1": 0.16889880952380948,
                "precision": 0.15364937641723353,
                "recall": 0.23214285714285715,
                "main_score": 0.16889880952380948
            },
            {
                "hf_subset": "arq-eng",
                "languages": [
                    "arq-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.33150384193194293,
                "f1": 0.27747873024072417,
                "precision": 0.2599320572578704,
                "recall": 0.33150384193194293,
                "main_score": 0.27747873024072417
            },
            {
                "hf_subset": "kab-eng",
                "languages": [
                    "kab-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.026000000000000002,
                "f1": 0.01687059048752127,
                "precision": 0.015384884521299,
                "recall": 0.026000000000000002,
                "main_score": 0.01687059048752127
            },
            {
                "hf_subset": "fra-eng",
                "languages": [
                    "fra-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9330000000000002,
                "f1": 0.9144000000000001,
                "precision": 0.9059166666666667,
                "recall": 0.9330000000000002,
                "main_score": 0.9144000000000001
            },
            {
                "hf_subset": "por-eng",
                "languages": [
                    "por-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.941,
                "f1": 0.9261666666666667,
                "precision": 0.9188333333333333,
                "recall": 0.941,
                "main_score": 0.9261666666666667
            },
            {
                "hf_subset": "tat-eng",
                "languages": [
                    "tat-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.05,
                "f1": 0.03589591971281927,
                "precision": 0.03304649161453285,
                "recall": 0.05,
                "main_score": 0.03589591971281927
            },
            {
                "hf_subset": "oci-eng",
                "languages": [
                    "oci-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.45899999999999996,
                "f1": 0.4017196914196914,
                "precision": 0.3830764368870302,
                "recall": 0.45899999999999996,
                "main_score": 0.4017196914196914
            },
            {
                "hf_subset": "pol-eng",
                "languages": [
                    "pol-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.169,
                "f1": 0.14094365204207351,
                "precision": 0.13276519841269843,
                "recall": 0.169,
                "main_score": 0.14094365204207351
            },
            {
                "hf_subset": "war-eng",
                "languages": [
                    "war-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.128,
                "f1": 0.10376574912567156,
                "precision": 0.09758423963284509,
                "recall": 0.128,
                "main_score": 0.10376574912567156
            },
            {
                "hf_subset": "aze-eng",
                "languages": [
                    "aze-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.081,
                "f1": 0.06319455355175778,
                "precision": 0.05849948830628881,
                "recall": 0.081,
                "main_score": 0.06319455355175778
            },
            {
                "hf_subset": "vie-eng",
                "languages": [
                    "vie-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.955,
                "f1": 0.9419666666666667,
                "precision": 0.936,
                "recall": 0.955,
                "main_score": 0.9419666666666667
            },
            {
                "hf_subset": "nno-eng",
                "languages": [
                    "nno-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.191,
                "f1": 0.16280080686081905,
                "precision": 0.15451573089395668,
                "recall": 0.191,
                "main_score": 0.16280080686081905
            },
            {
                "hf_subset": "cha-eng",
                "languages": [
                    "cha-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.30656934306569344,
                "f1": 0.232568647897115,
                "precision": 0.21260309034031663,
                "recall": 0.30656934306569344,
                "main_score": 0.232568647897115
            },
            {
                "hf_subset": "mhr-eng",
                "languages": [
                    "mhr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.022,
                "f1": 0.015568610472955212,
                "precision": 0.014555993437238521,
                "recall": 0.022,
                "main_score": 0.015568610472955212
            },
            {
                "hf_subset": "dan-eng",
                "languages": [
                    "dan-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.275,
                "f1": 0.23521682636223493,
                "precision": 0.22345341306967684,
                "recall": 0.275,
                "main_score": 0.23521682636223493
            },
            {
                "hf_subset": "ell-eng",
                "languages": [
                    "ell-Grek",
                    "eng-Latn"
                ],
                "accuracy": 0.074,
                "f1": 0.05344253880846173,
                "precision": 0.04999794279068863,
                "recall": 0.074,
                "main_score": 0.05344253880846173
            },
            {
                "hf_subset": "amh-eng",
                "languages": [
                    "amh-Ethi",
                    "eng-Latn"
                ],
                "accuracy": 0.005952380952380952,
                "f1": 0.00026455026455026457,
                "precision": 0.00013528138528138528,
                "recall": 0.005952380952380952,
                "main_score": 0.00026455026455026457
            },
            {
                "hf_subset": "pam-eng",
                "languages": [
                    "pam-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.073,
                "f1": 0.05853140211779251,
                "precision": 0.05505563080945322,
                "recall": 0.073,
                "main_score": 0.05853140211779251
            },
            {
                "hf_subset": "hsb-eng",
                "languages": [
                    "hsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.13250517598343686,
                "f1": 0.09676349506190704,
                "precision": 0.08930392053553217,
                "recall": 0.13250517598343686,
                "main_score": 0.09676349506190704
            },
            {
                "hf_subset": "srp-eng",
                "languages": [
                    "srp-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.145,
                "f1": 0.1168912588067557,
                "precision": 0.11024716513105519,
                "recall": 0.145,
                "main_score": 0.1168912588067557
            },
            {
                "hf_subset": "epo-eng",
                "languages": [
                    "epo-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.301,
                "f1": 0.2619688093631515,
                "precision": 0.2527171408616948,
                "recall": 0.301,
                "main_score": 0.2619688093631515
            },
            {
                "hf_subset": "kzj-eng",
                "languages": [
                    "kzj-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.064,
                "f1": 0.05174944594202333,
                "precision": 0.049753381420296255,
                "recall": 0.064,
                "main_score": 0.05174944594202333
            },
            {
                "hf_subset": "awa-eng",
                "languages": [
                    "awa-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.3939393939393939,
                "f1": 0.35005707393767094,
                "precision": 0.3364342032053631,
                "recall": 0.3939393939393939,
                "main_score": 0.35005707393767094
            },
            {
                "hf_subset": "fao-eng",
                "languages": [
                    "fao-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.18320610687022898,
                "f1": 0.12610893447220345,
                "precision": 0.11079228765297466,
                "recall": 0.18320610687022898,
                "main_score": 0.12610893447220345
            },
            {
                "hf_subset": "mal-eng",
                "languages": [
                    "mal-Mlym",
                    "eng-Latn"
                ],
                "accuracy": 0.8558951965065502,
                "f1": 0.8330363944928548,
                "precision": 0.8240026591554976,
                "recall": 0.8558951965065502,
                "main_score": 0.8330363944928548
            },
            {
                "hf_subset": "ile-eng",
                "languages": [
                    "ile-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.657,
                "f1": 0.5958964285714285,
                "precision": 0.5739282679738562,
                "recall": 0.657,
                "main_score": 0.5958964285714285
            },
            {
                "hf_subset": "bos-eng",
                "languages": [
                    "bos-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.1807909604519774,
                "f1": 0.1365194306689995,
                "precision": 0.12567953943826327,
                "recall": 0.1807909604519774,
                "main_score": 0.1365194306689995
            },
            {
                "hf_subset": "cor-eng",
                "languages": [
                    "cor-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.046,
                "f1": 0.02833538639250501,
                "precision": 0.02558444143575722,
                "recall": 0.046,
                "main_score": 0.02833538639250501
            },
            {
                "hf_subset": "cat-eng",
                "languages": [
                    "cat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.907,
                "f1": 0.8830666666666666,
                "precision": 0.8719499999999999,
                "recall": 0.907,
                "main_score": 0.8830666666666666
            },
            {
                "hf_subset": "eus-eng",
                "languages": [
                    "eus-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.577,
                "f1": 0.5338433067253876,
                "precision": 0.5181545133535035,
                "recall": 0.577,
                "main_score": 0.5338433067253876
            },
            {
                "hf_subset": "yue-eng",
                "languages": [
                    "yue-Hant",
                    "eng-Latn"
                ],
                "accuracy": 0.806,
                "f1": 0.7702903540903541,
                "precision": 0.7561685897435898,
                "recall": 0.806,
                "main_score": 0.7702903540903541
            },
            {
                "hf_subset": "swe-eng",
                "languages": [
                    "swe-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.24600000000000002,
                "f1": 0.1952814960069739,
                "precision": 0.18169084599880503,
                "recall": 0.24600000000000002,
                "main_score": 0.1952814960069739
            },
            {
                "hf_subset": "dtp-eng",
                "languages": [
                    "dtp-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.05,
                "f1": 0.034078491753102375,
                "precision": 0.031757682319102386,
                "recall": 0.05,
                "main_score": 0.034078491753102375
            },
            {
                "hf_subset": "kat-eng",
                "languages": [
                    "kat-Geor",
                    "eng-Latn"
                ],
                "accuracy": 0.012064343163538873,
                "f1": 0.004224313053283095,
                "precision": 0.003360484946842894,
                "recall": 0.012064343163538873,
                "main_score": 0.004224313053283095
            },
            {
                "hf_subset": "jpn-eng",
                "languages": [
                    "jpn-Jpan",
                    "eng-Latn"
                ],
                "accuracy": 0.7609999999999999,
                "f1": 0.7136246031746032,
                "precision": 0.695086544011544,
                "recall": 0.7609999999999999,
                "main_score": 0.7136246031746032
            },
            {
                "hf_subset": "csb-eng",
                "languages": [
                    "csb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.1422924901185771,
                "f1": 0.10026578603653703,
                "precision": 0.0909171178352764,
                "recall": 0.1422924901185771,
                "main_score": 0.10026578603653703
            },
            {
                "hf_subset": "xho-eng",
                "languages": [
                    "xho-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.08450704225352113,
                "f1": 0.0551214407186151,
                "precision": 0.04928281812084629,
                "recall": 0.08450704225352113,
                "main_score": 0.0551214407186151
            },
            {
                "hf_subset": "orv-eng",
                "languages": [
                    "orv-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.07664670658682635,
                "f1": 0.05786190079917295,
                "precision": 0.053643643579244,
                "recall": 0.07664670658682635,
                "main_score": 0.05786190079917295
            },
            {
                "hf_subset": "ind-eng",
                "languages": [
                    "ind-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.905,
                "f1": 0.8804,
                "precision": 0.8694833333333334,
                "recall": 0.905,
                "main_score": 0.8804
            },
            {
                "hf_subset": "tuk-eng",
                "languages": [
                    "tuk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.07389162561576355,
                "f1": 0.05482366349556517,
                "precision": 0.051568144499178986,
                "recall": 0.07389162561576355,
                "main_score": 0.05482366349556517
            },
            {
                "hf_subset": "max-eng",
                "languages": [
                    "max-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.4154929577464789,
                "f1": 0.3613520282534367,
                "precision": 0.34818226488560994,
                "recall": 0.4154929577464789,
                "main_score": 0.3613520282534367
            },
            {
                "hf_subset": "swh-eng",
                "languages": [
                    "swh-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.2076923076923077,
                "f1": 0.16742497560177644,
                "precision": 0.15965759712090138,
                "recall": 0.2076923076923077,
                "main_score": 0.16742497560177644
            },
            {
                "hf_subset": "hin-eng",
                "languages": [
                    "hin-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.8809999999999999,
                "f1": 0.8523176470588235,
                "precision": 0.8404458333333333,
                "recall": 0.8809999999999999,
                "main_score": 0.8523176470588235
            },
            {
                "hf_subset": "dsb-eng",
                "languages": [
                    "dsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.11899791231732777,
                "f1": 0.08776706659565102,
                "precision": 0.08167815946521582,
                "recall": 0.11899791231732777,
                "main_score": 0.08776706659565102
            },
            {
                "hf_subset": "ber-eng",
                "languages": [
                    "ber-Tfng",
                    "eng-Latn"
                ],
                "accuracy": 0.061,
                "f1": 0.049165895371784354,
                "precision": 0.0472523017415345,
                "recall": 0.061,
                "main_score": 0.049165895371784354
            },
            {
                "hf_subset": "tam-eng",
                "languages": [
                    "tam-Taml",
                    "eng-Latn"
                ],
                "accuracy": 0.7654723127035831,
                "f1": 0.7275787187839307,
                "precision": 0.7143338442869005,
                "recall": 0.7654723127035831,
                "main_score": 0.7275787187839307
            },
            {
                "hf_subset": "slk-eng",
                "languages": [
                    "slk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.117,
                "f1": 0.09975679190026007,
                "precision": 0.09569927715653521,
                "recall": 0.117,
                "main_score": 0.09975679190026007
            },
            {
                "hf_subset": "tgl-eng",
                "languages": [
                    "tgl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.131,
                "f1": 0.10697335850115408,
                "precision": 0.10113816082086341,
                "recall": 0.131,
                "main_score": 0.10697335850115408
            },
            {
                "hf_subset": "ast-eng",
                "languages": [
                    "ast-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7637795275590551,
                "f1": 0.7112860892388452,
                "precision": 0.6889763779527559,
                "recall": 0.7637795275590551,
                "main_score": 0.7112860892388452
            },
            {
                "hf_subset": "mkd-eng",
                "languages": [
                    "mkd-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.137,
                "f1": 0.10471861684067568,
                "precision": 0.09602902567641697,
                "recall": 0.137,
                "main_score": 0.10471861684067568
            },
            {
                "hf_subset": "khm-eng",
                "languages": [
                    "khm-Khmr",
                    "eng-Latn"
                ],
                "accuracy": 0.00554016620498615,
                "f1": 0.0037034084643642424,
                "precision": 0.0034676040281208438,
                "recall": 0.00554016620498615,
                "main_score": 0.0037034084643642424
            },
            {
                "hf_subset": "ces-eng",
                "languages": [
                    "ces-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.124,
                "f1": 0.09552607451092535,
                "precision": 0.08985175505050505,
                "recall": 0.124,
                "main_score": 0.09552607451092535
            },
            {
                "hf_subset": "tzl-eng",
                "languages": [
                    "tzl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.3365384615384615,
                "f1": 0.27820512820512816,
                "precision": 0.2609432234432234,
                "recall": 0.3365384615384615,
                "main_score": 0.27820512820512816
            },
            {
                "hf_subset": "urd-eng",
                "languages": [
                    "urd-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.745,
                "f1": 0.7009686507936508,
                "precision": 0.683117857142857,
                "recall": 0.745,
                "main_score": 0.7009686507936508
            },
            {
                "hf_subset": "ara-eng",
                "languages": [
                    "ara-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.883,
                "f1": 0.8537333333333333,
                "precision": 0.8405833333333333,
                "recall": 0.883,
                "main_score": 0.8537333333333333
            },
            {
                "hf_subset": "kor-eng",
                "languages": [
                    "kor-Hang",
                    "eng-Latn"
                ],
                "accuracy": 0.25,
                "f1": 0.22393124632031994,
                "precision": 0.2158347686592367,
                "recall": 0.25,
                "main_score": 0.22393124632031994
            },
            {
                "hf_subset": "yid-eng",
                "languages": [
                    "yid-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.00589622641509434,
                "f1": 0.001580498003376294,
                "precision": 0.0013932753848729651,
                "recall": 0.00589622641509434,
                "main_score": 0.001580498003376294
            },
            {
                "hf_subset": "fin-eng",
                "languages": [
                    "fin-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.04100000000000001,
                "f1": 0.034069011332551774,
                "precision": 0.03178450704225352,
                "recall": 0.04100000000000001,
                "main_score": 0.034069011332551774
            },
            {
                "hf_subset": "tha-eng",
                "languages": [
                    "tha-Thai",
                    "eng-Latn"
                ],
                "accuracy": 0.03102189781021898,
                "f1": 0.022238518116947513,
                "precision": 0.02103465682299194,
                "recall": 0.03102189781021898,
                "main_score": 0.022238518116947513
            },
            {
                "hf_subset": "wuu-eng",
                "languages": [
                    "wuu-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.831,
                "f1": 0.7958255835667599,
                "precision": 0.7809708333333333,
                "recall": 0.831,
                "main_score": 0.7958255835667599
            }
        ]
    }
}