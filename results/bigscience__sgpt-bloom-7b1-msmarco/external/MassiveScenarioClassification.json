{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 0.5147276395427035,
                "f1": 0.49374632081307995,
                "main_score": 0.5147276395427035
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 0.34868863483523865,
                "f1": 0.33741780743496363,
                "main_score": 0.34868863483523865
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 0.6520511096166779,
                "f1": 0.6585812500602437,
                "main_score": 0.6520511096166779
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 0.4557834566240754,
                "f1": 0.44445149170280035,
                "main_score": 0.4557834566240754
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 0.6729657027572293,
                "f1": 0.6724477523937467,
                "main_score": 0.6729657027572293
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 0.4629455279085407,
                "f1": 0.438563839951935,
                "main_score": 0.4629455279085407
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 0.5352387357094821,
                "f1": 0.5170977848027551,
                "main_score": 0.5352387357094821
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 0.6174176193678547,
                "f1": 0.6021916964479229,
                "main_score": 0.6174176193678547
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 0.48957632817753877,
                "f1": 0.4687842826446003,
                "main_score": 0.48957632817753877
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 0.7533624747814394,
                "f1": 0.7591438462111709,
                "main_score": 0.7533624747814394
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 0.7334229993275049,
                "f1": 0.7378165397558983,
                "main_score": 0.7334229993275049
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 0.5317417619367856,
                "f1": 0.5170967922777898,
                "main_score": 0.5317417619367856
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 0.44690652320107604,
                "f1": 0.4154881682785664,
                "main_score": 0.44690652320107604
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 0.729119031607263,
                "f1": 0.732742013056326,
                "main_score": 0.729119031607263
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 0.4310356422326832,
                "f1": 0.408859122581252,
                "main_score": 0.4310356422326832
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 0.6927370544720914,
                "f1": 0.6939544506405082,
                "main_score": 0.6927370544720914
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 0.4516476126429052,
                "f1": 0.42740225315790537,
                "main_score": 0.4516476126429052
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 0.38732347007397444,
                "f1": 0.37405467549510263,
                "main_score": 0.38732347007397444
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 0.7012777404169468,
                "f1": 0.7027219152812738,
                "main_score": 0.7012777404169468
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 0.44213180901143245,
                "f1": 0.4193459321382937,
                "main_score": 0.44213180901143245
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 0.6557162071284466,
                "f1": 0.6483341759045335,
                "main_score": 0.6557162071284466
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 0.6575991930060525,
                "f1": 0.6516549875504951,
                "main_score": 0.6575991930060525
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 0.5479488903833223,
                "f1": 0.540361640142686,
                "main_score": 0.5479488903833223
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 0.3299260255548084,
                "f1": 0.3182006847001885,
                "main_score": 0.3299260255548084
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 0.39344317417619373,
                "f1": 0.364362216652901,
                "main_score": 0.39344317417619373
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 0.6050100874243444,
                "f1": 0.6005101371257908,
                "main_score": 0.6050100874243444
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 0.5568930733019504,
                "f1": 0.5394058032286942,
                "main_score": 0.5568930733019504
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 0.4435104236718225,
                "f1": 0.4205421666771541,
                "main_score": 0.4435104236718225
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 0.6553127101546738,
                "f1": 0.6598462024333497,
                "main_score": 0.6553127101546738
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 0.3871553463349025,
                "f1": 0.3744327037149584,
                "main_score": 0.3871553463349025
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 0.6498991257565566,
                "f1": 0.6387720198978004,
                "main_score": 0.6498991257565566
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 0.36839273705447206,
                "f1": 0.35233967279698375,
                "main_score": 0.36839273705447206
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 0.5179892400806994,
                "f1": 0.4966926632125972,
                "main_score": 0.5179892400806994
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 0.5631809011432415,
                "f1": 0.5383218533617983,
                "main_score": 0.5631809011432415
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 0.49979825151311374,
                "f1": 0.4883013175441888,
                "main_score": 0.49979825151311374
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 0.7145595158036315,
                "f1": 0.7208708814699701,
                "main_score": 0.7145595158036315
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 0.5368527236045729,
                "f1": 0.5223278593929981,
                "main_score": 0.5368527236045729
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 0.6160390047074646,
                "f1": 0.6050391482195117,
                "main_score": 0.6160390047074646
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 0.48036314727639534,
                "f1": 0.4643480413383716,
                "main_score": 0.48036314727639534
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 0.5005716207128446,
                "f1": 0.4885821859948888,
                "main_score": 0.5005716207128446
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 0.5172831203765971,
                "f1": 0.4989292996950847,
                "main_score": 0.5172831203765971
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 0.5421990585070613,
                "f1": 0.528711542984193,
                "main_score": 0.5421990585070613
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 0.6277067921990584,
                "f1": 0.6309441501491594,
                "main_score": 0.6277067921990584
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 0.6258574310692671,
                "f1": 0.6161370697612978,
                "main_score": 0.6258574310692671
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 0.45178211163416276,
                "f1": 0.4385143229183324,
                "main_score": 0.45178211163416276
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 0.5206455951580364,
                "f1": 0.5094356892049626,
                "main_score": 0.5206455951580364
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 0.47205783456624073,
                "f1": 0.47042236441204893,
                "main_score": 0.47205783456624073
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 0.6425689307330196,
                "f1": 0.6389944944984115,
                "main_score": 0.6425689307330196
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 0.7060524546065905,
                "f1": 0.7156341573343581,
                "main_score": 0.7060524546065905
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 0.7395427034297242,
                "f1": 0.7439706882311063,
                "main_score": 0.7395427034297242
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 0.7029926025554808,
                "f1": 0.7132045932560297,
                "main_score": 0.7029926025554808
            }
        ]
    }
}