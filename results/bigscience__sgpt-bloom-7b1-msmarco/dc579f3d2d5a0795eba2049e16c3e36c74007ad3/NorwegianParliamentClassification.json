{
  "dataset_revision": "f7393532774c66312378d30b197610b43d751972",
  "task_name": "NorwegianParliamentClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.536917,
        "f1": 0.532181,
        "f1_weighted": 0.532181,
        "ap": 0.520402,
        "ap_weighted": 0.520402,
        "scores_per_experiment": [
          {
            "accuracy": 0.570833,
            "f1": 0.566571,
            "f1_weighted": 0.566571,
            "ap": 0.539604,
            "ap_weighted": 0.539604
          },
          {
            "accuracy": 0.535,
            "f1": 0.526584,
            "f1_weighted": 0.526584,
            "ap": 0.518467,
            "ap_weighted": 0.518467
          },
          {
            "accuracy": 0.529167,
            "f1": 0.523131,
            "f1_weighted": 0.523131,
            "ap": 0.515681,
            "ap_weighted": 0.515681
          },
          {
            "accuracy": 0.513333,
            "f1": 0.496407,
            "f1_weighted": 0.496407,
            "ap": 0.506947,
            "ap_weighted": 0.506947
          },
          {
            "accuracy": 0.550833,
            "f1": 0.550083,
            "f1_weighted": 0.550083,
            "ap": 0.527806,
            "ap_weighted": 0.527806
          },
          {
            "accuracy": 0.5425,
            "f1": 0.542429,
            "f1_weighted": 0.542429,
            "ap": 0.523012,
            "ap_weighted": 0.523012
          },
          {
            "accuracy": 0.515,
            "f1": 0.514347,
            "f1_weighted": 0.514347,
            "ap": 0.50771,
            "ap_weighted": 0.50771
          },
          {
            "accuracy": 0.514167,
            "f1": 0.51342,
            "f1_weighted": 0.51342,
            "ap": 0.507269,
            "ap_weighted": 0.507269
          },
          {
            "accuracy": 0.576667,
            "f1": 0.570128,
            "f1_weighted": 0.570128,
            "ap": 0.546136,
            "ap_weighted": 0.546136
          },
          {
            "accuracy": 0.521667,
            "f1": 0.518713,
            "f1_weighted": 0.518713,
            "ap": 0.51139,
            "ap_weighted": 0.51139
          }
        ],
        "main_score": 0.536917,
        "hf_subset": "default",
        "languages": [
          "nob-Latn"
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.543667,
        "f1": 0.538726,
        "f1_weighted": 0.538726,
        "ap": 0.524023,
        "ap_weighted": 0.524023,
        "scores_per_experiment": [
          {
            "accuracy": 0.565833,
            "f1": 0.562221,
            "f1_weighted": 0.562221,
            "ap": 0.536584,
            "ap_weighted": 0.536584
          },
          {
            "accuracy": 0.540833,
            "f1": 0.531765,
            "f1_weighted": 0.531765,
            "ap": 0.521721,
            "ap_weighted": 0.521721
          },
          {
            "accuracy": 0.544167,
            "f1": 0.534262,
            "f1_weighted": 0.534262,
            "ap": 0.524837,
            "ap_weighted": 0.524837
          },
          {
            "accuracy": 0.5375,
            "f1": 0.518396,
            "f1_weighted": 0.518396,
            "ap": 0.521087,
            "ap_weighted": 0.521087
          },
          {
            "accuracy": 0.5775,
            "f1": 0.577006,
            "f1_weighted": 0.577006,
            "ap": 0.544372,
            "ap_weighted": 0.544372
          },
          {
            "accuracy": 0.54,
            "f1": 0.539937,
            "f1_weighted": 0.539937,
            "ap": 0.521638,
            "ap_weighted": 0.521638
          },
          {
            "accuracy": 0.5325,
            "f1": 0.532055,
            "f1_weighted": 0.532055,
            "ap": 0.517245,
            "ap_weighted": 0.517245
          },
          {
            "accuracy": 0.535,
            "f1": 0.534844,
            "f1_weighted": 0.534844,
            "ap": 0.518682,
            "ap_weighted": 0.518682
          },
          {
            "accuracy": 0.531667,
            "f1": 0.527094,
            "f1_weighted": 0.527094,
            "ap": 0.517082,
            "ap_weighted": 0.517082
          },
          {
            "accuracy": 0.531667,
            "f1": 0.52968,
            "f1_weighted": 0.52968,
            "ap": 0.516986,
            "ap_weighted": 0.516986
          }
        ],
        "main_score": 0.543667,
        "hf_subset": "default",
        "languages": [
          "nob-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 229.6247136592865,
  "kg_co2_emissions": 0.020613522959699718
}