{
  "dataset_revision": "edfaf9da55d3dd50d43143d90c1ac476895ae6de",
  "task_name": "ToxicConversationsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.618848,
        "f1": 0.473369,
        "f1_weighted": 0.699423,
        "ap": 0.105842,
        "ap_weighted": 0.105842,
        "scores_per_experiment": [
          {
            "accuracy": 0.715332,
            "f1": 0.526658,
            "f1_weighted": 0.778223,
            "ap": 0.114098,
            "ap_weighted": 0.114098
          },
          {
            "accuracy": 0.658203,
            "f1": 0.498385,
            "f1_weighted": 0.73673,
            "ap": 0.110157,
            "ap_weighted": 0.110157
          },
          {
            "accuracy": 0.70459,
            "f1": 0.523288,
            "f1_weighted": 0.770766,
            "ap": 0.115413,
            "ap_weighted": 0.115413
          },
          {
            "accuracy": 0.721191,
            "f1": 0.535448,
            "f1_weighted": 0.782724,
            "ap": 0.121161,
            "ap_weighted": 0.121161
          },
          {
            "accuracy": 0.358398,
            "f1": 0.316686,
            "f1_weighted": 0.458804,
            "ap": 0.08157,
            "ap_weighted": 0.08157
          },
          {
            "accuracy": 0.56543,
            "f1": 0.44819,
            "f1_weighted": 0.662301,
            "ap": 0.10188,
            "ap_weighted": 0.10188
          },
          {
            "accuracy": 0.726074,
            "f1": 0.513927,
            "f1_weighted": 0.784246,
            "ap": 0.098375,
            "ap_weighted": 0.098375
          },
          {
            "accuracy": 0.534668,
            "f1": 0.436446,
            "f1_weighted": 0.634498,
            "ap": 0.106724,
            "ap_weighted": 0.106724
          },
          {
            "accuracy": 0.623535,
            "f1": 0.486844,
            "f1_weighted": 0.709791,
            "ap": 0.115489,
            "ap_weighted": 0.115489
          },
          {
            "accuracy": 0.581055,
            "f1": 0.44782,
            "f1_weighted": 0.676146,
            "ap": 0.093551,
            "ap_weighted": 0.093551
          }
        ],
        "main_score": 0.618848,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 408.8211944103241,
  "kg_co2_emissions": null
}