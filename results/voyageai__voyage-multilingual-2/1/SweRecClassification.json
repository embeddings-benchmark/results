{
  "dataset_revision": "b07c6ce548f6a7ac8d546e1bbe197a0086409190",
  "task_name": "SweRecClassification",
  "mteb_version": "2.1.11",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.819824,
            "f1": 0.733343,
            "f1_weighted": 0.823106,
            "precision": 0.730286,
            "precision_weighted": 0.826894,
            "recall": 0.737481,
            "recall_weighted": 0.819824,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.738281,
            "f1": 0.685527,
            "f1_weighted": 0.765478,
            "precision": 0.704881,
            "precision_weighted": 0.823827,
            "recall": 0.721737,
            "recall_weighted": 0.738281,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.791016,
            "f1": 0.711641,
            "f1_weighted": 0.801881,
            "precision": 0.714327,
            "precision_weighted": 0.830687,
            "recall": 0.728778,
            "recall_weighted": 0.791016,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.795898,
            "f1": 0.720916,
            "f1_weighted": 0.807757,
            "precision": 0.720365,
            "precision_weighted": 0.826751,
            "recall": 0.734543,
            "recall_weighted": 0.795898,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.791016,
            "f1": 0.679591,
            "f1_weighted": 0.789599,
            "precision": 0.680378,
            "precision_weighted": 0.788239,
            "recall": 0.678924,
            "recall_weighted": 0.791016,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.819824,
            "f1": 0.705913,
            "f1_weighted": 0.811747,
            "precision": 0.716296,
            "precision_weighted": 0.806189,
            "recall": 0.701241,
            "recall_weighted": 0.819824,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.795898,
            "f1": 0.716513,
            "f1_weighted": 0.80642,
            "precision": 0.713035,
            "precision_weighted": 0.821346,
            "recall": 0.728882,
            "recall_weighted": 0.795898,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.739258,
            "f1": 0.692014,
            "f1_weighted": 0.773526,
            "precision": 0.718281,
            "precision_weighted": 0.848383,
            "recall": 0.737773,
            "recall_weighted": 0.739258,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.695312,
            "f1": 0.632632,
            "f1_weighted": 0.72703,
            "precision": 0.668719,
            "precision_weighted": 0.80942,
            "recall": 0.66013,
            "recall_weighted": 0.695312,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.785645,
            "f1": 0.689786,
            "f1_weighted": 0.791517,
            "precision": 0.687705,
            "precision_weighted": 0.798628,
            "recall": 0.694078,
            "recall_weighted": 0.785645,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.777197,
        "f1": 0.696788,
        "f1_weighted": 0.789806,
        "precision": 0.705427,
        "precision_weighted": 0.818036,
        "recall": 0.712357,
        "recall_weighted": 0.777197,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.777197,
        "hf_subset": "default",
        "languages": [
          "swe-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 116.59562277793884,
  "kg_co2_emissions": null
}