{
  "dataset_revision": "a76d16fae880597b9c73047b50159220a441cb54",
  "task_name": "MTOPDomainClassification",
  "mteb_version": "2.1.8",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.98974,
            "f1": 0.98865,
            "f1_weighted": 0.989741,
            "precision": 0.989087,
            "precision_weighted": 0.989872,
            "recall": 0.988381,
            "recall_weighted": 0.98974,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.990652,
            "f1": 0.989823,
            "f1_weighted": 0.990651,
            "precision": 0.989964,
            "precision_weighted": 0.990739,
            "recall": 0.989788,
            "recall_weighted": 0.990652,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.98974,
            "f1": 0.988748,
            "f1_weighted": 0.989738,
            "precision": 0.988956,
            "precision_weighted": 0.989854,
            "recall": 0.98868,
            "recall_weighted": 0.98974,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.990196,
            "f1": 0.989063,
            "f1_weighted": 0.990197,
            "precision": 0.98957,
            "precision_weighted": 0.990334,
            "recall": 0.988722,
            "recall_weighted": 0.990196,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.989284,
            "f1": 0.988061,
            "f1_weighted": 0.989272,
            "precision": 0.988563,
            "precision_weighted": 0.989353,
            "recall": 0.987676,
            "recall_weighted": 0.989284,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.989284,
            "f1": 0.988583,
            "f1_weighted": 0.989293,
            "precision": 0.988332,
            "precision_weighted": 0.989431,
            "recall": 0.988975,
            "recall_weighted": 0.989284,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.990652,
            "f1": 0.989655,
            "f1_weighted": 0.990638,
            "precision": 0.990367,
            "precision_weighted": 0.990695,
            "recall": 0.989047,
            "recall_weighted": 0.990652,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.990652,
            "f1": 0.989998,
            "f1_weighted": 0.990653,
            "precision": 0.989836,
            "precision_weighted": 0.990713,
            "recall": 0.990224,
            "recall_weighted": 0.990652,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.987004,
            "f1": 0.986139,
            "f1_weighted": 0.987027,
            "precision": 0.986259,
            "precision_weighted": 0.987282,
            "recall": 0.986287,
            "recall_weighted": 0.987004,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.990196,
            "f1": 0.989047,
            "f1_weighted": 0.990185,
            "precision": 0.989511,
            "precision_weighted": 0.990254,
            "recall": 0.988679,
            "recall_weighted": 0.990196,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.98974,
        "f1": 0.988777,
        "f1_weighted": 0.989739,
        "precision": 0.989044,
        "precision_weighted": 0.989853,
        "recall": 0.988646,
        "recall_weighted": 0.98974,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.98974,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 60.46223330497742,
  "kg_co2_emissions": null
}