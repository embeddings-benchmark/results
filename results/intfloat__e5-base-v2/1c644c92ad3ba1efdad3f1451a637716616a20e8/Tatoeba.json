{
  "dataset_revision": "69e8f12da6e31d59addadda9a9c8a2e601a0e282",
  "evaluation_time": 227.0123109817505,
  "kg_co2_emissions": 0.008227897259187518,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.018,
        "f1": 0.012282272199345369,
        "hf_subset": "kor-eng",
        "languages": [
          "kor-Hang",
          "eng-Latn"
        ],
        "main_score": 0.012282272199345369,
        "precision": 0.011419445459096015,
        "recall": 0.018
      },
      {
        "accuracy": 0.019,
        "f1": 0.01424981684981685,
        "hf_subset": "kab-eng",
        "languages": [
          "kab-Latn",
          "eng-Latn"
        ],
        "main_score": 0.01424981684981685,
        "precision": 0.013464242424242424,
        "recall": 0.019
      },
      {
        "accuracy": 0.01,
        "f1": 0.005218626868987736,
        "hf_subset": "pes-eng",
        "languages": [
          "pes-Arab",
          "eng-Latn"
        ],
        "main_score": 0.005218626868987736,
        "precision": 0.004782171828464062,
        "recall": 0.01
      },
      {
        "accuracy": 0.135,
        "f1": 0.10451412126192451,
        "hf_subset": "afr-eng",
        "languages": [
          "afr-Latn",
          "eng-Latn"
        ],
        "main_score": 0.10451412126192451,
        "precision": 0.09714172494172493,
        "recall": 0.135
      },
      {
        "accuracy": 0.13740458015267176,
        "f1": 0.11702714164546225,
        "hf_subset": "fao-eng",
        "languages": [
          "fao-Latn",
          "eng-Latn"
        ],
        "main_score": 0.11702714164546225,
        "precision": 0.11338336861462141,
        "recall": 0.13740458015267176
      },
      {
        "accuracy": 0.051,
        "f1": 0.03764158189591626,
        "hf_subset": "cor-eng",
        "languages": [
          "cor-Latn",
          "eng-Latn"
        ],
        "main_score": 0.03764158189591626,
        "precision": 0.03485239565989566,
        "recall": 0.051
      },
      {
        "accuracy": 0.188,
        "f1": 0.15515699953869508,
        "hf_subset": "oci-eng",
        "languages": [
          "oci-Latn",
          "eng-Latn"
        ],
        "main_score": 0.15515699953869508,
        "precision": 0.14657141234758883,
        "recall": 0.188
      },
      {
        "accuracy": 0.009695290858725761,
        "f1": 0.00470265889640341,
        "hf_subset": "khm-eng",
        "languages": [
          "khm-Khmr",
          "eng-Latn"
        ],
        "main_score": 0.00470265889640341,
        "precision": 0.004475788734051527,
        "recall": 0.009695290858725761
      },
      {
        "accuracy": 0.1619718309859155,
        "f1": 0.13151546209967158,
        "hf_subset": "max-eng",
        "languages": [
          "max-Deva",
          "eng-Latn"
        ],
        "main_score": 0.13151546209967158,
        "precision": 0.122888721216186,
        "recall": 0.1619718309859155
      },
      {
        "accuracy": 0.033,
        "f1": 0.01842958055098429,
        "hf_subset": "bul-eng",
        "languages": [
          "bul-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.01842958055098429,
        "precision": 0.016302008565620182,
        "recall": 0.033
      },
      {
        "accuracy": 0.008385744234800839,
        "f1": 0.005254572050664995,
        "hf_subset": "arz-eng",
        "languages": [
          "arz-Arab",
          "eng-Latn"
        ],
        "main_score": 0.005254572050664995,
        "precision": 0.004898446833930705,
        "recall": 0.008385744234800839
      },
      {
        "accuracy": 0.385,
        "f1": 0.3476411677753142,
        "hf_subset": "fra-eng",
        "languages": [
          "fra-Latn",
          "eng-Latn"
        ],
        "main_score": 0.3476411677753142,
        "precision": 0.334317609685001,
        "recall": 0.385
      },
      {
        "accuracy": 0.43190661478599224,
        "f1": 0.3706560624848562,
        "hf_subset": "nov-eng",
        "languages": [
          "nov-Latn",
          "eng-Latn"
        ],
        "main_score": 0.3706560624848562,
        "precision": 0.34895929837564077,
        "recall": 0.43190661478599224
      },
      {
        "accuracy": 0.017391304347826087,
        "f1": 0.011532050992836338,
        "hf_subset": "kaz-eng",
        "languages": [
          "kaz-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.011532050992836338,
        "precision": 0.010725377107364684,
        "recall": 0.017391304347826087
      },
      {
        "accuracy": 0.012773722627737226,
        "f1": 0.010349041900725442,
        "hf_subset": "tha-eng",
        "languages": [
          "tha-Thai",
          "eng-Latn"
        ],
        "main_score": 0.010349041900725442,
        "precision": 0.01004071070952951,
        "recall": 0.012773722627737226
      },
      {
        "accuracy": 0.044,
        "f1": 0.02478685897435897,
        "hf_subset": "yue-eng",
        "languages": [
          "yue-Hant",
          "eng-Latn"
        ],
        "main_score": 0.02478685897435897,
        "precision": 0.02231205522533048,
        "recall": 0.044
      },
      {
        "accuracy": 0.03636363636363636,
        "f1": 0.026152471124381235,
        "hf_subset": "mon-eng",
        "languages": [
          "mon-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.026152471124381235,
        "precision": 0.024391190614276737,
        "recall": 0.03636363636363636
      },
      {
        "accuracy": 0.064,
        "f1": 0.05145464073670181,
        "hf_subset": "lvs-eng",
        "languages": [
          "lvs-Latn",
          "eng-Latn"
        ],
        "main_score": 0.05145464073670181,
        "precision": 0.04875927033031728,
        "recall": 0.064
      },
      {
        "accuracy": 0.10814094775212636,
        "f1": 0.08125293984909095,
        "hf_subset": "slv-eng",
        "languages": [
          "slv-Latn",
          "eng-Latn"
        ],
        "main_score": 0.08125293984909095,
        "precision": 0.07476464272225196,
        "recall": 0.10814094775212636
      },
      {
        "accuracy": 0.007,
        "f1": 0.0028161616161616163,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.0028161616161616163,
        "precision": 0.002147764227642276,
        "recall": 0.007
      },
      {
        "accuracy": 0.098,
        "f1": 0.08377625420388576,
        "hf_subset": "ind-eng",
        "languages": [
          "ind-Latn",
          "eng-Latn"
        ],
        "main_score": 0.08377625420388576,
        "precision": 0.08036906113210461,
        "recall": 0.098
      },
      {
        "accuracy": 0.356,
        "f1": 0.30563968253968254,
        "hf_subset": "por-eng",
        "languages": [
          "por-Latn",
          "eng-Latn"
        ],
        "main_score": 0.30563968253968254,
        "precision": 0.2895379728523111,
        "recall": 0.356
      },
      {
        "accuracy": 0.197,
        "f1": 0.16299058775558495,
        "hf_subset": "ron-eng",
        "languages": [
          "ron-Latn",
          "eng-Latn"
        ],
        "main_score": 0.16299058775558495,
        "precision": 0.1537486930184511,
        "recall": 0.197
      },
      {
        "accuracy": 0.27167630057803466,
        "f1": 0.21288803254121172,
        "hf_subset": "fry-eng",
        "languages": [
          "fry-Latn",
          "eng-Latn"
        ],
        "main_score": 0.21288803254121172,
        "precision": 0.19809848147998435,
        "recall": 0.27167630057803466
      },
      {
        "accuracy": 0.09881422924901186,
        "f1": 0.07914971840898474,
        "hf_subset": "csb-eng",
        "languages": [
          "csb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.07914971840898474,
        "precision": 0.07632800335369506,
        "recall": 0.09881422924901186
      },
      {
        "accuracy": 0.306,
        "f1": 0.2667903543332501,
        "hf_subset": "glg-eng",
        "languages": [
          "glg-Latn",
          "eng-Latn"
        ],
        "main_score": 0.2667903543332501,
        "precision": 0.2567052500658383,
        "recall": 0.306
      },
      {
        "accuracy": 0.358,
        "f1": 0.3161617675707945,
        "hf_subset": "spa-eng",
        "languages": [
          "spa-Latn",
          "eng-Latn"
        ],
        "main_score": 0.3161617675707945,
        "precision": 0.3044287622999966,
        "recall": 0.358
      },
      {
        "accuracy": 0.002,
        "f1": 1.3885573885573886e-05,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 1.3885573885573886e-05,
        "precision": 6.97713873690461e-06,
        "recall": 0.002
      },
      {
        "accuracy": 0.0918580375782881,
        "f1": 0.0732776617954071,
        "hf_subset": "dsb-eng",
        "languages": [
          "dsb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.0732776617954071,
        "precision": 0.06867813235245386,
        "recall": 0.0918580375782881
      },
      {
        "accuracy": 0.147,
        "f1": 0.11030079407387798,
        "hf_subset": "lat-eng",
        "languages": [
          "lat-Latn",
          "eng-Latn"
        ],
        "main_score": 0.11030079407387798,
        "precision": 0.10099715287566988,
        "recall": 0.147
      },
      {
        "accuracy": 0.2222222222222222,
        "f1": 0.17848447848447851,
        "hf_subset": "gsw-eng",
        "languages": [
          "gsw-Latn",
          "eng-Latn"
        ],
        "main_score": 0.17848447848447851,
        "precision": 0.16766381766381766,
        "recall": 0.2222222222222222
      },
      {
        "accuracy": 0.011904761904761904,
        "f1": 0.006802721088435374,
        "hf_subset": "amh-eng",
        "languages": [
          "amh-Ethi",
          "eng-Latn"
        ],
        "main_score": 0.006802721088435374,
        "precision": 0.00641025641025641,
        "recall": 0.011904761904761904
      },
      {
        "accuracy": 0.045,
        "f1": 0.03358536944025035,
        "hf_subset": "gle-eng",
        "languages": [
          "gle-Latn",
          "eng-Latn"
        ],
        "main_score": 0.03358536944025035,
        "precision": 0.03089350112253338,
        "recall": 0.045
      },
      {
        "accuracy": 0.19047619047619047,
        "f1": 0.16127313251492453,
        "hf_subset": "pms-eng",
        "languages": [
          "pms-Latn",
          "eng-Latn"
        ],
        "main_score": 0.16127313251492453,
        "precision": 0.15291125541125541,
        "recall": 0.19047619047619047
      },
      {
        "accuracy": 0.04,
        "f1": 0.031523864905709964,
        "hf_subset": "cmn-eng",
        "languages": [
          "cmn-Hans",
          "eng-Latn"
        ],
        "main_score": 0.031523864905709964,
        "precision": 0.03003103057415451,
        "recall": 0.04
      },
      {
        "accuracy": 0.002911208151382824,
        "f1": 0.0014603687371503036,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.0014603687371503036,
        "precision": 0.00145799031188107,
        "recall": 0.002911208151382824
      },
      {
        "accuracy": 0.007,
        "f1": 0.005127272727272727,
        "hf_subset": "ara-eng",
        "languages": [
          "ara-Arab",
          "eng-Latn"
        ],
        "main_score": 0.005127272727272727,
        "precision": 0.00478125,
        "recall": 0.007
      },
      {
        "accuracy": 0.10975609756097561,
        "f1": 0.08554831969466116,
        "hf_subset": "kur-eng",
        "languages": [
          "kur-Latn",
          "eng-Latn"
        ],
        "main_score": 0.08554831969466116,
        "precision": 0.0809451453135221,
        "recall": 0.10975609756097561
      },
      {
        "accuracy": 0.003257328990228013,
        "f1": 0.00010679767181075453,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.00010679767181075453,
        "precision": 5.428881650380022e-05,
        "recall": 0.003257328990228013
      },
      {
        "accuracy": 0.18,
        "f1": 0.15989145504882307,
        "hf_subset": "nob-eng",
        "languages": [
          "nob-Latn",
          "eng-Latn"
        ],
        "main_score": 0.15989145504882307,
        "precision": 0.1543396020339221,
        "recall": 0.18
      },
      {
        "accuracy": 0.085,
        "f1": 0.06832517256352219,
        "hf_subset": "ces-eng",
        "languages": [
          "ces-Latn",
          "eng-Latn"
        ],
        "main_score": 0.06832517256352219,
        "precision": 0.06530267967052956,
        "recall": 0.085
      },
      {
        "accuracy": 0.081,
        "f1": 0.06569253246753246,
        "hf_subset": "isl-eng",
        "languages": [
          "isl-Latn",
          "eng-Latn"
        ],
        "main_score": 0.06569253246753246,
        "precision": 0.06194202551834131,
        "recall": 0.081
      },
      {
        "accuracy": 0.279,
        "f1": 0.2343519461545039,
        "hf_subset": "ita-eng",
        "languages": [
          "ita-Latn",
          "eng-Latn"
        ],
        "main_score": 0.2343519461545039,
        "precision": 0.22183583833559323,
        "recall": 0.279
      },
      {
        "accuracy": 0.149,
        "f1": 0.12525453322571967,
        "hf_subset": "nno-eng",
        "languages": [
          "nno-Latn",
          "eng-Latn"
        ],
        "main_score": 0.12525453322571967,
        "precision": 0.11917189911956132,
        "recall": 0.149
      },
      {
        "accuracy": 0.249,
        "f1": 0.21160204876822525,
        "hf_subset": "lfn-eng",
        "languages": [
          "lfn-Latn",
          "eng-Latn"
        ],
        "main_score": 0.21160204876822525,
        "precision": 0.20063370787709023,
        "recall": 0.249
      },
      {
        "accuracy": 0.19230769230769232,
        "f1": 0.1648422090729783,
        "hf_subset": "tzl-eng",
        "languages": [
          "tzl-Latn",
          "eng-Latn"
        ],
        "main_score": 0.1648422090729783,
        "precision": 0.15663461538461537,
        "recall": 0.19230769230769232
      },
      {
        "accuracy": 0.267,
        "f1": 0.21921255536847084,
        "hf_subset": "ido-eng",
        "languages": [
          "ido-Latn",
          "eng-Latn"
        ],
        "main_score": 0.21921255536847084,
        "precision": 0.20644889012947837,
        "recall": 0.267
      },
      {
        "accuracy": 0.2992125984251969,
        "f1": 0.25630930749040987,
        "hf_subset": "ast-eng",
        "languages": [
          "ast-Latn",
          "eng-Latn"
        ],
        "main_score": 0.25630930749040987,
        "precision": 0.2403646912556983,
        "recall": 0.2992125984251969
      },
      {
        "accuracy": 0.111,
        "f1": 0.08281571017153816,
        "hf_subset": "eus-eng",
        "languages": [
          "eus-Latn",
          "eng-Latn"
        ],
        "main_score": 0.08281571017153816,
        "precision": 0.07619545741498122,
        "recall": 0.111
      },
      {
        "accuracy": 0.242,
        "f1": 0.20370676927486897,
        "hf_subset": "cbk-eng",
        "languages": [
          "cbk-Latn",
          "eng-Latn"
        ],
        "main_score": 0.20370676927486897,
        "precision": 0.19313889110529556,
        "recall": 0.242
      },
      {
        "accuracy": 0.035,
        "f1": 0.019180785679957665,
        "hf_subset": "wuu-eng",
        "languages": [
          "wuu-Hans",
          "eng-Latn"
        ],
        "main_score": 0.019180785679957665,
        "precision": 0.016772609827630115,
        "recall": 0.035
      },
      {
        "accuracy": 0.013,
        "f1": 0.006570476790968594,
        "hf_subset": "ell-eng",
        "languages": [
          "ell-Grek",
          "eng-Latn"
        ],
        "main_score": 0.006570476790968594,
        "precision": 0.0060083177659284405,
        "recall": 0.013
      },
      {
        "accuracy": 0.04225352112676056,
        "f1": 0.03034297740465783,
        "hf_subset": "xho-eng",
        "languages": [
          "xho-Latn",
          "eng-Latn"
        ],
        "main_score": 0.03034297740465783,
        "precision": 0.029424609673545148,
        "recall": 0.04225352112676056
      },
      {
        "accuracy": 0.069,
        "f1": 0.054883655233655224,
        "hf_subset": "kzj-eng",
        "languages": [
          "kzj-Latn",
          "eng-Latn"
        ],
        "main_score": 0.054883655233655224,
        "precision": 0.05191509074410163,
        "recall": 0.069
      },
      {
        "accuracy": 0.119,
        "f1": 0.09743091010192921,
        "hf_subset": "hrv-eng",
        "languages": [
          "hrv-Latn",
          "eng-Latn"
        ],
        "main_score": 0.09743091010192921,
        "precision": 0.09179587182390686,
        "recall": 0.119
      },
      {
        "accuracy": 0.046,
        "f1": 0.033623455812480205,
        "hf_subset": "dtp-eng",
        "languages": [
          "dtp-Latn",
          "eng-Latn"
        ],
        "main_score": 0.033623455812480205,
        "precision": 0.03141740876500626,
        "recall": 0.046
      },
      {
        "accuracy": 0.006738544474393531,
        "f1": 0.004722765290422591,
        "hf_subset": "hye-eng",
        "languages": [
          "hye-Armn",
          "eng-Latn"
        ],
        "main_score": 0.004722765290422591,
        "precision": 0.004495261281627685,
        "recall": 0.006738544474393531
      },
      {
        "accuracy": 0.004,
        "f1": 0.0012719904209265913,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.0012719904209265913,
        "precision": 0.0011461293743372219,
        "recall": 0.004
      },
      {
        "accuracy": 0.07666666666666666,
        "f1": 0.061110780423280414,
        "hf_subset": "ceb-eng",
        "languages": [
          "ceb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.061110780423280414,
        "precision": 0.057726683465665056,
        "recall": 0.07666666666666666
      },
      {
        "accuracy": 0.1384180790960452,
        "f1": 0.12342481103697454,
        "hf_subset": "bos-eng",
        "languages": [
          "bos-Latn",
          "eng-Latn"
        ],
        "main_score": 0.12342481103697454,
        "precision": 0.12023593167660963,
        "recall": 0.1384180790960452
      },
      {
        "accuracy": 0.092,
        "f1": 0.07492359139706076,
        "hf_subset": "sqi-eng",
        "languages": [
          "sqi-Latn",
          "eng-Latn"
        ],
        "main_score": 0.07492359139706076,
        "precision": 0.07065025861281074,
        "recall": 0.092
      },
      {
        "accuracy": 0.083,
        "f1": 0.0645862041982785,
        "hf_subset": "tgl-eng",
        "languages": [
          "tgl-Latn",
          "eng-Latn"
        ],
        "main_score": 0.0645862041982785,
        "precision": 0.06002903327666485,
        "recall": 0.083
      },
      {
        "accuracy": 0.291044776119403,
        "f1": 0.2241767353707652,
        "hf_subset": "ang-eng",
        "languages": [
          "ang-Latn",
          "eng-Latn"
        ],
        "main_score": 0.2241767353707652,
        "precision": 0.20429470295580918,
        "recall": 0.291044776119403
      },
      {
        "accuracy": 0.005,
        "f1": 0.00270593471810089,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.00270593471810089,
        "precision": 0.002447420634920635,
        "recall": 0.005
      },
      {
        "accuracy": 0.078,
        "f1": 0.061498140601218415,
        "hf_subset": "pam-eng",
        "languages": [
          "pam-Latn",
          "eng-Latn"
        ],
        "main_score": 0.061498140601218415,
        "precision": 0.05785964005130672,
        "recall": 0.078
      },
      {
        "accuracy": 0.346,
        "f1": 0.29226887905080357,
        "hf_subset": "ile-eng",
        "languages": [
          "ile-Latn",
          "eng-Latn"
        ],
        "main_score": 0.29226887905080357,
        "precision": 0.27653422336106154,
        "recall": 0.346
      },
      {
        "accuracy": 0.013172338090010977,
        "f1": 0.008866070481448303,
        "hf_subset": "arq-eng",
        "languages": [
          "arq-Arab",
          "eng-Latn"
        ],
        "main_score": 0.008866070481448303,
        "precision": 0.008387564685588834,
        "recall": 0.013172338090010977
      },
      {
        "accuracy": 0.037,
        "f1": 0.026743290043290045,
        "hf_subset": "bel-eng",
        "languages": [
          "bel-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.026743290043290045,
        "precision": 0.02487129329004329,
        "recall": 0.037
      },
      {
        "accuracy": 0.17857142857142858,
        "f1": 0.12102688977688977,
        "hf_subset": "swg-eng",
        "languages": [
          "swg-Latn",
          "eng-Latn"
        ],
        "main_score": 0.12102688977688977,
        "precision": 0.10852040816326529,
        "recall": 0.17857142857142858
      },
      {
        "accuracy": 0.005361930294906166,
        "f1": 0.0025947912677135193,
        "hf_subset": "kat-eng",
        "languages": [
          "kat-Geor",
          "eng-Latn"
        ],
        "main_score": 0.0025947912677135193,
        "precision": 0.002090514489978297,
        "recall": 0.005361930294906166
      },
      {
        "accuracy": 0.17,
        "f1": 0.13867500081295855,
        "hf_subset": "swe-eng",
        "languages": [
          "swe-Latn",
          "eng-Latn"
        ],
        "main_score": 0.13867500081295855,
        "precision": 0.13027776459721782,
        "recall": 0.17
      },
      {
        "accuracy": 0.08695652173913043,
        "f1": 0.06969009826152683,
        "hf_subset": "hsb-eng",
        "languages": [
          "hsb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.06969009826152683,
        "precision": 0.06426846100759144,
        "recall": 0.08695652173913043
      },
      {
        "accuracy": 0.013,
        "f1": 0.004605472683520128,
        "hf_subset": "mhr-eng",
        "languages": [
          "mhr-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.004605472683520128,
        "precision": 0.0040070391138487494,
        "recall": 0.013
      },
      {
        "accuracy": 0.039,
        "f1": 0.025779066054728374,
        "hf_subset": "rus-eng",
        "languages": [
          "rus-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.025779066054728374,
        "precision": 0.023684223106658433,
        "recall": 0.039
      },
      {
        "accuracy": 0.063,
        "f1": 0.048208010898368514,
        "hf_subset": "aze-eng",
        "languages": [
          "aze-Latn",
          "eng-Latn"
        ],
        "main_score": 0.048208010898368514,
        "precision": 0.045114042864252836,
        "recall": 0.063
      },
      {
        "accuracy": 0.093,
        "f1": 0.07647243544183843,
        "hf_subset": "pol-eng",
        "languages": [
          "pol-Latn",
          "eng-Latn"
        ],
        "main_score": 0.07647243544183843,
        "precision": 0.07199589057763296,
        "recall": 0.093
      },
      {
        "accuracy": 0.06896551724137931,
        "f1": 0.05002052545155993,
        "hf_subset": "tuk-eng",
        "languages": [
          "tuk-Latn",
          "eng-Latn"
        ],
        "main_score": 0.05002052545155993,
        "precision": 0.04578822204142938,
        "recall": 0.06896551724137931
      },
      {
        "accuracy": 0.043,
        "f1": 0.0348416067761807,
        "hf_subset": "lit-eng",
        "languages": [
          "lit-Latn",
          "eng-Latn"
        ],
        "main_score": 0.0348416067761807,
        "precision": 0.03309644215944006,
        "recall": 0.043
      },
      {
        "accuracy": 0.014150943396226415,
        "f1": 0.004868078781122259,
        "hf_subset": "yid-eng",
        "languages": [
          "yid-Hebr",
          "eng-Latn"
        ],
        "main_score": 0.004868078781122259,
        "precision": 0.003891219683637744,
        "recall": 0.014150943396226415
      },
      {
        "accuracy": 0.121,
        "f1": 0.09906899149183038,
        "hf_subset": "zsm-eng",
        "languages": [
          "zsm-Latn",
          "eng-Latn"
        ],
        "main_score": 0.09906899149183038,
        "precision": 0.0946164725087721,
        "recall": 0.121
      },
      {
        "accuracy": 0.06829268292682927,
        "f1": 0.05964523281596452,
        "hf_subset": "jav-eng",
        "languages": [
          "jav-Latn",
          "eng-Latn"
        ],
        "main_score": 0.05964523281596452,
        "precision": 0.0591378332387975,
        "recall": 0.06829268292682927
      },
      {
        "accuracy": 0.053738317757009345,
        "f1": 0.039366798926186566,
        "hf_subset": "uzb-eng",
        "languages": [
          "uzb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.039366798926186566,
        "precision": 0.036725211171841764,
        "recall": 0.053738317757009345
      },
      {
        "accuracy": 0.0782608695652174,
        "f1": 0.06761185904509674,
        "hf_subset": "cym-eng",
        "languages": [
          "cym-Latn",
          "eng-Latn"
        ],
        "main_score": 0.06761185904509674,
        "precision": 0.06421177166518782,
        "recall": 0.0782608695652174
      },
      {
        "accuracy": 0.065,
        "f1": 0.05120054007261012,
        "hf_subset": "bre-eng",
        "languages": [
          "bre-Latn",
          "eng-Latn"
        ],
        "main_score": 0.05120054007261012,
        "precision": 0.04806491332741332,
        "recall": 0.065
      },
      {
        "accuracy": 0.063,
        "f1": 0.0543006998776522,
        "hf_subset": "ber-eng",
        "languages": [
          "ber-Tfng",
          "eng-Latn"
        ],
        "main_score": 0.0543006998776522,
        "precision": 0.053196745409461225,
        "recall": 0.063
      },
      {
        "accuracy": 0.06,
        "f1": 0.051482230392156866,
        "hf_subset": "fin-eng",
        "languages": [
          "fin-Latn",
          "eng-Latn"
        ],
        "main_score": 0.051482230392156866,
        "precision": 0.04861170634920634,
        "recall": 0.06
      },
      {
        "accuracy": 0.007,
        "f1": 0.005022496652283887,
        "hf_subset": "uig-eng",
        "languages": [
          "uig-Arab",
          "eng-Latn"
        ],
        "main_score": 0.005022496652283887,
        "precision": 0.005011315757794631,
        "recall": 0.007
      },
      {
        "accuracy": 0.221,
        "f1": 0.1857201576661691,
        "hf_subset": "nds-eng",
        "languages": [
          "nds-Latn",
          "eng-Latn"
        ],
        "main_score": 0.1857201576661691,
        "precision": 0.17577585192192577,
        "recall": 0.221
      },
      {
        "accuracy": 0.202,
        "f1": 0.16577276416128264,
        "hf_subset": "dan-eng",
        "languages": [
          "dan-Latn",
          "eng-Latn"
        ],
        "main_score": 0.16577276416128264,
        "precision": 0.15636189756294874,
        "recall": 0.202
      },
      {
        "accuracy": 0.088,
        "f1": 0.07187864442320964,
        "hf_subset": "slk-eng",
        "languages": [
          "slk-Latn",
          "eng-Latn"
        ],
        "main_score": 0.07187864442320964,
        "precision": 0.06792831576635924,
        "recall": 0.088
      },
      {
        "accuracy": 0.013,
        "f1": 0.009139953379953379,
        "hf_subset": "heb-eng",
        "languages": [
          "heb-Hebr",
          "eng-Latn"
        ],
        "main_score": 0.009139953379953379,
        "precision": 0.008572032032032032,
        "recall": 0.013
      },
      {
        "accuracy": 0.028,
        "f1": 0.017181468987611047,
        "hf_subset": "jpn-eng",
        "languages": [
          "jpn-Jpan",
          "eng-Latn"
        ],
        "main_score": 0.017181468987611047,
        "precision": 0.015502900998157376,
        "recall": 0.028
      },
      {
        "accuracy": 0.012,
        "f1": 0.0066492478644140625,
        "hf_subset": "mkd-eng",
        "languages": [
          "mkd-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.0066492478644140625,
        "precision": 0.006170033670033669,
        "recall": 0.012
      },
      {
        "accuracy": 0.005988023952095809,
        "f1": 0.0021304960035745885,
        "hf_subset": "orv-eng",
        "languages": [
          "orv-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.0021304960035745885,
        "precision": 0.0017854163452571566,
        "recall": 0.005988023952095809
      },
      {
        "accuracy": 0.11538461538461539,
        "f1": 0.08512962191045373,
        "hf_subset": "swh-eng",
        "languages": [
          "swh-Latn",
          "eng-Latn"
        ],
        "main_score": 0.08512962191045373,
        "precision": 0.08000401530836314,
        "recall": 0.11538461538461539
      },
      {
        "accuracy": 0.072,
        "f1": 0.054935704205797666,
        "hf_subset": "tur-eng",
        "languages": [
          "tur-Latn",
          "eng-Latn"
        ],
        "main_score": 0.054935704205797666,
        "precision": 0.050765474102679294,
        "recall": 0.072
      },
      {
        "accuracy": 0.21,
        "f1": 0.16558266599785584,
        "hf_subset": "epo-eng",
        "languages": [
          "epo-Latn",
          "eng-Latn"
        ],
        "main_score": 0.16558266599785584,
        "precision": 0.15281593740506957,
        "recall": 0.21
      },
      {
        "accuracy": 0.086,
        "f1": 0.06961738437001594,
        "hf_subset": "hun-eng",
        "languages": [
          "hun-Latn",
          "eng-Latn"
        ],
        "main_score": 0.06961738437001594,
        "precision": 0.06696348923980502,
        "recall": 0.086
      },
      {
        "accuracy": 0.038600723763570564,
        "f1": 0.029876946173689238,
        "hf_subset": "gla-eng",
        "languages": [
          "gla-Latn",
          "eng-Latn"
        ],
        "main_score": 0.029876946173689238,
        "precision": 0.02815783604526181,
        "recall": 0.038600723763570564
      },
      {
        "accuracy": 0.063,
        "f1": 0.048606776552518244,
        "hf_subset": "srp-eng",
        "languages": [
          "srp-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.048606776552518244,
        "precision": 0.04604256082344306,
        "recall": 0.063
      },
      {
        "accuracy": 0.008658008658008658,
        "f1": 0.0002679859822716966,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.0002679859822716966,
        "precision": 0.0001361121479028627,
        "recall": 0.008658008658008658
      },
      {
        "accuracy": 0.017094017094017096,
        "f1": 0.0035542598850092384,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.0035542598850092384,
        "precision": 0.002499992884608269,
        "recall": 0.017094017094017096
      },
      {
        "accuracy": 0.23357664233576642,
        "f1": 0.1853609083536091,
        "hf_subset": "cha-eng",
        "languages": [
          "cha-Latn",
          "eng-Latn"
        ],
        "main_score": 0.1853609083536091,
        "precision": 0.1701663633823359,
        "recall": 0.23357664233576642
      },
      {
        "accuracy": 0.09,
        "f1": 0.07467102206736353,
        "hf_subset": "war-eng",
        "languages": [
          "war-Latn",
          "eng-Latn"
        ],
        "main_score": 0.07467102206736353,
        "precision": 0.07035216517825947,
        "recall": 0.09
      },
      {
        "accuracy": 0.058,
        "f1": 0.046757103613266696,
        "hf_subset": "est-eng",
        "languages": [
          "est-Latn",
          "eng-Latn"
        ],
        "main_score": 0.046757103613266696,
        "precision": 0.043995093268319165,
        "recall": 0.058
      },
      {
        "accuracy": 0.264,
        "f1": 0.2342288988758875,
        "hf_subset": "cat-eng",
        "languages": [
          "cat-Latn",
          "eng-Latn"
        ],
        "main_score": 0.2342288988758875,
        "precision": 0.22519143748995474,
        "recall": 0.264
      },
      {
        "accuracy": 0.028,
        "f1": 0.014005165801733423,
        "hf_subset": "ukr-eng",
        "languages": [
          "ukr-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.014005165801733423,
        "precision": 0.01248515884021102,
        "recall": 0.028
      },
      {
        "accuracy": 0.016,
        "f1": 0.010275301931604109,
        "hf_subset": "tat-eng",
        "languages": [
          "tat-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.010275301931604109,
        "precision": 0.009858169158091273,
        "recall": 0.016
      },
      {
        "accuracy": 0.37,
        "f1": 0.3361271111717116,
        "hf_subset": "deu-eng",
        "languages": [
          "deu-Latn",
          "eng-Latn"
        ],
        "main_score": 0.3361271111717116,
        "precision": 0.32410217560217564,
        "recall": 0.37
      },
      {
        "accuracy": 0.249,
        "f1": 0.21165321826052372,
        "hf_subset": "nld-eng",
        "languages": [
          "nld-Latn",
          "eng-Latn"
        ],
        "main_score": 0.21165321826052372,
        "precision": 0.20187692155142092,
        "recall": 0.249
      },
      {
        "accuracy": 0.474,
        "f1": 0.4166232442394207,
        "hf_subset": "ina-eng",
        "languages": [
          "ina-Latn",
          "eng-Latn"
        ],
        "main_score": 0.4166232442394207,
        "precision": 0.398036747647777,
        "recall": 0.474
      },
      {
        "accuracy": 0.079,
        "f1": 0.0636767327366665,
        "hf_subset": "vie-eng",
        "languages": [
          "vie-Latn",
          "eng-Latn"
        ],
        "main_score": 0.0636767327366665,
        "precision": 0.06001785328366724,
        "recall": 0.079
      }
    ]
  },
  "task_name": "Tatoeba"
}