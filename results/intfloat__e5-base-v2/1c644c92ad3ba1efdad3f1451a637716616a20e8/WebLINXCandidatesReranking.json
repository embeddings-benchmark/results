{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 4248.140825510025,
  "kg_co2_emissions": 0.25991811875949344,
  "mteb_version": "1.18.0",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.052162029605849834,
        "map": 0.06983641073059821,
        "mrr": 0.052162029605849834,
        "nAUC_map_diff1": 0.032071146591299254,
        "nAUC_map_max": -0.034281189725270986,
        "nAUC_map_std": 0.29135712737059716,
        "nAUC_mrr_diff1": 0.03008408279204459,
        "nAUC_mrr_max": -0.04204260476299199,
        "nAUC_mrr_std": 0.2673270198213723
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07731597181861624,
        "map": 0.09604634811580037,
        "mrr": 0.07731597181861624,
        "nAUC_map_diff1": 0.06233106514411541,
        "nAUC_map_max": -0.19339800433951884,
        "nAUC_map_std": 0.04662895864558295,
        "nAUC_mrr_diff1": 0.05568990069028682,
        "nAUC_mrr_max": -0.19488474415864238,
        "nAUC_mrr_std": 0.04544136878825737
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.05684868755105194,
        "map": 0.07468561697384823,
        "mrr": 0.05684868755105194,
        "nAUC_map_diff1": 0.11833835356272794,
        "nAUC_map_max": -0.03425929774312102,
        "nAUC_map_std": 0.218959552436612,
        "nAUC_mrr_diff1": 0.11920892006639264,
        "nAUC_mrr_max": -0.017395630646190405,
        "nAUC_mrr_std": 0.20447237330951173
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07515676775303048,
        "map": 0.09332677231191437,
        "mrr": 0.07515676775303048,
        "nAUC_map_diff1": 0.09048671039877056,
        "nAUC_map_max": -0.09927265673278303,
        "nAUC_map_std": 0.09517604668816446,
        "nAUC_mrr_diff1": 0.09388551420178866,
        "nAUC_mrr_max": -0.10113858888561653,
        "nAUC_mrr_std": 0.08019463283406263
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.047546472999717274,
        "map": 0.06664751180374086,
        "mrr": 0.047546472999717274,
        "nAUC_map_diff1": 0.16719645322647073,
        "nAUC_map_max": -0.06250253507826725,
        "nAUC_map_std": 0.19871870712820605,
        "nAUC_mrr_diff1": 0.18623241457814024,
        "nAUC_mrr_max": -0.04766245746389028,
        "nAUC_mrr_std": 0.18698227988931904
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07710125300440443,
        "map": 0.09463369770099418,
        "mrr": 0.07710125300440443,
        "nAUC_map_diff1": 0.1459996614414972,
        "nAUC_map_max": -0.12174914920681583,
        "nAUC_map_std": 0.1500714574946478,
        "nAUC_mrr_diff1": 0.1445205780276923,
        "nAUC_mrr_max": -0.1187913682501668,
        "nAUC_mrr_std": 0.1515223584915788
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}