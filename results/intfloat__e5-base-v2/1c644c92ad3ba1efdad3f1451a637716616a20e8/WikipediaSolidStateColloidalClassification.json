{
  "dataset_revision": "7d8df44e588b6143d4856c781f72f919fa0599a7",
  "task_name": "WikipediaSolidStateColloidalClassification",
  "mteb_version": "1.26.4",
  "scores": {
    "test": [
      {
        "accuracy": 0.792568,
        "f1": 0.790218,
        "f1_weighted": 0.791126,
        "ap": 0.774293,
        "ap_weighted": 0.774293,
        "scores_per_experiment": [
          {
            "accuracy": 0.745495,
            "f1": 0.744551,
            "f1_weighted": 0.742941,
            "ap": 0.757727,
            "ap_weighted": 0.757727
          },
          {
            "accuracy": 0.662162,
            "f1": 0.659669,
            "f1_weighted": 0.656652,
            "ap": 0.678877,
            "ap_weighted": 0.678877
          },
          {
            "accuracy": 0.808559,
            "f1": 0.807362,
            "f1_weighted": 0.808935,
            "ap": 0.785049,
            "ap_weighted": 0.785049
          },
          {
            "accuracy": 0.815315,
            "f1": 0.812747,
            "f1_weighted": 0.815019,
            "ap": 0.782374,
            "ap_weighted": 0.782374
          },
          {
            "accuracy": 0.837838,
            "f1": 0.836765,
            "f1_weighted": 0.838136,
            "ap": 0.815156,
            "ap_weighted": 0.815156
          },
          {
            "accuracy": 0.871622,
            "f1": 0.867641,
            "f1_weighted": 0.870019,
            "ap": 0.825257,
            "ap_weighted": 0.825257
          },
          {
            "accuracy": 0.817568,
            "f1": 0.811911,
            "f1_weighted": 0.81529,
            "ap": 0.772071,
            "ap_weighted": 0.772071
          },
          {
            "accuracy": 0.806306,
            "f1": 0.805739,
            "f1_weighted": 0.806827,
            "ap": 0.789344,
            "ap_weighted": 0.789344
          },
          {
            "accuracy": 0.765766,
            "f1": 0.76496,
            "f1_weighted": 0.763534,
            "ap": 0.779748,
            "ap_weighted": 0.779748
          },
          {
            "accuracy": 0.795045,
            "f1": 0.790834,
            "f1_weighted": 0.793909,
            "ap": 0.757332,
            "ap_weighted": 0.757332
          }
        ],
        "main_score": 0.792568,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 3.4342191219329834,
  "kg_co2_emissions": null
}