{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 17.835263967514038,
  "kg_co2_emissions": 0.0006079478510717312,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.3041015625,
        "f1": 0.29699995589806844,
        "f1_weighted": 0.29699729742254605,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.3041015625,
        "scores_per_experiment": [
          {
            "accuracy": 0.2958984375,
            "f1": 0.29724142946523885,
            "f1_weighted": 0.29721513425307333
          },
          {
            "accuracy": 0.2939453125,
            "f1": 0.2770440329914478,
            "f1_weighted": 0.27701784873306207
          },
          {
            "accuracy": 0.326171875,
            "f1": 0.3178802208542304,
            "f1_weighted": 0.3178692059148286
          },
          {
            "accuracy": 0.31396484375,
            "f1": 0.30913217189217146,
            "f1_weighted": 0.3091390967250107
          },
          {
            "accuracy": 0.29296875,
            "f1": 0.2910102553722854,
            "f1_weighted": 0.2910034717102903
          },
          {
            "accuracy": 0.29736328125,
            "f1": 0.2834641958905331,
            "f1_weighted": 0.28345507401913744
          },
          {
            "accuracy": 0.25830078125,
            "f1": 0.2581651396697069,
            "f1_weighted": 0.25815086814953025
          },
          {
            "accuracy": 0.34130859375,
            "f1": 0.33295866170937255,
            "f1_weighted": 0.3329836726690707
          },
          {
            "accuracy": 0.322265625,
            "f1": 0.3176652215352774,
            "f1_weighted": 0.3176696247357731
          },
          {
            "accuracy": 0.298828125,
            "f1": 0.2854382296004201,
            "f1_weighted": 0.2854689773156842
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.300732421875,
        "f1": 0.2934546565200823,
        "f1_weighted": 0.29343805161539055,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.300732421875,
        "scores_per_experiment": [
          {
            "accuracy": 0.30712890625,
            "f1": 0.3071474061510388,
            "f1_weighted": 0.3071087556644657
          },
          {
            "accuracy": 0.2978515625,
            "f1": 0.2799476227778472,
            "f1_weighted": 0.2799222329279251
          },
          {
            "accuracy": 0.306640625,
            "f1": 0.29696302534971836,
            "f1_weighted": 0.2969365813517757
          },
          {
            "accuracy": 0.32080078125,
            "f1": 0.31796104283247884,
            "f1_weighted": 0.3179529971701383
          },
          {
            "accuracy": 0.27685546875,
            "f1": 0.2740407427270338,
            "f1_weighted": 0.27402385823012043
          },
          {
            "accuracy": 0.29833984375,
            "f1": 0.2887152857334149,
            "f1_weighted": 0.2886965511340549
          },
          {
            "accuracy": 0.23583984375,
            "f1": 0.2347224603502675,
            "f1_weighted": 0.2346873125162991
          },
          {
            "accuracy": 0.3291015625,
            "f1": 0.3204573362915237,
            "f1_weighted": 0.32047520215038117
          },
          {
            "accuracy": 0.3271484375,
            "f1": 0.3227246207962323,
            "f1_weighted": 0.3227119990225472
          },
          {
            "accuracy": 0.3076171875,
            "f1": 0.2918670221912673,
            "f1_weighted": 0.29186502598619785
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}