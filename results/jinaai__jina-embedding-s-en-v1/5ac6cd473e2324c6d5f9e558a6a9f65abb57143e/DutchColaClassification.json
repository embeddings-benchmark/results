{
  "dataset_revision": "2269ed7d95d8abaab829f1592b4b2047372e9f81",
  "task_name": "DutchColaClassification",
  "mteb_version": "2.1.17",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.532083,
            "f1": 0.531904,
            "f1_weighted": 0.531904,
            "precision": 0.532133,
            "precision_weighted": 0.532133,
            "recall": 0.532083,
            "recall_weighted": 0.532083,
            "ap": 0.517032,
            "ap_weighted": 0.517032
          },
          {
            "accuracy": 0.55,
            "f1": 0.549638,
            "f1_weighted": 0.549638,
            "precision": 0.550161,
            "precision_weighted": 0.550161,
            "recall": 0.55,
            "recall_weighted": 0.55,
            "ap": 0.52765,
            "ap_weighted": 0.52765
          },
          {
            "accuracy": 0.519583,
            "f1": 0.519209,
            "f1_weighted": 0.519209,
            "precision": 0.519645,
            "precision_weighted": 0.519645,
            "recall": 0.519583,
            "recall_weighted": 0.519583,
            "ap": 0.510155,
            "ap_weighted": 0.510155
          },
          {
            "accuracy": 0.553333,
            "f1": 0.551438,
            "f1_weighted": 0.551438,
            "precision": 0.55425,
            "precision_weighted": 0.55425,
            "recall": 0.553333,
            "recall_weighted": 0.553333,
            "ap": 0.529936,
            "ap_weighted": 0.529936
          },
          {
            "accuracy": 0.537917,
            "f1": 0.537336,
            "f1_weighted": 0.537336,
            "precision": 0.538108,
            "precision_weighted": 0.538108,
            "recall": 0.537917,
            "recall_weighted": 0.537917,
            "ap": 0.520506,
            "ap_weighted": 0.520506
          },
          {
            "accuracy": 0.525417,
            "f1": 0.524763,
            "f1_weighted": 0.524763,
            "precision": 0.525557,
            "precision_weighted": 0.525557,
            "recall": 0.525417,
            "recall_weighted": 0.525417,
            "ap": 0.513406,
            "ap_weighted": 0.513406
          },
          {
            "accuracy": 0.54375,
            "f1": 0.54217,
            "f1_weighted": 0.54217,
            "precision": 0.544362,
            "precision_weighted": 0.544362,
            "recall": 0.54375,
            "recall_weighted": 0.54375,
            "ap": 0.524044,
            "ap_weighted": 0.524044
          },
          {
            "accuracy": 0.513333,
            "f1": 0.512942,
            "f1_weighted": 0.512942,
            "precision": 0.513376,
            "precision_weighted": 0.513376,
            "recall": 0.513333,
            "recall_weighted": 0.513333,
            "ap": 0.506855,
            "ap_weighted": 0.506855
          },
          {
            "accuracy": 0.50875,
            "f1": 0.506417,
            "f1_weighted": 0.506417,
            "precision": 0.508919,
            "precision_weighted": 0.508919,
            "recall": 0.50875,
            "recall_weighted": 0.50875,
            "ap": 0.504464,
            "ap_weighted": 0.504464
          },
          {
            "accuracy": 0.541667,
            "f1": 0.541182,
            "f1_weighted": 0.541182,
            "precision": 0.541843,
            "precision_weighted": 0.541843,
            "recall": 0.541667,
            "recall_weighted": 0.541667,
            "ap": 0.52269,
            "ap_weighted": 0.52269
          }
        ],
        "accuracy": 0.532583,
        "f1": 0.5317,
        "f1_weighted": 0.5317,
        "precision": 0.532835,
        "precision_weighted": 0.532835,
        "recall": 0.532583,
        "recall_weighted": 0.532583,
        "ap": 0.517674,
        "ap_weighted": 0.517674,
        "main_score": 0.5317,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 16.665196418762207,
  "kg_co2_emissions": null
}