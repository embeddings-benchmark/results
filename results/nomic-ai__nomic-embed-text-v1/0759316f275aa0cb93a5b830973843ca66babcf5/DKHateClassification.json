{
  "dataset_revision": "59d12749a3c91a186063c7d729ec392fda94681c",
  "task_name": "DKHateClassification",
  "mteb_version": "1.34.21",
  "scores": {
    "test": [
      {
        "accuracy": 0.585106,
        "f1": 0.478942,
        "f1_weighted": 0.652595,
        "ap": 0.890748,
        "ap_weighted": 0.890748,
        "scores_per_experiment": [
          {
            "accuracy": 0.601824,
            "f1": 0.500805,
            "f1_weighted": 0.669397,
            "ap": 0.899386,
            "ap_weighted": 0.899386
          },
          {
            "accuracy": 0.504559,
            "f1": 0.449556,
            "f1_weighted": 0.580189,
            "ap": 0.901468,
            "ap_weighted": 0.901468
          },
          {
            "accuracy": 0.632219,
            "f1": 0.525547,
            "f1_weighted": 0.694444,
            "ap": 0.906015,
            "ap_weighted": 0.906015
          },
          {
            "accuracy": 0.547112,
            "f1": 0.460222,
            "f1_weighted": 0.622812,
            "ap": 0.889655,
            "ap_weighted": 0.889655
          },
          {
            "accuracy": 0.699088,
            "f1": 0.533534,
            "f1_weighted": 0.742166,
            "ap": 0.892652,
            "ap_weighted": 0.892652
          },
          {
            "accuracy": 0.522796,
            "f1": 0.424447,
            "f1_weighted": 0.603067,
            "ap": 0.872533,
            "ap_weighted": 0.872533
          },
          {
            "accuracy": 0.504559,
            "f1": 0.422527,
            "f1_weighted": 0.58593,
            "ap": 0.877096,
            "ap_weighted": 0.877096
          },
          {
            "accuracy": 0.528875,
            "f1": 0.444844,
            "f1_weighted": 0.606999,
            "ap": 0.884876,
            "ap_weighted": 0.884876
          },
          {
            "accuracy": 0.68693,
            "f1": 0.548877,
            "f1_weighted": 0.736235,
            "ap": 0.903301,
            "ap_weighted": 0.903301
          },
          {
            "accuracy": 0.6231,
            "f1": 0.479058,
            "f1_weighted": 0.684714,
            "ap": 0.880497,
            "ap_weighted": 0.880497
          }
        ],
        "main_score": 0.585106,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 34.74356150627136,
  "kg_co2_emissions": null
}