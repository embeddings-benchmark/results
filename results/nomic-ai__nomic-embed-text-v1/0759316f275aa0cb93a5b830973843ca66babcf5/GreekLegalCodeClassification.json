{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "task_name": "GreekLegalCodeClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.034424,
        "f1": 0.020634,
        "f1_weighted": 0.024198,
        "scores_per_experiment": [
          {
            "accuracy": 0.028809,
            "f1": 0.014366,
            "f1_weighted": 0.017206
          },
          {
            "accuracy": 0.033691,
            "f1": 0.019678,
            "f1_weighted": 0.02429
          },
          {
            "accuracy": 0.03418,
            "f1": 0.022469,
            "f1_weighted": 0.025408
          },
          {
            "accuracy": 0.033203,
            "f1": 0.025175,
            "f1_weighted": 0.025351
          },
          {
            "accuracy": 0.03418,
            "f1": 0.019436,
            "f1_weighted": 0.025488
          },
          {
            "accuracy": 0.03418,
            "f1": 0.017039,
            "f1_weighted": 0.026084
          },
          {
            "accuracy": 0.04248,
            "f1": 0.026104,
            "f1_weighted": 0.030714
          },
          {
            "accuracy": 0.029785,
            "f1": 0.017512,
            "f1_weighted": 0.017066
          },
          {
            "accuracy": 0.040527,
            "f1": 0.020408,
            "f1_weighted": 0.026579
          },
          {
            "accuracy": 0.033203,
            "f1": 0.024155,
            "f1_weighted": 0.023795
          }
        ],
        "main_score": 0.034424,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.034473,
        "f1": 0.021661,
        "f1_weighted": 0.021673,
        "scores_per_experiment": [
          {
            "accuracy": 0.033203,
            "f1": 0.023504,
            "f1_weighted": 0.017238
          },
          {
            "accuracy": 0.039551,
            "f1": 0.025513,
            "f1_weighted": 0.027719
          },
          {
            "accuracy": 0.037598,
            "f1": 0.026223,
            "f1_weighted": 0.025173
          },
          {
            "accuracy": 0.035645,
            "f1": 0.020352,
            "f1_weighted": 0.026164
          },
          {
            "accuracy": 0.035156,
            "f1": 0.026424,
            "f1_weighted": 0.024139
          },
          {
            "accuracy": 0.03125,
            "f1": 0.016338,
            "f1_weighted": 0.017419
          },
          {
            "accuracy": 0.033691,
            "f1": 0.014922,
            "f1_weighted": 0.020734
          },
          {
            "accuracy": 0.032227,
            "f1": 0.022386,
            "f1_weighted": 0.01992
          },
          {
            "accuracy": 0.031738,
            "f1": 0.019966,
            "f1_weighted": 0.017644
          },
          {
            "accuracy": 0.034668,
            "f1": 0.020978,
            "f1_weighted": 0.020583
          }
        ],
        "main_score": 0.034473,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ]
  },
  "evaluation_time": 1755.2935070991516,
  "kg_co2_emissions": 0.15456274293352773
}