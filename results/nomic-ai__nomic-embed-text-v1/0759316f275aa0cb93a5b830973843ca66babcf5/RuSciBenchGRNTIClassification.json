{
  "dataset_revision": "673a610d6d3dd91a547a0d57ae1b56f37ebbf6a1",
  "task_name": "RuSciBenchGRNTIClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.192871,
        "f1": 0.179447,
        "f1_weighted": 0.179537,
        "scores_per_experiment": [
          {
            "accuracy": 0.20459,
            "f1": 0.189423,
            "f1_weighted": 0.189538
          },
          {
            "accuracy": 0.185059,
            "f1": 0.175863,
            "f1_weighted": 0.175893
          },
          {
            "accuracy": 0.185547,
            "f1": 0.168727,
            "f1_weighted": 0.168852
          },
          {
            "accuracy": 0.1875,
            "f1": 0.1762,
            "f1_weighted": 0.176298
          },
          {
            "accuracy": 0.197754,
            "f1": 0.183913,
            "f1_weighted": 0.183909
          },
          {
            "accuracy": 0.181641,
            "f1": 0.163183,
            "f1_weighted": 0.163365
          },
          {
            "accuracy": 0.20459,
            "f1": 0.196058,
            "f1_weighted": 0.19625
          },
          {
            "accuracy": 0.197754,
            "f1": 0.181039,
            "f1_weighted": 0.181091
          },
          {
            "accuracy": 0.18457,
            "f1": 0.171174,
            "f1_weighted": 0.171259
          },
          {
            "accuracy": 0.199707,
            "f1": 0.188884,
            "f1_weighted": 0.188915
          }
        ],
        "main_score": 0.192871,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 46.1879997253418,
  "kg_co2_emissions": 0.0036098538726612958
}