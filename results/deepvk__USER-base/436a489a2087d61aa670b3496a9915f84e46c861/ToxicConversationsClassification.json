{
  "dataset_revision": "edfaf9da55d3dd50d43143d90c1ac476895ae6de",
  "task_name": "ToxicConversationsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.624023,
        "f1": 0.467801,
        "f1_weighted": 0.70606,
        "ap": 0.096465,
        "ap_weighted": 0.096465,
        "scores_per_experiment": [
          {
            "accuracy": 0.703613,
            "f1": 0.515077,
            "f1_weighted": 0.769608,
            "ap": 0.107217,
            "ap_weighted": 0.107217
          },
          {
            "accuracy": 0.688965,
            "f1": 0.496111,
            "f1_weighted": 0.758526,
            "ap": 0.095642,
            "ap_weighted": 0.095642
          },
          {
            "accuracy": 0.730469,
            "f1": 0.50985,
            "f1_weighted": 0.786667,
            "ap": 0.094178,
            "ap_weighted": 0.094178
          },
          {
            "accuracy": 0.647461,
            "f1": 0.483455,
            "f1_weighted": 0.728469,
            "ap": 0.099479,
            "ap_weighted": 0.099479
          },
          {
            "accuracy": 0.520996,
            "f1": 0.412228,
            "f1_weighted": 0.625073,
            "ap": 0.086815,
            "ap_weighted": 0.086815
          },
          {
            "accuracy": 0.506348,
            "f1": 0.409612,
            "f1_weighted": 0.610785,
            "ap": 0.091485,
            "ap_weighted": 0.091485
          },
          {
            "accuracy": 0.75293,
            "f1": 0.526647,
            "f1_weighted": 0.802149,
            "ap": 0.100468,
            "ap_weighted": 0.100468
          },
          {
            "accuracy": 0.512695,
            "f1": 0.413003,
            "f1_weighted": 0.61664,
            "ap": 0.091657,
            "ap_weighted": 0.091657
          },
          {
            "accuracy": 0.614258,
            "f1": 0.465663,
            "f1_weighted": 0.702864,
            "ap": 0.096308,
            "ap_weighted": 0.096308
          },
          {
            "accuracy": 0.5625,
            "f1": 0.446363,
            "f1_weighted": 0.659817,
            "ap": 0.101405,
            "ap_weighted": 0.101405
          }
        ],
        "main_score": 0.624023,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 9.750489950180054,
  "kg_co2_emissions": 0.00042476526318723954
}