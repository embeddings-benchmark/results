{
  "dataset_revision": "59d12749a3c91a186063c7d729ec392fda94681c",
  "task_name": "DKHateClassification",
  "mteb_version": "1.34.21",
  "scores": {
    "test": [
      {
        "accuracy": 0.564438,
        "f1": 0.456928,
        "f1_weighted": 0.635732,
        "ap": 0.88245,
        "ap_weighted": 0.88245,
        "scores_per_experiment": [
          {
            "accuracy": 0.568389,
            "f1": 0.481003,
            "f1_weighted": 0.640887,
            "ap": 0.897402,
            "ap_weighted": 0.897402
          },
          {
            "accuracy": 0.638298,
            "f1": 0.497839,
            "f1_weighted": 0.697226,
            "ap": 0.887142,
            "ap_weighted": 0.887142
          },
          {
            "accuracy": 0.604863,
            "f1": 0.47208,
            "f1_weighted": 0.670853,
            "ap": 0.880508,
            "ap_weighted": 0.880508
          },
          {
            "accuracy": 0.547112,
            "f1": 0.44343,
            "f1_weighted": 0.623779,
            "ap": 0.87784,
            "ap_weighted": 0.87784
          },
          {
            "accuracy": 0.613982,
            "f1": 0.473374,
            "f1_weighted": 0.677669,
            "ap": 0.879343,
            "ap_weighted": 0.879343
          },
          {
            "accuracy": 0.571429,
            "f1": 0.462698,
            "f1_weighted": 0.64416,
            "ap": 0.88326,
            "ap_weighted": 0.88326
          },
          {
            "accuracy": 0.416413,
            "f1": 0.370335,
            "f1_weighted": 0.498216,
            "ap": 0.872967,
            "ap_weighted": 0.872967
          },
          {
            "accuracy": 0.504559,
            "f1": 0.419401,
            "f1_weighted": 0.586338,
            "ap": 0.874808,
            "ap_weighted": 0.874808
          },
          {
            "accuracy": 0.574468,
            "f1": 0.464668,
            "f1_weighted": 0.646686,
            "ap": 0.88365,
            "ap_weighted": 0.88365
          },
          {
            "accuracy": 0.604863,
            "f1": 0.48445,
            "f1_weighted": 0.671507,
            "ap": 0.887574,
            "ap_weighted": 0.887574
          }
        ],
        "main_score": 0.564438,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 26.73285484313965,
  "kg_co2_emissions": null
}