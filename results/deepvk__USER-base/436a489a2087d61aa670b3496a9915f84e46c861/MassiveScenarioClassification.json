{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 12.991098165512085,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test": [
      {
        "accuracy": 0.683322125084062,
        "f1": 0.6789216595117911,
        "f1_weighted": 0.6789220857044962,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.683322125084062,
        "scores_per_experiment": [
          {
            "accuracy": 0.6983860121049092,
            "f1": 0.6968361395616279,
            "f1_weighted": 0.6929963003740454
          },
          {
            "accuracy": 0.6926698049764627,
            "f1": 0.688248480634289,
            "f1_weighted": 0.6876800769668283
          },
          {
            "accuracy": 0.6987222595830531,
            "f1": 0.6848560416757307,
            "f1_weighted": 0.6951191442932766
          },
          {
            "accuracy": 0.6926698049764627,
            "f1": 0.689563552303926,
            "f1_weighted": 0.6894947221162891
          },
          {
            "accuracy": 0.6802286482851378,
            "f1": 0.6735849206101423,
            "f1_weighted": 0.672357646723595
          },
          {
            "accuracy": 0.6553463349024883,
            "f1": 0.6498833588673948,
            "f1_weighted": 0.646846684218786
          },
          {
            "accuracy": 0.6775386684599866,
            "f1": 0.671397045034112,
            "f1_weighted": 0.6746504777597576
          },
          {
            "accuracy": 0.6731674512441157,
            "f1": 0.6745934047572872,
            "f1_weighted": 0.6708795063517968
          },
          {
            "accuracy": 0.691324815063887,
            "f1": 0.6927110898650731,
            "f1_weighted": 0.6907736149313112
          },
          {
            "accuracy": 0.6731674512441157,
            "f1": 0.6675425618083283,
            "f1_weighted": 0.6684226833092751
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.679390063944909,
        "f1": 0.6722476179254533,
        "f1_weighted": 0.6753394907636693,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.679390063944909,
        "scores_per_experiment": [
          {
            "accuracy": 0.7122479094933596,
            "f1": 0.7024048170048803,
            "f1_weighted": 0.7107818271296087
          },
          {
            "accuracy": 0.6896212493851451,
            "f1": 0.684277242929466,
            "f1_weighted": 0.686303230631246
          },
          {
            "accuracy": 0.6965076242006887,
            "f1": 0.6886425162146992,
            "f1_weighted": 0.695663445160082
          },
          {
            "accuracy": 0.6724053123462863,
            "f1": 0.6714164004634391,
            "f1_weighted": 0.6681062450471776
          },
          {
            "accuracy": 0.6748647319232661,
            "f1": 0.6664117338549709,
            "f1_weighted": 0.6640226694768981
          },
          {
            "accuracy": 0.6424003935071323,
            "f1": 0.6350949840373314,
            "f1_weighted": 0.634009695905311
          },
          {
            "accuracy": 0.6606000983767831,
            "f1": 0.6471564479083354,
            "f1_weighted": 0.657953708288241
          },
          {
            "accuracy": 0.6660108214461387,
            "f1": 0.6605051304087408,
            "f1_weighted": 0.6631458167556192
          },
          {
            "accuracy": 0.6950319724545008,
            "f1": 0.694118079813324,
            "f1_weighted": 0.692255566688595
          },
          {
            "accuracy": 0.6842105263157895,
            "f1": 0.6724488266193469,
            "f1_weighted": 0.6811527025539142
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}