{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 20.951470613479614,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test": [
      {
        "accuracy": 0.655682582380632,
        "f1": 0.6338961487774227,
        "f1_weighted": 0.6480597449616844,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.655682582380632,
        "scores_per_experiment": [
          {
            "accuracy": 0.6704774714189644,
            "f1": 0.647475414512512,
            "f1_weighted": 0.6659397649485388
          },
          {
            "accuracy": 0.6819098856758574,
            "f1": 0.662489895036288,
            "f1_weighted": 0.6783883254671151
          },
          {
            "accuracy": 0.6466039004707465,
            "f1": 0.6242232867603106,
            "f1_weighted": 0.6410093192677391
          },
          {
            "accuracy": 0.6735036987222596,
            "f1": 0.6410911362221813,
            "f1_weighted": 0.6709072150514942
          },
          {
            "accuracy": 0.6607262945527909,
            "f1": 0.6231508432557643,
            "f1_weighted": 0.6480460006458304
          },
          {
            "accuracy": 0.6062542030934768,
            "f1": 0.6011023141096451,
            "f1_weighted": 0.5998313956299846
          },
          {
            "accuracy": 0.6503026227303296,
            "f1": 0.632475051390601,
            "f1_weighted": 0.6379971091877489
          },
          {
            "accuracy": 0.6455951580363147,
            "f1": 0.621413080428444,
            "f1_weighted": 0.6368622535829532
          },
          {
            "accuracy": 0.6472763954270343,
            "f1": 0.6290294095590201,
            "f1_weighted": 0.6363984985256705
          },
          {
            "accuracy": 0.6741761936785474,
            "f1": 0.6565110564994615,
            "f1_weighted": 0.6652175673097682
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6546483030004919,
        "f1": 0.6170084072887285,
        "f1_weighted": 0.6467102103838726,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6546483030004919,
        "scores_per_experiment": [
          {
            "accuracy": 0.6655189375307428,
            "f1": 0.6407137156386896,
            "f1_weighted": 0.6620056024271209
          },
          {
            "accuracy": 0.6886374815543532,
            "f1": 0.6482929448250859,
            "f1_weighted": 0.6897276180359616
          },
          {
            "accuracy": 0.663059517953763,
            "f1": 0.6233875880499811,
            "f1_weighted": 0.6584387792587484
          },
          {
            "accuracy": 0.6660108214461387,
            "f1": 0.6206756768910606,
            "f1_weighted": 0.6620542885960824
          },
          {
            "accuracy": 0.6581406787998032,
            "f1": 0.5963637854752606,
            "f1_weighted": 0.6468825589459524
          },
          {
            "accuracy": 0.6242006886374816,
            "f1": 0.5999780060723752,
            "f1_weighted": 0.612971527080006
          },
          {
            "accuracy": 0.6350221347761928,
            "f1": 0.6009547207122825,
            "f1_weighted": 0.6205747529672364
          },
          {
            "accuracy": 0.6305951795376291,
            "f1": 0.5976554502118897,
            "f1_weighted": 0.6185263433996023
          },
          {
            "accuracy": 0.6433841613379242,
            "f1": 0.6086049008903142,
            "f1_weighted": 0.632679114520136
          },
          {
            "accuracy": 0.6719134284308903,
            "f1": 0.6334572841203467,
            "f1_weighted": 0.6632415186078794
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}