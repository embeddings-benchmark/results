{
  "dataset_revision": "ed9a4403ed4adbfaf4aab56d5b2709e9f6c3ba33",
  "evaluation_time": 206.1810121536255,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test": [
      {
        "accuracy": 0.01902854281422133,
        "f1": 0.015805837559365984,
        "hf_subset": "arb_Arab-rus_Cyrl",
        "languages": [
          "arb-Arab",
          "rus-Cyrl"
        ],
        "main_score": 0.015805837559365984,
        "precision": 0.015357514721981982,
        "recall": 0.01902854281422133
      },
      {
        "accuracy": 0.7551326990485728,
        "f1": 0.7090540503284619,
        "hf_subset": "bel_Cyrl-rus_Cyrl",
        "languages": [
          "bel-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.7090540503284619,
        "precision": 0.6913888690178124,
        "recall": 0.7551326990485728
      },
      {
        "accuracy": 0.005508262393590386,
        "f1": 0.0041821710489997705,
        "hf_subset": "ben_Beng-rus_Cyrl",
        "languages": [
          "ben-Beng",
          "rus-Cyrl"
        ],
        "main_score": 0.0041821710489997705,
        "precision": 0.004010661503976949,
        "recall": 0.005508262393590386
      },
      {
        "accuracy": 0.4231347020530796,
        "f1": 0.3663640622973363,
        "hf_subset": "bos_Latn-rus_Cyrl",
        "languages": [
          "bos-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.3663640622973363,
        "precision": 0.34938795283646934,
        "recall": 0.4231347020530796
      },
      {
        "accuracy": 0.786179268903355,
        "f1": 0.7439750393518002,
        "hf_subset": "bul_Cyrl-rus_Cyrl",
        "languages": [
          "bul-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.7439750393518002,
        "precision": 0.727057649967014,
        "recall": 0.786179268903355
      },
      {
        "accuracy": 0.4246369554331497,
        "f1": 0.3718851059964122,
        "hf_subset": "ces_Latn-rus_Cyrl",
        "languages": [
          "ces-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.3718851059964122,
        "precision": 0.35562021631656093,
        "recall": 0.4246369554331497
      },
      {
        "accuracy": 0.3059589384076114,
        "f1": 0.25829773402530115,
        "hf_subset": "deu_Latn-rus_Cyrl",
        "languages": [
          "deu-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.25829773402530115,
        "precision": 0.24523138456625793,
        "recall": 0.3059589384076114
      },
      {
        "accuracy": 0.06159238858287431,
        "f1": 0.0523194873610041,
        "hf_subset": "ell_Grek-rus_Cyrl",
        "languages": [
          "ell-Grek",
          "rus-Cyrl"
        ],
        "main_score": 0.0523194873610041,
        "precision": 0.049787173754108204,
        "recall": 0.06159238858287431
      },
      {
        "accuracy": 0.9439158738107161,
        "f1": 0.9272909364046069,
        "hf_subset": "eng_Latn-rus_Cyrl",
        "languages": [
          "eng-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9272909364046069,
        "precision": 0.9195626773493575,
        "recall": 0.9439158738107161
      },
      {
        "accuracy": 0.0485728592889334,
        "f1": 0.04155212197600031,
        "hf_subset": "fas_Arab-rus_Cyrl",
        "languages": [
          "fas-Arab",
          "rus-Cyrl"
        ],
        "main_score": 0.04155212197600031,
        "precision": 0.03943692467654882,
        "recall": 0.0485728592889334
      },
      {
        "accuracy": 0.21832749123685527,
        "f1": 0.17695534921479172,
        "hf_subset": "fin_Latn-rus_Cyrl",
        "languages": [
          "fin-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.17695534921479172,
        "precision": 0.16729502741468127,
        "recall": 0.21832749123685527
      },
      {
        "accuracy": 0.46770155232849275,
        "f1": 0.4243674104595282,
        "hf_subset": "fra_Latn-rus_Cyrl",
        "languages": [
          "fra-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.4243674104595282,
        "precision": 0.4095742382341088,
        "recall": 0.46770155232849275
      },
      {
        "accuracy": 0.0385578367551327,
        "f1": 0.03010296560049329,
        "hf_subset": "heb_Hebr-rus_Cyrl",
        "languages": [
          "heb-Hebr",
          "rus-Cyrl"
        ],
        "main_score": 0.03010296560049329,
        "precision": 0.028058531652919712,
        "recall": 0.0385578367551327
      },
      {
        "accuracy": 0.018027040560841263,
        "f1": 0.013821404132585188,
        "hf_subset": "hin_Deva-rus_Cyrl",
        "languages": [
          "hin-Deva",
          "rus-Cyrl"
        ],
        "main_score": 0.013821404132585188,
        "precision": 0.012947806396803423,
        "recall": 0.018027040560841263
      },
      {
        "accuracy": 0.4186279419128693,
        "f1": 0.35970747088768656,
        "hf_subset": "hrv_Latn-rus_Cyrl",
        "languages": [
          "hrv-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.35970747088768656,
        "precision": 0.34177662052980246,
        "recall": 0.4186279419128693
      },
      {
        "accuracy": 0.24186279419128692,
        "f1": 0.19663349732692878,
        "hf_subset": "hun_Latn-rus_Cyrl",
        "languages": [
          "hun-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.19663349732692878,
        "precision": 0.18486219130899026,
        "recall": 0.24186279419128692
      },
      {
        "accuracy": 0.24386579869804706,
        "f1": 0.19859922428160195,
        "hf_subset": "ind_Latn-rus_Cyrl",
        "languages": [
          "ind-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.19859922428160195,
        "precision": 0.18734507184781804,
        "recall": 0.24386579869804706
      },
      {
        "accuracy": 0.022033049574361543,
        "f1": 0.016642363617318197,
        "hf_subset": "jpn_Jpan-rus_Cyrl",
        "languages": [
          "jpn-Jpan",
          "rus-Cyrl"
        ],
        "main_score": 0.016642363617318197,
        "precision": 0.015534611929180065,
        "recall": 0.022033049574361543
      },
      {
        "accuracy": 0.04206309464196294,
        "f1": 0.03360046117359905,
        "hf_subset": "kor_Hang-rus_Cyrl",
        "languages": [
          "kor-Hang",
          "rus-Cyrl"
        ],
        "main_score": 0.03360046117359905,
        "precision": 0.031607953589601674,
        "recall": 0.04206309464196294
      },
      {
        "accuracy": 0.2398597896845268,
        "f1": 0.19866208325214219,
        "hf_subset": "lit_Latn-rus_Cyrl",
        "languages": [
          "lit-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.19866208325214219,
        "precision": 0.18847325620288022,
        "recall": 0.2398597896845268
      },
      {
        "accuracy": 0.6950425638457687,
        "f1": 0.6461346782077878,
        "hf_subset": "mkd_Cyrl-rus_Cyrl",
        "languages": [
          "mkd-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.6461346782077878,
        "precision": 0.6284680558966989,
        "recall": 0.6950425638457687
      },
      {
        "accuracy": 0.33750625938908363,
        "f1": 0.283786914335562,
        "hf_subset": "nld_Latn-rus_Cyrl",
        "languages": [
          "nld-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.283786914335562,
        "precision": 0.2697783401136325,
        "recall": 0.33750625938908363
      },
      {
        "accuracy": 0.35903855783675515,
        "f1": 0.30756749042386894,
        "hf_subset": "pol_Latn-rus_Cyrl",
        "languages": [
          "pol-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.30756749042386894,
        "precision": 0.2929384452061517,
        "recall": 0.35903855783675515
      },
      {
        "accuracy": 0.37656484727090633,
        "f1": 0.3235873384276051,
        "hf_subset": "por_Latn-rus_Cyrl",
        "languages": [
          "por-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.3235873384276051,
        "precision": 0.30725658003959583,
        "recall": 0.37656484727090633
      },
      {
        "accuracy": 0.06509764646970456,
        "f1": 0.03357307419729944,
        "hf_subset": "rus_Cyrl-arb_Arab",
        "languages": [
          "rus-Cyrl",
          "arb-Arab"
        ],
        "main_score": 0.03357307419729944,
        "precision": 0.02829936906622449,
        "recall": 0.06509764646970456
      },
      {
        "accuracy": 0.7205808713069605,
        "f1": 0.658584305028972,
        "hf_subset": "rus_Cyrl-bel_Cyrl",
        "languages": [
          "rus-Cyrl",
          "bel-Cyrl"
        ],
        "main_score": 0.658584305028972,
        "precision": 0.6341029401244724,
        "recall": 0.7205808713069605
      },
      {
        "accuracy": 0.028042063094641963,
        "f1": 0.008526173895053783,
        "hf_subset": "rus_Cyrl-ben_Beng",
        "languages": [
          "rus-Cyrl",
          "ben-Beng"
        ],
        "main_score": 0.008526173895053783,
        "precision": 0.0064279404848427805,
        "recall": 0.028042063094641963
      },
      {
        "accuracy": 0.4521782674011017,
        "f1": 0.3717925485382553,
        "hf_subset": "rus_Cyrl-bos_Latn",
        "languages": [
          "rus-Cyrl",
          "bos-Latn"
        ],
        "main_score": 0.3717925485382553,
        "precision": 0.34530293122556516,
        "recall": 0.4521782674011017
      },
      {
        "accuracy": 0.7396094141211818,
        "f1": 0.6765242852623924,
        "hf_subset": "rus_Cyrl-bul_Cyrl",
        "languages": [
          "rus-Cyrl",
          "bul-Cyrl"
        ],
        "main_score": 0.6765242852623924,
        "precision": 0.6505234041538498,
        "recall": 0.7396094141211818
      },
      {
        "accuracy": 0.4672008012018027,
        "f1": 0.3881765274729721,
        "hf_subset": "rus_Cyrl-ces_Latn",
        "languages": [
          "rus-Cyrl",
          "ces-Latn"
        ],
        "main_score": 0.3881765274729721,
        "precision": 0.3597449584855694,
        "recall": 0.4672008012018027
      },
      {
        "accuracy": 0.400600901352028,
        "f1": 0.31853640474881156,
        "hf_subset": "rus_Cyrl-deu_Latn",
        "languages": [
          "rus-Cyrl",
          "deu-Latn"
        ],
        "main_score": 0.31853640474881156,
        "precision": 0.2926897095838007,
        "recall": 0.400600901352028
      },
      {
        "accuracy": 0.12468703054581873,
        "f1": 0.07115000960864572,
        "hf_subset": "rus_Cyrl-ell_Grek",
        "languages": [
          "rus-Cyrl",
          "ell-Grek"
        ],
        "main_score": 0.07115000960864572,
        "precision": 0.059726650123349136,
        "recall": 0.12468703054581873
      },
      {
        "accuracy": 0.9238858287431146,
        "f1": 0.9013770655983976,
        "hf_subset": "rus_Cyrl-eng_Latn",
        "languages": [
          "rus-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.9013770655983976,
        "precision": 0.8909077902568138,
        "recall": 0.9238858287431146
      },
      {
        "accuracy": 0.12068102153229844,
        "f1": 0.06449366453970648,
        "hf_subset": "rus_Cyrl-fas_Arab",
        "languages": [
          "rus-Cyrl",
          "fas-Arab"
        ],
        "main_score": 0.06449366453970648,
        "precision": 0.052931425440326474,
        "recall": 0.12068102153229844
      },
      {
        "accuracy": 0.29494241362043067,
        "f1": 0.21977350165812393,
        "hf_subset": "rus_Cyrl-fin_Latn",
        "languages": [
          "rus-Cyrl",
          "fin-Latn"
        ],
        "main_score": 0.21977350165812393,
        "precision": 0.19819757194946122,
        "recall": 0.29494241362043067
      },
      {
        "accuracy": 0.5438157235853781,
        "f1": 0.4634967463960954,
        "hf_subset": "rus_Cyrl-fra_Latn",
        "languages": [
          "rus-Cyrl",
          "fra-Latn"
        ],
        "main_score": 0.4634967463960954,
        "precision": 0.4340455163918299,
        "recall": 0.5438157235853781
      },
      {
        "accuracy": 0.07811717576364546,
        "f1": 0.03761504446670749,
        "hf_subset": "rus_Cyrl-heb_Hebr",
        "languages": [
          "rus-Cyrl",
          "heb-Hebr"
        ],
        "main_score": 0.03761504446670749,
        "precision": 0.03052979740946969,
        "recall": 0.07811717576364546
      },
      {
        "accuracy": 0.06259389083625438,
        "f1": 0.030461150047900974,
        "hf_subset": "rus_Cyrl-hin_Deva",
        "languages": [
          "rus-Cyrl",
          "hin-Deva"
        ],
        "main_score": 0.030461150047900974,
        "precision": 0.02506616021295115,
        "recall": 0.06259389083625438
      },
      {
        "accuracy": 0.44516775162744116,
        "f1": 0.3671781779842785,
        "hf_subset": "rus_Cyrl-hrv_Latn",
        "languages": [
          "rus-Cyrl",
          "hrv-Latn"
        ],
        "main_score": 0.3671781779842785,
        "precision": 0.3419389517771611,
        "recall": 0.44516775162744116
      },
      {
        "accuracy": 0.30796194291437157,
        "f1": 0.23067497817604288,
        "hf_subset": "rus_Cyrl-hun_Latn",
        "languages": [
          "rus-Cyrl",
          "hun-Latn"
        ],
        "main_score": 0.23067497817604288,
        "precision": 0.20794841493443744,
        "recall": 0.30796194291437157
      },
      {
        "accuracy": 0.3189784677015523,
        "f1": 0.2370467052789065,
        "hf_subset": "rus_Cyrl-ind_Latn",
        "languages": [
          "rus-Cyrl",
          "ind-Latn"
        ],
        "main_score": 0.2370467052789065,
        "precision": 0.21231007671984928,
        "recall": 0.3189784677015523
      },
      {
        "accuracy": 0.06509764646970456,
        "f1": 0.028625009905395973,
        "hf_subset": "rus_Cyrl-jpn_Jpan",
        "languages": [
          "rus-Cyrl",
          "jpn-Jpan"
        ],
        "main_score": 0.028625009905395973,
        "precision": 0.022578930464020285,
        "recall": 0.06509764646970456
      },
      {
        "accuracy": 0.11717576364546821,
        "f1": 0.06808210376716319,
        "hf_subset": "rus_Cyrl-kor_Hang",
        "languages": [
          "rus-Cyrl",
          "kor-Hang"
        ],
        "main_score": 0.06808210376716319,
        "precision": 0.05809107630620068,
        "recall": 0.11717576364546821
      },
      {
        "accuracy": 0.32498748122183274,
        "f1": 0.24852648044459522,
        "hf_subset": "rus_Cyrl-lit_Latn",
        "languages": [
          "rus-Cyrl",
          "lit-Latn"
        ],
        "main_score": 0.24852648044459522,
        "precision": 0.2257200812596406,
        "recall": 0.32498748122183274
      },
      {
        "accuracy": 0.6619929894842264,
        "f1": 0.587641418838214,
        "hf_subset": "rus_Cyrl-mkd_Cyrl",
        "languages": [
          "rus-Cyrl",
          "mkd-Cyrl"
        ],
        "main_score": 0.587641418838214,
        "precision": 0.5580424207740181,
        "recall": 0.6619929894842264
      },
      {
        "accuracy": 0.4021031547320981,
        "f1": 0.31699625556661115,
        "hf_subset": "rus_Cyrl-nld_Latn",
        "languages": [
          "rus-Cyrl",
          "nld-Latn"
        ],
        "main_score": 0.31699625556661115,
        "precision": 0.29008344721915075,
        "recall": 0.4021031547320981
      },
      {
        "accuracy": 0.3900851276915373,
        "f1": 0.3102457080755411,
        "hf_subset": "rus_Cyrl-pol_Latn",
        "languages": [
          "rus-Cyrl",
          "pol-Latn"
        ],
        "main_score": 0.3102457080755411,
        "precision": 0.2844206596115495,
        "recall": 0.3900851276915373
      },
      {
        "accuracy": 0.42263395092638956,
        "f1": 0.3407229600443334,
        "hf_subset": "rus_Cyrl-por_Latn",
        "languages": [
          "rus-Cyrl",
          "por-Latn"
        ],
        "main_score": 0.3407229600443334,
        "precision": 0.31435903531546994,
        "recall": 0.42263395092638956
      },
      {
        "accuracy": 0.48122183274912367,
        "f1": 0.40094258978394,
        "hf_subset": "rus_Cyrl-slk_Latn",
        "languages": [
          "rus-Cyrl",
          "slk-Latn"
        ],
        "main_score": 0.40094258978394,
        "precision": 0.3740007328953247,
        "recall": 0.48122183274912367
      },
      {
        "accuracy": 0.4401602403605408,
        "f1": 0.3599705789002892,
        "hf_subset": "rus_Cyrl-slv_Latn",
        "languages": [
          "rus-Cyrl",
          "slv-Latn"
        ],
        "main_score": 0.3599705789002892,
        "precision": 0.3331744712293477,
        "recall": 0.4401602403605408
      },
      {
        "accuracy": 0.4636955433149725,
        "f1": 0.384466375164923,
        "hf_subset": "rus_Cyrl-spa_Latn",
        "languages": [
          "rus-Cyrl",
          "spa-Latn"
        ],
        "main_score": 0.384466375164923,
        "precision": 0.35778251265231736,
        "recall": 0.4636955433149725
      },
      {
        "accuracy": 0.5598397596394592,
        "f1": 0.47482338225953646,
        "hf_subset": "rus_Cyrl-srp_Cyrl",
        "languages": [
          "rus-Cyrl",
          "srp-Cyrl"
        ],
        "main_score": 0.47482338225953646,
        "precision": 0.4442684228362746,
        "recall": 0.5598397596394592
      },
      {
        "accuracy": 0.45968953430145215,
        "f1": 0.38236763836479204,
        "hf_subset": "rus_Cyrl-srp_Latn",
        "languages": [
          "rus-Cyrl",
          "srp-Latn"
        ],
        "main_score": 0.38236763836479204,
        "precision": 0.35651602904762053,
        "recall": 0.45968953430145215
      },
      {
        "accuracy": 0.25037556334501754,
        "f1": 0.1807586844072455,
        "hf_subset": "rus_Cyrl-swa_Latn",
        "languages": [
          "rus-Cyrl",
          "swa-Latn"
        ],
        "main_score": 0.1807586844072455,
        "precision": 0.16146506451023582,
        "recall": 0.25037556334501754
      },
      {
        "accuracy": 0.37255883825738606,
        "f1": 0.29400763927554113,
        "hf_subset": "rus_Cyrl-swe_Latn",
        "languages": [
          "rus-Cyrl",
          "swe-Latn"
        ],
        "main_score": 0.29400763927554113,
        "precision": 0.2696190050559065,
        "recall": 0.37255883825738606
      },
      {
        "accuracy": 0.058587881822734104,
        "f1": 0.02773563497851167,
        "hf_subset": "rus_Cyrl-tam_Taml",
        "languages": [
          "rus-Cyrl",
          "tam-Taml"
        ],
        "main_score": 0.02773563497851167,
        "precision": 0.022237305530935947,
        "recall": 0.058587881822734104
      },
      {
        "accuracy": 0.33800701051577364,
        "f1": 0.2583173322532361,
        "hf_subset": "rus_Cyrl-tur_Latn",
        "languages": [
          "rus-Cyrl",
          "tur-Latn"
        ],
        "main_score": 0.2583173322532361,
        "precision": 0.23275253620371975,
        "recall": 0.33800701051577364
      },
      {
        "accuracy": 0.9474211316975463,
        "f1": 0.9326823568686362,
        "hf_subset": "rus_Cyrl-ukr_Cyrl",
        "languages": [
          "rus-Cyrl",
          "ukr-Cyrl"
        ],
        "main_score": 0.9326823568686362,
        "precision": 0.9258637956935403,
        "recall": 0.9474211316975463
      },
      {
        "accuracy": 0.3014521782674011,
        "f1": 0.22664195208671553,
        "hf_subset": "rus_Cyrl-vie_Latn",
        "languages": [
          "rus-Cyrl",
          "vie-Latn"
        ],
        "main_score": 0.22664195208671553,
        "precision": 0.20371458510905588,
        "recall": 0.3014521782674011
      },
      {
        "accuracy": 0.21181772658988482,
        "f1": 0.14190581427377025,
        "hf_subset": "rus_Cyrl-zho_Hant",
        "languages": [
          "rus-Cyrl",
          "zho-Hant"
        ],
        "main_score": 0.14190581427377025,
        "precision": 0.12309609244075538,
        "recall": 0.21181772658988482
      },
      {
        "accuracy": 0.23234852278417625,
        "f1": 0.16051400045572864,
        "hf_subset": "rus_Cyrl-zul_Latn",
        "languages": [
          "rus-Cyrl",
          "zul-Latn"
        ],
        "main_score": 0.16051400045572864,
        "precision": 0.14186293916558126,
        "recall": 0.23234852278417625
      },
      {
        "accuracy": 0.4171256885327992,
        "f1": 0.36366702486952523,
        "hf_subset": "slk_Latn-rus_Cyrl",
        "languages": [
          "slk-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.36366702486952523,
        "precision": 0.34733957218631034,
        "recall": 0.4171256885327992
      },
      {
        "accuracy": 0.39359038557836756,
        "f1": 0.3422262108728573,
        "hf_subset": "slv_Latn-rus_Cyrl",
        "languages": [
          "slv-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.3422262108728573,
        "precision": 0.3271410289577658,
        "recall": 0.39359038557836756
      },
      {
        "accuracy": 0.4261392088132198,
        "f1": 0.3724901387600273,
        "hf_subset": "spa_Latn-rus_Cyrl",
        "languages": [
          "spa-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.3724901387600273,
        "precision": 0.3562872299786975,
        "recall": 0.4261392088132198
      },
      {
        "accuracy": 0.5983975963945919,
        "f1": 0.5411725452536669,
        "hf_subset": "srp_Cyrl-rus_Cyrl",
        "languages": [
          "srp-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.5411725452536669,
        "precision": 0.5203938503953424,
        "recall": 0.5983975963945919
      },
      {
        "accuracy": 0.39809714571857785,
        "f1": 0.34188195470890087,
        "hf_subset": "srp_Latn-rus_Cyrl",
        "languages": [
          "srp-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.34188195470890087,
        "precision": 0.3257640106065995,
        "recall": 0.39809714571857785
      },
      {
        "accuracy": 0.17826740110165248,
        "f1": 0.14287862952018443,
        "hf_subset": "swa_Latn-rus_Cyrl",
        "languages": [
          "swa-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.14287862952018443,
        "precision": 0.13465847025467204,
        "recall": 0.17826740110165248
      },
      {
        "accuracy": 0.3199799699549324,
        "f1": 0.26999136033063714,
        "hf_subset": "swe_Latn-rus_Cyrl",
        "languages": [
          "swe-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.26999136033063714,
        "precision": 0.2557953236909122,
        "recall": 0.3199799699549324
      },
      {
        "accuracy": 0.022533800701051578,
        "f1": 0.016920245884614785,
        "hf_subset": "tam_Taml-rus_Cyrl",
        "languages": [
          "tam-Taml",
          "rus-Cyrl"
        ],
        "main_score": 0.016920245884614785,
        "precision": 0.015895533403181956,
        "recall": 0.022533800701051578
      },
      {
        "accuracy": 0.2613920881321983,
        "f1": 0.21558122703557245,
        "hf_subset": "tur_Latn-rus_Cyrl",
        "languages": [
          "tur-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.21558122703557245,
        "precision": 0.20267771279270996,
        "recall": 0.2613920881321983
      },
      {
        "accuracy": 0.9624436654982473,
        "f1": 0.9522951093306626,
        "hf_subset": "ukr_Cyrl-rus_Cyrl",
        "languages": [
          "ukr-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.9522951093306626,
        "precision": 0.9475463194792189,
        "recall": 0.9624436654982473
      },
      {
        "accuracy": 0.2073109664496745,
        "f1": 0.17647490050131906,
        "hf_subset": "vie_Latn-rus_Cyrl",
        "languages": [
          "vie-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.17647490050131906,
        "precision": 0.16795252939556707,
        "recall": 0.2073109664496745
      },
      {
        "accuracy": 0.07411116675012519,
        "f1": 0.06452126180467214,
        "hf_subset": "zho_Hant-rus_Cyrl",
        "languages": [
          "zho-Hant",
          "rus-Cyrl"
        ],
        "main_score": 0.06452126180467214,
        "precision": 0.06245383020135485,
        "recall": 0.07411116675012519
      },
      {
        "accuracy": 0.1727591387080621,
        "f1": 0.13955683591870136,
        "hf_subset": "zul_Latn-rus_Cyrl",
        "languages": [
          "zul-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.13955683591870136,
        "precision": 0.13194662538324317,
        "recall": 0.1727591387080621
      }
    ]
  },
  "task_name": "NTREXBitextMining"
}