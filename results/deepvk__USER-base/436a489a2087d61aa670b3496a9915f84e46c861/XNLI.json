{
  "dataset_revision": "09698e0180d87dc247ca447d3a1248b931ac0cdb",
  "evaluation_time": 4.988911151885986,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.7091575091575092,
        "cosine_accuracy_threshold": 0.5526376962661743,
        "cosine_ap": 0.779997823475449,
        "cosine_f1": 0.7288343558282209,
        "cosine_f1_threshold": 0.4470091462135315,
        "cosine_precision": 0.6265822784810127,
        "cosine_recall": 0.8709677419354839,
        "dot_accuracy": 0.7091575091575092,
        "dot_accuracy_threshold": 0.5526376962661743,
        "dot_ap": 0.779997823475449,
        "dot_f1": 0.7288343558282209,
        "dot_f1_threshold": 0.4470091462135315,
        "dot_precision": 0.6265822784810127,
        "dot_recall": 0.8709677419354839,
        "euclidean_accuracy": 0.7091575091575092,
        "euclidean_accuracy_threshold": 0.9458987712860107,
        "euclidean_ap": 0.779997823475449,
        "euclidean_f1": 0.7288343558282209,
        "euclidean_f1_threshold": 1.0516566038131714,
        "euclidean_precision": 0.6265822784810127,
        "euclidean_recall": 0.8709677419354839,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.779997823475449,
        "manhattan_accuracy": 0.7091575091575092,
        "manhattan_accuracy_threshold": 21.135772705078125,
        "manhattan_ap": 0.7799948581200891,
        "manhattan_f1": 0.7301790281329924,
        "manhattan_f1_threshold": 22.842853546142578,
        "manhattan_precision": 0.6473922902494331,
        "manhattan_recall": 0.8372434017595308,
        "max_ap": 0.779997823475449,
        "max_f1": 0.7301790281329924,
        "max_precision": 0.6473922902494331,
        "max_recall": 0.8709677419354839,
        "similarity_accuracy": 0.7091575091575092,
        "similarity_accuracy_threshold": 0.5526376962661743,
        "similarity_ap": 0.779997823475449,
        "similarity_f1": 0.7288343558282209,
        "similarity_f1_threshold": 0.4470091462135315,
        "similarity_precision": 0.6265822784810127,
        "similarity_recall": 0.8709677419354839
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.7318681318681318,
        "cosine_accuracy_threshold": 0.520270049571991,
        "cosine_ap": 0.796444009722443,
        "cosine_f1": 0.7362482369534556,
        "cosine_f1_threshold": 0.4961276352405548,
        "cosine_precision": 0.7092391304347826,
        "cosine_recall": 0.7653958944281525,
        "dot_accuracy": 0.7318681318681318,
        "dot_accuracy_threshold": 0.5202701091766357,
        "dot_ap": 0.796444009722443,
        "dot_f1": 0.7362482369534556,
        "dot_f1_threshold": 0.496127724647522,
        "dot_precision": 0.7092391304347826,
        "dot_recall": 0.7653958944281525,
        "euclidean_accuracy": 0.7318681318681318,
        "euclidean_accuracy_threshold": 0.9795200824737549,
        "euclidean_ap": 0.796444009722443,
        "euclidean_f1": 0.7362482369534556,
        "euclidean_f1_threshold": 1.0038648843765259,
        "euclidean_precision": 0.7092391304347826,
        "euclidean_recall": 0.7653958944281525,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.7965237716705318,
        "manhattan_accuracy": 0.7289377289377289,
        "manhattan_accuracy_threshold": 21.463184356689453,
        "manhattan_ap": 0.7965237716705318,
        "manhattan_f1": 0.7345505617977528,
        "manhattan_f1_threshold": 22.17138671875,
        "manhattan_precision": 0.7048517520215634,
        "manhattan_recall": 0.7668621700879765,
        "max_ap": 0.7965237716705318,
        "max_f1": 0.7362482369534556,
        "max_precision": 0.7092391304347826,
        "max_recall": 0.7668621700879765,
        "similarity_accuracy": 0.7318681318681318,
        "similarity_accuracy_threshold": 0.520270049571991,
        "similarity_ap": 0.796444009722443,
        "similarity_f1": 0.7362482369534556,
        "similarity_f1_threshold": 0.4961276352405548,
        "similarity_precision": 0.7092391304347826,
        "similarity_recall": 0.7653958944281525
      }
    ]
  },
  "task_name": "XNLI"
}