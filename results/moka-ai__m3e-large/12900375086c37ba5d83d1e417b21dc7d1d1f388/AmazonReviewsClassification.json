{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "task_name": "AmazonReviewsClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.27254,
        "f1": 0.270379,
        "f1_weighted": 0.270379,
        "scores_per_experiment": [
          {
            "accuracy": 0.2802,
            "f1": 0.277761,
            "f1_weighted": 0.277761
          },
          {
            "accuracy": 0.2798,
            "f1": 0.277963,
            "f1_weighted": 0.277963
          },
          {
            "accuracy": 0.2854,
            "f1": 0.286964,
            "f1_weighted": 0.286964
          },
          {
            "accuracy": 0.2692,
            "f1": 0.267944,
            "f1_weighted": 0.267944
          },
          {
            "accuracy": 0.2586,
            "f1": 0.257909,
            "f1_weighted": 0.257909
          },
          {
            "accuracy": 0.279,
            "f1": 0.279569,
            "f1_weighted": 0.279569
          },
          {
            "accuracy": 0.242,
            "f1": 0.23708,
            "f1_weighted": 0.23708
          },
          {
            "accuracy": 0.3068,
            "f1": 0.305188,
            "f1_weighted": 0.305188
          },
          {
            "accuracy": 0.2522,
            "f1": 0.248123,
            "f1_weighted": 0.248123
          },
          {
            "accuracy": 0.2722,
            "f1": 0.265291,
            "f1_weighted": 0.265291
          }
        ],
        "main_score": 0.27254,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.26902,
        "f1": 0.26683,
        "f1_weighted": 0.26683,
        "scores_per_experiment": [
          {
            "accuracy": 0.2846,
            "f1": 0.282986,
            "f1_weighted": 0.282986
          },
          {
            "accuracy": 0.26,
            "f1": 0.258727,
            "f1_weighted": 0.258727
          },
          {
            "accuracy": 0.28,
            "f1": 0.279767,
            "f1_weighted": 0.279767
          },
          {
            "accuracy": 0.266,
            "f1": 0.263769,
            "f1_weighted": 0.263769
          },
          {
            "accuracy": 0.2572,
            "f1": 0.255289,
            "f1_weighted": 0.255289
          },
          {
            "accuracy": 0.2798,
            "f1": 0.280817,
            "f1_weighted": 0.280817
          },
          {
            "accuracy": 0.2416,
            "f1": 0.235538,
            "f1_weighted": 0.235538
          },
          {
            "accuracy": 0.3012,
            "f1": 0.301279,
            "f1_weighted": 0.301279
          },
          {
            "accuracy": 0.251,
            "f1": 0.24619,
            "f1_weighted": 0.24619
          },
          {
            "accuracy": 0.2688,
            "f1": 0.263936,
            "f1_weighted": 0.263936
          }
        ],
        "main_score": 0.26902,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 158.50289154052734,
  "kg_co2_emissions": null
}