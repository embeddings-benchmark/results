{
  "dataset_revision": "human_evaluation",
  "task_name": "HUMESTSBenchmark",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "spearman": 0.8035322665045446,
        "pearson": 0.8211643019994898,
        "mse": 1.10678222,
        "main_score": 0.8035322665045446,
        "scores_per_experiment": [
          {
            "spearman": 0.8303382109645605,
            "pearson": 0.8081182020523571,
            "mse": 1.0917422199999998
          },
          {
            "spearman": 0.635562250024232,
            "pearson": 0.6671868628832128,
            "mse": 1.81182222
          }
        ],
        "agreement": {
          "metric": "mean_pairwise_spearman",
          "value": 0.5794091505852625,
          "n_annotators": 2,
          "n_items": 50,
          "total_annotations": 100
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 0,
  "kg_co2_emissions": null
}