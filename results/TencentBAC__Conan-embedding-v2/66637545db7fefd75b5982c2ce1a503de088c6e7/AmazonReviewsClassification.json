{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "task_name": "AmazonReviewsClassification",
  "mteb_version": "1.36.38",
  "scores": {
    "test": [
      {
        "accuracy": 0.68802,
        "f1": 0.672001,
        "f1_weighted": 0.672001,
        "scores_per_experiment": [
          {
            "accuracy": 0.6738,
            "f1": 0.655025,
            "f1_weighted": 0.655025
          },
          {
            "accuracy": 0.6552,
            "f1": 0.637171,
            "f1_weighted": 0.637171
          },
          {
            "accuracy": 0.6508,
            "f1": 0.61535,
            "f1_weighted": 0.61535
          },
          {
            "accuracy": 0.6572,
            "f1": 0.646781,
            "f1_weighted": 0.646781
          },
          {
            "accuracy": 0.6456,
            "f1": 0.612069,
            "f1_weighted": 0.612069
          },
          {
            "accuracy": 0.7042,
            "f1": 0.69107,
            "f1_weighted": 0.69107
          },
          {
            "accuracy": 0.7126,
            "f1": 0.710021,
            "f1_weighted": 0.710021
          },
          {
            "accuracy": 0.737,
            "f1": 0.73482,
            "f1_weighted": 0.73482
          },
          {
            "accuracy": 0.7372,
            "f1": 0.72574,
            "f1_weighted": 0.72574
          },
          {
            "accuracy": 0.7066,
            "f1": 0.691957,
            "f1_weighted": 0.691957
          }
        ],
        "main_score": 0.68802,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      },
      {
        "accuracy": 0.56528,
        "f1": 0.511231,
        "f1_weighted": 0.511231,
        "scores_per_experiment": [
          {
            "accuracy": 0.5582,
            "f1": 0.49541,
            "f1_weighted": 0.49541
          },
          {
            "accuracy": 0.5644,
            "f1": 0.506135,
            "f1_weighted": 0.506135
          },
          {
            "accuracy": 0.578,
            "f1": 0.524545,
            "f1_weighted": 0.524545
          },
          {
            "accuracy": 0.5478,
            "f1": 0.497233,
            "f1_weighted": 0.497233
          },
          {
            "accuracy": 0.5654,
            "f1": 0.526459,
            "f1_weighted": 0.526459
          },
          {
            "accuracy": 0.541,
            "f1": 0.473218,
            "f1_weighted": 0.473218
          },
          {
            "accuracy": 0.5856,
            "f1": 0.533651,
            "f1_weighted": 0.533651
          },
          {
            "accuracy": 0.5604,
            "f1": 0.504627,
            "f1_weighted": 0.504627
          },
          {
            "accuracy": 0.588,
            "f1": 0.548541,
            "f1_weighted": 0.548541
          },
          {
            "accuracy": 0.564,
            "f1": 0.502492,
            "f1_weighted": 0.502492
          }
        ],
        "main_score": 0.56528,
        "hf_subset": "zh",
        "languages": [
          "cmn-Hans"
        ]
      }
    ]
  },
  "evaluation_time": 126.74265789985657,
  "kg_co2_emissions": null
}
