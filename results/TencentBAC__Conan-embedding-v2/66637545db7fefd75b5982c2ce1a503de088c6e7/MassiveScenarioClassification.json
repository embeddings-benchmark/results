{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "task_name": "MassiveScenarioClassification",
  "mteb_version": "1.36.38",
  "scores": {
    "test": [
      {
        "accuracy": 0.909987,
        "f1": 0.898458,
        "f1_weighted": 0.907396,
        "scores_per_experiment": [
          {
            "accuracy": 0.909886,
            "f1": 0.9027,
            "f1_weighted": 0.906961
          },
          {
            "accuracy": 0.909213,
            "f1": 0.899745,
            "f1_weighted": 0.908151
          },
          {
            "accuracy": 0.905851,
            "f1": 0.892712,
            "f1_weighted": 0.902498
          },
          {
            "accuracy": 0.897108,
            "f1": 0.874221,
            "f1_weighted": 0.892538
          },
          {
            "accuracy": 0.903497,
            "f1": 0.88545,
            "f1_weighted": 0.896396
          },
          {
            "accuracy": 0.896772,
            "f1": 0.886308,
            "f1_weighted": 0.895913
          },
          {
            "accuracy": 0.911903,
            "f1": 0.897721,
            "f1_weighted": 0.908932
          },
          {
            "accuracy": 0.914929,
            "f1": 0.908802,
            "f1_weighted": 0.913832
          },
          {
            "accuracy": 0.915602,
            "f1": 0.908967,
            "f1_weighted": 0.914219
          },
          {
            "accuracy": 0.935104,
            "f1": 0.927951,
            "f1_weighted": 0.934518
          }
        ],
        "main_score": 0.909987,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      },
      {
        "accuracy": 0.801412,
        "f1": 0.77917,
        "f1_weighted": 0.798487,
        "scores_per_experiment": [
          {
            "accuracy": 0.830868,
            "f1": 0.809921,
            "f1_weighted": 0.829775
          },
          {
            "accuracy": 0.805985,
            "f1": 0.783855,
            "f1_weighted": 0.80025
          },
          {
            "accuracy": 0.846671,
            "f1": 0.831353,
            "f1_weighted": 0.844173
          },
          {
            "accuracy": 0.813719,
            "f1": 0.782772,
            "f1_weighted": 0.806534
          },
          {
            "accuracy": 0.769334,
            "f1": 0.744382,
            "f1_weighted": 0.765776
          },
          {
            "accuracy": 0.781103,
            "f1": 0.757208,
            "f1_weighted": 0.770706
          },
          {
            "accuracy": 0.783457,
            "f1": 0.768367,
            "f1_weighted": 0.786104
          },
          {
            "accuracy": 0.795225,
            "f1": 0.789757,
            "f1_weighted": 0.798814
          },
          {
            "accuracy": 0.79119,
            "f1": 0.750061,
            "f1_weighted": 0.785418
          },
          {
            "accuracy": 0.79657,
            "f1": 0.774027,
            "f1_weighted": 0.797321
          }
        ],
        "main_score": 0.801412,
        "hf_subset": "zh-CN",
        "languages": [
          "cmo-Hans"
        ]
      }
    ]
  },
  "evaluation_time": 106.10910964012146,
  "kg_co2_emissions": null
}
