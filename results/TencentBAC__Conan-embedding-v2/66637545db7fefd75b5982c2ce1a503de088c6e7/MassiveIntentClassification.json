{
  "dataset_revision": "4815e20407010da34463acc759c162ca9864bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.36.38",
  "scores": {
    "test": [
      {
        "accuracy": 0.863788,
        "f1": 0.784931,
        "f1_weighted": 0.851016,
        "scores_per_experiment": [
          {
            "accuracy": 0.849832,
            "f1": 0.801809,
            "f1_weighted": 0.840755
          },
          {
            "accuracy": 0.878985,
            "f1": 0.815597,
            "f1_weighted": 0.856088
          },
          {
            "accuracy": 0.85226,
            "f1": 0.782681,
            "f1_weighted": 0.840849
          },
          {
            "accuracy": 0.85226,
            "f1": 0.781184,
            "f1_weighted": 0.840055
          },
          {
            "accuracy": 0.854613,
            "f1": 0.769491,
            "f1_weighted": 0.846186
          },
          {
            "accuracy": 0.859321,
            "f1": 0.789419,
            "f1_weighted": 0.850364
          },
          {
            "accuracy": 0.864364,
            "f1": 0.788645,
            "f1_weighted": 0.85187
          },
          {
            "accuracy": 0.851251,
            "f1": 0.768596,
            "f1_weighted": 0.843043
          },
          {
            "accuracy": 0.851074,
            "f1": 0.780666,
            "f1_weighted": 0.841802
          },
          {
            "accuracy": 0.858985,
            "f1": 0.811234,
            "f1_weighted": 0.852162
          }
        ],
        "main_score": 0.863788,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      },
      {
        "accuracy": 0.767956,
        "f1": 0.722474,
        "f1_weighted": 0.76416,
        "scores_per_experiment": [
          {
            "accuracy": 0.76113,
            "f1": 0.804577,
            "f1_weighted": 0.755777
          },
          {
            "accuracy": 0.77458,
            "f1": 0.716077,
            "f1_weighted": 0.772605
          },
          {
            "accuracy": 0.764156,
            "f1": 0.713382,
            "f1_weighted": 0.759012
          },
          {
            "accuracy": 0.772226,
            "f1": 0.717912,
            "f1_weighted": 0.769483
          },
          {
            "accuracy": 0.759775,
            "f1": 0.706415,
            "f1_weighted": 0.753976
          },
          {
            "accuracy": 0.779623,
            "f1": 0.720916,
            "f1_weighted": 0.779211
          },
          {
            "accuracy": 0.760457,
            "f1": 0.709201,
            "f1_weighted": 0.753845
          },
          {
            "accuracy": 0.759775,
            "f1": 0.704168,
            "f1_weighted": 0.754014
          },
          {
            "accuracy": 0.771217,
            "f1": 0.703656,
            "f1_weighted": 0.767658
          },
          {
            "accuracy": 0.776597,
            "f1": 0.72844,
            "f1_weighted": 0.776012
          }
        ],
        "main_score": 0.767956,
        "hf_subset": "zh-CN",
        "languages": [
          "cmo-Hans"
        ]
      }
    ]
  },
  "evaluation_time": 149.87785488594055,
  "kg_co2_emissions": null
}
