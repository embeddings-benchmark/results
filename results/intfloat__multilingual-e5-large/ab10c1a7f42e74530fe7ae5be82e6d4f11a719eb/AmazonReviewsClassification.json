{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "task_name": "AmazonReviewsClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.43376,
        "f1": 0.414462,
        "f1_weighted": 0.414462,
        "scores_per_experiment": [
          {
            "accuracy": 0.476,
            "f1": 0.452301,
            "f1_weighted": 0.452301
          },
          {
            "accuracy": 0.4248,
            "f1": 0.397215,
            "f1_weighted": 0.397215
          },
          {
            "accuracy": 0.4154,
            "f1": 0.405041,
            "f1_weighted": 0.405041
          },
          {
            "accuracy": 0.462,
            "f1": 0.425891,
            "f1_weighted": 0.425891
          },
          {
            "accuracy": 0.449,
            "f1": 0.428747,
            "f1_weighted": 0.428747
          },
          {
            "accuracy": 0.4288,
            "f1": 0.401068,
            "f1_weighted": 0.401068
          },
          {
            "accuracy": 0.392,
            "f1": 0.389674,
            "f1_weighted": 0.389674
          },
          {
            "accuracy": 0.4622,
            "f1": 0.442961,
            "f1_weighted": 0.442961
          },
          {
            "accuracy": 0.3996,
            "f1": 0.384882,
            "f1_weighted": 0.384882
          },
          {
            "accuracy": 0.4278,
            "f1": 0.416841,
            "f1_weighted": 0.416841
          }
        ],
        "main_score": 0.43376,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.42862,
        "f1": 0.409553,
        "f1_weighted": 0.409553,
        "scores_per_experiment": [
          {
            "accuracy": 0.4572,
            "f1": 0.432479,
            "f1_weighted": 0.432479
          },
          {
            "accuracy": 0.4264,
            "f1": 0.400572,
            "f1_weighted": 0.400572
          },
          {
            "accuracy": 0.4216,
            "f1": 0.410873,
            "f1_weighted": 0.410873
          },
          {
            "accuracy": 0.4564,
            "f1": 0.421387,
            "f1_weighted": 0.421387
          },
          {
            "accuracy": 0.4346,
            "f1": 0.414504,
            "f1_weighted": 0.414504
          },
          {
            "accuracy": 0.4172,
            "f1": 0.389329,
            "f1_weighted": 0.389329
          },
          {
            "accuracy": 0.391,
            "f1": 0.387954,
            "f1_weighted": 0.387954
          },
          {
            "accuracy": 0.4646,
            "f1": 0.44493,
            "f1_weighted": 0.44493
          },
          {
            "accuracy": 0.3988,
            "f1": 0.383216,
            "f1_weighted": 0.383216
          },
          {
            "accuracy": 0.4184,
            "f1": 0.410285,
            "f1_weighted": 0.410285
          }
        ],
        "main_score": 0.42862,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 159.76749777793884,
  "kg_co2_emissions": null
}