{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.25.8",
  "scores": {
    "validation": [
      {
        "accuracy": 0.656665,
        "f1": 0.606756,
        "f1_weighted": 0.651854,
        "scores_per_experiment": [
          {
            "accuracy": 0.674373,
            "f1": 0.628454,
            "f1_weighted": 0.675079
          },
          {
            "accuracy": 0.699951,
            "f1": 0.645244,
            "f1_weighted": 0.695409
          },
          {
            "accuracy": 0.662568,
            "f1": 0.608065,
            "f1_weighted": 0.656373
          },
          {
            "accuracy": 0.658141,
            "f1": 0.59248,
            "f1_weighted": 0.662259
          },
          {
            "accuracy": 0.649779,
            "f1": 0.598547,
            "f1_weighted": 0.643054
          },
          {
            "accuracy": 0.637973,
            "f1": 0.603178,
            "f1_weighted": 0.631276
          },
          {
            "accuracy": 0.667978,
            "f1": 0.627905,
            "f1_weighted": 0.658208
          },
          {
            "accuracy": 0.647319,
            "f1": 0.580637,
            "f1_weighted": 0.639761
          },
          {
            "accuracy": 0.630103,
            "f1": 0.588263,
            "f1_weighted": 0.624023
          },
          {
            "accuracy": 0.638465,
            "f1": 0.594789,
            "f1_weighted": 0.633096
          }
        ],
        "main_score": 0.656665,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.653093,
        "f1": 0.621473,
        "f1_weighted": 0.64874,
        "scores_per_experiment": [
          {
            "accuracy": 0.678547,
            "f1": 0.640734,
            "f1_weighted": 0.675366
          },
          {
            "accuracy": 0.679892,
            "f1": 0.646688,
            "f1_weighted": 0.672277
          },
          {
            "accuracy": 0.650303,
            "f1": 0.616191,
            "f1_weighted": 0.646585
          },
          {
            "accuracy": 0.648958,
            "f1": 0.616942,
            "f1_weighted": 0.655676
          },
          {
            "accuracy": 0.638534,
            "f1": 0.605671,
            "f1_weighted": 0.631529
          },
          {
            "accuracy": 0.636853,
            "f1": 0.613947,
            "f1_weighted": 0.63209
          },
          {
            "accuracy": 0.676194,
            "f1": 0.652282,
            "f1_weighted": 0.665491
          },
          {
            "accuracy": 0.641224,
            "f1": 0.59011,
            "f1_weighted": 0.636521
          },
          {
            "accuracy": 0.640551,
            "f1": 0.614572,
            "f1_weighted": 0.633157
          },
          {
            "accuracy": 0.639879,
            "f1": 0.617593,
            "f1_weighted": 0.63871
          }
        ],
        "main_score": 0.653093,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 39.99903416633606,
  "kg_co2_emissions": null
}