{
  "dataset_revision": "a1bc0e8fd36c3d5015bd64c14ca098596774784a",
  "task_name": "WebFAQBitextMiningQAs",
  "mteb_version": "1.36.1",
  "scores": {
    "default": [
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "ara-fas",
        "languages": [
          "ara-Arab",
          "fas-Arab"
        ]
      },
      {
        "precision": 0.976312,
        "recall": 0.98364,
        "f1": 0.978698,
        "accuracy": 0.98364,
        "main_score": 0.978698,
        "hf_subset": "ara-heb",
        "languages": [
          "ara-Arab",
          "heb-Hebr"
        ]
      },
      {
        "precision": 0.994398,
        "recall": 0.996266,
        "f1": 0.995021,
        "accuracy": 0.996266,
        "main_score": 0.995021,
        "hf_subset": "jpn-kor",
        "languages": [
          "jpn-Jpan",
          "kor-Kore"
        ]
      },
      {
        "precision": 0.993363,
        "recall": 0.995575,
        "f1": 0.9941,
        "accuracy": 0.995575,
        "main_score": 0.9941,
        "hf_subset": "jpn-vie",
        "languages": [
          "jpn-Jpan",
          "vie-Latn"
        ]
      },
      {
        "precision": 0.991802,
        "recall": 0.994213,
        "f1": 0.992573,
        "accuracy": 0.994213,
        "main_score": 0.992573,
        "hf_subset": "jpn-zho",
        "languages": [
          "jpn-Jpan",
          "zho-Hans"
        ]
      },
      {
        "precision": 0.993867,
        "recall": 0.995671,
        "f1": 0.994468,
        "accuracy": 0.995671,
        "main_score": 0.994468,
        "hf_subset": "kor-vie",
        "languages": [
          "kor-Kore",
          "vie-Latn"
        ]
      },
      {
        "precision": 0.9931,
        "recall": 0.9954,
        "f1": 0.993867,
        "accuracy": 0.9954,
        "main_score": 0.993867,
        "hf_subset": "kor-zho",
        "languages": [
          "kor-Kore",
          "zho-Hans"
        ]
      },
      {
        "precision": 0.991228,
        "recall": 0.993808,
        "f1": 0.992002,
        "accuracy": 0.993808,
        "main_score": 0.992002,
        "hf_subset": "vie-zho",
        "languages": [
          "vie-Latn",
          "zho-Hans"
        ]
      },
      {
        "precision": 0.938462,
        "recall": 0.949451,
        "f1": 0.941685,
        "accuracy": 0.949451,
        "main_score": 0.941685,
        "hf_subset": "ind-msa",
        "languages": [
          "ind-Latn",
          "msa-Latn"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "ind-tgl",
        "languages": [
          "ind-Latn",
          "tgl-Latn"
        ]
      },
      {
        "precision": 0.978802,
        "recall": 0.985692,
        "f1": 0.981055,
        "accuracy": 0.985692,
        "main_score": 0.981055,
        "hf_subset": "ind-tha",
        "languages": [
          "ind-Latn",
          "tha-Thai"
        ]
      },
      {
        "precision": 0.99798,
        "recall": 0.998653,
        "f1": 0.998204,
        "accuracy": 0.998653,
        "main_score": 0.998204,
        "hf_subset": "bul-ces",
        "languages": [
          "bul-Cyrl",
          "ces-Latn"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "bul-lav",
        "languages": [
          "bul-Cyrl",
          "lav-Latn"
        ]
      },
      {
        "precision": 0.996264,
        "recall": 0.997509,
        "f1": 0.996679,
        "accuracy": 0.997509,
        "main_score": 0.996679,
        "hf_subset": "bul-lit",
        "languages": [
          "bul-Cyrl",
          "lit-Latn"
        ]
      },
      {
        "precision": 0.994801,
        "recall": 0.99633,
        "f1": 0.995311,
        "accuracy": 0.99633,
        "main_score": 0.995311,
        "hf_subset": "bul-pol",
        "languages": [
          "bul-Cyrl",
          "pol-Latn"
        ]
      },
      {
        "precision": 0.997177,
        "recall": 0.997967,
        "f1": 0.997403,
        "accuracy": 0.997967,
        "main_score": 0.997403,
        "hf_subset": "bul-rus",
        "languages": [
          "bul-Cyrl",
          "rus-Cyrl"
        ]
      },
      {
        "precision": 0.993501,
        "recall": 0.995667,
        "f1": 0.994223,
        "accuracy": 0.995667,
        "main_score": 0.994223,
        "hf_subset": "bul-slk",
        "languages": [
          "bul-Cyrl",
          "slk-Latn"
        ]
      },
      {
        "precision": 0.994197,
        "recall": 0.996132,
        "f1": 0.994842,
        "accuracy": 0.996132,
        "main_score": 0.994842,
        "hf_subset": "bul-slv",
        "languages": [
          "bul-Cyrl",
          "slv-Latn"
        ]
      },
      {
        "precision": 0.989865,
        "recall": 0.993243,
        "f1": 0.990991,
        "accuracy": 0.993243,
        "main_score": 0.990991,
        "hf_subset": "bul-srp",
        "languages": [
          "bul-Cyrl",
          "srp-Cyrl"
        ]
      },
      {
        "precision": 0.99581,
        "recall": 0.997207,
        "f1": 0.996276,
        "accuracy": 0.997207,
        "main_score": 0.996276,
        "hf_subset": "bul-ukr",
        "languages": [
          "bul-Cyrl",
          "ukr-Cyrl"
        ]
      },
      {
        "precision": 0.996571,
        "recall": 0.997714,
        "f1": 0.996952,
        "accuracy": 0.997714,
        "main_score": 0.996952,
        "hf_subset": "ces-lav",
        "languages": [
          "ces-Latn",
          "lav-Latn"
        ]
      },
      {
        "precision": 0.998503,
        "recall": 0.999002,
        "f1": 0.998669,
        "accuracy": 0.999002,
        "main_score": 0.998669,
        "hf_subset": "ces-lit",
        "languages": [
          "ces-Latn",
          "lit-Latn"
        ]
      },
      {
        "precision": 0.996881,
        "recall": 0.997921,
        "f1": 0.997228,
        "accuracy": 0.997921,
        "main_score": 0.997228,
        "hf_subset": "ces-pol",
        "languages": [
          "ces-Latn",
          "pol-Latn"
        ]
      },
      {
        "precision": 0.99246,
        "recall": 0.994869,
        "f1": 0.993237,
        "accuracy": 0.994869,
        "main_score": 0.993237,
        "hf_subset": "ces-rus",
        "languages": [
          "ces-Latn",
          "rus-Cyrl"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "ces-slk",
        "languages": [
          "ces-Latn",
          "slk-Latn"
        ]
      },
      {
        "precision": 0.989927,
        "recall": 0.992701,
        "f1": 0.990754,
        "accuracy": 0.992701,
        "main_score": 0.990754,
        "hf_subset": "ces-slv",
        "languages": [
          "ces-Latn",
          "slv-Latn"
        ]
      },
      {
        "precision": 0.987569,
        "recall": 0.991713,
        "f1": 0.98895,
        "accuracy": 0.991713,
        "main_score": 0.98895,
        "hf_subset": "ces-srp",
        "languages": [
          "ces-Latn",
          "srp-Cyrl"
        ]
      },
      {
        "precision": 0.983528,
        "recall": 0.988327,
        "f1": 0.985084,
        "accuracy": 0.988327,
        "main_score": 0.985084,
        "hf_subset": "ces-ukr",
        "languages": [
          "ces-Latn",
          "ukr-Cyrl"
        ]
      },
      {
        "precision": 0.990415,
        "recall": 0.99361,
        "f1": 0.99148,
        "accuracy": 0.99361,
        "main_score": 0.99148,
        "hf_subset": "hrv-slk",
        "languages": [
          "hrv-Latn",
          "slk-Latn"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "kat-rus",
        "languages": [
          "kat-Geor",
          "rus-Cyrl"
        ]
      },
      {
        "precision": 0.997172,
        "recall": 0.998115,
        "f1": 0.997487,
        "accuracy": 0.998115,
        "main_score": 0.997487,
        "hf_subset": "lav-lit",
        "languages": [
          "lav-Latn",
          "lit-Latn"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "lav-pol",
        "languages": [
          "lav-Latn",
          "pol-Latn"
        ]
      },
      {
        "precision": 0.997875,
        "recall": 0.998584,
        "f1": 0.998111,
        "accuracy": 0.998584,
        "main_score": 0.998111,
        "hf_subset": "lav-rus",
        "languages": [
          "lav-Latn",
          "rus-Cyrl"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "lav-slk",
        "languages": [
          "lav-Latn",
          "slk-Latn"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "lav-slv",
        "languages": [
          "lav-Latn",
          "slv-Latn"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "lav-ukr",
        "languages": [
          "lav-Latn",
          "ukr-Cyrl"
        ]
      },
      {
        "precision": 0.997076,
        "recall": 0.998051,
        "f1": 0.997401,
        "accuracy": 0.998051,
        "main_score": 0.997401,
        "hf_subset": "lit-pol",
        "languages": [
          "lit-Latn",
          "pol-Latn"
        ]
      },
      {
        "precision": 0.995317,
        "recall": 0.996878,
        "f1": 0.995838,
        "accuracy": 0.996878,
        "main_score": 0.995838,
        "hf_subset": "lit-rus",
        "languages": [
          "lit-Latn",
          "rus-Cyrl"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "lit-slk",
        "languages": [
          "lit-Latn",
          "slk-Latn"
        ]
      },
      {
        "precision": 0.990115,
        "recall": 0.99341,
        "f1": 0.991214,
        "accuracy": 0.99341,
        "main_score": 0.991214,
        "hf_subset": "lit-slv",
        "languages": [
          "lit-Latn",
          "slv-Latn"
        ]
      },
      {
        "precision": 0.997653,
        "recall": 0.998435,
        "f1": 0.997913,
        "accuracy": 0.998435,
        "main_score": 0.997913,
        "hf_subset": "lit-ukr",
        "languages": [
          "lit-Latn",
          "ukr-Cyrl"
        ]
      },
      {
        "precision": 0.991652,
        "recall": 0.994017,
        "f1": 0.992371,
        "accuracy": 0.994017,
        "main_score": 0.992371,
        "hf_subset": "pol-rus",
        "languages": [
          "pol-Latn",
          "rus-Cyrl"
        ]
      },
      {
        "precision": 0.997654,
        "recall": 0.998436,
        "f1": 0.997914,
        "accuracy": 0.998436,
        "main_score": 0.997914,
        "hf_subset": "pol-slk",
        "languages": [
          "pol-Latn",
          "slk-Latn"
        ]
      },
      {
        "precision": 0.990232,
        "recall": 0.993488,
        "f1": 0.991317,
        "accuracy": 0.993488,
        "main_score": 0.991317,
        "hf_subset": "pol-slv",
        "languages": [
          "pol-Latn",
          "slv-Latn"
        ]
      },
      {
        "precision": 0.970867,
        "recall": 0.979675,
        "f1": 0.973577,
        "accuracy": 0.979675,
        "main_score": 0.973577,
        "hf_subset": "pol-srp",
        "languages": [
          "pol-Latn",
          "srp-Cyrl"
        ]
      },
      {
        "precision": 0.992194,
        "recall": 0.994515,
        "f1": 0.992968,
        "accuracy": 0.994515,
        "main_score": 0.992968,
        "hf_subset": "pol-ukr",
        "languages": [
          "pol-Latn",
          "ukr-Cyrl"
        ]
      },
      {
        "precision": 0.995249,
        "recall": 0.996041,
        "f1": 0.995513,
        "accuracy": 0.996041,
        "main_score": 0.995513,
        "hf_subset": "rus-slk",
        "languages": [
          "rus-Cyrl",
          "slk-Latn"
        ]
      },
      {
        "precision": 0.991788,
        "recall": 0.994526,
        "f1": 0.992701,
        "accuracy": 0.994526,
        "main_score": 0.992701,
        "hf_subset": "rus-slv",
        "languages": [
          "rus-Cyrl",
          "slv-Latn"
        ]
      },
      {
        "precision": 0.973626,
        "recall": 0.982418,
        "f1": 0.976557,
        "accuracy": 0.982418,
        "main_score": 0.976557,
        "hf_subset": "rus-srp",
        "languages": [
          "rus-Cyrl",
          "srp-Cyrl"
        ]
      },
      {
        "precision": 0.995153,
        "recall": 0.996459,
        "f1": 0.995551,
        "accuracy": 0.996459,
        "main_score": 0.995551,
        "hf_subset": "rus-ukr",
        "languages": [
          "rus-Cyrl",
          "ukr-Cyrl"
        ]
      },
      {
        "precision": 0.995234,
        "recall": 0.996823,
        "f1": 0.995764,
        "accuracy": 0.996823,
        "main_score": 0.995764,
        "hf_subset": "slk-slv",
        "languages": [
          "slk-Latn",
          "slv-Latn"
        ]
      },
      {
        "precision": 0.973856,
        "recall": 0.982175,
        "f1": 0.97653,
        "accuracy": 0.982175,
        "main_score": 0.97653,
        "hf_subset": "slk-srp",
        "languages": [
          "slk-Latn",
          "srp-Cyrl"
        ]
      },
      {
        "precision": 0.998411,
        "recall": 0.998941,
        "f1": 0.998588,
        "accuracy": 0.998941,
        "main_score": 0.998588,
        "hf_subset": "slk-ukr",
        "languages": [
          "slk-Latn",
          "ukr-Cyrl"
        ]
      },
      {
        "precision": 0.985972,
        "recall": 0.98998,
        "f1": 0.987308,
        "accuracy": 0.98998,
        "main_score": 0.987308,
        "hf_subset": "slv-srp",
        "languages": [
          "slv-Latn",
          "srp-Cyrl"
        ]
      },
      {
        "precision": 0.983629,
        "recall": 0.989086,
        "f1": 0.985448,
        "accuracy": 0.989086,
        "main_score": 0.985448,
        "hf_subset": "slv-ukr",
        "languages": [
          "slv-Latn",
          "ukr-Cyrl"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "cat-deu",
        "languages": [
          "cat-Latn",
          "deu-Latn"
        ]
      },
      {
        "precision": 0.992475,
        "recall": 0.994983,
        "f1": 0.993311,
        "accuracy": 0.994983,
        "main_score": 0.993311,
        "hf_subset": "cat-fra",
        "languages": [
          "cat-Latn",
          "fra-Latn"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "cat-ita",
        "languages": [
          "cat-Latn",
          "ita-Latn"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "cat-por",
        "languages": [
          "cat-Latn",
          "por-Latn"
        ]
      },
      {
        "precision": 0.998426,
        "recall": 0.998867,
        "f1": 0.998552,
        "accuracy": 0.998867,
        "main_score": 0.998552,
        "hf_subset": "cat-spa",
        "languages": [
          "cat-Latn",
          "spa-Latn"
        ]
      },
      {
        "precision": 0.995927,
        "recall": 0.997233,
        "f1": 0.996349,
        "accuracy": 0.997233,
        "main_score": 0.996349,
        "hf_subset": "dan-deu",
        "languages": [
          "dan-Latn",
          "deu-Latn"
        ]
      },
      {
        "precision": 0.996142,
        "recall": 0.997107,
        "f1": 0.996449,
        "accuracy": 0.997107,
        "main_score": 0.996449,
        "hf_subset": "dan-fra",
        "languages": [
          "dan-Latn",
          "fra-Latn"
        ]
      },
      {
        "precision": 0.990826,
        "recall": 0.993884,
        "f1": 0.991845,
        "accuracy": 0.993884,
        "main_score": 0.991845,
        "hf_subset": "dan-isl",
        "languages": [
          "dan-Latn",
          "isl-Latn"
        ]
      },
      {
        "precision": 0.995416,
        "recall": 0.996857,
        "f1": 0.995897,
        "accuracy": 0.996857,
        "main_score": 0.995897,
        "hf_subset": "dan-ita",
        "languages": [
          "dan-Latn",
          "ita-Latn"
        ]
      },
      {
        "precision": 0.998414,
        "recall": 0.99878,
        "f1": 0.998536,
        "accuracy": 0.99878,
        "main_score": 0.998536,
        "hf_subset": "dan-nld",
        "languages": [
          "dan-Latn",
          "nld-Latn"
        ]
      },
      {
        "precision": 0.99635,
        "recall": 0.997311,
        "f1": 0.996671,
        "accuracy": 0.997311,
        "main_score": 0.996671,
        "hf_subset": "dan-nor",
        "languages": [
          "dan-Latn",
          "nor-Latn"
        ]
      },
      {
        "precision": 0.994386,
        "recall": 0.996257,
        "f1": 0.995009,
        "accuracy": 0.996257,
        "main_score": 0.995009,
        "hf_subset": "dan-por",
        "languages": [
          "dan-Latn",
          "por-Latn"
        ]
      },
      {
        "precision": 0.996345,
        "recall": 0.997563,
        "f1": 0.996751,
        "accuracy": 0.997563,
        "main_score": 0.996751,
        "hf_subset": "dan-ron",
        "languages": [
          "dan-Latn",
          "ron-Latn"
        ]
      },
      {
        "precision": 0.99664,
        "recall": 0.99776,
        "f1": 0.997013,
        "accuracy": 0.99776,
        "main_score": 0.997013,
        "hf_subset": "dan-spa",
        "languages": [
          "dan-Latn",
          "spa-Latn"
        ]
      },
      {
        "precision": 0.998243,
        "recall": 0.998828,
        "f1": 0.998438,
        "accuracy": 0.998828,
        "main_score": 0.998438,
        "hf_subset": "dan-swe",
        "languages": [
          "dan-Latn",
          "swe-Latn"
        ]
      },
      {
        "precision": 0.996189,
        "recall": 0.997439,
        "f1": 0.996604,
        "accuracy": 0.997439,
        "main_score": 0.996604,
        "hf_subset": "deu-fra",
        "languages": [
          "deu-Latn",
          "fra-Latn"
        ]
      },
      {
        "precision": 0.97449,
        "recall": 0.982993,
        "f1": 0.977324,
        "accuracy": 0.982993,
        "main_score": 0.977324,
        "hf_subset": "deu-isl",
        "languages": [
          "deu-Latn",
          "isl-Latn"
        ]
      },
      {
        "precision": 0.996602,
        "recall": 0.997658,
        "f1": 0.996948,
        "accuracy": 0.997658,
        "main_score": 0.996948,
        "hf_subset": "deu-ita",
        "languages": [
          "deu-Latn",
          "ita-Latn"
        ]
      },
      {
        "precision": 0.996974,
        "recall": 0.997889,
        "f1": 0.997279,
        "accuracy": 0.997889,
        "main_score": 0.997279,
        "hf_subset": "deu-nld",
        "languages": [
          "deu-Latn",
          "nld-Latn"
        ]
      },
      {
        "precision": 0.997305,
        "recall": 0.998203,
        "f1": 0.997605,
        "accuracy": 0.998203,
        "main_score": 0.997605,
        "hf_subset": "deu-nor",
        "languages": [
          "deu-Latn",
          "nor-Latn"
        ]
      },
      {
        "precision": 0.99598,
        "recall": 0.997261,
        "f1": 0.996407,
        "accuracy": 0.997261,
        "main_score": 0.996407,
        "hf_subset": "deu-por",
        "languages": [
          "deu-Latn",
          "por-Latn"
        ]
      },
      {
        "precision": 0.997916,
        "recall": 0.99861,
        "f1": 0.998147,
        "accuracy": 0.99861,
        "main_score": 0.998147,
        "hf_subset": "deu-ron",
        "languages": [
          "deu-Latn",
          "ron-Latn"
        ]
      },
      {
        "precision": 0.996054,
        "recall": 0.997214,
        "f1": 0.996415,
        "accuracy": 0.997214,
        "main_score": 0.996415,
        "hf_subset": "deu-spa",
        "languages": [
          "deu-Latn",
          "spa-Latn"
        ]
      },
      {
        "precision": 0.997401,
        "recall": 0.998267,
        "f1": 0.99769,
        "accuracy": 0.998267,
        "main_score": 0.99769,
        "hf_subset": "deu-swe",
        "languages": [
          "deu-Latn",
          "swe-Latn"
        ]
      },
      {
        "precision": 0.987032,
        "recall": 0.991354,
        "f1": 0.988473,
        "accuracy": 0.991354,
        "main_score": 0.988473,
        "hf_subset": "fra-isl",
        "languages": [
          "fra-Latn",
          "isl-Latn"
        ]
      },
      {
        "precision": 0.995925,
        "recall": 0.9972,
        "f1": 0.996342,
        "accuracy": 0.9972,
        "main_score": 0.996342,
        "hf_subset": "fra-ita",
        "languages": [
          "fra-Latn",
          "ita-Latn"
        ]
      },
      {
        "precision": 0.996731,
        "recall": 0.997821,
        "f1": 0.997094,
        "accuracy": 0.997821,
        "main_score": 0.997094,
        "hf_subset": "fra-nld",
        "languages": [
          "fra-Latn",
          "nld-Latn"
        ]
      },
      {
        "precision": 0.996286,
        "recall": 0.997263,
        "f1": 0.996612,
        "accuracy": 0.997263,
        "main_score": 0.996612,
        "hf_subset": "fra-nor",
        "languages": [
          "fra-Latn",
          "nor-Latn"
        ]
      },
      {
        "precision": 0.995213,
        "recall": 0.996683,
        "f1": 0.995703,
        "accuracy": 0.996683,
        "main_score": 0.995703,
        "hf_subset": "fra-por",
        "languages": [
          "fra-Latn",
          "por-Latn"
        ]
      },
      {
        "precision": 0.996358,
        "recall": 0.997572,
        "f1": 0.996763,
        "accuracy": 0.997572,
        "main_score": 0.996763,
        "hf_subset": "fra-ron",
        "languages": [
          "fra-Latn",
          "ron-Latn"
        ]
      },
      {
        "precision": 0.99517,
        "recall": 0.996654,
        "f1": 0.995656,
        "accuracy": 0.996654,
        "main_score": 0.995656,
        "hf_subset": "fra-spa",
        "languages": [
          "fra-Latn",
          "spa-Latn"
        ]
      },
      {
        "precision": 0.997603,
        "recall": 0.998402,
        "f1": 0.997869,
        "accuracy": 0.998402,
        "main_score": 0.997869,
        "hf_subset": "fra-swe",
        "languages": [
          "fra-Latn",
          "swe-Latn"
        ]
      },
      {
        "precision": 0.996437,
        "recall": 0.997625,
        "f1": 0.996833,
        "accuracy": 0.997625,
        "main_score": 0.996833,
        "hf_subset": "isl-ita",
        "languages": [
          "isl-Latn",
          "ita-Latn"
        ]
      },
      {
        "precision": 0.985531,
        "recall": 0.990354,
        "f1": 0.987138,
        "accuracy": 0.990354,
        "main_score": 0.987138,
        "hf_subset": "isl-nld",
        "languages": [
          "isl-Latn",
          "nld-Latn"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "isl-por",
        "languages": [
          "isl-Latn",
          "por-Latn"
        ]
      },
      {
        "precision": 0.995902,
        "recall": 0.997268,
        "f1": 0.996357,
        "accuracy": 0.997268,
        "main_score": 0.996357,
        "hf_subset": "isl-spa",
        "languages": [
          "isl-Latn",
          "spa-Latn"
        ]
      },
      {
        "precision": 0.990385,
        "recall": 0.99359,
        "f1": 0.991453,
        "accuracy": 0.99359,
        "main_score": 0.991453,
        "hf_subset": "isl-swe",
        "languages": [
          "isl-Latn",
          "swe-Latn"
        ]
      },
      {
        "precision": 0.996889,
        "recall": 0.997926,
        "f1": 0.997234,
        "accuracy": 0.997926,
        "main_score": 0.997234,
        "hf_subset": "ita-nld",
        "languages": [
          "ita-Latn",
          "nld-Latn"
        ]
      },
      {
        "precision": 0.992647,
        "recall": 0.994833,
        "f1": 0.993376,
        "accuracy": 0.994833,
        "main_score": 0.993376,
        "hf_subset": "ita-nor",
        "languages": [
          "ita-Latn",
          "nor-Latn"
        ]
      },
      {
        "precision": 0.995469,
        "recall": 0.996979,
        "f1": 0.995972,
        "accuracy": 0.996979,
        "main_score": 0.995972,
        "hf_subset": "ita-por",
        "languages": [
          "ita-Latn",
          "por-Latn"
        ]
      },
      {
        "precision": 0.995982,
        "recall": 0.997321,
        "f1": 0.996429,
        "accuracy": 0.997321,
        "main_score": 0.996429,
        "hf_subset": "ita-ron",
        "languages": [
          "ita-Latn",
          "ron-Latn"
        ]
      },
      {
        "precision": 0.99496,
        "recall": 0.996553,
        "f1": 0.995484,
        "accuracy": 0.996553,
        "main_score": 0.995484,
        "hf_subset": "ita-spa",
        "languages": [
          "ita-Latn",
          "spa-Latn"
        ]
      },
      {
        "precision": 0.997152,
        "recall": 0.998102,
        "f1": 0.997469,
        "accuracy": 0.998102,
        "main_score": 0.997469,
        "hf_subset": "ita-swe",
        "languages": [
          "ita-Latn",
          "swe-Latn"
        ]
      },
      {
        "precision": 0.997748,
        "recall": 0.998498,
        "f1": 0.997998,
        "accuracy": 0.998498,
        "main_score": 0.997998,
        "hf_subset": "nld-nor",
        "languages": [
          "nld-Latn",
          "nor-Latn"
        ]
      },
      {
        "precision": 0.995205,
        "recall": 0.996724,
        "f1": 0.995703,
        "accuracy": 0.996724,
        "main_score": 0.995703,
        "hf_subset": "nld-por",
        "languages": [
          "nld-Latn",
          "por-Latn"
        ]
      },
      {
        "precision": 0.994922,
        "recall": 0.996537,
        "f1": 0.995441,
        "accuracy": 0.996537,
        "main_score": 0.995441,
        "hf_subset": "nld-ron",
        "languages": [
          "nld-Latn",
          "ron-Latn"
        ]
      },
      {
        "precision": 0.996703,
        "recall": 0.997802,
        "f1": 0.99707,
        "accuracy": 0.997802,
        "main_score": 0.99707,
        "hf_subset": "nld-spa",
        "languages": [
          "nld-Latn",
          "spa-Latn"
        ]
      },
      {
        "precision": 0.997404,
        "recall": 0.998226,
        "f1": 0.997667,
        "accuracy": 0.998226,
        "main_score": 0.997667,
        "hf_subset": "nld-swe",
        "languages": [
          "nld-Latn",
          "swe-Latn"
        ]
      },
      {
        "precision": 0.996422,
        "recall": 0.997615,
        "f1": 0.996819,
        "accuracy": 0.997615,
        "main_score": 0.996819,
        "hf_subset": "nor-por",
        "languages": [
          "nor-Latn",
          "por-Latn"
        ]
      },
      {
        "precision": 0.997875,
        "recall": 0.998584,
        "f1": 0.998111,
        "accuracy": 0.998584,
        "main_score": 0.998111,
        "hf_subset": "nor-ron",
        "languages": [
          "nor-Latn",
          "ron-Latn"
        ]
      },
      {
        "precision": 0.997119,
        "recall": 0.998079,
        "f1": 0.997439,
        "accuracy": 0.998079,
        "main_score": 0.997439,
        "hf_subset": "nor-spa",
        "languages": [
          "nor-Latn",
          "spa-Latn"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "nor-swe",
        "languages": [
          "nor-Latn",
          "swe-Latn"
        ]
      },
      {
        "precision": 0.996034,
        "recall": 0.997356,
        "f1": 0.996475,
        "accuracy": 0.997356,
        "main_score": 0.996475,
        "hf_subset": "por-ron",
        "languages": [
          "por-Latn",
          "ron-Latn"
        ]
      },
      {
        "precision": 0.996062,
        "recall": 0.997264,
        "f1": 0.996456,
        "accuracy": 0.997264,
        "main_score": 0.996456,
        "hf_subset": "por-spa",
        "languages": [
          "por-Latn",
          "spa-Latn"
        ]
      },
      {
        "precision": 0.996104,
        "recall": 0.997403,
        "f1": 0.996537,
        "accuracy": 0.997403,
        "main_score": 0.996537,
        "hf_subset": "por-swe",
        "languages": [
          "por-Latn",
          "swe-Latn"
        ]
      },
      {
        "precision": 0.997037,
        "recall": 0.997926,
        "f1": 0.997333,
        "accuracy": 0.997926,
        "main_score": 0.997333,
        "hf_subset": "ron-spa",
        "languages": [
          "ron-Latn",
          "spa-Latn"
        ]
      },
      {
        "precision": 0.994429,
        "recall": 0.996286,
        "f1": 0.995048,
        "accuracy": 0.996286,
        "main_score": 0.995048,
        "hf_subset": "ron-swe",
        "languages": [
          "ron-Latn",
          "swe-Latn"
        ]
      },
      {
        "precision": 0.997816,
        "recall": 0.998362,
        "f1": 0.997987,
        "accuracy": 0.998362,
        "main_score": 0.997987,
        "hf_subset": "spa-swe",
        "languages": [
          "spa-Latn",
          "swe-Latn"
        ]
      },
      {
        "precision": 0.998722,
        "recall": 0.999148,
        "f1": 0.998864,
        "accuracy": 0.999148,
        "main_score": 0.998864,
        "hf_subset": "ben-hin",
        "languages": [
          "ben-Beng",
          "hin-Deva"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "ben-mar",
        "languages": [
          "ben-Beng",
          "mar-Deva"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "ben-urd",
        "languages": [
          "ben-Beng",
          "urd-Arab"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "hin-mar",
        "languages": [
          "hin-Deva",
          "mar-Deva"
        ]
      },
      {
        "precision": 0.997248,
        "recall": 0.998165,
        "f1": 0.997554,
        "accuracy": 0.998165,
        "main_score": 0.997554,
        "hf_subset": "hin-urd",
        "languages": [
          "hin-Deva",
          "urd-Arab"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "mar-urd",
        "languages": [
          "mar-Deva",
          "urd-Arab"
        ]
      },
      {
        "precision": 0.996359,
        "recall": 0.997573,
        "f1": 0.996764,
        "accuracy": 0.997573,
        "main_score": 0.996764,
        "hf_subset": "aze-kaz",
        "languages": [
          "aze-Latn",
          "kaz-Cyrl"
        ]
      },
      {
        "precision": 0.992268,
        "recall": 0.994845,
        "f1": 0.993127,
        "accuracy": 0.994845,
        "main_score": 0.993127,
        "hf_subset": "aze-tur",
        "languages": [
          "aze-Latn",
          "tur-Latn"
        ]
      },
      {
        "precision": 0.995588,
        "recall": 0.997059,
        "f1": 0.996078,
        "accuracy": 0.997059,
        "main_score": 0.996078,
        "hf_subset": "kaz-tur",
        "languages": [
          "kaz-Cyrl",
          "tur-Latn"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "est-fin",
        "languages": [
          "est-Latn",
          "fin-Latn"
        ]
      },
      {
        "precision": 0.995549,
        "recall": 0.997033,
        "f1": 0.996044,
        "accuracy": 0.997033,
        "main_score": 0.996044,
        "hf_subset": "est-hun",
        "languages": [
          "est-Latn",
          "hun-Latn"
        ]
      },
      {
        "precision": 0.996109,
        "recall": 0.997406,
        "f1": 0.996541,
        "accuracy": 0.997406,
        "main_score": 0.996541,
        "hf_subset": "fin-hun",
        "languages": [
          "fin-Latn",
          "hun-Latn"
        ]
      },
      {
        "precision": 0.9905,
        "recall": 0.993331,
        "f1": 0.991371,
        "accuracy": 0.993331,
        "main_score": 0.991371,
        "hf_subset": "ara-eng",
        "languages": [
          "ara-Arab",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.997512,
        "recall": 0.998342,
        "f1": 0.997789,
        "accuracy": 0.998342,
        "main_score": 0.997789,
        "hf_subset": "aze-eng",
        "languages": [
          "aze-Latn",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.998903,
        "recall": 0.999268,
        "f1": 0.999025,
        "accuracy": 0.999268,
        "main_score": 0.999025,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.99789,
        "recall": 0.998594,
        "f1": 0.998125,
        "accuracy": 0.998594,
        "main_score": 0.998125,
        "hf_subset": "bul-eng",
        "languages": [
          "bul-Cyrl",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.996094,
        "recall": 0.997396,
        "f1": 0.996528,
        "accuracy": 0.997396,
        "main_score": 0.996528,
        "hf_subset": "cat-eng",
        "languages": [
          "cat-Latn",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.997219,
        "recall": 0.998146,
        "f1": 0.997528,
        "accuracy": 0.998146,
        "main_score": 0.997528,
        "hf_subset": "ces-eng",
        "languages": [
          "ces-Latn",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.995493,
        "recall": 0.996897,
        "f1": 0.995937,
        "accuracy": 0.996897,
        "main_score": 0.995937,
        "hf_subset": "dan-eng",
        "languages": [
          "dan-Latn",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.99776,
        "recall": 0.998474,
        "f1": 0.997996,
        "accuracy": 0.998474,
        "main_score": 0.997996,
        "hf_subset": "deu-eng",
        "languages": [
          "deu-Latn",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.995759,
        "recall": 0.996774,
        "f1": 0.99604,
        "accuracy": 0.996774,
        "main_score": 0.99604,
        "hf_subset": "ell-eng",
        "languages": [
          "ell-Grek",
          "eng-Latn"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "eng-est",
        "languages": [
          "eng-Latn",
          "est-Latn"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "eng-fas",
        "languages": [
          "eng-Latn",
          "fas-Arab"
        ]
      },
      {
        "precision": 0.997822,
        "recall": 0.998548,
        "f1": 0.998064,
        "accuracy": 0.998548,
        "main_score": 0.998064,
        "hf_subset": "eng-fin",
        "languages": [
          "eng-Latn",
          "fin-Latn"
        ]
      },
      {
        "precision": 0.996646,
        "recall": 0.997662,
        "f1": 0.996975,
        "accuracy": 0.997662,
        "main_score": 0.996975,
        "hf_subset": "eng-fra",
        "languages": [
          "eng-Latn",
          "fra-Latn"
        ]
      },
      {
        "precision": 0.981293,
        "recall": 0.987528,
        "f1": 0.983371,
        "accuracy": 0.987528,
        "main_score": 0.983371,
        "hf_subset": "eng-heb",
        "languages": [
          "eng-Latn",
          "heb-Hebr"
        ]
      },
      {
        "precision": 0.99632,
        "recall": 0.997296,
        "f1": 0.99662,
        "accuracy": 0.997296,
        "main_score": 0.99662,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "eng-hrv",
        "languages": [
          "eng-Latn",
          "hrv-Latn"
        ]
      },
      {
        "precision": 0.996796,
        "recall": 0.997712,
        "f1": 0.997101,
        "accuracy": 0.997712,
        "main_score": 0.997101,
        "hf_subset": "eng-hun",
        "languages": [
          "eng-Latn",
          "hun-Latn"
        ]
      },
      {
        "precision": 0.996526,
        "recall": 0.997684,
        "f1": 0.996912,
        "accuracy": 0.997684,
        "main_score": 0.996912,
        "hf_subset": "eng-ind",
        "languages": [
          "eng-Latn",
          "ind-Latn"
        ]
      },
      {
        "precision": 0.988827,
        "recall": 0.99162,
        "f1": 0.989758,
        "accuracy": 0.99162,
        "main_score": 0.989758,
        "hf_subset": "eng-isl",
        "languages": [
          "eng-Latn",
          "isl-Latn"
        ]
      },
      {
        "precision": 0.997872,
        "recall": 0.998525,
        "f1": 0.998084,
        "accuracy": 0.998525,
        "main_score": 0.998084,
        "hf_subset": "eng-ita",
        "languages": [
          "eng-Latn",
          "ita-Latn"
        ]
      },
      {
        "precision": 0.996541,
        "recall": 0.997373,
        "f1": 0.996804,
        "accuracy": 0.997373,
        "main_score": 0.996804,
        "hf_subset": "eng-jpn",
        "languages": [
          "eng-Latn",
          "jpn-Jpan"
        ]
      },
      {
        "precision": 0.995665,
        "recall": 0.99711,
        "f1": 0.996146,
        "accuracy": 0.99711,
        "main_score": 0.996146,
        "hf_subset": "eng-kaz",
        "languages": [
          "eng-Latn",
          "kaz-Cyrl"
        ]
      },
      {
        "precision": 0.995439,
        "recall": 0.996873,
        "f1": 0.995895,
        "accuracy": 0.996873,
        "main_score": 0.995895,
        "hf_subset": "eng-kor",
        "languages": [
          "eng-Latn",
          "kor-Kore"
        ]
      },
      {
        "precision": 0.994439,
        "recall": 0.996293,
        "f1": 0.995057,
        "accuracy": 0.996293,
        "main_score": 0.995057,
        "hf_subset": "eng-lav",
        "languages": [
          "eng-Latn",
          "lav-Latn"
        ]
      },
      {
        "precision": 0.998734,
        "recall": 0.999156,
        "f1": 0.998875,
        "accuracy": 0.999156,
        "main_score": 0.998875,
        "hf_subset": "eng-lit",
        "languages": [
          "eng-Latn",
          "lit-Latn"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ]
      },
      {
        "precision": 0.99467,
        "recall": 0.995736,
        "f1": 0.995025,
        "accuracy": 0.995736,
        "main_score": 0.995025,
        "hf_subset": "eng-msa",
        "languages": [
          "eng-Latn",
          "msa-Latn"
        ]
      },
      {
        "precision": 0.998324,
        "recall": 0.998847,
        "f1": 0.998495,
        "accuracy": 0.998847,
        "main_score": 0.998495,
        "hf_subset": "eng-nld",
        "languages": [
          "eng-Latn",
          "nld-Latn"
        ]
      },
      {
        "precision": 0.999437,
        "recall": 0.999625,
        "f1": 0.9995,
        "accuracy": 0.999625,
        "main_score": 0.9995,
        "hf_subset": "eng-nor",
        "languages": [
          "eng-Latn",
          "nor-Latn"
        ]
      },
      {
        "precision": 0.996942,
        "recall": 0.997962,
        "f1": 0.997282,
        "accuracy": 0.997962,
        "main_score": 0.997282,
        "hf_subset": "eng-pol",
        "languages": [
          "eng-Latn",
          "pol-Latn"
        ]
      },
      {
        "precision": 0.997542,
        "recall": 0.998307,
        "f1": 0.997797,
        "accuracy": 0.998307,
        "main_score": 0.997797,
        "hf_subset": "eng-por",
        "languages": [
          "eng-Latn",
          "por-Latn"
        ]
      },
      {
        "precision": 0.998026,
        "recall": 0.998684,
        "f1": 0.998245,
        "accuracy": 0.998684,
        "main_score": 0.998245,
        "hf_subset": "eng-ron",
        "languages": [
          "eng-Latn",
          "ron-Latn"
        ]
      },
      {
        "precision": 0.995949,
        "recall": 0.997115,
        "f1": 0.996321,
        "accuracy": 0.997115,
        "main_score": 0.996321,
        "hf_subset": "eng-rus",
        "languages": [
          "eng-Latn",
          "rus-Cyrl"
        ]
      },
      {
        "precision": 0.999177,
        "recall": 0.999451,
        "f1": 0.999269,
        "accuracy": 0.999451,
        "main_score": 0.999269,
        "hf_subset": "eng-slk",
        "languages": [
          "eng-Latn",
          "slk-Latn"
        ]
      },
      {
        "precision": 0.995057,
        "recall": 0.996552,
        "f1": 0.995517,
        "accuracy": 0.996552,
        "main_score": 0.995517,
        "hf_subset": "eng-slv",
        "languages": [
          "eng-Latn",
          "slv-Latn"
        ]
      },
      {
        "precision": 0.995267,
        "recall": 0.996474,
        "f1": 0.995655,
        "accuracy": 0.996474,
        "main_score": 0.995655,
        "hf_subset": "eng-spa",
        "languages": [
          "eng-Latn",
          "spa-Latn"
        ]
      },
      {
        "precision": 0.986249,
        "recall": 0.990099,
        "f1": 0.987349,
        "accuracy": 0.990099,
        "main_score": 0.987349,
        "hf_subset": "eng-srp",
        "languages": [
          "eng-Latn",
          "srp-Cyrl"
        ]
      },
      {
        "precision": 0.998751,
        "recall": 0.999167,
        "f1": 0.99889,
        "accuracy": 0.999167,
        "main_score": 0.99889,
        "hf_subset": "eng-swe",
        "languages": [
          "eng-Latn",
          "swe-Latn"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "eng-tgl",
        "languages": [
          "eng-Latn",
          "tgl-Latn"
        ]
      },
      {
        "precision": 0.982596,
        "recall": 0.987715,
        "f1": 0.984234,
        "accuracy": 0.987715,
        "main_score": 0.984234,
        "hf_subset": "eng-tha",
        "languages": [
          "eng-Latn",
          "tha-Thai"
        ]
      },
      {
        "precision": 0.99649,
        "recall": 0.997612,
        "f1": 0.996852,
        "accuracy": 0.997612,
        "main_score": 0.996852,
        "hf_subset": "eng-tur",
        "languages": [
          "eng-Latn",
          "tur-Latn"
        ]
      },
      {
        "precision": 0.994471,
        "recall": 0.9955,
        "f1": 0.994759,
        "accuracy": 0.9955,
        "main_score": 0.994759,
        "hf_subset": "eng-ukr",
        "languages": [
          "eng-Latn",
          "ukr-Cyrl"
        ]
      },
      {
        "precision": 0.994403,
        "recall": 0.996269,
        "f1": 0.995025,
        "accuracy": 0.996269,
        "main_score": 0.995025,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ]
      },
      {
        "precision": 0.994066,
        "recall": 0.996044,
        "f1": 0.994726,
        "accuracy": 0.996044,
        "main_score": 0.994726,
        "hf_subset": "eng-vie",
        "languages": [
          "eng-Latn",
          "vie-Latn"
        ]
      },
      {
        "precision": 0.994253,
        "recall": 0.995564,
        "f1": 0.99469,
        "accuracy": 0.995564,
        "main_score": 0.99469,
        "hf_subset": "eng-zho",
        "languages": [
          "eng-Latn",
          "zho-Hans"
        ]
      }
    ]
  },
  "evaluation_time": 10145.059701681137,
  "kg_co2_emissions": null
}