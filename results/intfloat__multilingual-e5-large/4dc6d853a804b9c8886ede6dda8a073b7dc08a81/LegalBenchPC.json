{
  "dataset_revision": "12ca3b695563788fead87a982ad1a068284413f4",
  "evaluation_time": 3.326507091522217,
  "kg_co2_emissions": 0.0002236993411214232,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.69384765625,
          "accuracy_threshold": 0.7717199325561523,
          "ap": 0.7867494798750736,
          "f1": 0.773421926910299,
          "f1_threshold": 0.7408546209335327,
          "precision": 0.650642817216322,
          "recall": 0.9533169533169533
        },
        "dot": {
          "accuracy": 0.69384765625,
          "accuracy_threshold": 0.7717200517654419,
          "ap": 0.7867495996368026,
          "f1": 0.773421926910299,
          "f1_threshold": 0.7408543229103088,
          "precision": 0.650642817216322,
          "recall": 0.9533169533169533
        },
        "euclidean": {
          "accuracy": 0.69384765625,
          "accuracy_threshold": 0.6756922602653503,
          "ap": 0.7867498350108902,
          "f1": 0.773421926910299,
          "f1_threshold": 0.7199240922927856,
          "precision": 0.650642817216322,
          "recall": 0.9533169533169533
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.69384765625,
        "manhattan": {
          "accuracy": 0.68896484375,
          "accuracy_threshold": 17.297101974487305,
          "ap": 0.7858867487040756,
          "f1": 0.7712287712287712,
          "f1_threshold": 18.31766128540039,
          "precision": 0.6498316498316499,
          "recall": 0.9484029484029484
        },
        "max": {
          "accuracy": 0.69384765625,
          "ap": 0.7867498350108902,
          "f1": 0.773421926910299
        },
        "similarity": {
          "accuracy": 0.69384765625,
          "accuracy_threshold": 0.7717199325561523,
          "ap": 0.7867494798750736,
          "f1": 0.773421926910299,
          "f1_threshold": 0.7408546209335327,
          "precision": 0.650642817216322,
          "recall": 0.9533169533169533
        }
      }
    ]
  },
  "task_name": "LegalBenchPC"
}