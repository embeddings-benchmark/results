{
  "dataset_revision": "b07c6ce548f6a7ac8d546e1bbe197a0086409190",
  "task_name": "SweRecClassification",
  "mteb_version": "2.3.9",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.615723,
            "f1": 0.529862,
            "f1_weighted": 0.624543,
            "precision": 0.537613,
            "precision_weighted": 0.647525,
            "recall": 0.53441,
            "recall_weighted": 0.615723,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.594238,
            "f1": 0.553597,
            "f1_weighted": 0.635863,
            "precision": 0.59772,
            "precision_weighted": 0.721437,
            "recall": 0.580913,
            "recall_weighted": 0.594238,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.635742,
            "f1": 0.581156,
            "f1_weighted": 0.668815,
            "precision": 0.606821,
            "precision_weighted": 0.73144,
            "recall": 0.602388,
            "recall_weighted": 0.635742,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.600098,
            "f1": 0.546523,
            "f1_weighted": 0.639144,
            "precision": 0.585806,
            "precision_weighted": 0.710846,
            "recall": 0.557855,
            "recall_weighted": 0.600098,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.503906,
            "f1": 0.454527,
            "f1_weighted": 0.541419,
            "precision": 0.489589,
            "precision_weighted": 0.607407,
            "recall": 0.464073,
            "recall_weighted": 0.503906,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.619141,
            "f1": 0.546118,
            "f1_weighted": 0.645955,
            "precision": 0.586552,
            "precision_weighted": 0.7122,
            "recall": 0.554574,
            "recall_weighted": 0.619141,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.589844,
            "f1": 0.54067,
            "f1_weighted": 0.625396,
            "precision": 0.574847,
            "precision_weighted": 0.698583,
            "recall": 0.560555,
            "recall_weighted": 0.589844,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.642578,
            "f1": 0.578293,
            "f1_weighted": 0.664526,
            "precision": 0.587875,
            "precision_weighted": 0.699109,
            "recall": 0.592244,
            "recall_weighted": 0.642578,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.592773,
            "f1": 0.540198,
            "f1_weighted": 0.622208,
            "precision": 0.579774,
            "precision_weighted": 0.705487,
            "recall": 0.56515,
            "recall_weighted": 0.592773,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.676758,
            "f1": 0.571471,
            "f1_weighted": 0.679242,
            "precision": 0.581911,
            "precision_weighted": 0.693573,
            "recall": 0.571073,
            "recall_weighted": 0.676758,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.60708,
        "f1": 0.544241,
        "f1_weighted": 0.634711,
        "precision": 0.572851,
        "precision_weighted": 0.692761,
        "recall": 0.558323,
        "recall_weighted": 0.60708,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.60708,
        "hf_subset": "default",
        "languages": [
          "swe-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 27.80238938331604,
  "kg_co2_emissions": null
}