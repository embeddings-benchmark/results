{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "task_name": "InappropriatenessClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.574756,
        "f1": 0.571249,
        "f1_weighted": 0.571249,
        "ap": 0.544651,
        "ap_weighted": 0.544651,
        "scores_per_experiment": [
          {
            "accuracy": 0.599121,
            "f1": 0.596502,
            "f1_weighted": 0.596502,
            "ap": 0.558022,
            "ap_weighted": 0.558022
          },
          {
            "accuracy": 0.633301,
            "f1": 0.633276,
            "f1_weighted": 0.633276,
            "ap": 0.584129,
            "ap_weighted": 0.584129
          },
          {
            "accuracy": 0.513672,
            "f1": 0.512265,
            "f1_weighted": 0.512265,
            "ap": 0.507005,
            "ap_weighted": 0.507005
          },
          {
            "accuracy": 0.577637,
            "f1": 0.565882,
            "f1_weighted": 0.565882,
            "ap": 0.547803,
            "ap_weighted": 0.547803
          },
          {
            "accuracy": 0.521484,
            "f1": 0.51733,
            "f1_weighted": 0.51733,
            "ap": 0.511132,
            "ap_weighted": 0.511132
          },
          {
            "accuracy": 0.54834,
            "f1": 0.546064,
            "f1_weighted": 0.546064,
            "ap": 0.526217,
            "ap_weighted": 0.526217
          },
          {
            "accuracy": 0.62793,
            "f1": 0.626773,
            "f1_weighted": 0.626773,
            "ap": 0.578691,
            "ap_weighted": 0.578691
          },
          {
            "accuracy": 0.568848,
            "f1": 0.565892,
            "f1_weighted": 0.565892,
            "ap": 0.538492,
            "ap_weighted": 0.538492
          },
          {
            "accuracy": 0.63916,
            "f1": 0.635508,
            "f1_weighted": 0.635508,
            "ap": 0.585715,
            "ap_weighted": 0.585715
          },
          {
            "accuracy": 0.518066,
            "f1": 0.512995,
            "f1_weighted": 0.512995,
            "ap": 0.509304,
            "ap_weighted": 0.509304
          }
        ],
        "main_score": 0.574756,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 18.163337469100952,
  "kg_co2_emissions": 0.0011105011480417835
}