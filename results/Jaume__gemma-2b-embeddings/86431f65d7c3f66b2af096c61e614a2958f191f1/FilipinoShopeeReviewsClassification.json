{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.339941,
        "f1": 0.335184,
        "f1_weighted": 0.335173,
        "scores_per_experiment": [
          {
            "accuracy": 0.354004,
            "f1": 0.344681,
            "f1_weighted": 0.344675
          },
          {
            "accuracy": 0.345215,
            "f1": 0.344328,
            "f1_weighted": 0.344332
          },
          {
            "accuracy": 0.323242,
            "f1": 0.316534,
            "f1_weighted": 0.316482
          },
          {
            "accuracy": 0.366211,
            "f1": 0.361105,
            "f1_weighted": 0.361101
          },
          {
            "accuracy": 0.340332,
            "f1": 0.334932,
            "f1_weighted": 0.334878
          },
          {
            "accuracy": 0.332031,
            "f1": 0.32635,
            "f1_weighted": 0.32636
          },
          {
            "accuracy": 0.302246,
            "f1": 0.301331,
            "f1_weighted": 0.301377
          },
          {
            "accuracy": 0.367676,
            "f1": 0.368515,
            "f1_weighted": 0.368499
          },
          {
            "accuracy": 0.383789,
            "f1": 0.368394,
            "f1_weighted": 0.368351
          },
          {
            "accuracy": 0.284668,
            "f1": 0.285669,
            "f1_weighted": 0.285678
          }
        ],
        "main_score": 0.339941,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.330859,
        "f1": 0.326138,
        "f1_weighted": 0.326131,
        "scores_per_experiment": [
          {
            "accuracy": 0.318848,
            "f1": 0.310455,
            "f1_weighted": 0.310441
          },
          {
            "accuracy": 0.341309,
            "f1": 0.339385,
            "f1_weighted": 0.339403
          },
          {
            "accuracy": 0.306641,
            "f1": 0.300984,
            "f1_weighted": 0.300947
          },
          {
            "accuracy": 0.351562,
            "f1": 0.348716,
            "f1_weighted": 0.348699
          },
          {
            "accuracy": 0.345703,
            "f1": 0.337535,
            "f1_weighted": 0.337463
          },
          {
            "accuracy": 0.324219,
            "f1": 0.319321,
            "f1_weighted": 0.319345
          },
          {
            "accuracy": 0.308105,
            "f1": 0.310154,
            "f1_weighted": 0.310203
          },
          {
            "accuracy": 0.351074,
            "f1": 0.349906,
            "f1_weighted": 0.349918
          },
          {
            "accuracy": 0.374023,
            "f1": 0.356729,
            "f1_weighted": 0.356678
          },
          {
            "accuracy": 0.287109,
            "f1": 0.288199,
            "f1_weighted": 0.288208
          }
        ],
        "main_score": 0.330859,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 48.031188011169434,
  "kg_co2_emissions": 0.003146340288370509
}