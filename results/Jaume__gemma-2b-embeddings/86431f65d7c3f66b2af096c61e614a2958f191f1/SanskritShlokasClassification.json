{
  "dataset_revision": "5a79d6472db143690c7ce6e974995d3610eee7f0",
  "task_name": "SanskritShlokasClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "train": [
      {
        "accuracy": 0.698695,
        "f1": 0.702828,
        "f1_weighted": 0.702713,
        "scores_per_experiment": [
          {
            "accuracy": 0.660574,
            "f1": 0.666313,
            "f1_weighted": 0.667637
          },
          {
            "accuracy": 0.697128,
            "f1": 0.704262,
            "f1_weighted": 0.703172
          },
          {
            "accuracy": 0.684073,
            "f1": 0.691876,
            "f1_weighted": 0.692079
          },
          {
            "accuracy": 0.725849,
            "f1": 0.728205,
            "f1_weighted": 0.724953
          },
          {
            "accuracy": 0.650131,
            "f1": 0.658335,
            "f1_weighted": 0.65848
          },
          {
            "accuracy": 0.660574,
            "f1": 0.669605,
            "f1_weighted": 0.666959
          },
          {
            "accuracy": 0.754569,
            "f1": 0.755773,
            "f1_weighted": 0.75482
          },
          {
            "accuracy": 0.72846,
            "f1": 0.726226,
            "f1_weighted": 0.72697
          },
          {
            "accuracy": 0.736292,
            "f1": 0.737788,
            "f1_weighted": 0.740204
          },
          {
            "accuracy": 0.689295,
            "f1": 0.689902,
            "f1_weighted": 0.691853
          }
        ],
        "main_score": 0.698695,
        "hf_subset": "default",
        "languages": [
          "san-Deva"
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.711458,
        "f1": 0.719422,
        "f1_weighted": 0.711271,
        "scores_per_experiment": [
          {
            "accuracy": 0.6875,
            "f1": 0.695259,
            "f1_weighted": 0.688103
          },
          {
            "accuracy": 0.708333,
            "f1": 0.724048,
            "f1_weighted": 0.713959
          },
          {
            "accuracy": 0.708333,
            "f1": 0.715217,
            "f1_weighted": 0.706623
          },
          {
            "accuracy": 0.6875,
            "f1": 0.699437,
            "f1_weighted": 0.683548
          },
          {
            "accuracy": 0.739583,
            "f1": 0.745397,
            "f1_weighted": 0.742556
          },
          {
            "accuracy": 0.697917,
            "f1": 0.712975,
            "f1_weighted": 0.700274
          },
          {
            "accuracy": 0.6875,
            "f1": 0.690648,
            "f1_weighted": 0.672302
          },
          {
            "accuracy": 0.6875,
            "f1": 0.695592,
            "f1_weighted": 0.689209
          },
          {
            "accuracy": 0.78125,
            "f1": 0.783888,
            "f1_weighted": 0.785088
          },
          {
            "accuracy": 0.729167,
            "f1": 0.731757,
            "f1_weighted": 0.731047
          }
        ],
        "main_score": 0.711458,
        "hf_subset": "default",
        "languages": [
          "san-Deva"
        ]
      }
    ]
  },
  "evaluation_time": 19.095913410186768,
  "kg_co2_emissions": 0.0008693542606455411
}