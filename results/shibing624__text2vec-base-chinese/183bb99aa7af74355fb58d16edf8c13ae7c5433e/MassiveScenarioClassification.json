{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "task_name": "MassiveScenarioClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.381505,
        "f1": 0.362205,
        "f1_weighted": 0.388857,
        "scores_per_experiment": [
          {
            "accuracy": 0.371372,
            "f1": 0.351806,
            "f1_weighted": 0.375352
          },
          {
            "accuracy": 0.400394,
            "f1": 0.379562,
            "f1_weighted": 0.415933
          },
          {
            "accuracy": 0.35514,
            "f1": 0.349721,
            "f1_weighted": 0.371012
          },
          {
            "accuracy": 0.364978,
            "f1": 0.334731,
            "f1_weighted": 0.379133
          },
          {
            "accuracy": 0.41515,
            "f1": 0.381942,
            "f1_weighted": 0.418966
          },
          {
            "accuracy": 0.380226,
            "f1": 0.358784,
            "f1_weighted": 0.386297
          },
          {
            "accuracy": 0.377767,
            "f1": 0.374419,
            "f1_weighted": 0.382887
          },
          {
            "accuracy": 0.397442,
            "f1": 0.374188,
            "f1_weighted": 0.399416
          },
          {
            "accuracy": 0.380226,
            "f1": 0.360865,
            "f1_weighted": 0.385861
          },
          {
            "accuracy": 0.372356,
            "f1": 0.356032,
            "f1_weighted": 0.373711
          }
        ],
        "main_score": 0.381505,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.394654,
        "f1": 0.364783,
        "f1_weighted": 0.401637,
        "scores_per_experiment": [
          {
            "accuracy": 0.38803,
            "f1": 0.354614,
            "f1_weighted": 0.392006
          },
          {
            "accuracy": 0.414257,
            "f1": 0.389441,
            "f1_weighted": 0.425842
          },
          {
            "accuracy": 0.362811,
            "f1": 0.346491,
            "f1_weighted": 0.378253
          },
          {
            "accuracy": 0.396772,
            "f1": 0.352233,
            "f1_weighted": 0.410579
          },
          {
            "accuracy": 0.411231,
            "f1": 0.369387,
            "f1_weighted": 0.412023
          },
          {
            "accuracy": 0.39879,
            "f1": 0.366652,
            "f1_weighted": 0.405704
          },
          {
            "accuracy": 0.387021,
            "f1": 0.374077,
            "f1_weighted": 0.394579
          },
          {
            "accuracy": 0.414929,
            "f1": 0.382321,
            "f1_weighted": 0.420284
          },
          {
            "accuracy": 0.390383,
            "f1": 0.35851,
            "f1_weighted": 0.394772
          },
          {
            "accuracy": 0.382313,
            "f1": 0.3541,
            "f1_weighted": 0.382326
          }
        ],
        "main_score": 0.394654,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 18.46432590484619,
  "kg_co2_emissions": null
}