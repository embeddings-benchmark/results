{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "task_name": "MTOPDomainClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.653278,
        "f1": 0.631535,
        "f1_weighted": 0.651111,
        "scores_per_experiment": [
          {
            "accuracy": 0.674931,
            "f1": 0.653569,
            "f1_weighted": 0.673208
          },
          {
            "accuracy": 0.616529,
            "f1": 0.600052,
            "f1_weighted": 0.613495
          },
          {
            "accuracy": 0.626446,
            "f1": 0.600747,
            "f1_weighted": 0.623558
          },
          {
            "accuracy": 0.683196,
            "f1": 0.66492,
            "f1_weighted": 0.683415
          },
          {
            "accuracy": 0.671074,
            "f1": 0.65214,
            "f1_weighted": 0.669046
          },
          {
            "accuracy": 0.624242,
            "f1": 0.598089,
            "f1_weighted": 0.625343
          },
          {
            "accuracy": 0.676033,
            "f1": 0.651983,
            "f1_weighted": 0.672995
          },
          {
            "accuracy": 0.661708,
            "f1": 0.643678,
            "f1_weighted": 0.663146
          },
          {
            "accuracy": 0.652893,
            "f1": 0.626512,
            "f1_weighted": 0.649429
          },
          {
            "accuracy": 0.64573,
            "f1": 0.623656,
            "f1_weighted": 0.637473
          }
        ],
        "main_score": 0.653278,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.65858,
        "f1": 0.631728,
        "f1_weighted": 0.65672,
        "scores_per_experiment": [
          {
            "accuracy": 0.673711,
            "f1": 0.648778,
            "f1_weighted": 0.67223
          },
          {
            "accuracy": 0.617639,
            "f1": 0.595972,
            "f1_weighted": 0.614411
          },
          {
            "accuracy": 0.65286,
            "f1": 0.622918,
            "f1_weighted": 0.65065
          },
          {
            "accuracy": 0.679346,
            "f1": 0.655509,
            "f1_weighted": 0.680007
          },
          {
            "accuracy": 0.673147,
            "f1": 0.644407,
            "f1_weighted": 0.670146
          },
          {
            "accuracy": 0.629473,
            "f1": 0.597687,
            "f1_weighted": 0.629707
          },
          {
            "accuracy": 0.681319,
            "f1": 0.653806,
            "f1_weighted": 0.679906
          },
          {
            "accuracy": 0.655959,
            "f1": 0.629276,
            "f1_weighted": 0.658373
          },
          {
            "accuracy": 0.669203,
            "f1": 0.644074,
            "f1_weighted": 0.666091
          },
          {
            "accuracy": 0.653142,
            "f1": 0.624855,
            "f1_weighted": 0.645677
          }
        ],
        "main_score": 0.65858,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 54.61255240440369,
  "kg_co2_emissions": null
}