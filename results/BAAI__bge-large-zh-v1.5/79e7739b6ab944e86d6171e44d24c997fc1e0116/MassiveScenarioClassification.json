{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "task_name": "MassiveScenarioClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.460994,
        "f1": 0.432362,
        "f1_weighted": 0.453524,
        "scores_per_experiment": [
          {
            "accuracy": 0.445155,
            "f1": 0.414334,
            "f1_weighted": 0.440396
          },
          {
            "accuracy": 0.455976,
            "f1": 0.428443,
            "f1_weighted": 0.452879
          },
          {
            "accuracy": 0.458436,
            "f1": 0.43941,
            "f1_weighted": 0.44914
          },
          {
            "accuracy": 0.468273,
            "f1": 0.431396,
            "f1_weighted": 0.466967
          },
          {
            "accuracy": 0.476144,
            "f1": 0.44239,
            "f1_weighted": 0.468229
          },
          {
            "accuracy": 0.468273,
            "f1": 0.445797,
            "f1_weighted": 0.464348
          },
          {
            "accuracy": 0.461387,
            "f1": 0.433685,
            "f1_weighted": 0.451084
          },
          {
            "accuracy": 0.455976,
            "f1": 0.433401,
            "f1_weighted": 0.441157
          },
          {
            "accuracy": 0.424496,
            "f1": 0.392932,
            "f1_weighted": 0.405462
          },
          {
            "accuracy": 0.495819,
            "f1": 0.461829,
            "f1_weighted": 0.495575
          }
        ],
        "main_score": 0.460994,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.477572,
        "f1": 0.439894,
        "f1_weighted": 0.472796,
        "scores_per_experiment": [
          {
            "accuracy": 0.471755,
            "f1": 0.437742,
            "f1_weighted": 0.464948
          },
          {
            "accuracy": 0.474781,
            "f1": 0.438838,
            "f1_weighted": 0.474541
          },
          {
            "accuracy": 0.478144,
            "f1": 0.440983,
            "f1_weighted": 0.46672
          },
          {
            "accuracy": 0.491594,
            "f1": 0.441069,
            "f1_weighted": 0.48944
          },
          {
            "accuracy": 0.497646,
            "f1": 0.453133,
            "f1_weighted": 0.494568
          },
          {
            "accuracy": 0.476463,
            "f1": 0.445105,
            "f1_weighted": 0.480268
          },
          {
            "accuracy": 0.474781,
            "f1": 0.438287,
            "f1_weighted": 0.469445
          },
          {
            "accuracy": 0.481506,
            "f1": 0.447025,
            "f1_weighted": 0.472311
          },
          {
            "accuracy": 0.439475,
            "f1": 0.411151,
            "f1_weighted": 0.427214
          },
          {
            "accuracy": 0.489576,
            "f1": 0.445603,
            "f1_weighted": 0.488502
          }
        ],
        "main_score": 0.477572,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 30.23071026802063,
  "kg_co2_emissions": null
}