{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.748282,
        "f1": 0.632999,
        "f1_weighted": 0.785522,
        "ap": 0.258241,
        "ap_weighted": 0.258241,
        "scores_per_experiment": [
          {
            "accuracy": 0.691581,
            "f1": 0.577421,
            "f1_weighted": 0.741962,
            "ap": 0.205362,
            "ap_weighted": 0.205362
          },
          {
            "accuracy": 0.803265,
            "f1": 0.669209,
            "f1_weighted": 0.826964,
            "ap": 0.27475,
            "ap_weighted": 0.27475
          },
          {
            "accuracy": 0.758591,
            "f1": 0.650658,
            "f1_weighted": 0.796126,
            "ap": 0.281117,
            "ap_weighted": 0.281117
          },
          {
            "accuracy": 0.798969,
            "f1": 0.679857,
            "f1_weighted": 0.826147,
            "ap": 0.300008,
            "ap_weighted": 0.300008
          },
          {
            "accuracy": 0.761168,
            "f1": 0.618384,
            "f1_weighted": 0.793255,
            "ap": 0.220107,
            "ap_weighted": 0.220107
          },
          {
            "accuracy": 0.790378,
            "f1": 0.663926,
            "f1_weighted": 0.81836,
            "ap": 0.275969,
            "ap_weighted": 0.275969
          },
          {
            "accuracy": 0.829897,
            "f1": 0.699961,
            "f1_weighted": 0.847878,
            "ap": 0.311317,
            "ap_weighted": 0.311317
          },
          {
            "accuracy": 0.608247,
            "f1": 0.539408,
            "f1_weighted": 0.672803,
            "ap": 0.215273,
            "ap_weighted": 0.215273
          },
          {
            "accuracy": 0.697595,
            "f1": 0.593038,
            "f1_weighted": 0.747569,
            "ap": 0.227243,
            "ap_weighted": 0.227243
          },
          {
            "accuracy": 0.743127,
            "f1": 0.638129,
            "f1_weighted": 0.784155,
            "ap": 0.27126,
            "ap_weighted": 0.27126
          }
        ],
        "main_score": 0.748282,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 7.190161228179932,
  "kg_co2_emissions": 0.00021416832594150993
}