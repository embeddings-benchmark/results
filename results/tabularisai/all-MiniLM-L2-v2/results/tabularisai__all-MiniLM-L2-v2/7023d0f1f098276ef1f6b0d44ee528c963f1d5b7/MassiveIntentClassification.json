{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.38.9",
  "scores": {
    "test": [
      {
        "accuracy": 0.651278,
        "f1": 0.627803,
        "f1_weighted": 0.652738,
        "scores_per_experiment": [
          {
            "accuracy": 0.661399,
            "f1": 0.641408,
            "f1_weighted": 0.663335
          },
          {
            "accuracy": 0.683255,
            "f1": 0.654961,
            "f1_weighted": 0.683575
          },
          {
            "accuracy": 0.645931,
            "f1": 0.624642,
            "f1_weighted": 0.641422
          },
          {
            "accuracy": 0.669132,
            "f1": 0.631521,
            "f1_weighted": 0.671292
          },
          {
            "accuracy": 0.653665,
            "f1": 0.620533,
            "f1_weighted": 0.648323
          },
          {
            "accuracy": 0.626765,
            "f1": 0.618059,
            "f1_weighted": 0.628714
          },
          {
            "accuracy": 0.644586,
            "f1": 0.627081,
            "f1_weighted": 0.648011
          },
          {
            "accuracy": 0.632145,
            "f1": 0.609611,
            "f1_weighted": 0.638609
          },
          {
            "accuracy": 0.631137,
            "f1": 0.607168,
            "f1_weighted": 0.636088
          },
          {
            "accuracy": 0.664761,
            "f1": 0.643048,
            "f1_weighted": 0.66801
          }
        ],
        "main_score": 0.651278,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 4.314992427825928,
  "kg_co2_emissions": null
}