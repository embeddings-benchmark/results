{
  "dataset_revision": "c723236c5ec417d79512e6104aca9d2cd88168f6",
  "task_name": "SDSGlovesClassification",
  "mteb_version": "1.25.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.65945,
        "f1": 0.400217,
        "f1_weighted": 0.783889,
        "ap": 0.996838,
        "ap_weighted": 0.996838,
        "scores_per_experiment": [
          {
            "accuracy": 0.758,
            "f1": 0.439145,
            "f1_weighted": 0.858647,
            "ap": 0.997033,
            "ap_weighted": 0.997033
          },
          {
            "accuracy": 0.832,
            "f1": 0.462845,
            "f1_weighted": 0.904584,
            "ap": 0.996833,
            "ap_weighted": 0.996833
          },
          {
            "accuracy": 0.4025,
            "f1": 0.291043,
            "f1_weighted": 0.569896,
            "ap": 0.996603,
            "ap_weighted": 0.996603
          },
          {
            "accuracy": 0.627,
            "f1": 0.389149,
            "f1_weighted": 0.767271,
            "ap": 0.996012,
            "ap_weighted": 0.996012
          },
          {
            "accuracy": 0.7605,
            "f1": 0.440038,
            "f1_weighted": 0.86026,
            "ap": 0.997043,
            "ap_weighted": 0.997043
          },
          {
            "accuracy": 0.7155,
            "f1": 0.422152,
            "f1_weighted": 0.830575,
            "ap": 0.996366,
            "ap_weighted": 0.996366
          },
          {
            "accuracy": 0.5255,
            "f1": 0.351029,
            "f1_weighted": 0.684829,
            "ap": 0.997595,
            "ap_weighted": 0.997595
          },
          {
            "accuracy": 0.569,
            "f1": 0.367946,
            "f1_weighted": 0.721573,
            "ap": 0.996773,
            "ap_weighted": 0.996773
          },
          {
            "accuracy": 0.6595,
            "f1": 0.404335,
            "f1_weighted": 0.791079,
            "ap": 0.997136,
            "ap_weighted": 0.997136
          },
          {
            "accuracy": 0.745,
            "f1": 0.434488,
            "f1_weighted": 0.85018,
            "ap": 0.996981,
            "ap_weighted": 0.996981
          }
        ],
        "main_score": 0.65945,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 33.490267515182495,
  "kg_co2_emissions": null
}