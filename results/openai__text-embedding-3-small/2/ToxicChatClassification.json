{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.798196,
        "f1": 0.673664,
        "f1_weighted": 0.824246,
        "ap": 0.290446,
        "ap_weighted": 0.290446,
        "scores_per_experiment": [
          {
            "accuracy": 0.738832,
            "f1": 0.608268,
            "f1_weighted": 0.77769,
            "ap": 0.219588,
            "ap_weighted": 0.219588
          },
          {
            "accuracy": 0.813574,
            "f1": 0.674406,
            "f1_weighted": 0.833873,
            "ap": 0.276037,
            "ap_weighted": 0.276037
          },
          {
            "accuracy": 0.85567,
            "f1": 0.736939,
            "f1_weighted": 0.869335,
            "ap": 0.365784,
            "ap_weighted": 0.365784
          },
          {
            "accuracy": 0.84622,
            "f1": 0.727117,
            "f1_weighted": 0.862173,
            "ap": 0.353491,
            "ap_weighted": 0.353491
          },
          {
            "accuracy": 0.782646,
            "f1": 0.64855,
            "f1_weighted": 0.811181,
            "ap": 0.25407,
            "ap_weighted": 0.25407
          },
          {
            "accuracy": 0.812715,
            "f1": 0.684518,
            "f1_weighted": 0.835175,
            "ap": 0.29599,
            "ap_weighted": 0.29599
          },
          {
            "accuracy": 0.804124,
            "f1": 0.667587,
            "f1_weighted": 0.827185,
            "ap": 0.271076,
            "ap_weighted": 0.271076
          },
          {
            "accuracy": 0.719072,
            "f1": 0.619703,
            "f1_weighted": 0.765333,
            "ap": 0.258548,
            "ap_weighted": 0.258548
          },
          {
            "accuracy": 0.785223,
            "f1": 0.666766,
            "f1_weighted": 0.815606,
            "ap": 0.28672,
            "ap_weighted": 0.28672
          },
          {
            "accuracy": 0.823883,
            "f1": 0.702783,
            "f1_weighted": 0.844909,
            "ap": 0.323157,
            "ap_weighted": 0.323157
          }
        ],
        "main_score": 0.798196,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 50.517465591430664,
  "kg_co2_emissions": null
}