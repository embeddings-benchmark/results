{
  "dataset_revision": "f8650438298df086750ff4973661bb58a201a5ee",
  "evaluation_time": 2588.7689065933228,
  "kg_co2_emissions": null,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.22332015810276679,
        "f1": 0.17403893942749657,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.17403893942749657,
        "precision": 0.15971537423670648,
        "recall": 0.22332015810276679
      },
      {
        "accuracy": 0.26679841897233203,
        "f1": 0.1926958352132577,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ],
        "main_score": 0.1926958352132577,
        "precision": 0.1683625854821507,
        "recall": 0.26679841897233203
      },
      {
        "accuracy": 0.05632411067193676,
        "f1": 0.04257542858905284,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ],
        "main_score": 0.04257542858905284,
        "precision": 0.0402193561055283,
        "recall": 0.05632411067193676
      },
      {
        "accuracy": 0.08300395256916997,
        "f1": 0.04233334636262658,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ],
        "main_score": 0.04233334636262658,
        "precision": 0.0340958985680606,
        "recall": 0.08300395256916997
      },
      {
        "accuracy": 0.5741106719367589,
        "f1": 0.505571086592636,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.505571086592636,
        "precision": 0.4810024118942103,
        "recall": 0.5741106719367589
      },
      {
        "accuracy": 0.5375494071146245,
        "f1": 0.45367404334795636,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ],
        "main_score": 0.45367404334795636,
        "precision": 0.42394598155467716,
        "recall": 0.5375494071146245
      },
      {
        "accuracy": 0.05138339920948617,
        "f1": 0.036514218408953185,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ],
        "main_score": 0.036514218408953185,
        "precision": 0.03332761788789457,
        "recall": 0.05138339920948617
      },
      {
        "accuracy": 0.07312252964426877,
        "f1": 0.03621808481258242,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ],
        "main_score": 0.03621808481258242,
        "precision": 0.029046860365759743,
        "recall": 0.07312252964426877
      },
      {
        "accuracy": 0.05237154150197629,
        "f1": 0.03736232282791223,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.03736232282791223,
        "precision": 0.03401054085658599,
        "recall": 0.05237154150197629
      },
      {
        "accuracy": 0.08893280632411067,
        "f1": 0.04790497436248424,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ],
        "main_score": 0.04790497436248424,
        "precision": 0.037646424302143905,
        "recall": 0.08893280632411067
      },
      {
        "accuracy": 0.2124505928853755,
        "f1": 0.1680969915162734,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.1680969915162734,
        "precision": 0.15578413336283825,
        "recall": 0.2124505928853755
      },
      {
        "accuracy": 0.2490118577075099,
        "f1": 0.17637973201474344,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ],
        "main_score": 0.17637973201474344,
        "precision": 0.15312091838771683,
        "recall": 0.2490118577075099
      },
      {
        "accuracy": 0.08498023715415019,
        "f1": 0.06421835837353682,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.06421835837353682,
        "precision": 0.05919156806632103,
        "recall": 0.08498023715415019
      },
      {
        "accuracy": 0.1007905138339921,
        "f1": 0.05705295726917075,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ],
        "main_score": 0.05705295726917075,
        "precision": 0.04731555221093777,
        "recall": 0.1007905138339921
      },
      {
        "accuracy": 0.12154150197628459,
        "f1": 0.09319273439403308,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.09319273439403308,
        "precision": 0.08635483020061285,
        "recall": 0.12154150197628459
      },
      {
        "accuracy": 0.11956521739130435,
        "f1": 0.058700719969289696,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ],
        "main_score": 0.058700719969289696,
        "precision": 0.04632766060554328,
        "recall": 0.11956521739130435
      },
      {
        "accuracy": 0.3922924901185771,
        "f1": 0.33146018887852186,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.33146018887852186,
        "precision": 0.3117497845997232,
        "recall": 0.3922924901185771
      },
      {
        "accuracy": 0.40711462450592883,
        "f1": 0.3174667339687102,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ],
        "main_score": 0.3174667339687102,
        "precision": 0.2875871219250666,
        "recall": 0.40711462450592883
      },
      {
        "accuracy": 0.09090909090909091,
        "f1": 0.06560148552021934,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ],
        "main_score": 0.06560148552021934,
        "precision": 0.060540083351038956,
        "recall": 0.09090909090909091
      },
      {
        "accuracy": 0.13636363636363635,
        "f1": 0.0792037617630503,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ],
        "main_score": 0.0792037617630503,
        "precision": 0.06530295709180532,
        "recall": 0.13636363636363635
      },
      {
        "accuracy": 0.30237154150197626,
        "f1": 0.24996683181704346,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ],
        "main_score": 0.24996683181704346,
        "precision": 0.2345807290043748,
        "recall": 0.30237154150197626
      },
      {
        "accuracy": 0.34980237154150196,
        "f1": 0.2669292088017384,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ],
        "main_score": 0.2669292088017384,
        "precision": 0.24090725822009246,
        "recall": 0.34980237154150196
      },
      {
        "accuracy": 0.23517786561264822,
        "f1": 0.19441786024508673,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ],
        "main_score": 0.19441786024508673,
        "precision": 0.18414094815321935,
        "recall": 0.23517786561264822
      },
      {
        "accuracy": 0.2826086956521739,
        "f1": 0.2034712597459633,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ],
        "main_score": 0.2034712597459633,
        "precision": 0.17947397989524055,
        "recall": 0.2826086956521739
      },
      {
        "accuracy": 0.044466403162055336,
        "f1": 0.03597855525815892,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ],
        "main_score": 0.03597855525815892,
        "precision": 0.03417953525013628,
        "recall": 0.044466403162055336
      },
      {
        "accuracy": 0.06818181818181818,
        "f1": 0.02780801644720814,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ],
        "main_score": 0.02780801644720814,
        "precision": 0.020632165619117526,
        "recall": 0.06818181818181818
      },
      {
        "accuracy": 0.05039525691699605,
        "f1": 0.03940093006207835,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ],
        "main_score": 0.03940093006207835,
        "precision": 0.03693663008086859,
        "recall": 0.05039525691699605
      },
      {
        "accuracy": 0.09387351778656126,
        "f1": 0.05227224344955533,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ],
        "main_score": 0.05227224344955533,
        "precision": 0.04313021092290568,
        "recall": 0.09387351778656126
      },
      {
        "accuracy": 0.2855731225296443,
        "f1": 0.23375676779393914,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ],
        "main_score": 0.23375676779393914,
        "precision": 0.2197961125010446,
        "recall": 0.2855731225296443
      },
      {
        "accuracy": 0.30434782608695654,
        "f1": 0.21372234109648294,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ],
        "main_score": 0.21372234109648294,
        "precision": 0.18579941139229675,
        "recall": 0.30434782608695654
      },
      {
        "accuracy": 0.191699604743083,
        "f1": 0.15190332280108898,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ],
        "main_score": 0.15190332280108898,
        "precision": 0.14241835373729056,
        "recall": 0.191699604743083
      },
      {
        "accuracy": 0.23023715415019763,
        "f1": 0.15578741307002178,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ],
        "main_score": 0.15578741307002178,
        "precision": 0.1346329221038591,
        "recall": 0.23023715415019763
      },
      {
        "accuracy": 0.4268774703557312,
        "f1": 0.3617036564665023,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.3617036564665023,
        "precision": 0.34105943495401575,
        "recall": 0.4268774703557312
      },
      {
        "accuracy": 0.43774703557312256,
        "f1": 0.3588002988447653,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ],
        "main_score": 0.3588002988447653,
        "precision": 0.33256511001076217,
        "recall": 0.43774703557312256
      },
      {
        "accuracy": 0.3616600790513834,
        "f1": 0.30016189184355335,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ],
        "main_score": 0.30016189184355335,
        "precision": 0.2809692802381016,
        "recall": 0.3616600790513834
      },
      {
        "accuracy": 0.38537549407114624,
        "f1": 0.30426134206038513,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ],
        "main_score": 0.30426134206038513,
        "precision": 0.27690873113433967,
        "recall": 0.38537549407114624
      },
      {
        "accuracy": 0.038537549407114624,
        "f1": 0.03209886103780066,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ],
        "main_score": 0.03209886103780066,
        "precision": 0.03047082805081164,
        "recall": 0.038537549407114624
      },
      {
        "accuracy": 0.06225296442687747,
        "f1": 0.02235787084483532,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ],
        "main_score": 0.02235787084483532,
        "precision": 0.016473820421291037,
        "recall": 0.06225296442687747
      },
      {
        "accuracy": 0.06422924901185771,
        "f1": 0.04317375222670531,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ],
        "main_score": 0.04317375222670531,
        "precision": 0.039904841482127106,
        "recall": 0.06422924901185771
      },
      {
        "accuracy": 0.09980237154150198,
        "f1": 0.05567016106067574,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ],
        "main_score": 0.05567016106067574,
        "precision": 0.04596972292929628,
        "recall": 0.09980237154150198
      },
      {
        "accuracy": 0.31126482213438733,
        "f1": 0.2654527441690766,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ],
        "main_score": 0.2654527441690766,
        "precision": 0.2522787830401953,
        "recall": 0.31126482213438733
      },
      {
        "accuracy": 0.33992094861660077,
        "f1": 0.2536424735782443,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ],
        "main_score": 0.2536424735782443,
        "precision": 0.22620040710019787,
        "recall": 0.33992094861660077
      },
      {
        "accuracy": 0.17094861660079053,
        "f1": 0.13814688612841064,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ],
        "main_score": 0.13814688612841064,
        "precision": 0.13012624151405583,
        "recall": 0.17094861660079053
      },
      {
        "accuracy": 0.20948616600790515,
        "f1": 0.13599753891255867,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ],
        "main_score": 0.13599753891255867,
        "precision": 0.11462765014700611,
        "recall": 0.20948616600790515
      },
      {
        "accuracy": 0.3181818181818182,
        "f1": 0.2591237998185216,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ],
        "main_score": 0.2591237998185216,
        "precision": 0.24186964849819492,
        "recall": 0.3181818181818182
      },
      {
        "accuracy": 0.35968379446640314,
        "f1": 0.27752182663509894,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ],
        "main_score": 0.27752182663509894,
        "precision": 0.2515653428696907,
        "recall": 0.35968379446640314
      },
      {
        "accuracy": 0.29150197628458496,
        "f1": 0.24774312305704713,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ],
        "main_score": 0.24774312305704713,
        "precision": 0.23529364305845935,
        "recall": 0.29150197628458496
      },
      {
        "accuracy": 0.33992094861660077,
        "f1": 0.25336127421107657,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ],
        "main_score": 0.25336127421107657,
        "precision": 0.22724841583537234,
        "recall": 0.33992094861660077
      },
      {
        "accuracy": 0.3073122529644269,
        "f1": 0.24955550441976487,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ],
        "main_score": 0.24955550441976487,
        "precision": 0.23200420101139393,
        "recall": 0.3073122529644269
      },
      {
        "accuracy": 0.34288537549407117,
        "f1": 0.25699857893533784,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ],
        "main_score": 0.25699857893533784,
        "precision": 0.22927692366921615,
        "recall": 0.34288537549407117
      },
      {
        "accuracy": 0.06027667984189723,
        "f1": 0.046544610440200807,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ],
        "main_score": 0.046544610440200807,
        "precision": 0.043711252088714674,
        "recall": 0.06027667984189723
      },
      {
        "accuracy": 0.09090909090909091,
        "f1": 0.04696107286898595,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ],
        "main_score": 0.04696107286898595,
        "precision": 0.0362736804806963,
        "recall": 0.09090909090909091
      },
      {
        "accuracy": 0.33794466403162055,
        "f1": 0.27696631209359174,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ],
        "main_score": 0.27696631209359174,
        "precision": 0.2592875479597673,
        "recall": 0.33794466403162055
      },
      {
        "accuracy": 0.37351778656126483,
        "f1": 0.2920658447832361,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ],
        "main_score": 0.2920658447832361,
        "precision": 0.26506420031181716,
        "recall": 0.37351778656126483
      },
      {
        "accuracy": 0.30533596837944665,
        "f1": 0.2569485203237188,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ],
        "main_score": 0.2569485203237188,
        "precision": 0.24344641743546144,
        "recall": 0.30533596837944665
      },
      {
        "accuracy": 0.34881422924901184,
        "f1": 0.26291716627685,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ],
        "main_score": 0.26291716627685,
        "precision": 0.2352249913020664,
        "recall": 0.34881422924901184
      },
      {
        "accuracy": 0.01383399209486166,
        "f1": 0.010896067117956173,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ],
        "main_score": 0.010896067117956173,
        "precision": 0.01056959345002823,
        "recall": 0.01383399209486166
      },
      {
        "accuracy": 0.022727272727272728,
        "f1": 0.004980129939406981,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ],
        "main_score": 0.004980129939406981,
        "precision": 0.003472560663721053,
        "recall": 0.022727272727272728
      }
    ],
    "validation": [
      {
        "accuracy": 0.24172517552657974,
        "f1": 0.18443638060251058,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.18443638060251058,
        "precision": 0.16854871696897775,
        "recall": 0.24172517552657974
      },
      {
        "accuracy": 0.25075225677031093,
        "f1": 0.1802307354965329,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ],
        "main_score": 0.1802307354965329,
        "precision": 0.15846229163681516,
        "recall": 0.25075225677031093
      },
      {
        "accuracy": 0.0712136409227683,
        "f1": 0.055038923609150414,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ],
        "main_score": 0.055038923609150414,
        "precision": 0.05130202227810843,
        "recall": 0.0712136409227683
      },
      {
        "accuracy": 0.09428284854563691,
        "f1": 0.050169578554734015,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ],
        "main_score": 0.050169578554734015,
        "precision": 0.040317202421006094,
        "recall": 0.09428284854563691
      },
      {
        "accuracy": 0.6328986960882648,
        "f1": 0.5605260948791541,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.5605260948791541,
        "precision": 0.5316262278900192,
        "recall": 0.6328986960882648
      },
      {
        "accuracy": 0.5977933801404213,
        "f1": 0.5228439286112305,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ],
        "main_score": 0.5228439286112305,
        "precision": 0.49482532401288676,
        "recall": 0.5977933801404213
      },
      {
        "accuracy": 0.07622868605817452,
        "f1": 0.055265105818782974,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ],
        "main_score": 0.055265105818782974,
        "precision": 0.05026122652226771,
        "recall": 0.07622868605817452
      },
      {
        "accuracy": 0.07723169508525576,
        "f1": 0.03876322401278073,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ],
        "main_score": 0.03876322401278073,
        "precision": 0.03055371993436186,
        "recall": 0.07723169508525576
      },
      {
        "accuracy": 0.08124373119358075,
        "f1": 0.06506510874615187,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.06506510874615187,
        "precision": 0.061560845207373024,
        "recall": 0.08124373119358075
      },
      {
        "accuracy": 0.09929789368104312,
        "f1": 0.052640055575176724,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ],
        "main_score": 0.052640055575176724,
        "precision": 0.04242442828332075,
        "recall": 0.09929789368104312
      },
      {
        "accuracy": 0.23671013039117353,
        "f1": 0.18643704618127888,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.18643704618127888,
        "precision": 0.1720291577362791,
        "recall": 0.23671013039117353
      },
      {
        "accuracy": 0.2587763289869609,
        "f1": 0.18634416206249352,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ],
        "main_score": 0.18634416206249352,
        "precision": 0.1645808438073391,
        "recall": 0.2587763289869609
      },
      {
        "accuracy": 0.10732196589769308,
        "f1": 0.07959497755324864,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.07959497755324864,
        "precision": 0.07306281609680937,
        "recall": 0.10732196589769308
      },
      {
        "accuracy": 0.11434302908726178,
        "f1": 0.06375762318590804,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ],
        "main_score": 0.06375762318590804,
        "precision": 0.05288524336525117,
        "recall": 0.11434302908726178
      },
      {
        "accuracy": 0.13139418254764293,
        "f1": 0.10586456338713109,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.10586456338713109,
        "precision": 0.09917849897789717,
        "recall": 0.13139418254764293
      },
      {
        "accuracy": 0.12537612838515547,
        "f1": 0.059967448724897544,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ],
        "main_score": 0.059967448724897544,
        "precision": 0.046915274685718364,
        "recall": 0.12537612838515547
      },
      {
        "accuracy": 0.40220661985957873,
        "f1": 0.34929729674290944,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.34929729674290944,
        "precision": 0.33106961577372807,
        "recall": 0.40220661985957873
      },
      {
        "accuracy": 0.4202607823470411,
        "f1": 0.33286760047040886,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ],
        "main_score": 0.33286760047040886,
        "precision": 0.302770251591716,
        "recall": 0.4202607823470411
      },
      {
        "accuracy": 0.11133400200601805,
        "f1": 0.0822242367241727,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ],
        "main_score": 0.0822242367241727,
        "precision": 0.07573877262880441,
        "recall": 0.11133400200601805
      },
      {
        "accuracy": 0.160481444332999,
        "f1": 0.09987167810195717,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ],
        "main_score": 0.09987167810195717,
        "precision": 0.08323203042360515,
        "recall": 0.160481444332999
      },
      {
        "accuracy": 0.3811434302908726,
        "f1": 0.313213747692183,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ],
        "main_score": 0.313213747692183,
        "precision": 0.29248650870707443,
        "recall": 0.3811434302908726
      },
      {
        "accuracy": 0.37713139418254765,
        "f1": 0.2904028324289107,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ],
        "main_score": 0.2904028324289107,
        "precision": 0.26158979164735563,
        "recall": 0.37713139418254765
      },
      {
        "accuracy": 0.29689067201604813,
        "f1": 0.2407571700345182,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ],
        "main_score": 0.2407571700345182,
        "precision": 0.22358883117588765,
        "recall": 0.29689067201604813
      },
      {
        "accuracy": 0.279839518555667,
        "f1": 0.20058983503434502,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ],
        "main_score": 0.20058983503434502,
        "precision": 0.1772596942230844,
        "recall": 0.279839518555667
      },
      {
        "accuracy": 0.06118355065195587,
        "f1": 0.04614891875738933,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ],
        "main_score": 0.04614891875738933,
        "precision": 0.043083363804080774,
        "recall": 0.06118355065195587
      },
      {
        "accuracy": 0.07723169508525576,
        "f1": 0.034543568603664726,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ],
        "main_score": 0.034543568603664726,
        "precision": 0.027103920642215413,
        "recall": 0.07723169508525576
      },
      {
        "accuracy": 0.07221664994984955,
        "f1": 0.051650369139893446,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ],
        "main_score": 0.051650369139893446,
        "precision": 0.04747829594995955,
        "recall": 0.07221664994984955
      },
      {
        "accuracy": 0.10230692076228685,
        "f1": 0.06081696787717288,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ],
        "main_score": 0.06081696787717288,
        "precision": 0.051390423298159967,
        "recall": 0.10230692076228685
      },
      {
        "accuracy": 0.3059177532597793,
        "f1": 0.2691846129856045,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ],
        "main_score": 0.2691846129856045,
        "precision": 0.2579447073753907,
        "recall": 0.3059177532597793
      },
      {
        "accuracy": 0.3319959879638917,
        "f1": 0.24095132550498646,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ],
        "main_score": 0.24095132550498646,
        "precision": 0.21292535615504524,
        "recall": 0.3319959879638917
      },
      {
        "accuracy": 0.2136409227683049,
        "f1": 0.1673643552515462,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ],
        "main_score": 0.1673643552515462,
        "precision": 0.1555472329469681,
        "recall": 0.2136409227683049
      },
      {
        "accuracy": 0.23269809428284854,
        "f1": 0.1554840955480956,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ],
        "main_score": 0.1554840955480956,
        "precision": 0.13428931710156963,
        "recall": 0.23269809428284854
      },
      {
        "accuracy": 0.47342026078234706,
        "f1": 0.3884893497230508,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.3884893497230508,
        "precision": 0.35930477635592983,
        "recall": 0.47342026078234706
      },
      {
        "accuracy": 0.46138415245737213,
        "f1": 0.37863498921673444,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ],
        "main_score": 0.37863498921673444,
        "precision": 0.3498800495052409,
        "recall": 0.46138415245737213
      },
      {
        "accuracy": 0.4092276830491474,
        "f1": 0.3423267602449672,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ],
        "main_score": 0.3423267602449672,
        "precision": 0.3204047387472663,
        "recall": 0.4092276830491474
      },
      {
        "accuracy": 0.3901705115346038,
        "f1": 0.3061269890256851,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ],
        "main_score": 0.3061269890256851,
        "precision": 0.2774550887846658,
        "recall": 0.3901705115346038
      },
      {
        "accuracy": 0.033099297893681046,
        "f1": 0.028463411382182804,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ],
        "main_score": 0.028463411382182804,
        "precision": 0.027104101760329756,
        "recall": 0.033099297893681046
      },
      {
        "accuracy": 0.07622868605817452,
        "f1": 0.03138858000780763,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ],
        "main_score": 0.03138858000780763,
        "precision": 0.025574033903884606,
        "recall": 0.07622868605817452
      },
      {
        "accuracy": 0.07422266800401203,
        "f1": 0.054755148658911026,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ],
        "main_score": 0.054755148658911026,
        "precision": 0.05028866145441678,
        "recall": 0.07422266800401203
      },
      {
        "accuracy": 0.09929789368104312,
        "f1": 0.05401167500491789,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ],
        "main_score": 0.05401167500491789,
        "precision": 0.04427183383611491,
        "recall": 0.09929789368104312
      },
      {
        "accuracy": 0.3811434302908726,
        "f1": 0.31447242891839844,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ],
        "main_score": 0.31447242891839844,
        "precision": 0.2921642412169877,
        "recall": 0.3811434302908726
      },
      {
        "accuracy": 0.3881644934804413,
        "f1": 0.3043167176414745,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ],
        "main_score": 0.3043167176414745,
        "precision": 0.27704377881969483,
        "recall": 0.3881644934804413
      },
      {
        "accuracy": 0.18756268806419257,
        "f1": 0.1530104084428823,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ],
        "main_score": 0.1530104084428823,
        "precision": 0.1427457099376,
        "recall": 0.18756268806419257
      },
      {
        "accuracy": 0.20561685055165496,
        "f1": 0.1341908926913942,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ],
        "main_score": 0.1341908926913942,
        "precision": 0.11413579078633063,
        "recall": 0.20561685055165496
      },
      {
        "accuracy": 0.37612838515546637,
        "f1": 0.29966335151006807,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ],
        "main_score": 0.29966335151006807,
        "precision": 0.27392792736072574,
        "recall": 0.37612838515546637
      },
      {
        "accuracy": 0.4092276830491474,
        "f1": 0.32182263295603314,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ],
        "main_score": 0.32182263295603314,
        "precision": 0.2923094680868001,
        "recall": 0.4092276830491474
      },
      {
        "accuracy": 0.3731193580742227,
        "f1": 0.30987023376071327,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ],
        "main_score": 0.30987023376071327,
        "precision": 0.2905912959355263,
        "recall": 0.3731193580742227
      },
      {
        "accuracy": 0.37011033099297896,
        "f1": 0.2801082363295318,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ],
        "main_score": 0.2801082363295318,
        "precision": 0.25151334233580974,
        "recall": 0.37011033099297896
      },
      {
        "accuracy": 0.38615847542627885,
        "f1": 0.3076521735023557,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ],
        "main_score": 0.3076521735023557,
        "precision": 0.28335109757007026,
        "recall": 0.38615847542627885
      },
      {
        "accuracy": 0.3901705115346038,
        "f1": 0.3042947022886842,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ],
        "main_score": 0.3042947022886842,
        "precision": 0.2755395214021092,
        "recall": 0.3901705115346038
      },
      {
        "accuracy": 0.06920762286860582,
        "f1": 0.05151172487903885,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ],
        "main_score": 0.05151172487903885,
        "precision": 0.04718924481582019,
        "recall": 0.06920762286860582
      },
      {
        "accuracy": 0.09227683049147442,
        "f1": 0.04709946323528489,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ],
        "main_score": 0.04709946323528489,
        "precision": 0.03741036744875848,
        "recall": 0.09227683049147442
      },
      {
        "accuracy": 0.43229689067201604,
        "f1": 0.3597434987499705,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ],
        "main_score": 0.3597434987499705,
        "precision": 0.33533068351404943,
        "recall": 0.43229689067201604
      },
      {
        "accuracy": 0.42527582748244735,
        "f1": 0.33916272627406024,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ],
        "main_score": 0.33916272627406024,
        "precision": 0.3089482483414279,
        "recall": 0.42527582748244735
      },
      {
        "accuracy": 0.4102306920762287,
        "f1": 0.34330581376719793,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ],
        "main_score": 0.34330581376719793,
        "precision": 0.32121121260856256,
        "recall": 0.4102306920762287
      },
      {
        "accuracy": 0.4012036108324975,
        "f1": 0.3191499100476032,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ],
        "main_score": 0.3191499100476032,
        "precision": 0.29225797693833383,
        "recall": 0.4012036108324975
      },
      {
        "accuracy": 0.010030090270812437,
        "f1": 0.008036084300806611,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ],
        "main_score": 0.008036084300806611,
        "precision": 0.007528609926164035,
        "recall": 0.010030090270812437
      },
      {
        "accuracy": 0.0160481444332999,
        "f1": 0.0028286121342706136,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ],
        "main_score": 0.0028286121342706136,
        "precision": 0.002069728770253069,
        "recall": 0.0160481444332999
      }
    ]
  },
  "task_name": "IndicGenBenchFloresBitextMining"
}