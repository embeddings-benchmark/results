{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 81.58458161354065,
  "kg_co2_emissions": null,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.34658203125,
        "f1": 0.33631283047658705,
        "f1_weighted": 0.3363130019507576,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.34658203125,
        "scores_per_experiment": [
          {
            "accuracy": 0.353515625,
            "f1": 0.3453800018276233,
            "f1_weighted": 0.34538662668489284
          },
          {
            "accuracy": 0.3427734375,
            "f1": 0.3272206391937828,
            "f1_weighted": 0.3272291696629457
          },
          {
            "accuracy": 0.32763671875,
            "f1": 0.3189684988480352,
            "f1_weighted": 0.31891366322782455
          },
          {
            "accuracy": 0.34814453125,
            "f1": 0.34525619649891687,
            "f1_weighted": 0.3452449577527212
          },
          {
            "accuracy": 0.35986328125,
            "f1": 0.3396406004931524,
            "f1_weighted": 0.3396011787769878
          },
          {
            "accuracy": 0.33251953125,
            "f1": 0.30907606954020983,
            "f1_weighted": 0.30904333764035485
          },
          {
            "accuracy": 0.33935546875,
            "f1": 0.3427805731276429,
            "f1_weighted": 0.34280479469438163
          },
          {
            "accuracy": 0.3720703125,
            "f1": 0.3633302449032128,
            "f1_weighted": 0.36335141474077964
          },
          {
            "accuracy": 0.36376953125,
            "f1": 0.36231552187875343,
            "f1_weighted": 0.36233241543150374
          },
          {
            "accuracy": 0.326171875,
            "f1": 0.3091599584545413,
            "f1_weighted": 0.30922246089518457
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.34140625,
        "f1": 0.33062222136039326,
        "f1_weighted": 0.3306183062120626,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.34140625,
        "scores_per_experiment": [
          {
            "accuracy": 0.3466796875,
            "f1": 0.33478709619436475,
            "f1_weighted": 0.33478549507633787
          },
          {
            "accuracy": 0.31982421875,
            "f1": 0.3004697886924083,
            "f1_weighted": 0.3004902268918087
          },
          {
            "accuracy": 0.330078125,
            "f1": 0.3199999736554628,
            "f1_weighted": 0.31993779203912026
          },
          {
            "accuracy": 0.36328125,
            "f1": 0.35911299388643614,
            "f1_weighted": 0.35909067825773994
          },
          {
            "accuracy": 0.35595703125,
            "f1": 0.3363899622220693,
            "f1_weighted": 0.33637218869187435
          },
          {
            "accuracy": 0.32958984375,
            "f1": 0.3084486289114049,
            "f1_weighted": 0.3084313059578575
          },
          {
            "accuracy": 0.31103515625,
            "f1": 0.3116009838015056,
            "f1_weighted": 0.3116109586848132
          },
          {
            "accuracy": 0.37841796875,
            "f1": 0.37308483671369774,
            "f1_weighted": 0.37308904872939447
          },
          {
            "accuracy": 0.3486328125,
            "f1": 0.3495099141333715,
            "f1_weighted": 0.34951044459950353
          },
          {
            "accuracy": 0.33056640625,
            "f1": 0.3128180353932115,
            "f1_weighted": 0.3128649231921764
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}