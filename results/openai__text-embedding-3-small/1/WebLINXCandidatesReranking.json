{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 50164.427752017975,
  "kg_co2_emissions": null,
  "mteb_version": "1.18.0",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08947810772248975,
        "map": 0.10723267399620579,
        "mrr": 0.08947810772248975,
        "nAUC_map_diff1": 0.08015125195061065,
        "nAUC_map_max": -0.08209135558761368,
        "nAUC_map_std": 0.24723087139536334,
        "nAUC_mrr_diff1": 0.07504052727081537,
        "nAUC_mrr_max": -0.07913376327243239,
        "nAUC_mrr_std": 0.22061373116253022
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.11007537745230993,
        "map": 0.1288317036746966,
        "mrr": 0.11007537745230993,
        "nAUC_map_diff1": 0.17925365446756253,
        "nAUC_map_max": -0.028167202879516018,
        "nAUC_map_std": -0.07866365097693433,
        "nAUC_mrr_diff1": 0.17790548423914346,
        "nAUC_mrr_max": -0.026998811771868234,
        "nAUC_mrr_std": -0.08524926655524465
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.12411197209528224,
        "map": 0.14154006886714582,
        "mrr": 0.12411197209528224,
        "nAUC_map_diff1": 0.15242575368525232,
        "nAUC_map_max": 0.08796283941752756,
        "nAUC_map_std": 0.02479151088545743,
        "nAUC_mrr_diff1": 0.13857334514625386,
        "nAUC_mrr_max": 0.09782366705789512,
        "nAUC_mrr_std": 0.019862177800866843
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.12060211400528502,
        "map": 0.1390532790499241,
        "mrr": 0.12060211400528502,
        "nAUC_map_diff1": 0.04320507599824889,
        "nAUC_map_max": -0.0015695169842105492,
        "nAUC_map_std": 0.08433278943699546,
        "nAUC_mrr_diff1": 0.04841368973722382,
        "nAUC_mrr_max": 0.008638087559981534,
        "nAUC_mrr_std": 0.07003207922826947
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.10020775273637868,
        "map": 0.11585734303564786,
        "mrr": 0.10020775273637868,
        "nAUC_map_diff1": 0.13674944694735136,
        "nAUC_map_max": 0.06774052877019456,
        "nAUC_map_std": 0.12080099804383794,
        "nAUC_mrr_diff1": 0.1510347507825389,
        "nAUC_mrr_max": 0.09331026849002473,
        "nAUC_mrr_std": 0.10834950404461385
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.1562946085428791,
        "map": 0.1732497317789141,
        "mrr": 0.1562946085428791,
        "nAUC_map_diff1": 0.20758742480640052,
        "nAUC_map_max": 0.05883458550656691,
        "nAUC_map_std": -0.04479960343918857,
        "nAUC_mrr_diff1": 0.21539329830853207,
        "nAUC_mrr_max": 0.0680628898165194,
        "nAUC_mrr_std": -0.04571707993596203
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}