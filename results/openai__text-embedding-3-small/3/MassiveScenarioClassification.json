{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "task_name": "MassiveScenarioClassification",
  "mteb_version": "1.38.49",
  "scores": {
    "validation": [
      {
        "accuracy": 0.562961,
        "f1": 0.550266,
        "f1_weighted": 0.550094,
        "scores_per_experiment": [
          {
            "accuracy": 0.59518,
            "f1": 0.583319,
            "f1_weighted": 0.589408
          },
          {
            "accuracy": 0.583374,
            "f1": 0.561481,
            "f1_weighted": 0.571212
          },
          {
            "accuracy": 0.560748,
            "f1": 0.545513,
            "f1_weighted": 0.553248
          },
          {
            "accuracy": 0.538613,
            "f1": 0.530847,
            "f1_weighted": 0.528095
          },
          {
            "accuracy": 0.558288,
            "f1": 0.530056,
            "f1_weighted": 0.531776
          },
          {
            "accuracy": 0.534678,
            "f1": 0.527452,
            "f1_weighted": 0.514896
          },
          {
            "accuracy": 0.560748,
            "f1": 0.549299,
            "f1_weighted": 0.553856
          },
          {
            "accuracy": 0.545499,
            "f1": 0.540474,
            "f1_weighted": 0.527368
          },
          {
            "accuracy": 0.594688,
            "f1": 0.58172,
            "f1_weighted": 0.583625
          },
          {
            "accuracy": 0.557796,
            "f1": 0.552495,
            "f1_weighted": 0.547456
          }
        ],
        "main_score": 0.562961,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.570444,
        "f1": 0.552735,
        "f1_weighted": 0.558052,
        "scores_per_experiment": [
          {
            "accuracy": 0.590451,
            "f1": 0.575509,
            "f1_weighted": 0.580709
          },
          {
            "accuracy": 0.600202,
            "f1": 0.572069,
            "f1_weighted": 0.590828
          },
          {
            "accuracy": 0.566913,
            "f1": 0.55087,
            "f1_weighted": 0.558638
          },
          {
            "accuracy": 0.56187,
            "f1": 0.547899,
            "f1_weighted": 0.554513
          },
          {
            "accuracy": 0.557162,
            "f1": 0.528122,
            "f1_weighted": 0.534293
          },
          {
            "accuracy": 0.548083,
            "f1": 0.533787,
            "f1_weighted": 0.528284
          },
          {
            "accuracy": 0.569939,
            "f1": 0.550307,
            "f1_weighted": 0.562481
          },
          {
            "accuracy": 0.554808,
            "f1": 0.545818,
            "f1_weighted": 0.541997
          },
          {
            "accuracy": 0.589442,
            "f1": 0.565109,
            "f1_weighted": 0.577018
          },
          {
            "accuracy": 0.565568,
            "f1": 0.557864,
            "f1_weighted": 0.55176
          }
        ],
        "main_score": 0.570444,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 553.9659695625305,
  "kg_co2_emissions": null
}