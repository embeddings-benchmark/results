{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.38.49",
  "scores": {
    "validation": [
      {
        "accuracy": 0.523315,
        "f1": 0.475063,
        "f1_weighted": 0.513341,
        "scores_per_experiment": [
          {
            "accuracy": 0.515494,
            "f1": 0.474439,
            "f1_weighted": 0.515504
          },
          {
            "accuracy": 0.553369,
            "f1": 0.49451,
            "f1_weighted": 0.550421
          },
          {
            "accuracy": 0.52484,
            "f1": 0.482594,
            "f1_weighted": 0.517534
          },
          {
            "accuracy": 0.516478,
            "f1": 0.461789,
            "f1_weighted": 0.50433
          },
          {
            "accuracy": 0.535662,
            "f1": 0.473121,
            "f1_weighted": 0.52121
          },
          {
            "accuracy": 0.512543,
            "f1": 0.48337,
            "f1_weighted": 0.501951
          },
          {
            "accuracy": 0.530251,
            "f1": 0.490076,
            "f1_weighted": 0.517186
          },
          {
            "accuracy": 0.533202,
            "f1": 0.471278,
            "f1_weighted": 0.519234
          },
          {
            "accuracy": 0.491392,
            "f1": 0.450083,
            "f1_weighted": 0.477479
          },
          {
            "accuracy": 0.519921,
            "f1": 0.469369,
            "f1_weighted": 0.50856
          }
        ],
        "main_score": 0.523315,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.520074,
        "f1": 0.487395,
        "f1_weighted": 0.508642,
        "scores_per_experiment": [
          {
            "accuracy": 0.520847,
            "f1": 0.48317,
            "f1_weighted": 0.516352
          },
          {
            "accuracy": 0.537323,
            "f1": 0.491263,
            "f1_weighted": 0.532181
          },
          {
            "accuracy": 0.515804,
            "f1": 0.488331,
            "f1_weighted": 0.506047
          },
          {
            "accuracy": 0.513114,
            "f1": 0.46995,
            "f1_weighted": 0.499227
          },
          {
            "accuracy": 0.528917,
            "f1": 0.487179,
            "f1_weighted": 0.51191
          },
          {
            "accuracy": 0.500336,
            "f1": 0.481146,
            "f1_weighted": 0.49072
          },
          {
            "accuracy": 0.532952,
            "f1": 0.508601,
            "f1_weighted": 0.51877
          },
          {
            "accuracy": 0.538332,
            "f1": 0.495836,
            "f1_weighted": 0.524887
          },
          {
            "accuracy": 0.503699,
            "f1": 0.486561,
            "f1_weighted": 0.484753
          },
          {
            "accuracy": 0.509415,
            "f1": 0.481914,
            "f1_weighted": 0.501567
          }
        ],
        "main_score": 0.520074,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 1006.5673806667328,
  "kg_co2_emissions": null
}