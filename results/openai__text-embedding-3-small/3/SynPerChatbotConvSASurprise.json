{
  "dataset_revision": "62dad66fc2837b0ac5e5175fe7c265d2d502a386",
  "task_name": "SynPerChatbotConvSASurprise",
  "mteb_version": "1.38.49",
  "scores": {
    "test": [
      {
        "accuracy": 0.682645,
        "f1": 0.673153,
        "f1_weighted": 0.686605,
        "ap": 0.739866,
        "ap_weighted": 0.739866,
        "scores_per_experiment": [
          {
            "accuracy": 0.644628,
            "f1": 0.638304,
            "f1_weighted": 0.650557,
            "ap": 0.716158,
            "ap_weighted": 0.716158
          },
          {
            "accuracy": 0.702479,
            "f1": 0.696488,
            "f1_weighted": 0.707413,
            "ap": 0.758606,
            "ap_weighted": 0.758606
          },
          {
            "accuracy": 0.669421,
            "f1": 0.659155,
            "f1_weighted": 0.67431,
            "ap": 0.725152,
            "ap_weighted": 0.725152
          },
          {
            "accuracy": 0.661157,
            "f1": 0.62825,
            "f1_weighted": 0.656587,
            "ap": 0.695219,
            "ap_weighted": 0.695219
          },
          {
            "accuracy": 0.743802,
            "f1": 0.738004,
            "f1_weighted": 0.747989,
            "ap": 0.790841,
            "ap_weighted": 0.790841
          },
          {
            "accuracy": 0.694215,
            "f1": 0.687295,
            "f1_weighted": 0.699213,
            "ap": 0.749845,
            "ap_weighted": 0.749845
          },
          {
            "accuracy": 0.661157,
            "f1": 0.657791,
            "f1_weighted": 0.666486,
            "ap": 0.736392,
            "ap_weighted": 0.736392
          },
          {
            "accuracy": 0.652893,
            "f1": 0.647475,
            "f1_weighted": 0.658671,
            "ap": 0.724099,
            "ap_weighted": 0.724099
          },
          {
            "accuracy": 0.710744,
            "f1": 0.706819,
            "f1_weighted": 0.71551,
            "ap": 0.772414,
            "ap_weighted": 0.772414
          },
          {
            "accuracy": 0.68595,
            "f1": 0.671946,
            "f1_weighted": 0.689311,
            "ap": 0.729938,
            "ap_weighted": 0.729938
          }
        ],
        "main_score": 0.682645,
        "hf_subset": "default",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 68.41716480255127,
  "kg_co2_emissions": null
}