{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 17.859944581985474,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.5",
  "scores": {
    "test": [
      {
        "accuracy": 0.6414800000000002,
        "f1": 0.6293543652382521,
        "f1_weighted": 0.6293543652382521,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6414800000000002,
        "scores_per_experiment": [
          {
            "accuracy": 0.6346,
            "f1": 0.6238348033951117,
            "f1_weighted": 0.6238348033951117
          },
          {
            "accuracy": 0.626,
            "f1": 0.6135405233732006,
            "f1_weighted": 0.6135405233732006
          },
          {
            "accuracy": 0.6532,
            "f1": 0.6480091351145788,
            "f1_weighted": 0.6480091351145788
          },
          {
            "accuracy": 0.642,
            "f1": 0.6338171021741126,
            "f1_weighted": 0.6338171021741127
          },
          {
            "accuracy": 0.645,
            "f1": 0.624999228804507,
            "f1_weighted": 0.624999228804507
          },
          {
            "accuracy": 0.6514,
            "f1": 0.6409126837788517,
            "f1_weighted": 0.6409126837788517
          },
          {
            "accuracy": 0.6586,
            "f1": 0.6575223285417018,
            "f1_weighted": 0.6575223285417017
          },
          {
            "accuracy": 0.6658,
            "f1": 0.6650231057821473,
            "f1_weighted": 0.6650231057821473
          },
          {
            "accuracy": 0.6458,
            "f1": 0.636723657087441,
            "f1_weighted": 0.636723657087441
          },
          {
            "accuracy": 0.5924,
            "f1": 0.5491610843308681,
            "f1_weighted": 0.5491610843308682
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}