{
  "dataset_revision": "5b740b7c42c73d586420812a35745fc37118862f",
  "task_name": "NoRecClassification",
  "mteb_version": "2.1.5",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.482422,
            "f1": 0.461839,
            "f1_weighted": 0.4971,
            "precision": 0.473729,
            "precision_weighted": 0.537095,
            "recall": 0.487183,
            "recall_weighted": 0.482422,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.402344,
            "f1": 0.381509,
            "f1_weighted": 0.414939,
            "precision": 0.388675,
            "precision_weighted": 0.444559,
            "recall": 0.395918,
            "recall_weighted": 0.402344,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.491699,
            "f1": 0.46579,
            "f1_weighted": 0.506312,
            "precision": 0.473364,
            "precision_weighted": 0.540065,
            "recall": 0.484671,
            "recall_weighted": 0.491699,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.500977,
            "f1": 0.47351,
            "f1_weighted": 0.511387,
            "precision": 0.474683,
            "precision_weighted": 0.533435,
            "recall": 0.48758,
            "recall_weighted": 0.500977,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.460449,
            "f1": 0.433028,
            "f1_weighted": 0.469849,
            "precision": 0.441089,
            "precision_weighted": 0.500236,
            "recall": 0.457875,
            "recall_weighted": 0.460449,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.490723,
            "f1": 0.468403,
            "f1_weighted": 0.500045,
            "precision": 0.469805,
            "precision_weighted": 0.524894,
            "recall": 0.485325,
            "recall_weighted": 0.490723,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.452148,
            "f1": 0.440095,
            "f1_weighted": 0.472186,
            "precision": 0.473809,
            "precision_weighted": 0.550657,
            "recall": 0.480753,
            "recall_weighted": 0.452148,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.480469,
            "f1": 0.440036,
            "f1_weighted": 0.485448,
            "precision": 0.439814,
            "precision_weighted": 0.493908,
            "recall": 0.445921,
            "recall_weighted": 0.480469,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.456543,
            "f1": 0.442289,
            "f1_weighted": 0.467122,
            "precision": 0.448941,
            "precision_weighted": 0.504225,
            "recall": 0.465779,
            "recall_weighted": 0.456543,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.447754,
            "f1": 0.417351,
            "f1_weighted": 0.451429,
            "precision": 0.443525,
            "precision_weighted": 0.515575,
            "recall": 0.441522,
            "recall_weighted": 0.447754,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.466553,
        "f1": 0.442385,
        "f1_weighted": 0.477582,
        "precision": 0.452743,
        "precision_weighted": 0.514465,
        "recall": 0.463253,
        "recall_weighted": 0.466553,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.466553,
        "hf_subset": "default",
        "languages": [
          "nob-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 25.77444291114807,
  "kg_co2_emissions": null
}