{
  "dataset_revision": "de7ba3406ee55ea2cc52a0a41408fa6aede6d3c6",
  "task_name": "LccSentimentClassification",
  "mteb_version": "2.1.5",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.593333,
            "f1": 0.583262,
            "f1_weighted": 0.601953,
            "precision": 0.583333,
            "precision_weighted": 0.645667,
            "recall": 0.623946,
            "recall_weighted": 0.593333,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.466667,
            "f1": 0.442241,
            "f1_weighted": 0.477454,
            "precision": 0.443388,
            "precision_weighted": 0.511686,
            "recall": 0.467419,
            "recall_weighted": 0.466667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.513333,
            "f1": 0.512061,
            "f1_weighted": 0.515801,
            "precision": 0.540417,
            "precision_weighted": 0.629174,
            "recall": 0.583642,
            "recall_weighted": 0.513333,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.586667,
            "f1": 0.572438,
            "f1_weighted": 0.593368,
            "precision": 0.564969,
            "precision_weighted": 0.625907,
            "recall": 0.607511,
            "recall_weighted": 0.586667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.593333,
            "f1": 0.567524,
            "f1_weighted": 0.596757,
            "precision": 0.562378,
            "precision_weighted": 0.60655,
            "recall": 0.582156,
            "recall_weighted": 0.593333,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.566667,
            "f1": 0.540085,
            "f1_weighted": 0.573525,
            "precision": 0.541986,
            "precision_weighted": 0.600804,
            "recall": 0.557192,
            "recall_weighted": 0.566667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.6,
            "f1": 0.559643,
            "f1_weighted": 0.608857,
            "precision": 0.562474,
            "precision_weighted": 0.627012,
            "recall": 0.570395,
            "recall_weighted": 0.6,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.526667,
            "f1": 0.495629,
            "f1_weighted": 0.541805,
            "precision": 0.498266,
            "precision_weighted": 0.57938,
            "recall": 0.514972,
            "recall_weighted": 0.526667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.526667,
            "f1": 0.517876,
            "f1_weighted": 0.535748,
            "precision": 0.532753,
            "precision_weighted": 0.611929,
            "recall": 0.573551,
            "recall_weighted": 0.526667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.513333,
            "f1": 0.503014,
            "f1_weighted": 0.524945,
            "precision": 0.515769,
            "precision_weighted": 0.595147,
            "recall": 0.539668,
            "recall_weighted": 0.513333,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.548667,
        "f1": 0.529377,
        "f1_weighted": 0.557021,
        "precision": 0.534573,
        "precision_weighted": 0.603326,
        "recall": 0.562045,
        "recall_weighted": 0.548667,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.548667,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 8.769345998764038,
  "kg_co2_emissions": null
}