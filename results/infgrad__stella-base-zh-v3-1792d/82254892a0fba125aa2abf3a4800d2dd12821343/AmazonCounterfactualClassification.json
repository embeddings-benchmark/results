{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "task_name": "AmazonCounterfactualClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.642704,
        "f1": 0.620213,
        "f1_weighted": 0.656128,
        "ap": 0.771572,
        "ap_weighted": 0.771572,
        "scores_per_experiment": [
          {
            "accuracy": 0.618026,
            "f1": 0.596344,
            "f1_weighted": 0.633283,
            "ap": 0.756981,
            "ap_weighted": 0.756981
          },
          {
            "accuracy": 0.67382,
            "f1": 0.639385,
            "f1_weighted": 0.683385,
            "ap": 0.772173,
            "ap_weighted": 0.772173
          },
          {
            "accuracy": 0.684549,
            "f1": 0.660927,
            "f1_weighted": 0.696265,
            "ap": 0.792418,
            "ap_weighted": 0.792418
          },
          {
            "accuracy": 0.650215,
            "f1": 0.640396,
            "f1_weighted": 0.663858,
            "ap": 0.799227,
            "ap_weighted": 0.799227
          },
          {
            "accuracy": 0.624464,
            "f1": 0.608179,
            "f1_weighted": 0.639719,
            "ap": 0.768907,
            "ap_weighted": 0.768907
          },
          {
            "accuracy": 0.650215,
            "f1": 0.628362,
            "f1_weighted": 0.663945,
            "ap": 0.775167,
            "ap_weighted": 0.775167
          },
          {
            "accuracy": 0.658798,
            "f1": 0.62854,
            "f1_weighted": 0.670401,
            "ap": 0.768837,
            "ap_weighted": 0.768837
          },
          {
            "accuracy": 0.592275,
            "f1": 0.570902,
            "f1_weighted": 0.608715,
            "ap": 0.743075,
            "ap_weighted": 0.743075
          },
          {
            "accuracy": 0.626609,
            "f1": 0.594036,
            "f1_weighted": 0.639441,
            "ap": 0.749043,
            "ap_weighted": 0.749043
          },
          {
            "accuracy": 0.648069,
            "f1": 0.635054,
            "f1_weighted": 0.662266,
            "ap": 0.789887,
            "ap_weighted": 0.789887
          }
        ],
        "main_score": 0.642704,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.613812,
        "f1": 0.59108,
        "f1_weighted": 0.628265,
        "ap": 0.753177,
        "ap_weighted": 0.753177,
        "scores_per_experiment": [
          {
            "accuracy": 0.577088,
            "f1": 0.549022,
            "f1_weighted": 0.593108,
            "ap": 0.72603,
            "ap_weighted": 0.72603
          },
          {
            "accuracy": 0.622056,
            "f1": 0.584442,
            "f1_weighted": 0.633434,
            "ap": 0.740292,
            "ap_weighted": 0.740292
          },
          {
            "accuracy": 0.657388,
            "f1": 0.63608,
            "f1_weighted": 0.670587,
            "ap": 0.778379,
            "ap_weighted": 0.778379
          },
          {
            "accuracy": 0.618844,
            "f1": 0.609825,
            "f1_weighted": 0.63307,
            "ap": 0.778532,
            "ap_weighted": 0.778532
          },
          {
            "accuracy": 0.598501,
            "f1": 0.580203,
            "f1_weighted": 0.614547,
            "ap": 0.748784,
            "ap_weighted": 0.748784
          },
          {
            "accuracy": 0.625268,
            "f1": 0.605315,
            "f1_weighted": 0.640089,
            "ap": 0.761719,
            "ap_weighted": 0.761719
          },
          {
            "accuracy": 0.633833,
            "f1": 0.602038,
            "f1_weighted": 0.646117,
            "ap": 0.75196,
            "ap_weighted": 0.75196
          },
          {
            "accuracy": 0.585653,
            "f1": 0.565109,
            "f1_weighted": 0.602149,
            "ap": 0.738675,
            "ap_weighted": 0.738675
          },
          {
            "accuracy": 0.604925,
            "f1": 0.577736,
            "f1_weighted": 0.619724,
            "ap": 0.741425,
            "ap_weighted": 0.741425
          },
          {
            "accuracy": 0.614561,
            "f1": 0.601031,
            "f1_weighted": 0.629822,
            "ap": 0.76597,
            "ap_weighted": 0.76597
          }
        ],
        "main_score": 0.613812,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 53.7367000579834,
  "kg_co2_emissions": null
}