{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 257.95423316955566,
  "kg_co2_emissions": 0.01583345889352739,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.730230712890625,
        "ap": 0.6652038469322569,
        "ap_weighted": 0.6652038469322569,
        "f1": 0.7289216251855238,
        "f1_weighted": 0.7289180989412405,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.730230712890625,
        "scores_per_experiment": [
          {
            "accuracy": 0.716461181640625,
            "ap": 0.6546769250018841,
            "ap_weighted": 0.6546769250018841,
            "f1": 0.7164599244163372,
            "f1_weighted": 0.7164596693273514
          },
          {
            "accuracy": 0.74713134765625,
            "ap": 0.6716212737452746,
            "ap_weighted": 0.6716212737452746,
            "f1": 0.7425525484252935,
            "f1_weighted": 0.7425378795032721
          },
          {
            "accuracy": 0.72967529296875,
            "ap": 0.6719399522854387,
            "ap_weighted": 0.6719399522854387,
            "f1": 0.7292425568698386,
            "f1_weighted": 0.7292471815304378
          },
          {
            "accuracy": 0.728668212890625,
            "ap": 0.6676026880317467,
            "ap_weighted": 0.6676026880317467,
            "f1": 0.7286341730580992,
            "f1_weighted": 0.7286354715803208
          },
          {
            "accuracy": 0.729217529296875,
            "ap": 0.6601783955969401,
            "ap_weighted": 0.6601783955969401,
            "f1": 0.727718923158051,
            "f1_weighted": 0.7277102927648204
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}