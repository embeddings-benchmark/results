{
  "dataset_revision": "87b7a0d1c402dbb481db649569c556d9aa27ac05",
  "task_name": "TweetTopicSingleClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test_2021": [
      {
        "accuracy": 0.462197,
        "f1": 0.360271,
        "f1_weighted": 0.511172,
        "scores_per_experiment": [
          {
            "accuracy": 0.385706,
            "f1": 0.311359,
            "f1_weighted": 0.435092
          },
          {
            "accuracy": 0.428825,
            "f1": 0.347228,
            "f1_weighted": 0.471003
          },
          {
            "accuracy": 0.478441,
            "f1": 0.35882,
            "f1_weighted": 0.51622
          },
          {
            "accuracy": 0.467218,
            "f1": 0.376065,
            "f1_weighted": 0.514385
          },
          {
            "accuracy": 0.475487,
            "f1": 0.390232,
            "f1_weighted": 0.526014
          },
          {
            "accuracy": 0.467809,
            "f1": 0.363646,
            "f1_weighted": 0.521962
          },
          {
            "accuracy": 0.491435,
            "f1": 0.352089,
            "f1_weighted": 0.533131
          },
          {
            "accuracy": 0.47785,
            "f1": 0.381372,
            "f1_weighted": 0.530107
          },
          {
            "accuracy": 0.434141,
            "f1": 0.336044,
            "f1_weighted": 0.496458
          },
          {
            "accuracy": 0.515062,
            "f1": 0.385855,
            "f1_weighted": 0.567346
          }
        ],
        "main_score": 0.462197,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 6.654687166213989,
  "kg_co2_emissions": 0.00014959928501728363
}