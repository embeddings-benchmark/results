{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "task_name": "GreekLegalCodeClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.104199,
        "f1": 0.07589,
        "f1_weighted": 0.111419,
        "scores_per_experiment": [
          {
            "accuracy": 0.111328,
            "f1": 0.076341,
            "f1_weighted": 0.118688
          },
          {
            "accuracy": 0.10791,
            "f1": 0.081251,
            "f1_weighted": 0.118071
          },
          {
            "accuracy": 0.111816,
            "f1": 0.079767,
            "f1_weighted": 0.121333
          },
          {
            "accuracy": 0.091309,
            "f1": 0.070805,
            "f1_weighted": 0.095699
          },
          {
            "accuracy": 0.099121,
            "f1": 0.069008,
            "f1_weighted": 0.104873
          },
          {
            "accuracy": 0.100098,
            "f1": 0.068799,
            "f1_weighted": 0.108987
          },
          {
            "accuracy": 0.106934,
            "f1": 0.074066,
            "f1_weighted": 0.113993
          },
          {
            "accuracy": 0.102539,
            "f1": 0.079741,
            "f1_weighted": 0.106976
          },
          {
            "accuracy": 0.10498,
            "f1": 0.078816,
            "f1_weighted": 0.111758
          },
          {
            "accuracy": 0.105957,
            "f1": 0.080307,
            "f1_weighted": 0.113815
          }
        ],
        "main_score": 0.104199,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.09624,
        "f1": 0.072313,
        "f1_weighted": 0.101248,
        "scores_per_experiment": [
          {
            "accuracy": 0.110352,
            "f1": 0.079107,
            "f1_weighted": 0.118323
          },
          {
            "accuracy": 0.091309,
            "f1": 0.070342,
            "f1_weighted": 0.099413
          },
          {
            "accuracy": 0.09668,
            "f1": 0.07633,
            "f1_weighted": 0.09868
          },
          {
            "accuracy": 0.092285,
            "f1": 0.065657,
            "f1_weighted": 0.094828
          },
          {
            "accuracy": 0.09668,
            "f1": 0.069206,
            "f1_weighted": 0.105249
          },
          {
            "accuracy": 0.092285,
            "f1": 0.069455,
            "f1_weighted": 0.093937
          },
          {
            "accuracy": 0.094727,
            "f1": 0.072118,
            "f1_weighted": 0.100992
          },
          {
            "accuracy": 0.092285,
            "f1": 0.070904,
            "f1_weighted": 0.096523
          },
          {
            "accuracy": 0.099121,
            "f1": 0.073475,
            "f1_weighted": 0.106188
          },
          {
            "accuracy": 0.09668,
            "f1": 0.07654,
            "f1_weighted": 0.098345
          }
        ],
        "main_score": 0.09624,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ]
  },
  "evaluation_time": 530.1028618812561,
  "kg_co2_emissions": 0.012189799420894444
}