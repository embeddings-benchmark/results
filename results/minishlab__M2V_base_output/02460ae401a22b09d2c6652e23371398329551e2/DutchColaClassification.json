{
  "dataset_revision": "2269ed7d95d8abaab829f1592b4b2047372e9f81",
  "task_name": "DutchColaClassification",
  "mteb_version": "2.1.17",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.525833,
            "f1": 0.525702,
            "f1_weighted": 0.525702,
            "precision": 0.525862,
            "precision_weighted": 0.525862,
            "recall": 0.525833,
            "recall_weighted": 0.525833,
            "ap": 0.513563,
            "ap_weighted": 0.513563
          },
          {
            "accuracy": 0.53375,
            "f1": 0.533079,
            "f1_weighted": 0.533079,
            "precision": 0.533945,
            "precision_weighted": 0.533945,
            "recall": 0.53375,
            "recall_weighted": 0.53375,
            "ap": 0.517934,
            "ap_weighted": 0.517934
          },
          {
            "accuracy": 0.54,
            "f1": 0.539831,
            "f1_weighted": 0.539831,
            "precision": 0.540059,
            "precision_weighted": 0.540059,
            "recall": 0.54,
            "recall_weighted": 0.54,
            "ap": 0.521664,
            "ap_weighted": 0.521664
          },
          {
            "accuracy": 0.524167,
            "f1": 0.522723,
            "f1_weighted": 0.522723,
            "precision": 0.524463,
            "precision_weighted": 0.524463,
            "recall": 0.524167,
            "recall_weighted": 0.524167,
            "ap": 0.51274,
            "ap_weighted": 0.51274
          },
          {
            "accuracy": 0.518333,
            "f1": 0.518186,
            "f1_weighted": 0.518186,
            "precision": 0.518356,
            "precision_weighted": 0.518356,
            "recall": 0.518333,
            "recall_weighted": 0.518333,
            "ap": 0.509515,
            "ap_weighted": 0.509515
          },
          {
            "accuracy": 0.517917,
            "f1": 0.517128,
            "f1_weighted": 0.517128,
            "precision": 0.518035,
            "precision_weighted": 0.518035,
            "recall": 0.517917,
            "recall_weighted": 0.517917,
            "ap": 0.509308,
            "ap_weighted": 0.509308
          },
          {
            "accuracy": 0.494583,
            "f1": 0.494545,
            "f1_weighted": 0.494545,
            "precision": 0.494582,
            "precision_weighted": 0.494582,
            "recall": 0.494583,
            "recall_weighted": 0.494583,
            "ap": 0.497321,
            "ap_weighted": 0.497321
          },
          {
            "accuracy": 0.51625,
            "f1": 0.515937,
            "f1_weighted": 0.515937,
            "precision": 0.516292,
            "precision_weighted": 0.516292,
            "recall": 0.51625,
            "recall_weighted": 0.51625,
            "ap": 0.508403,
            "ap_weighted": 0.508403
          },
          {
            "accuracy": 0.51375,
            "f1": 0.513748,
            "f1_weighted": 0.513748,
            "precision": 0.51375,
            "precision_weighted": 0.51375,
            "recall": 0.51375,
            "recall_weighted": 0.51375,
            "ap": 0.507065,
            "ap_weighted": 0.507065
          },
          {
            "accuracy": 0.511667,
            "f1": 0.511341,
            "f1_weighted": 0.511341,
            "precision": 0.511698,
            "precision_weighted": 0.511698,
            "recall": 0.511667,
            "recall_weighted": 0.511667,
            "ap": 0.505963,
            "ap_weighted": 0.505963
          }
        ],
        "accuracy": 0.519625,
        "f1": 0.519222,
        "f1_weighted": 0.519222,
        "precision": 0.519704,
        "precision_weighted": 0.519704,
        "recall": 0.519625,
        "recall_weighted": 0.519625,
        "ap": 0.510347,
        "ap_weighted": 0.510347,
        "main_score": 0.519222,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 13.611520767211914,
  "kg_co2_emissions": null
}