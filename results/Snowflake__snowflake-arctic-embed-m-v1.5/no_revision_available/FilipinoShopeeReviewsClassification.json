{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 21.50929284095764,
  "kg_co2_emissions": 0.0007443072571894719,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.27998046875,
        "f1": 0.27540021081008265,
        "f1_weighted": 0.27539392918370165,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.27998046875,
        "scores_per_experiment": [
          {
            "accuracy": 0.275390625,
            "f1": 0.2783642735907549,
            "f1_weighted": 0.27837369912035076
          },
          {
            "accuracy": 0.29541015625,
            "f1": 0.28564382127716764,
            "f1_weighted": 0.2856198217510346
          },
          {
            "accuracy": 0.2685546875,
            "f1": 0.25855997401479885,
            "f1_weighted": 0.25849799729664347
          },
          {
            "accuracy": 0.27783203125,
            "f1": 0.28022395111466836,
            "f1_weighted": 0.28021323561699596
          },
          {
            "accuracy": 0.25927734375,
            "f1": 0.258718084052176,
            "f1_weighted": 0.2587359141608093
          },
          {
            "accuracy": 0.28125,
            "f1": 0.27167719478621694,
            "f1_weighted": 0.27169207055076355
          },
          {
            "accuracy": 0.24462890625,
            "f1": 0.24498478714113037,
            "f1_weighted": 0.24495647689728295
          },
          {
            "accuracy": 0.294921875,
            "f1": 0.2899022596990736,
            "f1_weighted": 0.28992028448692886
          },
          {
            "accuracy": 0.3173828125,
            "f1": 0.3057476523900316,
            "f1_weighted": 0.3057323890735173
          },
          {
            "accuracy": 0.28515625,
            "f1": 0.28018011003480847,
            "f1_weighted": 0.2801974028826895
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.28544921875,
        "f1": 0.2807467373992889,
        "f1_weighted": 0.28073316540370513,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.28544921875,
        "scores_per_experiment": [
          {
            "accuracy": 0.28564453125,
            "f1": 0.2868146529189342,
            "f1_weighted": 0.2868231134500074
          },
          {
            "accuracy": 0.28271484375,
            "f1": 0.27582481571549294,
            "f1_weighted": 0.2758018535688999
          },
          {
            "accuracy": 0.27392578125,
            "f1": 0.2641423186527372,
            "f1_weighted": 0.2641055168449907
          },
          {
            "accuracy": 0.3056640625,
            "f1": 0.3060627166524451,
            "f1_weighted": 0.3060384186735189
          },
          {
            "accuracy": 0.26611328125,
            "f1": 0.2640371697281748,
            "f1_weighted": 0.26403571560004935
          },
          {
            "accuracy": 0.2978515625,
            "f1": 0.2878409404032892,
            "f1_weighted": 0.28784500207474956
          },
          {
            "accuracy": 0.23681640625,
            "f1": 0.2366981960672986,
            "f1_weighted": 0.23664271212380433
          },
          {
            "accuracy": 0.3037109375,
            "f1": 0.3006822163290776,
            "f1_weighted": 0.3006815829362045
          },
          {
            "accuracy": 0.318359375,
            "f1": 0.3073505747845407,
            "f1_weighted": 0.30732286070875564
          },
          {
            "accuracy": 0.28369140625,
            "f1": 0.2780137727408986,
            "f1_weighted": 0.2780348780560713
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}