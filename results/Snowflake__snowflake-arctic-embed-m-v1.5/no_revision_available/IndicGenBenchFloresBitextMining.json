{
  "dataset_revision": "f8650438298df086750ff4973661bb58a201a5ee",
  "evaluation_time": 324.8283860683441,
  "kg_co2_emissions": 0.015309094688307861,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.0029644268774703555,
        "f1": 0.0019782510373036006,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.0019782510373036006,
        "precision": 0.0019772687904508444,
        "recall": 0.0029644268774703555
      },
      {
        "accuracy": 0.041501976284584984,
        "f1": 0.02091067283076058,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ],
        "main_score": 0.02091067283076058,
        "precision": 0.01752254264486211,
        "recall": 0.041501976284584984
      },
      {
        "accuracy": 0.00691699604743083,
        "f1": 0.005930839970604008,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ],
        "main_score": 0.005930839970604008,
        "precision": 0.005929847861874805,
        "recall": 0.00691699604743083
      },
      {
        "accuracy": 0.037549407114624504,
        "f1": 0.016423268139402685,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ],
        "main_score": 0.016423268139402685,
        "precision": 0.013041526573222332,
        "recall": 0.037549407114624504
      },
      {
        "accuracy": 0.00691699604743083,
        "f1": 0.00465569888932451,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.00465569888932451,
        "precision": 0.004469282824670348,
        "recall": 0.00691699604743083
      },
      {
        "accuracy": 0.041501976284584984,
        "f1": 0.023860582921870356,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ],
        "main_score": 0.023860582921870356,
        "precision": 0.020349017560115954,
        "recall": 0.041501976284584984
      },
      {
        "accuracy": 0.010869565217391304,
        "f1": 0.009883468768156859,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ],
        "main_score": 0.009883468768156859,
        "precision": 0.009882446906551436,
        "recall": 0.010869565217391304
      },
      {
        "accuracy": 0.05533596837944664,
        "f1": 0.025033092009235973,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ],
        "main_score": 0.025033092009235973,
        "precision": 0.020896780286415468,
        "recall": 0.05533596837944664
      },
      {
        "accuracy": 0.005928853754940711,
        "f1": 0.004942730139248325,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.004942730139248325,
        "precision": 0.004941721832892812,
        "recall": 0.005928853754940711
      },
      {
        "accuracy": 0.036561264822134384,
        "f1": 0.012255039948162974,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ],
        "main_score": 0.012255039948162974,
        "precision": 0.009317728916309794,
        "recall": 0.036561264822134384
      },
      {
        "accuracy": 0.007905138339920948,
        "f1": 0.006590717770841994,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.006590717770841994,
        "precision": 0.006424478584035598,
        "recall": 0.007905138339920948
      },
      {
        "accuracy": 0.038537549407114624,
        "f1": 0.022529028601238805,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ],
        "main_score": 0.022529028601238805,
        "precision": 0.01930460111636305,
        "recall": 0.038537549407114624
      },
      {
        "accuracy": 0.009881422924901186,
        "f1": 0.007539969951858354,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.007539969951858354,
        "precision": 0.007314952806756084,
        "recall": 0.009881422924901186
      },
      {
        "accuracy": 0.0266798418972332,
        "f1": 0.015598575659473117,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ],
        "main_score": 0.015598575659473117,
        "precision": 0.012777789645585843,
        "recall": 0.0266798418972332
      },
      {
        "accuracy": 0.03162055335968379,
        "f1": 0.028987634571010803,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.028987634571010803,
        "precision": 0.028327810526554902,
        "recall": 0.03162055335968379
      },
      {
        "accuracy": 0.10968379446640317,
        "f1": 0.05561079687073871,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ],
        "main_score": 0.05561079687073871,
        "precision": 0.046406081732379874,
        "recall": 0.10968379446640317
      },
      {
        "accuracy": 0.003952569169960474,
        "f1": 0.0029868846568451312,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.0029868846568451312,
        "precision": 0.0029757848348553,
        "recall": 0.003952569169960474
      },
      {
        "accuracy": 0.02865612648221344,
        "f1": 0.017723487388466717,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ],
        "main_score": 0.017723487388466717,
        "precision": 0.015834930383274097,
        "recall": 0.02865612648221344
      },
      {
        "accuracy": 0.004940711462450593,
        "f1": 0.003954551401239291,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ],
        "main_score": 0.003954551401239291,
        "precision": 0.003953561280695907,
        "recall": 0.004940711462450593
      },
      {
        "accuracy": 0.029644268774703556,
        "f1": 0.014361958408223371,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ],
        "main_score": 0.014361958408223371,
        "precision": 0.011876028940291095,
        "recall": 0.029644268774703556
      },
      {
        "accuracy": 0.009881422924901186,
        "f1": 0.00830277345738027,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ],
        "main_score": 0.00830277345738027,
        "precision": 0.008153364445925996,
        "recall": 0.009881422924901186
      },
      {
        "accuracy": 0.06027667984189723,
        "f1": 0.03508216666435685,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ],
        "main_score": 0.03508216666435685,
        "precision": 0.03048153699267305,
        "recall": 0.06027667984189723
      },
      {
        "accuracy": 0.019762845849802372,
        "f1": 0.018447661591474297,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ],
        "main_score": 0.018447661591474297,
        "precision": 0.018281803195773935,
        "recall": 0.019762845849802372
      },
      {
        "accuracy": 0.07213438735177866,
        "f1": 0.042848886308316736,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ],
        "main_score": 0.042848886308316736,
        "precision": 0.03746300940420837,
        "recall": 0.07213438735177866
      },
      {
        "accuracy": 0.00691699604743083,
        "f1": 0.005601493731661776,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ],
        "main_score": 0.005601493731661776,
        "precision": 0.005435794013293902,
        "recall": 0.00691699604743083
      },
      {
        "accuracy": 0.05138339920948617,
        "f1": 0.02013498280047727,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ],
        "main_score": 0.02013498280047727,
        "precision": 0.01601845610079124,
        "recall": 0.05138339920948617
      },
      {
        "accuracy": 0.009881422924901186,
        "f1": 0.00856590218292346,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ],
        "main_score": 0.00856590218292346,
        "precision": 0.008400211658876444,
        "recall": 0.009881422924901186
      },
      {
        "accuracy": 0.05138339920948617,
        "f1": 0.0246854980318842,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ],
        "main_score": 0.0246854980318842,
        "precision": 0.020551279729384704,
        "recall": 0.05138339920948617
      },
      {
        "accuracy": 0.03359683794466403,
        "f1": 0.03228147712497441,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ],
        "main_score": 0.03228147712497441,
        "precision": 0.032115706808549326,
        "recall": 0.03359683794466403
      },
      {
        "accuracy": 0.12549407114624506,
        "f1": 0.08255060846059604,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ],
        "main_score": 0.08255060846059604,
        "precision": 0.07482322443461725,
        "recall": 0.12549407114624506
      },
      {
        "accuracy": 0.020750988142292492,
        "f1": 0.018456190807497046,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ],
        "main_score": 0.018456190807497046,
        "precision": 0.01828608443743365,
        "recall": 0.020750988142292492
      },
      {
        "accuracy": 0.05632411067193676,
        "f1": 0.02872161968415705,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ],
        "main_score": 0.02872161968415705,
        "precision": 0.02415640598581464,
        "recall": 0.05632411067193676
      },
      {
        "accuracy": 0.00691699604743083,
        "f1": 0.005931117540261422,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.005931117540261422,
        "precision": 0.005929986945643109,
        "recall": 0.00691699604743083
      },
      {
        "accuracy": 0.03162055335968379,
        "f1": 0.020642709563164108,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ],
        "main_score": 0.020642709563164108,
        "precision": 0.018961988130638607,
        "recall": 0.03162055335968379
      },
      {
        "accuracy": 0.001976284584980237,
        "f1": 0.0009903529016455775,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ],
        "main_score": 0.0009903529016455775,
        "precision": 0.0009892488348109362,
        "recall": 0.001976284584980237
      },
      {
        "accuracy": 0.020750988142292492,
        "f1": 0.012820413151226007,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ],
        "main_score": 0.012820413151226007,
        "precision": 0.011195606106673299,
        "recall": 0.020750988142292492
      },
      {
        "accuracy": 0.008893280632411068,
        "f1": 0.007249136985595572,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ],
        "main_score": 0.007249136985595572,
        "precision": 0.006918378064623124,
        "recall": 0.008893280632411068
      },
      {
        "accuracy": 0.038537549407114624,
        "f1": 0.022527762628296828,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ],
        "main_score": 0.022527762628296828,
        "precision": 0.018592254683853324,
        "recall": 0.038537549407114624
      },
      {
        "accuracy": 0.007905138339920948,
        "f1": 0.00658991061960309,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ],
        "main_score": 0.00658991061960309,
        "precision": 0.006424073903851457,
        "recall": 0.007905138339920948
      },
      {
        "accuracy": 0.0533596837944664,
        "f1": 0.02964461162535879,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ],
        "main_score": 0.02964461162535879,
        "precision": 0.024964185325133184,
        "recall": 0.0533596837944664
      },
      {
        "accuracy": 0.007905138339920948,
        "f1": 0.005976099837965003,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ],
        "main_score": 0.005976099837965003,
        "precision": 0.005953000446144034,
        "recall": 0.007905138339920948
      },
      {
        "accuracy": 0.036561264822134384,
        "f1": 0.02021751117730939,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ],
        "main_score": 0.02021751117730939,
        "precision": 0.016910156999671064,
        "recall": 0.036561264822134384
      },
      {
        "accuracy": 0.012845849802371542,
        "f1": 0.010608620560186228,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ],
        "main_score": 0.010608620560186228,
        "precision": 0.010129740135238359,
        "recall": 0.012845849802371542
      },
      {
        "accuracy": 0.06818181818181818,
        "f1": 0.04202301648699568,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ],
        "main_score": 0.04202301648699568,
        "precision": 0.03593508100974706,
        "recall": 0.06818181818181818
      },
      {
        "accuracy": 0.0029644268774703555,
        "f1": 0.0016491218731007925,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ],
        "main_score": 0.0016491218731007925,
        "precision": 0.0014833237109739307,
        "recall": 0.0029644268774703555
      },
      {
        "accuracy": 0.023715415019762844,
        "f1": 0.010239279545398113,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ],
        "main_score": 0.010239279545398113,
        "precision": 0.00838838601689069,
        "recall": 0.023715415019762844
      },
      {
        "accuracy": 0.008893280632411068,
        "f1": 0.005793170601846007,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ],
        "main_score": 0.005793170601846007,
        "precision": 0.005284780103395079,
        "recall": 0.008893280632411068
      },
      {
        "accuracy": 0.05434782608695652,
        "f1": 0.032085811835524565,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ],
        "main_score": 0.032085811835524565,
        "precision": 0.027884679509624435,
        "recall": 0.05434782608695652
      },
      {
        "accuracy": 0.010869565217391304,
        "f1": 0.008420305646350263,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ],
        "main_score": 0.008420305646350263,
        "precision": 0.008245157489013822,
        "recall": 0.010869565217391304
      },
      {
        "accuracy": 0.06422924901185771,
        "f1": 0.04396577587607213,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ],
        "main_score": 0.04396577587607213,
        "precision": 0.03925242578813521,
        "recall": 0.06422924901185771
      },
      {
        "accuracy": 0.00691699604743083,
        "f1": 0.005107388060195576,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ],
        "main_score": 0.005107388060195576,
        "precision": 0.004777015187303001,
        "recall": 0.00691699604743083
      },
      {
        "accuracy": 0.05237154150197629,
        "f1": 0.029043876201430056,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ],
        "main_score": 0.029043876201430056,
        "precision": 0.024192821463867385,
        "recall": 0.05237154150197629
      },
      {
        "accuracy": 0.003952569169960474,
        "f1": 0.002966665025358216,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ],
        "main_score": 0.002966665025358216,
        "precision": 0.002965547220205605,
        "recall": 0.003952569169960474
      },
      {
        "accuracy": 0.025691699604743084,
        "f1": 0.012511458068253246,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ],
        "main_score": 0.012511458068253246,
        "precision": 0.010542738717145832,
        "recall": 0.025691699604743084
      },
      {
        "accuracy": 0.004940711462450593,
        "f1": 0.003954755325474834,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ],
        "main_score": 0.003954755325474834,
        "precision": 0.003953663458213508,
        "recall": 0.004940711462450593
      },
      {
        "accuracy": 0.05138339920948617,
        "f1": 0.031423910862704164,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ],
        "main_score": 0.031423910862704164,
        "precision": 0.027341350022778894,
        "recall": 0.05138339920948617
      },
      {
        "accuracy": 0.003952569169960474,
        "f1": 0.002966415091338545,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ],
        "main_score": 0.002966415091338545,
        "precision": 0.0029654219855191877,
        "recall": 0.003952569169960474
      },
      {
        "accuracy": 0.021739130434782608,
        "f1": 0.006186999873861466,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ],
        "main_score": 0.006186999873861466,
        "precision": 0.004447921783677978,
        "recall": 0.021739130434782608
      }
    ],
    "validation": [
      {
        "accuracy": 0.0050150451354062184,
        "f1": 0.002705430286356215,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.002705430286356215,
        "precision": 0.0025230956961632862,
        "recall": 0.0050150451354062184
      },
      {
        "accuracy": 0.033099297893681046,
        "f1": 0.016124345243009288,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ],
        "main_score": 0.016124345243009288,
        "precision": 0.012429818710811058,
        "recall": 0.033099297893681046
      },
      {
        "accuracy": 0.003009027081243731,
        "f1": 0.0020125951625367907,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ],
        "main_score": 0.0020125951625367907,
        "precision": 0.002009317425962097,
        "recall": 0.003009027081243731
      },
      {
        "accuracy": 0.034102306920762285,
        "f1": 0.010191334488607508,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ],
        "main_score": 0.010191334488607508,
        "precision": 0.007907721893908343,
        "recall": 0.034102306920762285
      },
      {
        "accuracy": 0.00802407221664995,
        "f1": 0.006704020106295899,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.006704020106295899,
        "precision": 0.006528280493654877,
        "recall": 0.00802407221664995
      },
      {
        "accuracy": 0.041123370110330994,
        "f1": 0.024033724834854876,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ],
        "main_score": 0.024033724834854876,
        "precision": 0.02088772792653131,
        "recall": 0.041123370110330994
      },
      {
        "accuracy": 0.011033099297893681,
        "f1": 0.00969935539713095,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ],
        "main_score": 0.00969935539713095,
        "precision": 0.009530389730342106,
        "recall": 0.011033099297893681
      },
      {
        "accuracy": 0.04613841524573721,
        "f1": 0.018941817191136686,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ],
        "main_score": 0.018941817191136686,
        "precision": 0.015032029162399813,
        "recall": 0.04613841524573721
      },
      {
        "accuracy": 0.006018054162487462,
        "f1": 0.003489982683376819,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.003489982683376819,
        "precision": 0.003087850521999892,
        "recall": 0.006018054162487462
      },
      {
        "accuracy": 0.041123370110330994,
        "f1": 0.01480739455936367,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ],
        "main_score": 0.01480739455936367,
        "precision": 0.01143337406650607,
        "recall": 0.041123370110330994
      },
      {
        "accuracy": 0.010030090270812437,
        "f1": 0.006391624391743276,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.006391624391743276,
        "precision": 0.006037848437720361,
        "recall": 0.010030090270812437
      },
      {
        "accuracy": 0.044132397191574725,
        "f1": 0.027402642307356447,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ],
        "main_score": 0.027402642307356447,
        "precision": 0.024424228707326447,
        "recall": 0.044132397191574725
      },
      {
        "accuracy": 0.0160481444332999,
        "f1": 0.01252324147077772,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.01252324147077772,
        "precision": 0.012042820869300306,
        "recall": 0.0160481444332999
      },
      {
        "accuracy": 0.029087261785356068,
        "f1": 0.016246951532809105,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ],
        "main_score": 0.016246951532809105,
        "precision": 0.013841732249265775,
        "recall": 0.029087261785356068
      },
      {
        "accuracy": 0.031093279839518557,
        "f1": 0.02875701101948071,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.02875701101948071,
        "precision": 0.028286901520889196,
        "recall": 0.031093279839518557
      },
      {
        "accuracy": 0.08726178535606821,
        "f1": 0.040694296881404926,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ],
        "main_score": 0.040694296881404926,
        "precision": 0.03305100098568551,
        "recall": 0.08726178535606821
      },
      {
        "accuracy": 0.013039117352056168,
        "f1": 0.008846863219982579,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.008846863219982579,
        "precision": 0.008510006897496624,
        "recall": 0.013039117352056168
      },
      {
        "accuracy": 0.03009027081243731,
        "f1": 0.018586977864811366,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ],
        "main_score": 0.018586977864811366,
        "precision": 0.016087395274957964,
        "recall": 0.03009027081243731
      },
      {
        "accuracy": 0.004012036108324975,
        "f1": 0.002020323368095498,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ],
        "main_score": 0.002020323368095498,
        "precision": 0.0020132051130461793,
        "recall": 0.004012036108324975
      },
      {
        "accuracy": 0.03510531594784353,
        "f1": 0.02164115628717827,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ],
        "main_score": 0.02164115628717827,
        "precision": 0.019437474113139036,
        "recall": 0.03510531594784353
      },
      {
        "accuracy": 0.015045135406218655,
        "f1": 0.01355256244925252,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01355256244925252,
        "precision": 0.013379459736494913,
        "recall": 0.015045135406218655
      },
      {
        "accuracy": 0.05717151454363089,
        "f1": 0.034765409180803064,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ],
        "main_score": 0.034765409180803064,
        "precision": 0.030604396100171818,
        "recall": 0.05717151454363089
      },
      {
        "accuracy": 0.011033099297893681,
        "f1": 0.01036442661317285,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01036442661317285,
        "precision": 0.010030090270812437,
        "recall": 0.011033099297893681
      },
      {
        "accuracy": 0.07422266800401203,
        "f1": 0.045116613184625266,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ],
        "main_score": 0.045116613184625266,
        "precision": 0.0377151591383573,
        "recall": 0.07422266800401203
      },
      {
        "accuracy": 0.013039117352056168,
        "f1": 0.009841392653369026,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ],
        "main_score": 0.009841392653369026,
        "precision": 0.009105413024918019,
        "recall": 0.013039117352056168
      },
      {
        "accuracy": 0.05115346038114343,
        "f1": 0.018975414616315543,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ],
        "main_score": 0.018975414616315543,
        "precision": 0.014288136740492391,
        "recall": 0.05115346038114343
      },
      {
        "accuracy": 0.012036108324974924,
        "f1": 0.009898761031295325,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ],
        "main_score": 0.009898761031295325,
        "precision": 0.009641235297194772,
        "recall": 0.012036108324974924
      },
      {
        "accuracy": 0.04914744232698094,
        "f1": 0.024058432188200748,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ],
        "main_score": 0.024058432188200748,
        "precision": 0.02092386223696088,
        "recall": 0.04914744232698094
      },
      {
        "accuracy": 0.044132397191574725,
        "f1": 0.040869336354464304,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ],
        "main_score": 0.040869336354464304,
        "precision": 0.04037785591652203,
        "recall": 0.044132397191574725
      },
      {
        "accuracy": 0.1584754262788365,
        "f1": 0.10272119460267122,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ],
        "main_score": 0.10272119460267122,
        "precision": 0.091118519049362,
        "recall": 0.1584754262788365
      },
      {
        "accuracy": 0.023069207622868605,
        "f1": 0.020296726642170984,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ],
        "main_score": 0.020296726642170984,
        "precision": 0.019643517017217064,
        "recall": 0.023069207622868605
      },
      {
        "accuracy": 0.0641925777331996,
        "f1": 0.037162863512331654,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ],
        "main_score": 0.037162863512331654,
        "precision": 0.032527319562150714,
        "recall": 0.0641925777331996
      },
      {
        "accuracy": 0.007021063189568706,
        "f1": 0.005194990491644786,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.005194990491644786,
        "precision": 0.004854306509271404,
        "recall": 0.007021063189568706
      },
      {
        "accuracy": 0.03811434302908726,
        "f1": 0.020103101315023308,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ],
        "main_score": 0.020103101315023308,
        "precision": 0.016312888347808897,
        "recall": 0.03811434302908726
      },
      {
        "accuracy": 0.010030090270812437,
        "f1": 0.00673334491828226,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ],
        "main_score": 0.00673334491828226,
        "precision": 0.006543117935680469,
        "recall": 0.010030090270812437
      },
      {
        "accuracy": 0.041123370110330994,
        "f1": 0.023513584131593768,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ],
        "main_score": 0.023513584131593768,
        "precision": 0.02065836596192369,
        "recall": 0.041123370110330994
      },
      {
        "accuracy": 0.00802407221664995,
        "f1": 0.004160543338421802,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ],
        "main_score": 0.004160543338421802,
        "precision": 0.003773014302083524,
        "recall": 0.00802407221664995
      },
      {
        "accuracy": 0.03811434302908726,
        "f1": 0.019652590387768714,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ],
        "main_score": 0.019652590387768714,
        "precision": 0.016037491985335516,
        "recall": 0.03811434302908726
      },
      {
        "accuracy": 0.004012036108324975,
        "f1": 0.0025230730952547566,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ],
        "main_score": 0.0025230730952547566,
        "precision": 0.0023481904045469745,
        "recall": 0.004012036108324975
      },
      {
        "accuracy": 0.04012036108324975,
        "f1": 0.026762900534214477,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ],
        "main_score": 0.026762900534214477,
        "precision": 0.0237409941481138,
        "recall": 0.04012036108324975
      },
      {
        "accuracy": 0.007021063189568706,
        "f1": 0.005353425869004864,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ],
        "main_score": 0.005353425869004864,
        "precision": 0.005017071416269009,
        "recall": 0.007021063189568706
      },
      {
        "accuracy": 0.03911735205616851,
        "f1": 0.023842392396152476,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ],
        "main_score": 0.023842392396152476,
        "precision": 0.021772562154503034,
        "recall": 0.03911735205616851
      },
      {
        "accuracy": 0.015045135406218655,
        "f1": 0.012238533783167685,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ],
        "main_score": 0.012238533783167685,
        "precision": 0.011845637923872629,
        "recall": 0.015045135406218655
      },
      {
        "accuracy": 0.05616850551654965,
        "f1": 0.036583958223877985,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ],
        "main_score": 0.036583958223877985,
        "precision": 0.03278925504931777,
        "recall": 0.05616850551654965
      },
      {
        "accuracy": 0.003009027081243731,
        "f1": 0.0016927976913195728,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ],
        "main_score": 0.0016927976913195728,
        "precision": 0.001515183849420602,
        "recall": 0.003009027081243731
      },
      {
        "accuracy": 0.01805416248746239,
        "f1": 0.008862061349725203,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ],
        "main_score": 0.008862061349725203,
        "precision": 0.006721951568992693,
        "recall": 0.01805416248746239
      },
      {
        "accuracy": 0.011033099297893681,
        "f1": 0.009542812835670132,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ],
        "main_score": 0.009542812835670132,
        "precision": 0.009368581936285045,
        "recall": 0.011033099297893681
      },
      {
        "accuracy": 0.05115346038114343,
        "f1": 0.028498496129981246,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ],
        "main_score": 0.028498496129981246,
        "precision": 0.024415047976469133,
        "recall": 0.05115346038114343
      },
      {
        "accuracy": 0.011033099297893681,
        "f1": 0.00872715115042096,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ],
        "main_score": 0.00872715115042096,
        "precision": 0.008543017676429936,
        "recall": 0.011033099297893681
      },
      {
        "accuracy": 0.05717151454363089,
        "f1": 0.03382922271086764,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ],
        "main_score": 0.03382922271086764,
        "precision": 0.029403759619406555,
        "recall": 0.05717151454363089
      },
      {
        "accuracy": 0.00802407221664995,
        "f1": 0.004701834327698221,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ],
        "main_score": 0.004701834327698221,
        "precision": 0.004189851520563167,
        "recall": 0.00802407221664995
      },
      {
        "accuracy": 0.04714142427281846,
        "f1": 0.024459155339842942,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ],
        "main_score": 0.024459155339842942,
        "precision": 0.02046565408671464,
        "recall": 0.04714142427281846
      },
      {
        "accuracy": 0.007021063189568706,
        "f1": 0.006185222333667669,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ],
        "main_score": 0.006185222333667669,
        "precision": 0.005850885991307256,
        "recall": 0.007021063189568706
      },
      {
        "accuracy": 0.02708124373119358,
        "f1": 0.015354131531339627,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ],
        "main_score": 0.015354131531339627,
        "precision": 0.01356928999848833,
        "recall": 0.02708124373119358
      },
      {
        "accuracy": 0.010030090270812437,
        "f1": 0.008041500345531081,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ],
        "main_score": 0.008041500345531081,
        "precision": 0.008032838562076627,
        "recall": 0.010030090270812437
      },
      {
        "accuracy": 0.03510531594784353,
        "f1": 0.02078709120187457,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ],
        "main_score": 0.02078709120187457,
        "precision": 0.01904542090662061,
        "recall": 0.03510531594784353
      },
      {
        "accuracy": 0.004012036108324975,
        "f1": 0.002046154232458859,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ],
        "main_score": 0.002046154232458859,
        "precision": 0.0020262920700960486,
        "recall": 0.004012036108324975
      },
      {
        "accuracy": 0.017051153460381142,
        "f1": 0.005675925087721797,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ],
        "main_score": 0.005675925087721797,
        "precision": 0.00468074318974911,
        "recall": 0.017051153460381142
      }
    ]
  },
  "task_name": "IndicGenBenchFloresBitextMining"
}