{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 4072.805805683136,
  "kg_co2_emissions": 0.25185653839335603,
  "mteb_version": "1.18.0",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08562767522739434,
        "map": 0.10241102957533489,
        "mrr": 0.08562767522739434,
        "nAUC_map_diff1": -0.022494821849185453,
        "nAUC_map_max": -0.09578033498269355,
        "nAUC_map_std": -0.047813401632338805,
        "nAUC_mrr_diff1": -0.021958585484057935,
        "nAUC_mrr_max": -0.09034636498042267,
        "nAUC_mrr_std": -0.04908281763698277
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.1031381171942604,
        "map": 0.12008645090088438,
        "mrr": 0.1031381171942604,
        "nAUC_map_diff1": 0.061778530999383234,
        "nAUC_map_max": -0.03470138322005413,
        "nAUC_map_std": -0.017634719669682154,
        "nAUC_mrr_diff1": 0.05951340353276084,
        "nAUC_mrr_max": -0.03702233266362137,
        "nAUC_mrr_std": -0.02750224229793013
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09065473430911539,
        "map": 0.1068397418813908,
        "mrr": 0.09065473430911539,
        "nAUC_map_diff1": 0.10089182727526201,
        "nAUC_map_max": -0.015636166783628305,
        "nAUC_map_std": 0.15772281203871721,
        "nAUC_mrr_diff1": 0.09217877524017926,
        "nAUC_mrr_max": -0.019753949346805667,
        "nAUC_mrr_std": 0.16172532713812215
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09280546118031963,
        "map": 0.11125781094367936,
        "mrr": 0.09280546118031963,
        "nAUC_map_diff1": 0.12632901252092282,
        "nAUC_map_max": -0.021001878970500668,
        "nAUC_map_std": 0.03957328959876099,
        "nAUC_mrr_diff1": 0.13208833037704798,
        "nAUC_mrr_max": -0.01521519323807466,
        "nAUC_mrr_std": 0.030949409355671986
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08077441536411002,
        "map": 0.09813681018679246,
        "mrr": 0.08077441536411002,
        "nAUC_map_diff1": 0.1019895321799215,
        "nAUC_map_max": 0.06752775459032181,
        "nAUC_map_std": 0.16923325996200636,
        "nAUC_mrr_diff1": 0.10189992683496324,
        "nAUC_mrr_max": 0.06986991552949412,
        "nAUC_mrr_std": 0.1597080689944045
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.11042940107121506,
        "map": 0.12800200384432456,
        "mrr": 0.11042940107121506,
        "nAUC_map_diff1": 0.09724720191765542,
        "nAUC_map_max": -0.11123900301173324,
        "nAUC_map_std": -0.003917587296151527,
        "nAUC_mrr_diff1": 0.09782675934009963,
        "nAUC_mrr_max": -0.11906546592375951,
        "nAUC_mrr_std": -0.0070580198800785825
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}