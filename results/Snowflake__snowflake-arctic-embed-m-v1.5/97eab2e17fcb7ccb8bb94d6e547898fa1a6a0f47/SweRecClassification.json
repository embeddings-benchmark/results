{
  "dataset_revision": "b07c6ce548f6a7ac8d546e1bbe197a0086409190",
  "task_name": "SweRecClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.55874,
        "f1": 0.483688,
        "f1_weighted": 0.574684,
        "scores_per_experiment": [
          {
            "accuracy": 0.555176,
            "f1": 0.469728,
            "f1_weighted": 0.571161
          },
          {
            "accuracy": 0.635742,
            "f1": 0.542972,
            "f1_weighted": 0.64298
          },
          {
            "accuracy": 0.561523,
            "f1": 0.50635,
            "f1_weighted": 0.593576
          },
          {
            "accuracy": 0.508301,
            "f1": 0.465176,
            "f1_weighted": 0.551309
          },
          {
            "accuracy": 0.557617,
            "f1": 0.490671,
            "f1_weighted": 0.583175
          },
          {
            "accuracy": 0.524414,
            "f1": 0.43101,
            "f1_weighted": 0.514771
          },
          {
            "accuracy": 0.5625,
            "f1": 0.477889,
            "f1_weighted": 0.577147
          },
          {
            "accuracy": 0.645996,
            "f1": 0.543625,
            "f1_weighted": 0.651954
          },
          {
            "accuracy": 0.435547,
            "f1": 0.415334,
            "f1_weighted": 0.462968
          },
          {
            "accuracy": 0.600586,
            "f1": 0.494124,
            "f1_weighted": 0.597795
          }
        ],
        "main_score": 0.55874,
        "hf_subset": "default",
        "languages": [
          "swe-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 9.078317880630493,
  "kg_co2_emissions": 0.0003744920220987892
}