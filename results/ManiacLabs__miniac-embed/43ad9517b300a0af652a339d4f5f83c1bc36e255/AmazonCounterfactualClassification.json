{
  "dataset_revision": "1f7e6a9d6fa6e64c53d146e428565640410c0df1",
  "task_name": "AmazonCounterfactualClassification",
  "mteb_version": "2.7.26",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.6850746268656717,
            "f1": 0.6223799386170403,
            "f1_weighted": 0.7160774946809288,
            "precision": 0.627782327104891,
            "precision_weighted": 0.7971682568324165,
            "recall": 0.6944794572929796,
            "recall_weighted": 0.6850746268656717,
            "ap": 0.3049228117576612,
            "ap_weighted": 0.3049228117576612
          },
          {
            "accuracy": 0.6880597014925374,
            "f1": 0.6352197981113643,
            "f1_weighted": 0.7197636435212411,
            "precision": 0.6450685981172006,
            "precision_weighted": 0.8170330116185474,
            "recall": 0.7252262459459842,
            "recall_weighted": 0.6880597014925374,
            "ap": 0.3269485547212866,
            "ap_weighted": 0.3269485547212866
          },
          {
            "accuracy": 0.664179104477612,
            "f1": 0.613313804504798,
            "f1_weighted": 0.6987172711258189,
            "precision": 0.6305011562364504,
            "precision_weighted": 0.8066627316517572,
            "recall": 0.7046056451727116,
            "recall_weighted": 0.664179104477612,
            "ap": 0.3078512174017472,
            "ap_weighted": 0.3078512174017472
          },
          {
            "accuracy": 0.7402985074626866,
            "f1": 0.6660019940179461,
            "f1_weighted": 0.7619291379592565,
            "precision": 0.6559324244339443,
            "precision_weighted": 0.8095992185220107,
            "recall": 0.720134827005056,
            "recall_weighted": 0.7402985074626866,
            "ap": 0.33846784630232607,
            "ap_weighted": 0.33846784630232607
          },
          {
            "accuracy": 0.6865671641791045,
            "f1": 0.6349209644106322,
            "f1_weighted": 0.7185386211786351,
            "precision": 0.6459366272140901,
            "precision_weighted": 0.818616409315401,
            "recall": 0.7271877522695407,
            "recall_weighted": 0.6865671641791045,
            "ap": 0.32798103532395206,
            "ap_weighted": 0.32798103532395206
          },
          {
            "accuracy": 0.7582089552238805,
            "f1": 0.6879025579683474,
            "f1_weighted": 0.7781069921829938,
            "precision": 0.6748430398371443,
            "precision_weighted": 0.8240241515818303,
            "recall": 0.7457123029642114,
            "recall_weighted": 0.7582089552238805,
            "ap": 0.3654649621568507,
            "ap_weighted": 0.3654649621568507
          },
          {
            "accuracy": 0.7582089552238805,
            "f1": 0.6879025579683474,
            "f1_weighted": 0.7781069921829938,
            "precision": 0.6748430398371443,
            "precision_weighted": 0.8240241515818303,
            "recall": 0.7457123029642114,
            "recall_weighted": 0.7582089552238805,
            "ap": 0.3654649621568507,
            "ap_weighted": 0.3654649621568507
          },
          {
            "accuracy": 0.7074626865671642,
            "f1": 0.6517412935323383,
            "f1_weighted": 0.736570876958491,
            "precision": 0.6549491810707581,
            "precision_weighted": 0.8223359798405525,
            "recall": 0.7372856151482106,
            "recall_weighted": 0.7074626865671642,
            "ap": 0.3406276972647817,
            "ap_weighted": 0.3406276972647817
          },
          {
            "accuracy": 0.6582089552238806,
            "f1": 0.6173233334746682,
            "f1_weighted": 0.6934938068704611,
            "precision": 0.6448660714285714,
            "precision_weighted": 0.8263292910447761,
            "recall": 0.7297865711169964,
            "recall_weighted": 0.6582089552238806,
            "ap": 0.32376773100148115,
            "ap_weighted": 0.32376773100148115
          },
          {
            "accuracy": 0.6746268656716418,
            "f1": 0.6190043926920629,
            "f1_weighted": 0.7076527090032667,
            "precision": 0.6305092174103597,
            "precision_weighted": 0.8035861452046202,
            "recall": 0.7024317013411888,
            "recall_weighted": 0.6746268656716418,
            "ap": 0.3083101875243318,
            "ap_weighted": 0.3083101875243318
          }
        ],
        "accuracy": 0.702089552238806,
        "f1": 0.6435710635297546,
        "f1_weighted": 0.7308957545664088,
        "precision": 0.6485231682690553,
        "precision_weighted": 0.8149379347193741,
        "recall": 0.7232562421221089,
        "recall_weighted": 0.702089552238806,
        "ap": 0.33098070056112694,
        "ap_weighted": 0.33098070056112694,
        "main_score": 0.702089552238806,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 6.938409090042114,
  "kg_co2_emissions": null
}