{
  "dataset_revision": "d604517c81ca91fe16a244d1248fc021f9ecee7a",
  "task_name": "TweetSentimentExtractionClassification",
  "mteb_version": "2.7.26",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.6293152235427278,
            "f1": 0.6305171853785395,
            "f1_weighted": 0.6191931503669376,
            "precision": 0.6290121817067295,
            "precision_weighted": 0.6297667231816637,
            "recall": 0.6511576063706617,
            "recall_weighted": 0.6293152235427278,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.6185625353706847,
            "f1": 0.6215975689977766,
            "f1_weighted": 0.6121982494937072,
            "precision": 0.6177202882937541,
            "precision_weighted": 0.6165537890422242,
            "recall": 0.6354238991591666,
            "recall_weighted": 0.6185625353706847,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.6304470854555744,
            "f1": 0.6355189602416251,
            "f1_weighted": 0.6291653371928039,
            "precision": 0.6330090026561125,
            "precision_weighted": 0.6287753243023624,
            "recall": 0.6388692298937086,
            "recall_weighted": 0.6304470854555744,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.5967741935483871,
            "f1": 0.5994930989166816,
            "f1_weighted": 0.5879739243863381,
            "precision": 0.5963226244131988,
            "precision_weighted": 0.5955146891004109,
            "recall": 0.6181788293302346,
            "recall_weighted": 0.5967741935483871,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.6211092246745897,
            "f1": 0.6251757227202988,
            "f1_weighted": 0.6184174829789271,
            "precision": 0.6213412509542252,
            "precision_weighted": 0.618937485929696,
            "recall": 0.6320137100735469,
            "recall_weighted": 0.6211092246745897,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.619128466327108,
            "f1": 0.6237376370036242,
            "f1_weighted": 0.6148103596190919,
            "precision": 0.6200798967002252,
            "precision_weighted": 0.6185035026336604,
            "recall": 0.6352695959223611,
            "recall_weighted": 0.619128466327108,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.5619694397283531,
            "f1": 0.558937133891261,
            "f1_weighted": 0.5474975732472122,
            "precision": 0.5562424189940618,
            "precision_weighted": 0.553463115423716,
            "recall": 0.5808923623973489,
            "recall_weighted": 0.5619694397283531,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.6474250141482739,
            "f1": 0.6514328305581929,
            "f1_weighted": 0.6432257443160369,
            "precision": 0.6474011136040123,
            "precision_weighted": 0.6458735202838923,
            "recall": 0.661835837175819,
            "recall_weighted": 0.6474250141482739,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.5812110922467459,
            "f1": 0.5867515995907953,
            "f1_weighted": 0.5771835681570358,
            "precision": 0.581694759099502,
            "precision_weighted": 0.5770701796013549,
            "recall": 0.595529070506405,
            "recall_weighted": 0.5812110922467459,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.5967741935483871,
            "f1": 0.6026887844199252,
            "f1_weighted": 0.5939349051743746,
            "precision": 0.5986408913583042,
            "precision_weighted": 0.5945422203353765,
            "recall": 0.6102108227221554,
            "recall_weighted": 0.5967741935483871,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.6102716468590831,
        "f1": 0.613585052171872,
        "f1_weighted": 0.6043600294932465,
        "precision": 0.6101464427780126,
        "precision_weighted": 0.6079000549834357,
        "recall": 0.6259380963551407,
        "recall_weighted": 0.6102716468590831,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.6102716468590831,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 29.237359523773193,
  "kg_co2_emissions": null
}