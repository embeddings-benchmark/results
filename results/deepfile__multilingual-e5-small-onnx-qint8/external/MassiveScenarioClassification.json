{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 0.6304303967720243,
                "f1": 0.603950085685985,
                "main_score": 0.6304303967720243
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 0.5683591123066578,
                "f1": 0.5495059828830849,
                "main_score": 0.5683591123066578
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 0.5962340282447881,
                "f1": 0.5952515999649822,
                "main_score": 0.5962340282447881
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 0.6085406859448554,
                "f1": 0.5912929909568128,
                "main_score": 0.6085406859448554
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 0.6276731674512441,
                "f1": 0.6115956061262772,
                "main_score": 0.6276731674512441
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 0.5018157363819771,
                "f1": 0.46984221762899575,
                "main_score": 0.5018157363819771
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 0.6892737054472091,
                "f1": 0.6769135611952979,
                "main_score": 0.6892737054472091
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 0.6918964357767318,
                "f1": 0.6846106138186214,
                "main_score": 0.6918964357767318
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 0.670712844653665,
                "f1": 0.6675545422473901,
                "main_score": 0.670712844653665
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 0.7447545393409549,
                "f1": 0.7438427146553253,
                "main_score": 0.7447545393409549
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 0.6982515131136517,
                "f1": 0.6963516462173847,
                "main_score": 0.6982515131136517
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 0.6870880968392737,
                "f1": 0.6745420662567926,
                "main_score": 0.6870880968392737
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 0.6595494283792871,
                "f1": 0.6506191009049221,
                "main_score": 0.6595494283792871
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 0.6875924680564895,
                "f1": 0.6830833379585945,
                "main_score": 0.6875924680564895
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 0.638063214525891,
                "f1": 0.6327304824376505,
                "main_score": 0.638063214525891
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 0.6768997982515133,
                "f1": 0.6654703855381324,
                "main_score": 0.6768997982515133
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 0.6646940147948891,
                "f1": 0.6591017343463396,
                "main_score": 0.6646940147948891
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 0.5949899125756556,
                "f1": 0.5790333469917769,
                "main_score": 0.5949899125756556
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 0.679219905850706,
                "f1": 0.6723169403762939,
                "main_score": 0.679219905850706
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 0.564862138533961,
                "f1": 0.5485282355583758,
                "main_score": 0.564862138533961
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 0.6904169468728985,
                "f1": 0.6883833333320462,
                "main_score": 0.6904169468728985
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 0.7388702084734365,
                "f1": 0.7404474735232298,
                "main_score": 0.7388702084734365
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 0.5663416274377943,
                "f1": 0.5511332211687954,
                "main_score": 0.5663416274377943
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 0.5223604572965702,
                "f1": 0.5086529813991055,
                "main_score": 0.5223604572965702
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 0.4662407531943511,
                "f1": 0.4363485467164535,
                "main_score": 0.4662407531943511
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 0.5915601882985878,
                "f1": 0.5752283751095992,
                "main_score": 0.5915601882985878
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 0.6984532616005381,
                "f1": 0.6960021127179696,
                "main_score": 0.6984532616005381
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 0.566577000672495,
                "f1": 0.5584219135523227,
                "main_score": 0.566577000672495
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 0.6653665097511768,
                "f1": 0.6509087787792639,
                "main_score": 0.6653665097511768
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 0.5931405514458642,
                "f1": 0.5806135303831491,
                "main_score": 0.5931405514458642
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 0.6488231338264964,
                "f1": 0.6275109940778792,
                "main_score": 0.6488231338264964
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 0.5886012104909213,
                "f1": 0.5629118323058282,
                "main_score": 0.5886012104909213
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 0.6737390719569603,
                "f1": 0.6627922244885102,
                "main_score": 0.6737390719569603
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 0.708675184936113,
                "f1": 0.7022146529932018,
                "main_score": 0.708675184936113
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 0.682212508406187,
                "f1": 0.6777454802056282,
                "main_score": 0.682212508406187
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 0.6818090114324142,
                "f1": 0.6803737625431622,
                "main_score": 0.6818090114324142
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 0.6465030262273034,
                "f1": 0.6379294548691286,
                "main_score": 0.6465030262273034
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 0.6948217888365837,
                "f1": 0.6996028997292197,
                "main_score": 0.6948217888365837
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 0.6017821116341627,
                "f1": 0.593935969827171,
                "main_score": 0.6017821116341627
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 0.6286146603900471,
                "f1": 0.6013369273503237,
                "main_score": 0.6286146603900471
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 0.7089441829186282,
                "f1": 0.7003064076194089,
                "main_score": 0.7089441829186282
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 0.5815063887020847,
                "f1": 0.5623326278499678,
                "main_score": 0.5815063887020847
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 0.5943846671149966,
                "f1": 0.5770440450281974,
                "main_score": 0.5943846671149966
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 0.608507061197041,
                "f1": 0.5922916396061171,
                "main_score": 0.608507061197041
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 0.7065568258238063,
                "f1": 0.6990736239440634,
                "main_score": 0.7065568258238063
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 0.608843308675185,
                "f1": 0.5930332663713599,
                "main_score": 0.608843308675185
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 0.6805312710154674,
                "f1": 0.6744024062594776,
                "main_score": 0.6805312710154674
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 0.6211163416274378,
                "f1": 0.6089083013084519,
                "main_score": 0.6211163416274378
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 0.6744115669132482,
                "f1": 0.6792227541674553,
                "main_score": 0.6744115669132482
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 0.7446872898453261,
                "f1": 0.7416376793486025,
                "main_score": 0.7446872898453261
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 0.6831876260928043,
                "f1": 0.685246745215607,
                "main_score": 0.6831876260928043
            }
        ]
    }
}