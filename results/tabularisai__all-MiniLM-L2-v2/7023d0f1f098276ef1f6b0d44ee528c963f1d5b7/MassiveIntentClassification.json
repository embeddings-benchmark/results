{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.36.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.651278,
        "f1": 0.627803,
        "f1_weighted": 0.652738,
        "scores_per_experiment": [
          {
            "accuracy": 0.661399,
            "f1": 0.641408,
            "f1_weighted": 0.663335
          },
          {
            "accuracy": 0.683255,
            "f1": 0.654961,
            "f1_weighted": 0.683575
          },
          {
            "accuracy": 0.645931,
            "f1": 0.624642,
            "f1_weighted": 0.641422
          },
          {
            "accuracy": 0.669132,
            "f1": 0.631521,
            "f1_weighted": 0.671292
          },
          {
            "accuracy": 0.653665,
            "f1": 0.620533,
            "f1_weighted": 0.648323
          },
          {
            "accuracy": 0.626765,
            "f1": 0.618059,
            "f1_weighted": 0.628714
          },
          {
            "accuracy": 0.644586,
            "f1": 0.627081,
            "f1_weighted": 0.648011
          },
          {
            "accuracy": 0.632145,
            "f1": 0.609611,
            "f1_weighted": 0.638609
          },
          {
            "accuracy": 0.631137,
            "f1": 0.607168,
            "f1_weighted": 0.636088
          },
          {
            "accuracy": 0.664761,
            "f1": 0.643048,
            "f1_weighted": 0.66801
          }
        ],
        "main_score": 0.651278,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.662125,
        "f1": 0.641079,
        "f1_weighted": 0.659594,
        "scores_per_experiment": [
          {
            "accuracy": 0.66847,
            "f1": 0.64241,
            "f1_weighted": 0.665
          },
          {
            "accuracy": 0.688146,
            "f1": 0.654565,
            "f1_weighted": 0.687215
          },
          {
            "accuracy": 0.654206,
            "f1": 0.634544,
            "f1_weighted": 0.650551
          },
          {
            "accuracy": 0.673389,
            "f1": 0.632997,
            "f1_weighted": 0.6707
          },
          {
            "accuracy": 0.674865,
            "f1": 0.650296,
            "f1_weighted": 0.667636
          },
          {
            "accuracy": 0.656173,
            "f1": 0.639999,
            "f1_weighted": 0.657242
          },
          {
            "accuracy": 0.658633,
            "f1": 0.649235,
            "f1_weighted": 0.65517
          },
          {
            "accuracy": 0.633546,
            "f1": 0.611302,
            "f1_weighted": 0.632902
          },
          {
            "accuracy": 0.645352,
            "f1": 0.636087,
            "f1_weighted": 0.643181
          },
          {
            "accuracy": 0.66847,
            "f1": 0.659353,
            "f1_weighted": 0.66634
          }
        ],
        "main_score": 0.662125,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 12.529825448989868,
  "kg_co2_emissions": null
}