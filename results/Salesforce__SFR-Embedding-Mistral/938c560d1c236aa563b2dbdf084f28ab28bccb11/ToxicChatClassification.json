{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.824914,
        "f1": 0.708751,
        "f1_weighted": 0.845964,
        "ap": 0.337364,
        "ap_weighted": 0.337364,
        "scores_per_experiment": [
          {
            "accuracy": 0.830756,
            "f1": 0.690879,
            "f1_weighted": 0.846655,
            "ap": 0.293084,
            "ap_weighted": 0.293084
          },
          {
            "accuracy": 0.769759,
            "f1": 0.662594,
            "f1_weighted": 0.805046,
            "ap": 0.294796,
            "ap_weighted": 0.294796
          },
          {
            "accuracy": 0.833333,
            "f1": 0.721302,
            "f1_weighted": 0.853675,
            "ap": 0.354932,
            "ap_weighted": 0.354932
          },
          {
            "accuracy": 0.861684,
            "f1": 0.746296,
            "f1_weighted": 0.874472,
            "ap": 0.381043,
            "ap_weighted": 0.381043
          },
          {
            "accuracy": 0.84622,
            "f1": 0.726011,
            "f1_weighted": 0.861967,
            "ap": 0.350986,
            "ap_weighted": 0.350986
          },
          {
            "accuracy": 0.847938,
            "f1": 0.730166,
            "f1_weighted": 0.863713,
            "ap": 0.358601,
            "ap_weighted": 0.358601
          },
          {
            "accuracy": 0.863402,
            "f1": 0.733292,
            "f1_weighted": 0.872844,
            "ap": 0.350874,
            "ap_weighted": 0.350874
          },
          {
            "accuracy": 0.734536,
            "f1": 0.642332,
            "f1_weighted": 0.778376,
            "ap": 0.291518,
            "ap_weighted": 0.291518
          },
          {
            "accuracy": 0.81701,
            "f1": 0.701024,
            "f1_weighted": 0.840528,
            "ap": 0.326845,
            "ap_weighted": 0.326845
          },
          {
            "accuracy": 0.844502,
            "f1": 0.733613,
            "f1_weighted": 0.862368,
            "ap": 0.370958,
            "ap_weighted": 0.370958
          }
        ],
        "main_score": 0.824914,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 35.22531747817993,
  "kg_co2_emissions": 0.008666484502578752
}