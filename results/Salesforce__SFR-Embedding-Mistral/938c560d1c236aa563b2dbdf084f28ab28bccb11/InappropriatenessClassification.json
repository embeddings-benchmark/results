{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "task_name": "InappropriatenessClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.706396,
        "f1": 0.704739,
        "f1_weighted": 0.704739,
        "ap": 0.64873,
        "ap_weighted": 0.64873,
        "scores_per_experiment": [
          {
            "accuracy": 0.712402,
            "f1": 0.712328,
            "f1_weighted": 0.712328,
            "ap": 0.652818,
            "ap_weighted": 0.652818
          },
          {
            "accuracy": 0.721191,
            "f1": 0.72109,
            "f1_weighted": 0.72109,
            "ap": 0.657726,
            "ap_weighted": 0.657726
          },
          {
            "accuracy": 0.706543,
            "f1": 0.702491,
            "f1_weighted": 0.702491,
            "ap": 0.65892,
            "ap_weighted": 0.65892
          },
          {
            "accuracy": 0.65625,
            "f1": 0.654445,
            "f1_weighted": 0.654445,
            "ap": 0.599456,
            "ap_weighted": 0.599456
          },
          {
            "accuracy": 0.692383,
            "f1": 0.68988,
            "f1_weighted": 0.68988,
            "ap": 0.64131,
            "ap_weighted": 0.64131
          },
          {
            "accuracy": 0.714844,
            "f1": 0.714408,
            "f1_weighted": 0.714408,
            "ap": 0.650235,
            "ap_weighted": 0.650235
          },
          {
            "accuracy": 0.708496,
            "f1": 0.708202,
            "f1_weighted": 0.708202,
            "ap": 0.650665,
            "ap_weighted": 0.650665
          },
          {
            "accuracy": 0.734375,
            "f1": 0.731331,
            "f1_weighted": 0.731331,
            "ap": 0.662477,
            "ap_weighted": 0.662477
          },
          {
            "accuracy": 0.70752,
            "f1": 0.706046,
            "f1_weighted": 0.706046,
            "ap": 0.653928,
            "ap_weighted": 0.653928
          },
          {
            "accuracy": 0.709961,
            "f1": 0.707168,
            "f1_weighted": 0.707168,
            "ap": 0.659764,
            "ap_weighted": 0.659764
          }
        ],
        "main_score": 0.706396,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 40.47022271156311,
  "kg_co2_emissions": 0.009815684304975945
}