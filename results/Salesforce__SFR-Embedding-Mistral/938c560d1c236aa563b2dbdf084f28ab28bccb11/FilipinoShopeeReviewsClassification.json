{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.395703,
        "f1": 0.355069,
        "f1_weighted": 0.355068,
        "scores_per_experiment": [
          {
            "accuracy": 0.411133,
            "f1": 0.376711,
            "f1_weighted": 0.376679
          },
          {
            "accuracy": 0.38916,
            "f1": 0.348219,
            "f1_weighted": 0.348276
          },
          {
            "accuracy": 0.412598,
            "f1": 0.356508,
            "f1_weighted": 0.356492
          },
          {
            "accuracy": 0.408691,
            "f1": 0.378006,
            "f1_weighted": 0.377974
          },
          {
            "accuracy": 0.390137,
            "f1": 0.34536,
            "f1_weighted": 0.345338
          },
          {
            "accuracy": 0.391602,
            "f1": 0.324108,
            "f1_weighted": 0.324125
          },
          {
            "accuracy": 0.391602,
            "f1": 0.352734,
            "f1_weighted": 0.352711
          },
          {
            "accuracy": 0.400391,
            "f1": 0.365806,
            "f1_weighted": 0.365814
          },
          {
            "accuracy": 0.385742,
            "f1": 0.365721,
            "f1_weighted": 0.365742
          },
          {
            "accuracy": 0.375977,
            "f1": 0.337516,
            "f1_weighted": 0.337528
          }
        ],
        "main_score": 0.395703,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.394336,
        "f1": 0.35374,
        "f1_weighted": 0.353747,
        "scores_per_experiment": [
          {
            "accuracy": 0.399902,
            "f1": 0.360146,
            "f1_weighted": 0.360112
          },
          {
            "accuracy": 0.402832,
            "f1": 0.363625,
            "f1_weighted": 0.363685
          },
          {
            "accuracy": 0.405273,
            "f1": 0.347168,
            "f1_weighted": 0.347154
          },
          {
            "accuracy": 0.383789,
            "f1": 0.350327,
            "f1_weighted": 0.350305
          },
          {
            "accuracy": 0.395996,
            "f1": 0.355876,
            "f1_weighted": 0.355869
          },
          {
            "accuracy": 0.40625,
            "f1": 0.341815,
            "f1_weighted": 0.34184
          },
          {
            "accuracy": 0.407715,
            "f1": 0.372694,
            "f1_weighted": 0.372673
          },
          {
            "accuracy": 0.381836,
            "f1": 0.343072,
            "f1_weighted": 0.343097
          },
          {
            "accuracy": 0.393555,
            "f1": 0.375478,
            "f1_weighted": 0.375532
          },
          {
            "accuracy": 0.366211,
            "f1": 0.327198,
            "f1_weighted": 0.327204
          }
        ],
        "main_score": 0.394336,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 51.74695324897766,
  "kg_co2_emissions": 0.013264919086744575
}