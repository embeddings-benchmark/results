{
  "dataset_revision": "9fa4d0824015fe537ae2c8166781f5c79873da2c",
  "task_name": "IndonesianIdClickbaitClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "train": [
      {
        "accuracy": 0.633447,
        "f1": 0.630673,
        "f1_weighted": 0.634445,
        "ap": 0.512002,
        "ap_weighted": 0.512002,
        "scores_per_experiment": [
          {
            "accuracy": 0.693848,
            "f1": 0.678389,
            "f1_weighted": 0.689751,
            "ap": 0.553705,
            "ap_weighted": 0.553705
          },
          {
            "accuracy": 0.557129,
            "f1": 0.556174,
            "f1_weighted": 0.559491,
            "ap": 0.455129,
            "ap_weighted": 0.455129
          },
          {
            "accuracy": 0.548828,
            "f1": 0.54827,
            "f1_weighted": 0.550829,
            "ap": 0.450951,
            "ap_weighted": 0.450951
          },
          {
            "accuracy": 0.64209,
            "f1": 0.641812,
            "f1_weighted": 0.640206,
            "ap": 0.524153,
            "ap_weighted": 0.524153
          },
          {
            "accuracy": 0.653809,
            "f1": 0.653521,
            "f1_weighted": 0.655129,
            "ap": 0.52891,
            "ap_weighted": 0.52891
          },
          {
            "accuracy": 0.611816,
            "f1": 0.611494,
            "f1_weighted": 0.613297,
            "ap": 0.495108,
            "ap_weighted": 0.495108
          },
          {
            "accuracy": 0.699219,
            "f1": 0.697847,
            "f1_weighted": 0.701127,
            "ap": 0.56719,
            "ap_weighted": 0.56719
          },
          {
            "accuracy": 0.672363,
            "f1": 0.669047,
            "f1_weighted": 0.674385,
            "ap": 0.539563,
            "ap_weighted": 0.539563
          },
          {
            "accuracy": 0.615234,
            "f1": 0.612628,
            "f1_weighted": 0.617748,
            "ap": 0.492744,
            "ap_weighted": 0.492744
          },
          {
            "accuracy": 0.640137,
            "f1": 0.63755,
            "f1_weighted": 0.642484,
            "ap": 0.512571,
            "ap_weighted": 0.512571
          }
        ],
        "main_score": 0.630673,
        "hf_subset": "default",
        "languages": [
          "ind-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 302.15175700187683,
  "kg_co2_emissions": null
}