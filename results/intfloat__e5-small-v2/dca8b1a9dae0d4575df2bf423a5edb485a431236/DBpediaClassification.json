{
  "dataset_revision": "9abd46cf7fc8b4c64290f26993c540b92aa145ac",
  "evaluation_time": 11.272807121276855,
  "kg_co2_emissions": 0.0003325739000414599,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.87236328125,
        "f1": 0.8651045166767946,
        "f1_weighted": 0.8651497212941646,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.87236328125,
        "scores_per_experiment": [
          {
            "accuracy": 0.86962890625,
            "f1": 0.8594661633860777,
            "f1_weighted": 0.859538485063928
          },
          {
            "accuracy": 0.8701171875,
            "f1": 0.8636368688605102,
            "f1_weighted": 0.8636998360281442
          },
          {
            "accuracy": 0.873046875,
            "f1": 0.8638452370215732,
            "f1_weighted": 0.8638868938444744
          },
          {
            "accuracy": 0.873046875,
            "f1": 0.8654841554160673,
            "f1_weighted": 0.8655195710596458
          },
          {
            "accuracy": 0.87353515625,
            "f1": 0.8668065923970236,
            "f1_weighted": 0.8668755866762122
          },
          {
            "accuracy": 0.8828125,
            "f1": 0.8781655363231551,
            "f1_weighted": 0.8782031437602599
          },
          {
            "accuracy": 0.8818359375,
            "f1": 0.876196243281254,
            "f1_weighted": 0.8762163187625692
          },
          {
            "accuracy": 0.869140625,
            "f1": 0.8633432827742673,
            "f1_weighted": 0.8633861999839325
          },
          {
            "accuracy": 0.869140625,
            "f1": 0.8617693494351563,
            "f1_weighted": 0.8617685514618236
          },
          {
            "accuracy": 0.861328125,
            "f1": 0.8523317378728613,
            "f1_weighted": 0.8524026263006559
          }
        ]
      }
    ]
  },
  "task_name": "DBpediaClassification"
}