{
  "dataset_revision": "f333c1fcfa3ab43f008a327c8bd0140441354d34",
  "evaluation_time": 5.9020280838012695,
  "kg_co2_emissions": 0.0001776106948687964,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.176708984375,
        "f1": 0.1643339140425095,
        "hf_subset": "default",
        "languages": [
          "por-Latn"
        ],
        "lrap": 0.8007493760850701,
        "main_score": 0.176708984375,
        "scores_per_experiment": [
          {
            "accuracy": 0.14404296875,
            "f1": 0.1683968770278098,
            "lrap": 0.822401258680556
          },
          {
            "accuracy": 0.150390625,
            "f1": 0.15728577673264474,
            "lrap": 0.7995198567708346
          },
          {
            "accuracy": 0.15283203125,
            "f1": 0.1445867193486626,
            "lrap": 0.8055691189236118
          },
          {
            "accuracy": 0.15380859375,
            "f1": 0.14602178058556328,
            "lrap": 0.8160671657986114
          },
          {
            "accuracy": 0.25146484375,
            "f1": 0.1703813773844304,
            "lrap": 0.8016289605034734
          },
          {
            "accuracy": 0.17724609375,
            "f1": 0.18050913221241047,
            "lrap": 0.7955457899305569
          },
          {
            "accuracy": 0.189453125,
            "f1": 0.16583464928257124,
            "lrap": 0.7638075086805562
          },
          {
            "accuracy": 0.1123046875,
            "f1": 0.17095404876445555,
            "lrap": 0.8003065321180556
          },
          {
            "accuracy": 0.19287109375,
            "f1": 0.15429873106337183,
            "lrap": 0.8053521050347237
          },
          {
            "accuracy": 0.24267578125,
            "f1": 0.18507004802317498,
            "lrap": 0.7972954644097232
          }
        ]
      }
    ]
  },
  "task_name": "BrazilianToxicTweetsClassification"
}