{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 3073.836204767227,
  "kg_co2_emissions": 0.13430301723412294,
  "mteb_version": "1.18.0",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.04579220171214553,
        "map": 0.06222271868368156,
        "mrr": 0.04579220171214553,
        "nAUC_map_diff1": 0.19734922254052595,
        "nAUC_map_max": 0.1031366526877617,
        "nAUC_map_std": 0.34237943065860704,
        "nAUC_mrr_diff1": 0.19755661959625975,
        "nAUC_mrr_max": 0.10511128183218829,
        "nAUC_mrr_std": 0.31910098735437403
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08004563976390666,
        "map": 0.09793948097145082,
        "mrr": 0.08004563976390666,
        "nAUC_map_diff1": 0.11005710309886235,
        "nAUC_map_max": -0.05290343097647737,
        "nAUC_map_std": 0.08918806102357642,
        "nAUC_mrr_diff1": 0.11324291391332919,
        "nAUC_mrr_max": -0.059230713704629685,
        "nAUC_mrr_std": 0.07942553909083501
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07201083956994944,
        "map": 0.08823035814702308,
        "mrr": 0.07201083956994944,
        "nAUC_map_diff1": 0.06153217554532927,
        "nAUC_map_max": 0.04372589350465127,
        "nAUC_map_std": 0.25542967905871317,
        "nAUC_mrr_diff1": 0.05975051220944572,
        "nAUC_mrr_max": 0.061705772694729294,
        "nAUC_mrr_std": 0.23620706677431844
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07893821867491177,
        "map": 0.09538752235594344,
        "mrr": 0.07893821867491177,
        "nAUC_map_diff1": 0.09189972379581676,
        "nAUC_map_max": -0.006392206044049636,
        "nAUC_map_std": 0.25705344493518784,
        "nAUC_mrr_diff1": 0.10537842166511022,
        "nAUC_mrr_max": 0.011848619812719934,
        "nAUC_mrr_std": 0.23475742043622458
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.04553318995112888,
        "map": 0.06195076270991331,
        "mrr": 0.04553318995112888,
        "nAUC_map_diff1": 0.17964298897586603,
        "nAUC_map_max": 0.05526032071661163,
        "nAUC_map_std": 0.23524497048774526,
        "nAUC_mrr_diff1": 0.1850556622641659,
        "nAUC_mrr_max": 0.07027187745531482,
        "nAUC_mrr_std": 0.20930730920805504
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09176335663653111,
        "map": 0.10838965862867063,
        "mrr": 0.09176335663653111,
        "nAUC_map_diff1": 0.20923332520104998,
        "nAUC_map_max": 0.027569393855926543,
        "nAUC_map_std": 0.10637788901720686,
        "nAUC_mrr_diff1": 0.21668503154305202,
        "nAUC_mrr_max": 0.026412829755916217,
        "nAUC_mrr_std": 0.09672264401342812
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}