{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 16.353887796401978,
  "kg_co2_emissions": 0.000500780103192447,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.296142578125,
        "f1": 0.28921535449662567,
        "f1_weighted": 0.2892149769081553,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.296142578125,
        "scores_per_experiment": [
          {
            "accuracy": 0.275390625,
            "f1": 0.277083781727251,
            "f1_weighted": 0.2770583238627569
          },
          {
            "accuracy": 0.2822265625,
            "f1": 0.26887064695610563,
            "f1_weighted": 0.26889656584888716
          },
          {
            "accuracy": 0.29345703125,
            "f1": 0.2859962509850381,
            "f1_weighted": 0.2859767859728596
          },
          {
            "accuracy": 0.314453125,
            "f1": 0.3083190443538629,
            "f1_weighted": 0.3083157000538729
          },
          {
            "accuracy": 0.30224609375,
            "f1": 0.3000253678406448,
            "f1_weighted": 0.300002683852212
          },
          {
            "accuracy": 0.30322265625,
            "f1": 0.2927120441213144,
            "f1_weighted": 0.29269545265116836
          },
          {
            "accuracy": 0.24658203125,
            "f1": 0.24615311136062917,
            "f1_weighted": 0.24615385503478956
          },
          {
            "accuracy": 0.33935546875,
            "f1": 0.32472912638461127,
            "f1_weighted": 0.3247519023973434
          },
          {
            "accuracy": 0.30615234375,
            "f1": 0.3005596202519885,
            "f1_weighted": 0.3005700373944561
          },
          {
            "accuracy": 0.29833984375,
            "f1": 0.28770455098481057,
            "f1_weighted": 0.287728462013207
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.29375,
        "f1": 0.2861389400099504,
        "f1_weighted": 0.2861308368396731,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.29375,
        "scores_per_experiment": [
          {
            "accuracy": 0.30859375,
            "f1": 0.30845387155803755,
            "f1_weighted": 0.30844432879182226
          },
          {
            "accuracy": 0.2783203125,
            "f1": 0.2656377133645726,
            "f1_weighted": 0.2656612404489738
          },
          {
            "accuracy": 0.29736328125,
            "f1": 0.2871667075099241,
            "f1_weighted": 0.28713561518021
          },
          {
            "accuracy": 0.31640625,
            "f1": 0.3103450504572242,
            "f1_weighted": 0.31032842201337535
          },
          {
            "accuracy": 0.27783203125,
            "f1": 0.27434832362259587,
            "f1_weighted": 0.2743423292841792
          },
          {
            "accuracy": 0.2919921875,
            "f1": 0.282151221552443,
            "f1_weighted": 0.28212906303036045
          },
          {
            "accuracy": 0.2392578125,
            "f1": 0.23937112954962375,
            "f1_weighted": 0.23933448268289134
          },
          {
            "accuracy": 0.3212890625,
            "f1": 0.30933893050191935,
            "f1_weighted": 0.3093398387094606
          },
          {
            "accuracy": 0.306640625,
            "f1": 0.3008770112547567,
            "f1_weighted": 0.30087298304052906
          },
          {
            "accuracy": 0.2998046875,
            "f1": 0.28369944072840664,
            "f1_weighted": 0.283720065214929
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}