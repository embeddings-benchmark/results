{
  "dataset_revision": "673a610d6d3dd91a547a0d57ae1b56f37ebbf6a1",
  "task_name": "RuSciBenchGRNTIClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.637891,
        "f1": 0.61815,
        "f1_weighted": 0.618295,
        "scores_per_experiment": [
          {
            "accuracy": 0.657227,
            "f1": 0.645176,
            "f1_weighted": 0.645313
          },
          {
            "accuracy": 0.632324,
            "f1": 0.613288,
            "f1_weighted": 0.613406
          },
          {
            "accuracy": 0.634766,
            "f1": 0.615097,
            "f1_weighted": 0.615278
          },
          {
            "accuracy": 0.639648,
            "f1": 0.618136,
            "f1_weighted": 0.618268
          },
          {
            "accuracy": 0.633301,
            "f1": 0.614111,
            "f1_weighted": 0.614217
          },
          {
            "accuracy": 0.612793,
            "f1": 0.593491,
            "f1_weighted": 0.593603
          },
          {
            "accuracy": 0.649414,
            "f1": 0.628858,
            "f1_weighted": 0.629036
          },
          {
            "accuracy": 0.639648,
            "f1": 0.618676,
            "f1_weighted": 0.618832
          },
          {
            "accuracy": 0.65625,
            "f1": 0.634177,
            "f1_weighted": 0.634368
          },
          {
            "accuracy": 0.623535,
            "f1": 0.600486,
            "f1_weighted": 0.60063
          }
        ],
        "main_score": 0.637891,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 44.70658874511719,
  "kg_co2_emissions": 0.002514214045552477
}