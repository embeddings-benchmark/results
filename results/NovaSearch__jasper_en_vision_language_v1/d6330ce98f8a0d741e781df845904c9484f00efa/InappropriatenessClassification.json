{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "task_name": "InappropriatenessClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.641162,
        "f1": 0.629908,
        "f1_weighted": 0.629908,
        "ap": 0.598229,
        "ap_weighted": 0.598229,
        "scores_per_experiment": [
          {
            "accuracy": 0.69043,
            "f1": 0.682583,
            "f1_weighted": 0.682583,
            "ap": 0.648112,
            "ap_weighted": 0.648112
          },
          {
            "accuracy": 0.665527,
            "f1": 0.663124,
            "f1_weighted": 0.663124,
            "ap": 0.606203,
            "ap_weighted": 0.606203
          },
          {
            "accuracy": 0.686523,
            "f1": 0.679365,
            "f1_weighted": 0.679365,
            "ap": 0.64288,
            "ap_weighted": 0.64288
          },
          {
            "accuracy": 0.577637,
            "f1": 0.577433,
            "f1_weighted": 0.577433,
            "ap": 0.544592,
            "ap_weighted": 0.544592
          },
          {
            "accuracy": 0.623535,
            "f1": 0.610475,
            "f1_weighted": 0.610475,
            "ap": 0.585846,
            "ap_weighted": 0.585846
          },
          {
            "accuracy": 0.562988,
            "f1": 0.551121,
            "f1_weighted": 0.551121,
            "ap": 0.534488,
            "ap_weighted": 0.534488
          },
          {
            "accuracy": 0.717773,
            "f1": 0.71404,
            "f1_weighted": 0.71404,
            "ap": 0.670359,
            "ap_weighted": 0.670359
          },
          {
            "accuracy": 0.646484,
            "f1": 0.616801,
            "f1_weighted": 0.616801,
            "ap": 0.587027,
            "ap_weighted": 0.587027
          },
          {
            "accuracy": 0.61377,
            "f1": 0.581462,
            "f1_weighted": 0.581462,
            "ap": 0.586015,
            "ap_weighted": 0.586015
          },
          {
            "accuracy": 0.626953,
            "f1": 0.622678,
            "f1_weighted": 0.622678,
            "ap": 0.576765,
            "ap_weighted": 0.576765
          }
        ],
        "main_score": 0.641162,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 14.4707510471344,
  "kg_co2_emissions": 0.0005154542756778102
}