{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "task_name": "GreekLegalCodeClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.148828,
        "f1": 0.109653,
        "f1_weighted": 0.132252,
        "scores_per_experiment": [
          {
            "accuracy": 0.144043,
            "f1": 0.104874,
            "f1_weighted": 0.13246
          },
          {
            "accuracy": 0.151367,
            "f1": 0.110088,
            "f1_weighted": 0.133565
          },
          {
            "accuracy": 0.144043,
            "f1": 0.112894,
            "f1_weighted": 0.125177
          },
          {
            "accuracy": 0.130859,
            "f1": 0.095864,
            "f1_weighted": 0.106819
          },
          {
            "accuracy": 0.148926,
            "f1": 0.111639,
            "f1_weighted": 0.138492
          },
          {
            "accuracy": 0.155273,
            "f1": 0.114467,
            "f1_weighted": 0.148563
          },
          {
            "accuracy": 0.159668,
            "f1": 0.110872,
            "f1_weighted": 0.138882
          },
          {
            "accuracy": 0.139648,
            "f1": 0.099908,
            "f1_weighted": 0.112751
          },
          {
            "accuracy": 0.168945,
            "f1": 0.123745,
            "f1_weighted": 0.151204
          },
          {
            "accuracy": 0.145508,
            "f1": 0.112179,
            "f1_weighted": 0.134604
          }
        ],
        "main_score": 0.148828,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.139355,
        "f1": 0.112645,
        "f1_weighted": 0.128548,
        "scores_per_experiment": [
          {
            "accuracy": 0.146484,
            "f1": 0.124644,
            "f1_weighted": 0.140182
          },
          {
            "accuracy": 0.146973,
            "f1": 0.121426,
            "f1_weighted": 0.134203
          },
          {
            "accuracy": 0.147461,
            "f1": 0.117623,
            "f1_weighted": 0.136183
          },
          {
            "accuracy": 0.135742,
            "f1": 0.11033,
            "f1_weighted": 0.121187
          },
          {
            "accuracy": 0.142578,
            "f1": 0.116214,
            "f1_weighted": 0.135341
          },
          {
            "accuracy": 0.146484,
            "f1": 0.114349,
            "f1_weighted": 0.143353
          },
          {
            "accuracy": 0.131836,
            "f1": 0.1019,
            "f1_weighted": 0.117849
          },
          {
            "accuracy": 0.124512,
            "f1": 0.10359,
            "f1_weighted": 0.110565
          },
          {
            "accuracy": 0.137207,
            "f1": 0.109076,
            "f1_weighted": 0.125981
          },
          {
            "accuracy": 0.134277,
            "f1": 0.1073,
            "f1_weighted": 0.120639
          }
        ],
        "main_score": 0.139355,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ]
  },
  "evaluation_time": 4474.818310976028,
  "kg_co2_emissions": 0.16329301066973284
}