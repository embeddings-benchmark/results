{
  "dataset_revision": "1cd02f1579dab39fedc95de8cc15fd620557a9f2",
  "task_name": "IconclassClassification",
  "mteb_version": "2.1.14",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.455446,
            "f1": 0.453235,
            "f1_weighted": 0.454716,
            "precision": 0.459037,
            "precision_weighted": 0.461027,
            "recall": 0.454545,
            "recall_weighted": 0.455446,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.455446,
            "f1": 0.438805,
            "f1_weighted": 0.440166,
            "precision": 0.451497,
            "precision_weighted": 0.452302,
            "recall": 0.453887,
            "recall_weighted": 0.455446,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.410891,
            "f1": 0.40355,
            "f1_weighted": 0.404795,
            "precision": 0.402624,
            "precision_weighted": 0.40398,
            "recall": 0.40975,
            "recall_weighted": 0.410891,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.440594,
            "f1": 0.4299,
            "f1_weighted": 0.431527,
            "precision": 0.427147,
            "precision_weighted": 0.428672,
            "recall": 0.438955,
            "recall_weighted": 0.440594,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.445545,
            "f1": 0.429538,
            "f1_weighted": 0.43121,
            "precision": 0.435259,
            "precision_weighted": 0.436366,
            "recall": 0.443566,
            "recall_weighted": 0.445545,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.440594,
            "f1": 0.436485,
            "f1_weighted": 0.437478,
            "precision": 0.479837,
            "precision_weighted": 0.479958,
            "recall": 0.439394,
            "recall_weighted": 0.440594,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.480198,
            "f1": 0.474014,
            "f1_weighted": 0.475373,
            "precision": 0.477929,
            "precision_weighted": 0.478864,
            "recall": 0.47848,
            "recall_weighted": 0.480198,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.460396,
            "f1": 0.443148,
            "f1_weighted": 0.444931,
            "precision": 0.446131,
            "precision_weighted": 0.447608,
            "recall": 0.458278,
            "recall_weighted": 0.460396,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.485149,
            "f1": 0.477829,
            "f1_weighted": 0.479983,
            "precision": 0.48253,
            "precision_weighted": 0.484326,
            "recall": 0.482653,
            "recall_weighted": 0.485149,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.480198,
            "f1": 0.465546,
            "f1_weighted": 0.467657,
            "precision": 0.464101,
            "precision_weighted": 0.465661,
            "recall": 0.477602,
            "recall_weighted": 0.480198,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.455446,
        "f1": 0.445205,
        "f1_weighted": 0.446784,
        "precision": 0.452609,
        "precision_weighted": 0.453876,
        "recall": 0.453711,
        "recall_weighted": 0.455446,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.445205,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 4.1780619621276855,
  "kg_co2_emissions": null
}