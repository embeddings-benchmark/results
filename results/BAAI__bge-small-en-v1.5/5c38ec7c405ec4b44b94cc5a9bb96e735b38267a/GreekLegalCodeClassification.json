{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "evaluation_time": 319.1468029022217,
  "kg_co2_emissions": 0.011447089093560368,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.02998046875,
        "f1": 0.015557184750637235,
        "f1_weighted": 0.020147931556031062,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ],
        "main_score": 0.02998046875,
        "scores_per_experiment": [
          {
            "accuracy": 0.02294921875,
            "f1": 0.008787637362913991,
            "f1_weighted": 0.010582478866938042
          },
          {
            "accuracy": 0.02880859375,
            "f1": 0.015784761001717367,
            "f1_weighted": 0.017513294971660022
          },
          {
            "accuracy": 0.02734375,
            "f1": 0.01632226197751496,
            "f1_weighted": 0.015693470715067347
          },
          {
            "accuracy": 0.02587890625,
            "f1": 0.016986359643938374,
            "f1_weighted": 0.022342918385824785
          },
          {
            "accuracy": 0.03125,
            "f1": 0.013937448428360017,
            "f1_weighted": 0.02117389659100851
          },
          {
            "accuracy": 0.0341796875,
            "f1": 0.016265427277916994,
            "f1_weighted": 0.026077265403339465
          },
          {
            "accuracy": 0.03466796875,
            "f1": 0.016389412693447424,
            "f1_weighted": 0.02363602947104681
          },
          {
            "accuracy": 0.0322265625,
            "f1": 0.01634504829257096,
            "f1_weighted": 0.022207223506517396
          },
          {
            "accuracy": 0.03125,
            "f1": 0.015002279073393306,
            "f1_weighted": 0.017691456864158305
          },
          {
            "accuracy": 0.03125,
            "f1": 0.019751211754598957,
            "f1_weighted": 0.02456128078474994
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.02958984375,
        "f1": 0.014947185511200573,
        "f1_weighted": 0.020613604244204622,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ],
        "main_score": 0.02958984375,
        "scores_per_experiment": [
          {
            "accuracy": 0.02734375,
            "f1": 0.013924685439239028,
            "f1_weighted": 0.01792717417705971
          },
          {
            "accuracy": 0.0322265625,
            "f1": 0.016701673509472256,
            "f1_weighted": 0.02351782070699364
          },
          {
            "accuracy": 0.02587890625,
            "f1": 0.012452598728175031,
            "f1_weighted": 0.016528373531418943
          },
          {
            "accuracy": 0.02587890625,
            "f1": 0.01537148513419975,
            "f1_weighted": 0.020947678873384848
          },
          {
            "accuracy": 0.03076171875,
            "f1": 0.016943899488179204,
            "f1_weighted": 0.020245065378221645
          },
          {
            "accuracy": 0.029296875,
            "f1": 0.015149378754716981,
            "f1_weighted": 0.019695631642441674
          },
          {
            "accuracy": 0.0361328125,
            "f1": 0.016664165382401393,
            "f1_weighted": 0.025577555626920753
          },
          {
            "accuracy": 0.02734375,
            "f1": 0.013349911877272906,
            "f1_weighted": 0.018282531046331613
          },
          {
            "accuracy": 0.02978515625,
            "f1": 0.011987878251369119,
            "f1_weighted": 0.017354309169941837
          },
          {
            "accuracy": 0.03125,
            "f1": 0.01692617854698006,
            "f1_weighted": 0.026059902289331548
          }
        ]
      }
    ]
  },
  "task_name": "GreekLegalCodeClassification"
}