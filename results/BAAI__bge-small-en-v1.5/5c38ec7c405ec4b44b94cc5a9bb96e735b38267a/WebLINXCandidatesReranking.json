{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 3022.635339975357,
  "kg_co2_emissions": 0.12984159701324322,
  "mteb_version": "1.18.0",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.0673992331014803,
        "map": 0.08371914849201863,
        "mrr": 0.0673992331014803,
        "nAUC_map_diff1": 0.227267693839379,
        "nAUC_map_max": -0.013548158298139417,
        "nAUC_map_std": 0.04892399426838471,
        "nAUC_mrr_diff1": 0.21979667076400433,
        "nAUC_mrr_max": -0.015043558492235685,
        "nAUC_mrr_std": 0.04407033528548093
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09027422604517804,
        "map": 0.1091497427079831,
        "mrr": 0.09027422604517804,
        "nAUC_map_diff1": 0.0746243434988163,
        "nAUC_map_max": -0.0006303698290992431,
        "nAUC_map_std": 0.051072904975298435,
        "nAUC_mrr_diff1": 0.07066982548434787,
        "nAUC_mrr_max": -0.003828148364086076,
        "nAUC_mrr_std": 0.043726722084386105
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09978309821842506,
        "map": 0.1146266927433604,
        "mrr": 0.09978309821842506,
        "nAUC_map_diff1": 0.13192006671983042,
        "nAUC_map_max": 0.06358021345013792,
        "nAUC_map_std": 0.130080162307879,
        "nAUC_mrr_diff1": 0.14281402845475968,
        "nAUC_mrr_max": 0.07486443323968618,
        "nAUC_mrr_std": 0.13454696613311345
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08533423813718265,
        "map": 0.10355690647963542,
        "mrr": 0.08533423813718265,
        "nAUC_map_diff1": 0.05331824203225714,
        "nAUC_map_max": -0.03193627483072161,
        "nAUC_map_std": 0.1687993837149483,
        "nAUC_mrr_diff1": 0.05117956989959804,
        "nAUC_mrr_max": -0.03056787874707516,
        "nAUC_mrr_std": 0.15719940253413964
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.06442568863847489,
        "map": 0.08151145990434411,
        "mrr": 0.06442568863847489,
        "nAUC_map_diff1": 0.1570946330921587,
        "nAUC_map_max": 0.12025238332293495,
        "nAUC_map_std": 0.2881943105714825,
        "nAUC_mrr_diff1": 0.161737058819269,
        "nAUC_mrr_max": 0.1436262827041355,
        "nAUC_mrr_std": 0.29659145871777876
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.12422526017837317,
        "map": 0.14049072301136517,
        "mrr": 0.12422526017837317,
        "nAUC_map_diff1": 0.2715839561755204,
        "nAUC_map_max": 0.02743469705112336,
        "nAUC_map_std": 0.002935130043077723,
        "nAUC_mrr_diff1": 0.27689680252514987,
        "nAUC_mrr_max": 0.037683863762863846,
        "nAUC_mrr_std": 0.008743431309445215
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}