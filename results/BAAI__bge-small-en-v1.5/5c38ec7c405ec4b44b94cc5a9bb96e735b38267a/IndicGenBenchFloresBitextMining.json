{
  "dataset_revision": "f8650438298df086750ff4973661bb58a201a5ee",
  "evaluation_time": 323.246928691864,
  "kg_co2_emissions": 0.010861974480287898,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.009881422924901186,
        "f1": 0.00823657132068347,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.00823657132068347,
        "precision": 0.007906165514861166,
        "recall": 0.009881422924901186
      },
      {
        "accuracy": 0.0741106719367589,
        "f1": 0.03173983220014008,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ],
        "main_score": 0.03173983220014008,
        "precision": 0.024379212289035966,
        "recall": 0.0741106719367589
      },
      {
        "accuracy": 0.040513833992094864,
        "f1": 0.03020649673932996,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ],
        "main_score": 0.03020649673932996,
        "precision": 0.028341707683402963,
        "recall": 0.040513833992094864
      },
      {
        "accuracy": 0.0958498023715415,
        "f1": 0.048998924400034005,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ],
        "main_score": 0.048998924400034005,
        "precision": 0.039324756234487085,
        "recall": 0.0958498023715415
      },
      {
        "accuracy": 0.016798418972332016,
        "f1": 0.012899569272139259,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.012899569272139259,
        "precision": 0.012307361034482163,
        "recall": 0.016798418972332016
      },
      {
        "accuracy": 0.07707509881422925,
        "f1": 0.03658831684172333,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ],
        "main_score": 0.03658831684172333,
        "precision": 0.029483047588211023,
        "recall": 0.07707509881422925
      },
      {
        "accuracy": 0.04841897233201581,
        "f1": 0.03719519589230851,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ],
        "main_score": 0.03719519589230851,
        "precision": 0.03462053367970103,
        "recall": 0.04841897233201581
      },
      {
        "accuracy": 0.11758893280632411,
        "f1": 0.06650166084200321,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ],
        "main_score": 0.06650166084200321,
        "precision": 0.055677884641707025,
        "recall": 0.11758893280632411
      },
      {
        "accuracy": 0.039525691699604744,
        "f1": 0.02808930872600276,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.02808930872600276,
        "precision": 0.02569497543943155,
        "recall": 0.039525691699604744
      },
      {
        "accuracy": 0.10869565217391304,
        "f1": 0.06323367211109396,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ],
        "main_score": 0.06323367211109396,
        "precision": 0.05300386675764495,
        "recall": 0.10869565217391304
      },
      {
        "accuracy": 0.016798418972332016,
        "f1": 0.013631474738319983,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.013631474738319983,
        "precision": 0.013333220911068387,
        "recall": 0.016798418972332016
      },
      {
        "accuracy": 0.07213438735177866,
        "f1": 0.03884824071977367,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ],
        "main_score": 0.03884824071977367,
        "precision": 0.033651639323492036,
        "recall": 0.07213438735177866
      },
      {
        "accuracy": 0.02766798418972332,
        "f1": 0.021903722918959433,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.021903722918959433,
        "precision": 0.020769318503642643,
        "recall": 0.02766798418972332
      },
      {
        "accuracy": 0.08201581027667984,
        "f1": 0.04651929784093816,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ],
        "main_score": 0.04651929784093816,
        "precision": 0.038360985394340553,
        "recall": 0.08201581027667984
      },
      {
        "accuracy": 0.1007905138339921,
        "f1": 0.08705872693286881,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.08705872693286881,
        "precision": 0.08362044568566307,
        "recall": 0.1007905138339921
      },
      {
        "accuracy": 0.16205533596837945,
        "f1": 0.09506156454902309,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ],
        "main_score": 0.09506156454902309,
        "precision": 0.08168376756621216,
        "recall": 0.16205533596837945
      },
      {
        "accuracy": 0.014822134387351778,
        "f1": 0.011432372552047881,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.011432372552047881,
        "precision": 0.010623961734547745,
        "recall": 0.014822134387351778
      },
      {
        "accuracy": 0.058300395256917,
        "f1": 0.0244423248273638,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ],
        "main_score": 0.0244423248273638,
        "precision": 0.01906898095671732,
        "recall": 0.058300395256917
      },
      {
        "accuracy": 0.015810276679841896,
        "f1": 0.013506821939853746,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ],
        "main_score": 0.013506821939853746,
        "precision": 0.012846956344692358,
        "recall": 0.015810276679841896
      },
      {
        "accuracy": 0.05632411067193676,
        "f1": 0.025965139396816415,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ],
        "main_score": 0.025965139396816415,
        "precision": 0.019873093853371757,
        "recall": 0.05632411067193676
      },
      {
        "accuracy": 0.025691699604743084,
        "f1": 0.021360535971746242,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ],
        "main_score": 0.021360535971746242,
        "precision": 0.020620352381590407,
        "recall": 0.025691699604743084
      },
      {
        "accuracy": 0.08695652173913043,
        "f1": 0.05024580734305456,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ],
        "main_score": 0.05024580734305456,
        "precision": 0.0424601338027854,
        "recall": 0.08695652173913043
      },
      {
        "accuracy": 0.029644268774703556,
        "f1": 0.02477003434264924,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ],
        "main_score": 0.02477003434264924,
        "precision": 0.02413836163836164,
        "recall": 0.029644268774703556
      },
      {
        "accuracy": 0.09683794466403162,
        "f1": 0.0507349058799235,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ],
        "main_score": 0.0507349058799235,
        "precision": 0.042504325077488914,
        "recall": 0.09683794466403162
      },
      {
        "accuracy": 0.036561264822134384,
        "f1": 0.030733829096949907,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ],
        "main_score": 0.030733829096949907,
        "precision": 0.02898681086417693,
        "recall": 0.036561264822134384
      },
      {
        "accuracy": 0.09980237154150198,
        "f1": 0.04832034980145113,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ],
        "main_score": 0.04832034980145113,
        "precision": 0.038690292265790016,
        "recall": 0.09980237154150198
      },
      {
        "accuracy": 0.044466403162055336,
        "f1": 0.03702811228547443,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ],
        "main_score": 0.03702811228547443,
        "precision": 0.03515492003860173,
        "recall": 0.044466403162055336
      },
      {
        "accuracy": 0.09683794466403162,
        "f1": 0.049409078338893235,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ],
        "main_score": 0.049409078338893235,
        "precision": 0.04071150218389478,
        "recall": 0.09683794466403162
      },
      {
        "accuracy": 0.09090909090909091,
        "f1": 0.08310482945682468,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ],
        "main_score": 0.08310482945682468,
        "precision": 0.08115944819509219,
        "recall": 0.09090909090909091
      },
      {
        "accuracy": 0.16205533596837945,
        "f1": 0.09404552393777599,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ],
        "main_score": 0.09404552393777599,
        "precision": 0.07746712542280096,
        "recall": 0.16205533596837945
      },
      {
        "accuracy": 0.036561264822134384,
        "f1": 0.033256936824989744,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ],
        "main_score": 0.033256936824989744,
        "precision": 0.03252742569757756,
        "recall": 0.036561264822134384
      },
      {
        "accuracy": 0.08992094861660078,
        "f1": 0.048955302032827296,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ],
        "main_score": 0.048955302032827296,
        "precision": 0.04276839921592027,
        "recall": 0.08992094861660078
      },
      {
        "accuracy": 0.012845849802371542,
        "f1": 0.01013080842024129,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01013080842024129,
        "precision": 0.009529691323169585,
        "recall": 0.012845849802371542
      },
      {
        "accuracy": 0.06719367588932806,
        "f1": 0.034471585974075396,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ],
        "main_score": 0.034471585974075396,
        "precision": 0.029488244790803034,
        "recall": 0.06719367588932806
      },
      {
        "accuracy": 0.00691699604743083,
        "f1": 0.004350059176883053,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ],
        "main_score": 0.004350059176883053,
        "precision": 0.00387134178694281,
        "recall": 0.00691699604743083
      },
      {
        "accuracy": 0.06225296442687747,
        "f1": 0.03310975096346307,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ],
        "main_score": 0.03310975096346307,
        "precision": 0.02856293853768616,
        "recall": 0.06225296442687747
      },
      {
        "accuracy": 0.00691699604743083,
        "f1": 0.0044628736655746865,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ],
        "main_score": 0.0044628736655746865,
        "precision": 0.0042901156511773696,
        "recall": 0.00691699604743083
      },
      {
        "accuracy": 0.040513833992094864,
        "f1": 0.02493681173590113,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ],
        "main_score": 0.02493681173590113,
        "precision": 0.02197447248625514,
        "recall": 0.040513833992094864
      },
      {
        "accuracy": 0.012845849802371542,
        "f1": 0.009050447884001129,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ],
        "main_score": 0.009050447884001129,
        "precision": 0.008483270166850581,
        "recall": 0.012845849802371542
      },
      {
        "accuracy": 0.08300395256916997,
        "f1": 0.046827549784883245,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ],
        "main_score": 0.046827549784883245,
        "precision": 0.04002815730368081,
        "recall": 0.08300395256916997
      },
      {
        "accuracy": 0.016798418972332016,
        "f1": 0.013371582338707994,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ],
        "main_score": 0.013371582338707994,
        "precision": 0.012697214762562173,
        "recall": 0.016798418972332016
      },
      {
        "accuracy": 0.0800395256916996,
        "f1": 0.040061142725715176,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ],
        "main_score": 0.040061142725715176,
        "precision": 0.03301340791218406,
        "recall": 0.0800395256916996
      },
      {
        "accuracy": 0.03557312252964427,
        "f1": 0.030217415977064236,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ],
        "main_score": 0.030217415977064236,
        "precision": 0.02899647346583501,
        "recall": 0.03557312252964427
      },
      {
        "accuracy": 0.11462450592885376,
        "f1": 0.05891294605479763,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ],
        "main_score": 0.05891294605479763,
        "precision": 0.04922814108539308,
        "recall": 0.11462450592885376
      },
      {
        "accuracy": 0.007905138339920948,
        "f1": 0.004301549549137367,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ],
        "main_score": 0.004301549549137367,
        "precision": 0.0038306946770549567,
        "recall": 0.007905138339920948
      },
      {
        "accuracy": 0.05533596837944664,
        "f1": 0.026807660683474097,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ],
        "main_score": 0.026807660683474097,
        "precision": 0.021537592366612714,
        "recall": 0.05533596837944664
      },
      {
        "accuracy": 0.01383399209486166,
        "f1": 0.010073044919069624,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ],
        "main_score": 0.010073044919069624,
        "precision": 0.009655496693276103,
        "recall": 0.01383399209486166
      },
      {
        "accuracy": 0.08695652173913043,
        "f1": 0.045113343471289105,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ],
        "main_score": 0.045113343471289105,
        "precision": 0.03805004084125101,
        "recall": 0.08695652173913043
      },
      {
        "accuracy": 0.017786561264822136,
        "f1": 0.015878373377152078,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ],
        "main_score": 0.015878373377152078,
        "precision": 0.015564352627858276,
        "recall": 0.017786561264822136
      },
      {
        "accuracy": 0.08596837944664032,
        "f1": 0.04602075645395299,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ],
        "main_score": 0.04602075645395299,
        "precision": 0.03871764778501048,
        "recall": 0.08596837944664032
      },
      {
        "accuracy": 0.008893280632411068,
        "f1": 0.006095569018769304,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ],
        "main_score": 0.006095569018769304,
        "precision": 0.005271105705888314,
        "recall": 0.008893280632411068
      },
      {
        "accuracy": 0.07312252964426877,
        "f1": 0.036523342051796234,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ],
        "main_score": 0.036523342051796234,
        "precision": 0.02965294378551142,
        "recall": 0.07312252964426877
      },
      {
        "accuracy": 0.009881422924901186,
        "f1": 0.006399788681671126,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ],
        "main_score": 0.006399788681671126,
        "precision": 0.005940934721653058,
        "recall": 0.009881422924901186
      },
      {
        "accuracy": 0.07312252964426877,
        "f1": 0.033760378037738556,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ],
        "main_score": 0.033760378037738556,
        "precision": 0.02723779449110383,
        "recall": 0.07312252964426877
      },
      {
        "accuracy": 0.015810276679841896,
        "f1": 0.012284201565936643,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ],
        "main_score": 0.012284201565936643,
        "precision": 0.011824137050118805,
        "recall": 0.015810276679841896
      },
      {
        "accuracy": 0.08794466403162056,
        "f1": 0.043746793951366936,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ],
        "main_score": 0.043746793951366936,
        "precision": 0.03662008797019731,
        "recall": 0.08794466403162056
      },
      {
        "accuracy": 0.005928853754940711,
        "f1": 0.005270092226613965,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ],
        "main_score": 0.005270092226613965,
        "precision": 0.004940711462450593,
        "recall": 0.005928853754940711
      },
      {
        "accuracy": 0.022727272727272728,
        "f1": 0.005298224406546248,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ],
        "main_score": 0.005298224406546248,
        "precision": 0.0036902131141176692,
        "recall": 0.022727272727272728
      }
    ],
    "validation": [
      {
        "accuracy": 0.012036108324974924,
        "f1": 0.011534603811434303,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.011534603811434303,
        "precision": 0.011367435640254095,
        "recall": 0.012036108324974924
      },
      {
        "accuracy": 0.06820461384152457,
        "f1": 0.03260139868743882,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ],
        "main_score": 0.03260139868743882,
        "precision": 0.02701664265097711,
        "recall": 0.06820461384152457
      },
      {
        "accuracy": 0.03510531594784353,
        "f1": 0.030378759704377003,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ],
        "main_score": 0.030378759704377003,
        "precision": 0.029175079350605933,
        "recall": 0.03510531594784353
      },
      {
        "accuracy": 0.09829488465396188,
        "f1": 0.049749593913686226,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ],
        "main_score": 0.049749593913686226,
        "precision": 0.04088985867670412,
        "recall": 0.09829488465396188
      },
      {
        "accuracy": 0.007021063189568706,
        "f1": 0.005685746149807697,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.005685746149807697,
        "precision": 0.005517564840269796,
        "recall": 0.007021063189568706
      },
      {
        "accuracy": 0.07522567703109329,
        "f1": 0.04159098664376006,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ],
        "main_score": 0.04159098664376006,
        "precision": 0.0347538814508159,
        "recall": 0.07522567703109329
      },
      {
        "accuracy": 0.05115346038114343,
        "f1": 0.045083237868021264,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ],
        "main_score": 0.045083237868021264,
        "precision": 0.0439735437314244,
        "recall": 0.05115346038114343
      },
      {
        "accuracy": 0.12036108324974924,
        "f1": 0.06437758454060041,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ],
        "main_score": 0.06437758454060041,
        "precision": 0.0519019612935298,
        "recall": 0.12036108324974924
      },
      {
        "accuracy": 0.04513540621865597,
        "f1": 0.03961659456507228,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.03961659456507228,
        "precision": 0.03855764613486882,
        "recall": 0.04513540621865597
      },
      {
        "accuracy": 0.1334002006018054,
        "f1": 0.07736247882862245,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ],
        "main_score": 0.07736247882862245,
        "precision": 0.06517833646326511,
        "recall": 0.1334002006018054
      },
      {
        "accuracy": 0.010030090270812437,
        "f1": 0.008694777341141966,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.008694777341141966,
        "precision": 0.008526593980725137,
        "recall": 0.010030090270812437
      },
      {
        "accuracy": 0.07221664994984955,
        "f1": 0.038263913889637326,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ],
        "main_score": 0.038263913889637326,
        "precision": 0.03304415103562546,
        "recall": 0.07221664994984955
      },
      {
        "accuracy": 0.03911735205616851,
        "f1": 0.0361148169427376,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.0361148169427376,
        "precision": 0.03527574064183893,
        "recall": 0.03911735205616851
      },
      {
        "accuracy": 0.08726178535606821,
        "f1": 0.05289741930447516,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ],
        "main_score": 0.05289741930447516,
        "precision": 0.04538987072063838,
        "recall": 0.08726178535606821
      },
      {
        "accuracy": 0.08124373119358075,
        "f1": 0.07378006757426843,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.07378006757426843,
        "precision": 0.07192792669654871,
        "recall": 0.08124373119358075
      },
      {
        "accuracy": 0.1464393179538616,
        "f1": 0.07809305566145089,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ],
        "main_score": 0.07809305566145089,
        "precision": 0.06332930786290866,
        "recall": 0.1464393179538616
      },
      {
        "accuracy": 0.020060180541624874,
        "f1": 0.015718065026291212,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.015718065026291212,
        "precision": 0.015145412046492649,
        "recall": 0.020060180541624874
      },
      {
        "accuracy": 0.09428284854563691,
        "f1": 0.055283967917968316,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ],
        "main_score": 0.055283967917968316,
        "precision": 0.04752246214141248,
        "recall": 0.09428284854563691
      },
      {
        "accuracy": 0.022066198595787363,
        "f1": 0.019072208353762696,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ],
        "main_score": 0.019072208353762696,
        "precision": 0.018563215983615,
        "recall": 0.022066198595787363
      },
      {
        "accuracy": 0.06619859578736209,
        "f1": 0.03301323844975139,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ],
        "main_score": 0.03301323844975139,
        "precision": 0.027214357067732984,
        "recall": 0.06619859578736209
      },
      {
        "accuracy": 0.013039117352056168,
        "f1": 0.01136948259745222,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01136948259745222,
        "precision": 0.01103412382192543,
        "recall": 0.013039117352056168
      },
      {
        "accuracy": 0.09528585757271815,
        "f1": 0.04951183525476711,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ],
        "main_score": 0.04951183525476711,
        "precision": 0.04081729809853513,
        "recall": 0.09528585757271815
      },
      {
        "accuracy": 0.020060180541624874,
        "f1": 0.018223402991343177,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ],
        "main_score": 0.018223402991343177,
        "precision": 0.017888031554159203,
        "recall": 0.020060180541624874
      },
      {
        "accuracy": 0.10130391173520562,
        "f1": 0.052737311887517464,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ],
        "main_score": 0.052737311887517464,
        "precision": 0.04451288359490112,
        "recall": 0.10130391173520562
      },
      {
        "accuracy": 0.04513540621865597,
        "f1": 0.039788940464805266,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ],
        "main_score": 0.039788940464805266,
        "precision": 0.037781466670754625,
        "recall": 0.04513540621865597
      },
      {
        "accuracy": 0.119358074222668,
        "f1": 0.06109550186160823,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ],
        "main_score": 0.06109550186160823,
        "precision": 0.049931490355224054,
        "recall": 0.119358074222668
      },
      {
        "accuracy": 0.03610832497492478,
        "f1": 0.03230868721586461,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ],
        "main_score": 0.03230868721586461,
        "precision": 0.031651298624271976,
        "recall": 0.03610832497492478
      },
      {
        "accuracy": 0.10832497492477432,
        "f1": 0.054316259073773306,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ],
        "main_score": 0.054316259073773306,
        "precision": 0.04407414669061854,
        "recall": 0.10832497492477432
      },
      {
        "accuracy": 0.11735205616850551,
        "f1": 0.10676178756757512,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ],
        "main_score": 0.10676178756757512,
        "precision": 0.10431791478190168,
        "recall": 0.11735205616850551
      },
      {
        "accuracy": 0.18756268806419257,
        "f1": 0.11156270301684808,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ],
        "main_score": 0.11156270301684808,
        "precision": 0.09365896013898846,
        "recall": 0.18756268806419257
      },
      {
        "accuracy": 0.031093279839518557,
        "f1": 0.028086380030866723,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ],
        "main_score": 0.028086380030866723,
        "precision": 0.027082308496615113,
        "recall": 0.031093279839518557
      },
      {
        "accuracy": 0.09528585757271815,
        "f1": 0.05304413795415425,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ],
        "main_score": 0.05304413795415425,
        "precision": 0.04597670905980393,
        "recall": 0.09528585757271815
      },
      {
        "accuracy": 0.003009027081243731,
        "f1": 0.002008042290342268,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.002008042290342268,
        "precision": 0.0020070311945938825,
        "recall": 0.003009027081243731
      },
      {
        "accuracy": 0.07823470411233702,
        "f1": 0.04686309545862712,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ],
        "main_score": 0.04686309545862712,
        "precision": 0.04062339624976187,
        "recall": 0.07823470411233702
      },
      {
        "accuracy": 0.011033099297893681,
        "f1": 0.010032126837364886,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ],
        "main_score": 0.010032126837364886,
        "precision": 0.01003110958892939,
        "recall": 0.011033099297893681
      },
      {
        "accuracy": 0.08324974924774323,
        "f1": 0.04523601330659335,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ],
        "main_score": 0.04523601330659335,
        "precision": 0.037768173505482845,
        "recall": 0.08324974924774323
      },
      {
        "accuracy": 0.009027081243731194,
        "f1": 0.00802676485833339,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ],
        "main_score": 0.00802676485833339,
        "precision": 0.008025420347062695,
        "recall": 0.009027081243731194
      },
      {
        "accuracy": 0.04613841524573721,
        "f1": 0.020837619462826616,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ],
        "main_score": 0.020837619462826616,
        "precision": 0.01711945495899955,
        "recall": 0.04613841524573721
      },
      {
        "accuracy": 0.006018054162487462,
        "f1": 0.004181238780574312,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ],
        "main_score": 0.004181238780574312,
        "precision": 0.003845886220420992,
        "recall": 0.006018054162487462
      },
      {
        "accuracy": 0.08425275827482448,
        "f1": 0.04347798897778616,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ],
        "main_score": 0.04347798897778616,
        "precision": 0.03707404268559261,
        "recall": 0.08425275827482448
      },
      {
        "accuracy": 0.010030090270812437,
        "f1": 0.008694783537604684,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ],
        "main_score": 0.008694783537604684,
        "precision": 0.008526597085253726,
        "recall": 0.010030090270812437
      },
      {
        "accuracy": 0.08425275827482448,
        "f1": 0.049473855012247915,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ],
        "main_score": 0.049473855012247915,
        "precision": 0.04325677516089396,
        "recall": 0.08425275827482448
      },
      {
        "accuracy": 0.012036108324974924,
        "f1": 0.01036647148376833,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01036647148376833,
        "precision": 0.0100311137494115,
        "recall": 0.012036108324974924
      },
      {
        "accuracy": 0.10832497492477432,
        "f1": 0.06522748378963392,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ],
        "main_score": 0.06522748378963392,
        "precision": 0.056392944903708386,
        "recall": 0.10832497492477432
      },
      {
        "accuracy": 0.0020060180541624875,
        "f1": 0.001005033263261024,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ],
        "main_score": 0.001005033263261024,
        "precision": 0.0010040221675126388,
        "recall": 0.0020060180541624875
      },
      {
        "accuracy": 0.06720160481444333,
        "f1": 0.03731987071386073,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ],
        "main_score": 0.03731987071386073,
        "precision": 0.03139052294012497,
        "recall": 0.06720160481444333
      },
      {
        "accuracy": 0.009027081243731194,
        "f1": 0.007524606339343233,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ],
        "main_score": 0.007524606339343233,
        "precision": 0.007356419886992275,
        "recall": 0.009027081243731194
      },
      {
        "accuracy": 0.08425275827482448,
        "f1": 0.044278335675597524,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ],
        "main_score": 0.044278335675597524,
        "precision": 0.037604956183210456,
        "recall": 0.08425275827482448
      },
      {
        "accuracy": 0.012036108324974924,
        "f1": 0.010366475661236347,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ],
        "main_score": 0.010366475661236347,
        "precision": 0.01003111584241477,
        "recall": 0.012036108324974924
      },
      {
        "accuracy": 0.08425275827482448,
        "f1": 0.0462617396869334,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ],
        "main_score": 0.0462617396869334,
        "precision": 0.039377346469122956,
        "recall": 0.08425275827482448
      },
      {
        "accuracy": 0.01905717151454363,
        "f1": 0.014727381801944285,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ],
        "main_score": 0.014727381801944285,
        "precision": 0.013716126735183907,
        "recall": 0.01905717151454363
      },
      {
        "accuracy": 0.09327983951855567,
        "f1": 0.050867374136825516,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ],
        "main_score": 0.050867374136825516,
        "precision": 0.04301134345061498,
        "recall": 0.09327983951855567
      },
      {
        "accuracy": 0.006018054162487462,
        "f1": 0.004682737122726453,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ],
        "main_score": 0.004682737122726453,
        "precision": 0.004514555813188553,
        "recall": 0.006018054162487462
      },
      {
        "accuracy": 0.07723169508525576,
        "f1": 0.04202408893285823,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ],
        "main_score": 0.04202408893285823,
        "precision": 0.035804489750458086,
        "recall": 0.07723169508525576
      },
      {
        "accuracy": 0.006018054162487462,
        "f1": 0.004682735073908595,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ],
        "main_score": 0.004682735073908595,
        "precision": 0.004514554786705921,
        "recall": 0.006018054162487462
      },
      {
        "accuracy": 0.08324974924774323,
        "f1": 0.04929577587516262,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ],
        "main_score": 0.04929577587516262,
        "precision": 0.042636071650557904,
        "recall": 0.08324974924774323
      },
      {
        "accuracy": 0.010030090270812437,
        "f1": 0.00641616932713958,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ],
        "main_score": 0.00641616932713958,
        "precision": 0.006050466897362729,
        "recall": 0.010030090270812437
      },
      {
        "accuracy": 0.01805416248746239,
        "f1": 0.003307950475323697,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ],
        "main_score": 0.003307950475323697,
        "precision": 0.0020806320867813345,
        "recall": 0.01805416248746239
      }
    ]
  },
  "task_name": "IndicGenBenchFloresBitextMining"
}