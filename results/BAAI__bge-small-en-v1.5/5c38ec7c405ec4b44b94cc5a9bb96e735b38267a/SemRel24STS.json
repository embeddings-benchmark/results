{
  "dataset_revision": "ef5c383d1b87eb8feccde3dfb7f95e42b1b050dd",
  "evaluation_time": 18.575292110443115,
  "kg_co2_emissions": 0.0006010535174435875,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "cosine_pearson": 0.7513484685868433,
        "cosine_spearman": 0.7464496389091262,
        "euclidean_pearson": 0.7353694273384676,
        "euclidean_spearman": 0.7464496389091262,
        "hf_subset": "afr",
        "languages": [
          "afr-Latn"
        ],
        "main_score": 0.7464496389091262,
        "manhattan_pearson": 0.7339874559503863,
        "manhattan_spearman": 0.7426516533711204,
        "pearson": 0.7513484685868433,
        "spearman": 0.7464496389091262
      },
      {
        "cosine_pearson": 0.17185959657575878,
        "cosine_spearman": 0.14950010536746902,
        "euclidean_pearson": 0.15990317614273325,
        "euclidean_spearman": 0.14861845379664165,
        "hf_subset": "amh",
        "languages": [
          "amh-Ethi"
        ],
        "main_score": 0.14950010536746902,
        "manhattan_pearson": 0.15592201271461145,
        "manhattan_spearman": 0.14506568271258716,
        "pearson": 0.17185959657575878,
        "spearman": 0.14950010536746902
      },
      {
        "cosine_pearson": 0.21918017127270553,
        "cosine_spearman": 0.21836864749227075,
        "euclidean_pearson": 0.2250822779561685,
        "euclidean_spearman": 0.21836864749227075,
        "hf_subset": "arb",
        "languages": [
          "arb-Arab"
        ],
        "main_score": 0.21836864749227075,
        "manhattan_pearson": 0.22661632819812513,
        "manhattan_spearman": 0.22194992297891955,
        "pearson": 0.21918017127270553,
        "spearman": 0.21836864749227075
      },
      {
        "cosine_pearson": 0.3199655258299361,
        "cosine_spearman": 0.32684655813237484,
        "euclidean_pearson": 0.33003331955897275,
        "euclidean_spearman": 0.32684655813237484,
        "hf_subset": "arq",
        "languages": [
          "arq-Arab"
        ],
        "main_score": 0.32684655813237484,
        "manhattan_pearson": 0.32906836141706197,
        "manhattan_spearman": 0.3252936661744205,
        "pearson": 0.3199655258299361,
        "spearman": 0.32684655813237484
      },
      {
        "cosine_pearson": -0.03627087355423002,
        "cosine_spearman": -0.035986942845355786,
        "euclidean_pearson": 0.02227955701464364,
        "euclidean_spearman": -0.035986942845355786,
        "hf_subset": "ary",
        "languages": [
          "ary-Arab"
        ],
        "main_score": -0.035986942845355786,
        "manhattan_pearson": 0.02402675610755288,
        "manhattan_spearman": -0.0355877556956139,
        "pearson": -0.03627087355423002,
        "spearman": -0.035986942845355786
      },
      {
        "cosine_pearson": 0.8126401398701886,
        "cosine_spearman": 0.796889297351028,
        "euclidean_pearson": 0.8113701268159446,
        "euclidean_spearman": 0.796889297351028,
        "hf_subset": "eng",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.796889297351028,
        "manhattan_pearson": 0.8101477199678965,
        "manhattan_spearman": 0.7950394417261528,
        "pearson": 0.8126401398701886,
        "spearman": 0.796889297351028
      },
      {
        "cosine_pearson": 0.3736053816236317,
        "cosine_spearman": 0.3317886555612914,
        "euclidean_pearson": 0.38867914773292916,
        "euclidean_spearman": 0.3317886555612914,
        "hf_subset": "hau",
        "languages": [
          "hau-Latn"
        ],
        "main_score": 0.3317886555612914,
        "manhattan_pearson": 0.3857966313601869,
        "manhattan_spearman": 0.32918396545372114,
        "pearson": 0.3736053816236317,
        "spearman": 0.3317886555612914
      },
      {
        "cosine_pearson": 0.44414988322594995,
        "cosine_spearman": 0.4370552930156271,
        "euclidean_pearson": 0.463078890647311,
        "euclidean_spearman": 0.4370552930156271,
        "hf_subset": "hin",
        "languages": [
          "hin-Deva"
        ],
        "main_score": 0.4370552930156271,
        "manhattan_pearson": 0.46133562530242533,
        "manhattan_spearman": 0.4369685582526483,
        "pearson": 0.44414988322594995,
        "spearman": 0.4370552930156271
      },
      {
        "cosine_pearson": 0.43208887227218656,
        "cosine_spearman": 0.4552939426523848,
        "euclidean_pearson": 0.46617299963707365,
        "euclidean_spearman": 0.4552939426523848,
        "hf_subset": "ind",
        "languages": [
          "ind-Latn"
        ],
        "main_score": 0.4552939426523848,
        "manhattan_pearson": 0.46455879666657723,
        "manhattan_spearman": 0.45497287074420045,
        "pearson": 0.43208887227218656,
        "spearman": 0.4552939426523848
      },
      {
        "cosine_pearson": 0.4677395605399086,
        "cosine_spearman": 0.44373075432223863,
        "euclidean_pearson": 0.4694528477787702,
        "euclidean_spearman": 0.44373075432223863,
        "hf_subset": "kin",
        "languages": [
          "kin-Latn"
        ],
        "main_score": 0.44373075432223863,
        "manhattan_pearson": 0.47906939618078126,
        "manhattan_spearman": 0.4570918274369198,
        "pearson": 0.4677395605399086,
        "spearman": 0.44373075432223863
      },
      {
        "cosine_pearson": 0.42859335092013623,
        "cosine_spearman": 0.4141373193960924,
        "euclidean_pearson": 0.4503561286242377,
        "euclidean_spearman": 0.4141373193960924,
        "hf_subset": "mar",
        "languages": [
          "mar-Deva"
        ],
        "main_score": 0.4141373193960924,
        "manhattan_pearson": 0.44750722138058724,
        "manhattan_spearman": 0.41452312442780953,
        "pearson": 0.42859335092013623,
        "spearman": 0.4141373193960924
      },
      {
        "cosine_pearson": 0.2273978037637052,
        "cosine_spearman": 0.2909541706635743,
        "euclidean_pearson": 0.2859205369605506,
        "euclidean_spearman": 0.29113886633469493,
        "hf_subset": "tel",
        "languages": [
          "tel-Telu"
        ],
        "main_score": 0.2909541706635743,
        "manhattan_pearson": 0.28711266775781497,
        "manhattan_spearman": 0.29406341270765574,
        "pearson": 0.2273978037637052,
        "spearman": 0.2909541706635743
      }
    ]
  },
  "task_name": "SemRel24STS"
}