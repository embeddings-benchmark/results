{
  "dataset_revision": "ed9a4403ed4adbfaf4aab56d5b2709e9f6c3ba33",
  "evaluation_time": 80.45443749427795,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test": [
      {
        "accuracy": 0.09514271407110667,
        "f1": 0.07848535266403642,
        "hf_subset": "arb_Arab-rus_Cyrl",
        "languages": [
          "arb-Arab",
          "rus-Cyrl"
        ],
        "main_score": 0.07848535266403642,
        "precision": 0.07351046320689057,
        "recall": 0.09514271407110667
      },
      {
        "accuracy": 0.9644466700050075,
        "f1": 0.9548489400767818,
        "hf_subset": "bel_Cyrl-rus_Cyrl",
        "languages": [
          "bel-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.9548489400767818,
        "precision": 0.9503421799365716,
        "recall": 0.9644466700050075
      },
      {
        "accuracy": 0.0500751126690035,
        "f1": 0.04127525875279084,
        "hf_subset": "ben_Beng-rus_Cyrl",
        "languages": [
          "ben-Beng",
          "rus-Cyrl"
        ],
        "main_score": 0.04127525875279084,
        "precision": 0.03849583899659012,
        "recall": 0.0500751126690035
      },
      {
        "accuracy": 0.927891837756635,
        "f1": 0.9093306626606577,
        "hf_subset": "bos_Latn-rus_Cyrl",
        "languages": [
          "bos-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9093306626606577,
        "precision": 0.9006009013520281,
        "recall": 0.927891837756635
      },
      {
        "accuracy": 0.9889834752128193,
        "f1": 0.985478217325989,
        "hf_subset": "bul_Cyrl-rus_Cyrl",
        "languages": [
          "bul-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.985478217325989,
        "precision": 0.9838090469036888,
        "recall": 0.9889834752128193
      },
      {
        "accuracy": 0.8738107160741112,
        "f1": 0.8419653289457997,
        "hf_subset": "ces_Latn-rus_Cyrl",
        "languages": [
          "ces-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.8419653289457997,
        "precision": 0.8276498080454014,
        "recall": 0.8738107160741112
      },
      {
        "accuracy": 0.9364046069103655,
        "f1": 0.9183942580537473,
        "hf_subset": "deu_Latn-rus_Cyrl",
        "languages": [
          "deu-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9183942580537473,
        "precision": 0.9099899849774662,
        "recall": 0.9364046069103655
      },
      {
        "accuracy": 0.1727591387080621,
        "f1": 0.14683017700043238,
        "hf_subset": "ell_Grek-rus_Cyrl",
        "languages": [
          "ell-Grek",
          "rus-Cyrl"
        ],
        "main_score": 0.14683017700043238,
        "precision": 0.1375996715130416,
        "recall": 0.1727591387080621
      },
      {
        "accuracy": 0.9934902353530295,
        "f1": 0.9914037723251544,
        "hf_subset": "eng_Latn-rus_Cyrl",
        "languages": [
          "eng-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9914037723251544,
        "precision": 0.9904022700717743,
        "recall": 0.9934902353530295
      },
      {
        "accuracy": 0.1842764146219329,
        "f1": 0.16405045506876623,
        "hf_subset": "fas_Arab-rus_Cyrl",
        "languages": [
          "fas-Arab",
          "rus-Cyrl"
        ],
        "main_score": 0.16405045506876623,
        "precision": 0.15660449099001736,
        "recall": 0.1842764146219329
      },
      {
        "accuracy": 0.8537806710065098,
        "f1": 0.8185246123152984,
        "hf_subset": "fin_Latn-rus_Cyrl",
        "languages": [
          "fin-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.8185246123152984,
        "precision": 0.8030516012113408,
        "recall": 0.8537806710065098
      },
      {
        "accuracy": 0.9594391587381071,
        "f1": 0.9480887998664664,
        "hf_subset": "fra_Latn-rus_Cyrl",
        "languages": [
          "fra-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9480887998664664,
        "precision": 0.942663995993991,
        "recall": 0.9594391587381071
      },
      {
        "accuracy": 0.1397095643465198,
        "f1": 0.11647687843231158,
        "hf_subset": "heb_Hebr-rus_Cyrl",
        "languages": [
          "heb-Hebr",
          "rus-Cyrl"
        ],
        "main_score": 0.11647687843231158,
        "precision": 0.1093385448953321,
        "recall": 0.1397095643465198
      },
      {
        "accuracy": 0.11967951927891837,
        "f1": 0.09727044779623648,
        "hf_subset": "hin_Deva-rus_Cyrl",
        "languages": [
          "hin-Deva",
          "rus-Cyrl"
        ],
        "main_score": 0.09727044779623648,
        "precision": 0.08968073072937266,
        "recall": 0.11967951927891837
      },
      {
        "accuracy": 0.9364046069103655,
        "f1": 0.920489066933734,
        "hf_subset": "hrv_Latn-rus_Cyrl",
        "languages": [
          "hrv-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.920489066933734,
        "precision": 0.9131912153945203,
        "recall": 0.9364046069103655
      },
      {
        "accuracy": 0.7821732598898348,
        "f1": 0.7379266702251178,
        "hf_subset": "hun_Latn-rus_Cyrl",
        "languages": [
          "hun-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.7379266702251178,
        "precision": 0.7195122048151593,
        "recall": 0.7821732598898348
      },
      {
        "accuracy": 0.7671507260891337,
        "f1": 0.7226335446553932,
        "hf_subset": "ind_Latn-rus_Cyrl",
        "languages": [
          "ind-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.7226335446553932,
        "precision": 0.7052374196214958,
        "recall": 0.7671507260891337
      },
      {
        "accuracy": 0.058587881822734104,
        "f1": 0.04682497118037737,
        "hf_subset": "jpn_Jpan-rus_Cyrl",
        "languages": [
          "jpn-Jpan",
          "rus-Cyrl"
        ],
        "main_score": 0.04682497118037737,
        "precision": 0.04343392521974263,
        "recall": 0.058587881822734104
      },
      {
        "accuracy": 0.12769153730595895,
        "f1": 0.11755224562057763,
        "hf_subset": "kor_Hang-rus_Cyrl",
        "languages": [
          "kor-Hang",
          "rus-Cyrl"
        ],
        "main_score": 0.11755224562057763,
        "precision": 0.11375775784889454,
        "recall": 0.12769153730595895
      },
      {
        "accuracy": 0.7786680020030045,
        "f1": 0.732855235233803,
        "hf_subset": "lit_Latn-rus_Cyrl",
        "languages": [
          "lit-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.732855235233803,
        "precision": 0.7131434942401392,
        "recall": 0.7786680020030045
      },
      {
        "accuracy": 0.9819729594391587,
        "f1": 0.9769654481722584,
        "hf_subset": "mkd_Cyrl-rus_Cyrl",
        "languages": [
          "mkd-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.9769654481722584,
        "precision": 0.9745451510599231,
        "recall": 0.9819729594391587
      },
      {
        "accuracy": 0.9253880821231848,
        "f1": 0.9062760807878484,
        "hf_subset": "nld_Latn-rus_Cyrl",
        "languages": [
          "nld-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9062760807878484,
        "precision": 0.8975546653313303,
        "recall": 0.9253880821231848
      },
      {
        "accuracy": 0.8147220831246871,
        "f1": 0.7754543292350002,
        "hf_subset": "pol_Latn-rus_Cyrl",
        "languages": [
          "pol-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.7754543292350002,
        "precision": 0.7586433221260461,
        "recall": 0.8147220831246871
      },
      {
        "accuracy": 0.9414121181772659,
        "f1": 0.9265660395354937,
        "hf_subset": "por_Latn-rus_Cyrl",
        "languages": [
          "por-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9265660395354937,
        "precision": 0.9200550826239359,
        "recall": 0.9414121181772659
      },
      {
        "accuracy": 0.08012018027040561,
        "f1": 0.05554254619934188,
        "hf_subset": "rus_Cyrl-arb_Arab",
        "languages": [
          "rus-Cyrl",
          "arb-Arab"
        ],
        "main_score": 0.05554254619934188,
        "precision": 0.05062793690305482,
        "recall": 0.08012018027040561
      },
      {
        "accuracy": 0.9168753129694541,
        "f1": 0.8937573026205976,
        "hf_subset": "rus_Cyrl-bel_Cyrl",
        "languages": [
          "rus-Cyrl",
          "bel-Cyrl"
        ],
        "main_score": 0.8937573026205976,
        "precision": 0.8834501752628944,
        "recall": 0.9168753129694541
      },
      {
        "accuracy": 0.05408112168252378,
        "f1": 0.030331400853362927,
        "hf_subset": "rus_Cyrl-ben_Beng",
        "languages": [
          "rus-Cyrl",
          "ben-Beng"
        ],
        "main_score": 0.030331400853362927,
        "precision": 0.02596748588870204,
        "recall": 0.05408112168252378
      },
      {
        "accuracy": 0.8017025538307461,
        "f1": 0.7554347755399333,
        "hf_subset": "rus_Cyrl-bos_Latn",
        "languages": [
          "rus-Cyrl",
          "bos-Latn"
        ],
        "main_score": 0.7554347755399333,
        "precision": 0.7373192600533612,
        "recall": 0.8017025538307461
      },
      {
        "accuracy": 0.9814722083124687,
        "f1": 0.9755466533133033,
        "hf_subset": "rus_Cyrl-bul_Cyrl",
        "languages": [
          "rus-Cyrl",
          "bul-Cyrl"
        ],
        "main_score": 0.9755466533133033,
        "precision": 0.9727090635953931,
        "recall": 0.9814722083124687
      },
      {
        "accuracy": 0.7346019028542814,
        "f1": 0.6775168163500662,
        "hf_subset": "rus_Cyrl-ces_Latn",
        "languages": [
          "rus-Cyrl",
          "ces-Latn"
        ],
        "main_score": 0.6775168163500662,
        "precision": 0.6564493565745444,
        "recall": 0.7346019028542814
      },
      {
        "accuracy": 0.8232348522784176,
        "f1": 0.779667950699498,
        "hf_subset": "rus_Cyrl-deu_Latn",
        "languages": [
          "rus-Cyrl",
          "deu-Latn"
        ],
        "main_score": 0.779667950699498,
        "precision": 0.762040878778485,
        "recall": 0.8232348522784176
      },
      {
        "accuracy": 0.12318477716574862,
        "f1": 0.08861347397450042,
        "hf_subset": "rus_Cyrl-ell_Grek",
        "languages": [
          "rus-Cyrl",
          "ell-Grek"
        ],
        "main_score": 0.08861347397450042,
        "precision": 0.08135409243472039,
        "recall": 0.12318477716574862
      },
      {
        "accuracy": 0.9924887330996495,
        "f1": 0.9899849774661993,
        "hf_subset": "rus_Cyrl-eng_Latn",
        "languages": [
          "rus-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.9899849774661993,
        "precision": 0.9887330996494742,
        "recall": 0.9924887330996495
      },
      {
        "accuracy": 0.1542313470205308,
        "f1": 0.10814350254373507,
        "hf_subset": "rus_Cyrl-fas_Arab",
        "languages": [
          "rus-Cyrl",
          "fas-Arab"
        ],
        "main_score": 0.10814350254373507,
        "precision": 0.09865138550647148,
        "recall": 0.1542313470205308
      },
      {
        "accuracy": 0.6935403104656985,
        "f1": 0.6304272316095934,
        "hf_subset": "rus_Cyrl-fin_Latn",
        "languages": [
          "rus-Cyrl",
          "fin-Latn"
        ],
        "main_score": 0.6304272316095934,
        "precision": 0.6068102108982509,
        "recall": 0.6935403104656985
      },
      {
        "accuracy": 0.9048572859288934,
        "f1": 0.8775663495242865,
        "hf_subset": "rus_Cyrl-fra_Latn",
        "languages": [
          "rus-Cyrl",
          "fra-Latn"
        ],
        "main_score": 0.8775663495242865,
        "precision": 0.8651365937795583,
        "recall": 0.9048572859288934
      },
      {
        "accuracy": 0.1071607411116675,
        "f1": 0.07525997150992085,
        "hf_subset": "rus_Cyrl-heb_Hebr",
        "languages": [
          "rus-Cyrl",
          "heb-Hebr"
        ],
        "main_score": 0.07525997150992085,
        "precision": 0.06977241944111207,
        "recall": 0.1071607411116675
      },
      {
        "accuracy": 0.09263895843765649,
        "f1": 0.06093062569366739,
        "hf_subset": "rus_Cyrl-hin_Deva",
        "languages": [
          "rus-Cyrl",
          "hin-Deva"
        ],
        "main_score": 0.06093062569366739,
        "precision": 0.05477764093962497,
        "recall": 0.09263895843765649
      },
      {
        "accuracy": 0.8062093139709564,
        "f1": 0.760823775345558,
        "hf_subset": "rus_Cyrl-hrv_Latn",
        "languages": [
          "rus-Cyrl",
          "hrv-Latn"
        ],
        "main_score": 0.760823775345558,
        "precision": 0.742709500759075,
        "recall": 0.8062093139709564
      },
      {
        "accuracy": 0.6009013520280421,
        "f1": 0.5362408152560132,
        "hf_subset": "rus_Cyrl-hun_Latn",
        "languages": [
          "rus-Cyrl",
          "hun-Latn"
        ],
        "main_score": 0.5362408152560132,
        "precision": 0.5144666591105942,
        "recall": 0.6009013520280421
      },
      {
        "accuracy": 0.5723585378067101,
        "f1": 0.5081204888664393,
        "hf_subset": "rus_Cyrl-ind_Latn",
        "languages": [
          "rus-Cyrl",
          "ind-Latn"
        ],
        "main_score": 0.5081204888664393,
        "precision": 0.48761504852678883,
        "recall": 0.5723585378067101
      },
      {
        "accuracy": 0.05458187280921382,
        "f1": 0.029073454663923255,
        "hf_subset": "rus_Cyrl-jpn_Jpan",
        "languages": [
          "rus-Cyrl",
          "jpn-Jpan"
        ],
        "main_score": 0.029073454663923255,
        "precision": 0.024996997567380982,
        "recall": 0.05458187280921382
      },
      {
        "accuracy": 0.11316975463194792,
        "f1": 0.06158723927964203,
        "hf_subset": "rus_Cyrl-kor_Hang",
        "languages": [
          "rus-Cyrl",
          "kor-Hang"
        ],
        "main_score": 0.06158723927964203,
        "precision": 0.05122002536187806,
        "recall": 0.11316975463194792
      },
      {
        "accuracy": 0.5813720580871307,
        "f1": 0.5057594077874499,
        "hf_subset": "rus_Cyrl-lit_Latn",
        "languages": [
          "rus-Cyrl",
          "lit-Latn"
        ],
        "main_score": 0.5057594077874499,
        "precision": 0.48121949865373825,
        "recall": 0.5813720580871307
      },
      {
        "accuracy": 0.9774661992989484,
        "f1": 0.9708729761308629,
        "hf_subset": "rus_Cyrl-mkd_Cyrl",
        "languages": [
          "rus-Cyrl",
          "mkd-Cyrl"
        ],
        "main_score": 0.9708729761308629,
        "precision": 0.9676180938073777,
        "recall": 0.9774661992989484
      },
      {
        "accuracy": 0.7981972959439159,
        "f1": 0.7496849169858685,
        "hf_subset": "rus_Cyrl-nld_Latn",
        "languages": [
          "rus-Cyrl",
          "nld-Latn"
        ],
        "main_score": 0.7496849169858685,
        "precision": 0.7302675504484797,
        "recall": 0.7981972959439159
      },
      {
        "accuracy": 0.6399599399098648,
        "f1": 0.5714536723938984,
        "hf_subset": "rus_Cyrl-pol_Latn",
        "languages": [
          "rus-Cyrl",
          "pol-Latn"
        ],
        "main_score": 0.5714536723938984,
        "precision": 0.5469884420184871,
        "recall": 0.6399599399098648
      },
      {
        "accuracy": 0.8432648973460191,
        "f1": 0.8022211889262465,
        "hf_subset": "rus_Cyrl-por_Latn",
        "languages": [
          "rus-Cyrl",
          "por-Latn"
        ],
        "main_score": 0.8022211889262465,
        "precision": 0.7849822352576484,
        "recall": 0.8432648973460191
      },
      {
        "accuracy": 0.7065598397596394,
        "f1": 0.6460592727448894,
        "hf_subset": "rus_Cyrl-slk_Latn",
        "languages": [
          "rus-Cyrl",
          "slk-Latn"
        ],
        "main_score": 0.6460592727448894,
        "precision": 0.624485093367461,
        "recall": 0.7065598397596394
      },
      {
        "accuracy": 0.7966950425638458,
        "f1": 0.7469728402126998,
        "hf_subset": "rus_Cyrl-slv_Latn",
        "languages": [
          "rus-Cyrl",
          "slv-Latn"
        ],
        "main_score": 0.7469728402126998,
        "precision": 0.7269090323797384,
        "recall": 0.7966950425638458
      },
      {
        "accuracy": 0.8983475212819229,
        "f1": 0.8707990557264468,
        "hf_subset": "rus_Cyrl-spa_Latn",
        "languages": [
          "rus-Cyrl",
          "spa-Latn"
        ],
        "main_score": 0.8707990557264468,
        "precision": 0.8587381071607412,
        "recall": 0.8983475212819229
      },
      {
        "accuracy": 0.85628442663996,
        "f1": 0.8156663566778739,
        "hf_subset": "rus_Cyrl-srp_Cyrl",
        "languages": [
          "rus-Cyrl",
          "srp-Cyrl"
        ],
        "main_score": 0.8156663566778739,
        "precision": 0.7979969954932398,
        "recall": 0.85628442663996
      },
      {
        "accuracy": 0.7986980470706059,
        "f1": 0.7518467928332725,
        "hf_subset": "rus_Cyrl-srp_Latn",
        "languages": [
          "rus-Cyrl",
          "srp-Latn"
        ],
        "main_score": 0.7518467928332725,
        "precision": 0.7334931329172691,
        "recall": 0.7986980470706059
      },
      {
        "accuracy": 0.5353029544316474,
        "f1": 0.4732293159771839,
        "hf_subset": "rus_Cyrl-swa_Latn",
        "languages": [
          "rus-Cyrl",
          "swa-Latn"
        ],
        "main_score": 0.4732293159771839,
        "precision": 0.45391583180265815,
        "recall": 0.5353029544316474
      },
      {
        "accuracy": 0.8302453680520782,
        "f1": 0.7863930816860212,
        "hf_subset": "rus_Cyrl-swe_Latn",
        "languages": [
          "rus-Cyrl",
          "swe-Latn"
        ],
        "main_score": 0.7863930816860212,
        "precision": 0.7681113119896296,
        "recall": 0.8302453680520782
      },
      {
        "accuracy": 0.10665998998497747,
        "f1": 0.0703978958153226,
        "hf_subset": "rus_Cyrl-tam_Taml",
        "languages": [
          "rus-Cyrl",
          "tam-Taml"
        ],
        "main_score": 0.0703978958153226,
        "precision": 0.06287130656993785,
        "recall": 0.10665998998497747
      },
      {
        "accuracy": 0.515272909364046,
        "f1": 0.4476050834300164,
        "hf_subset": "rus_Cyrl-tur_Latn",
        "languages": [
          "rus-Cyrl",
          "tur-Latn"
        ],
        "main_score": 0.4476050834300164,
        "precision": 0.42624109012140415,
        "recall": 0.515272909364046
      },
      {
        "accuracy": 0.9654481722583875,
        "f1": 0.9554164580203639,
        "hf_subset": "rus_Cyrl-ukr_Cyrl",
        "languages": [
          "rus-Cyrl",
          "ukr-Cyrl"
        ],
        "main_score": 0.9554164580203639,
        "precision": 0.9505925554999166,
        "recall": 0.9654481722583875
      },
      {
        "accuracy": 0.3775663495242864,
        "f1": 0.31729537671966684,
        "hf_subset": "rus_Cyrl-vie_Latn",
        "languages": [
          "rus-Cyrl",
          "vie-Latn"
        ],
        "main_score": 0.31729537671966684,
        "precision": 0.3011144456218952,
        "recall": 0.3775663495242864
      },
      {
        "accuracy": 0.26639959939909863,
        "f1": 0.20437796576968842,
        "hf_subset": "rus_Cyrl-zho_Hant",
        "languages": [
          "rus-Cyrl",
          "zho-Hant"
        ],
        "main_score": 0.20437796576968842,
        "precision": 0.18811807072929268,
        "recall": 0.26639959939909863
      },
      {
        "accuracy": 0.5488232348522785,
        "f1": 0.4846359311221897,
        "hf_subset": "rus_Cyrl-zul_Latn",
        "languages": [
          "rus-Cyrl",
          "zul-Latn"
        ],
        "main_score": 0.4846359311221897,
        "precision": 0.4648919917486489,
        "recall": 0.5488232348522785
      },
      {
        "accuracy": 0.8728092138207311,
        "f1": 0.8438741445501586,
        "hf_subset": "slk_Latn-rus_Cyrl",
        "languages": [
          "slk-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.8438741445501586,
        "precision": 0.8309734442934243,
        "recall": 0.8728092138207311
      },
      {
        "accuracy": 0.9193790686029043,
        "f1": 0.8989412690464268,
        "hf_subset": "slv_Latn-rus_Cyrl",
        "languages": [
          "slv-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.8989412690464268,
        "precision": 0.8894925721916207,
        "recall": 0.9193790686029043
      },
      {
        "accuracy": 0.9559339008512769,
        "f1": 0.9441996327825071,
        "hf_subset": "spa_Latn-rus_Cyrl",
        "languages": [
          "spa-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9441996327825071,
        "precision": 0.9386162577199132,
        "recall": 0.9559339008512769
      },
      {
        "accuracy": 0.9594391587381071,
        "f1": 0.9475880487397763,
        "hf_subset": "srp_Cyrl-rus_Cyrl",
        "languages": [
          "srp-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.9475880487397763,
        "precision": 0.9419128693039559,
        "recall": 0.9594391587381071
      },
      {
        "accuracy": 0.927391086629945,
        "f1": 0.9083625438157236,
        "hf_subset": "srp_Latn-rus_Cyrl",
        "languages": [
          "srp-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9083625438157236,
        "precision": 0.899215489901519,
        "recall": 0.927391086629945
      },
      {
        "accuracy": 0.7270906359539309,
        "f1": 0.6762973825818092,
        "hf_subset": "swa_Latn-rus_Cyrl",
        "languages": [
          "swa-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.6762973825818092,
        "precision": 0.6561993172909548,
        "recall": 0.7270906359539309
      },
      {
        "accuracy": 0.9384076114171257,
        "f1": 0.9230774733528865,
        "hf_subset": "swe_Latn-rus_Cyrl",
        "languages": [
          "swe-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9230774733528865,
        "precision": 0.9161241862794192,
        "recall": 0.9384076114171257
      },
      {
        "accuracy": 0.13019529293940912,
        "f1": 0.10455831655130604,
        "hf_subset": "tam_Taml-rus_Cyrl",
        "languages": [
          "tam-Taml",
          "rus-Cyrl"
        ],
        "main_score": 0.10455831655130604,
        "precision": 0.09604484843593507,
        "recall": 0.13019529293940912
      },
      {
        "accuracy": 0.7035553329994992,
        "f1": 0.6496398749528444,
        "hf_subset": "tur_Latn-rus_Cyrl",
        "languages": [
          "tur-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.6496398749528444,
        "precision": 0.628537418459802,
        "recall": 0.7035553329994992
      },
      {
        "accuracy": 0.9769654481722584,
        "f1": 0.97085628442664,
        "hf_subset": "ukr_Cyrl-rus_Cyrl",
        "languages": [
          "ukr-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.97085628442664,
        "precision": 0.9680353864129527,
        "recall": 0.9769654481722584
      },
      {
        "accuracy": 0.5338007010515774,
        "f1": 0.4798683941805601,
        "hf_subset": "vie_Latn-rus_Cyrl",
        "languages": [
          "vie-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.4798683941805601,
        "precision": 0.46017887319906137,
        "recall": 0.5338007010515774
      },
      {
        "accuracy": 0.3169754631947922,
        "f1": 0.2756942828160639,
        "hf_subset": "zho_Hant-rus_Cyrl",
        "languages": [
          "zho-Hant",
          "rus-Cyrl"
        ],
        "main_score": 0.2756942828160639,
        "precision": 0.2622339052545842,
        "recall": 0.3169754631947922
      },
      {
        "accuracy": 0.7305958938407612,
        "f1": 0.6823373949813609,
        "hf_subset": "zul_Latn-rus_Cyrl",
        "languages": [
          "zul-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.6823373949813609,
        "precision": 0.663143524811026,
        "recall": 0.7305958938407612
      }
    ]
  },
  "task_name": "NTREXBitextMining"
}