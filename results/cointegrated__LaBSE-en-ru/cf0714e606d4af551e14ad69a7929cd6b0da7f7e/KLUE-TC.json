{
  "dataset_revision": "349481ec73fff722f88e0453ca05c77a447d967c",
  "task_name": "KLUE-TC",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.191797,
        "f1": 0.169441,
        "f1_weighted": 0.181356,
        "scores_per_experiment": [
          {
            "accuracy": 0.194336,
            "f1": 0.190345,
            "f1_weighted": 0.216524
          },
          {
            "accuracy": 0.162598,
            "f1": 0.152048,
            "f1_weighted": 0.13843
          },
          {
            "accuracy": 0.155762,
            "f1": 0.154644,
            "f1_weighted": 0.124545
          },
          {
            "accuracy": 0.153809,
            "f1": 0.154055,
            "f1_weighted": 0.138164
          },
          {
            "accuracy": 0.20752,
            "f1": 0.178737,
            "f1_weighted": 0.212118
          },
          {
            "accuracy": 0.182617,
            "f1": 0.188909,
            "f1_weighted": 0.137355
          },
          {
            "accuracy": 0.273438,
            "f1": 0.173865,
            "f1_weighted": 0.261807
          },
          {
            "accuracy": 0.206543,
            "f1": 0.164314,
            "f1_weighted": 0.203889
          },
          {
            "accuracy": 0.205566,
            "f1": 0.178057,
            "f1_weighted": 0.211948
          },
          {
            "accuracy": 0.175781,
            "f1": 0.159438,
            "f1_weighted": 0.168785
          }
        ],
        "main_score": 0.191797,
        "hf_subset": "default",
        "languages": [
          "kor-Hang"
        ]
      }
    ]
  },
  "evaluation_time": 7.5072174072265625,
  "kg_co2_emissions": 0.00022187708641035548
}