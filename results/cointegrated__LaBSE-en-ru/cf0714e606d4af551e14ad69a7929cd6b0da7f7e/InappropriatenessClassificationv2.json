{
  "dataset_revision": "698cb161a90150ec46618f714cdd8606cf21a9eb",
  "task_name": "InappropriatenessClassificationv2",
  "mteb_version": "1.38.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.572433,
        "f1": 0.537324,
        "f1_weighted": 0.586527,
        "ap": 0.325883,
        "ap_weighted": 0.325883,
        "scores_per_experiment": [
          {
            "accuracy": 0.525667,
            "f1": 0.519159,
            "f1_weighted": 0.541907,
            "ap": 0.33254,
            "ap_weighted": 0.33254
          },
          {
            "accuracy": 0.583667,
            "f1": 0.531975,
            "f1_weighted": 0.595228,
            "ap": 0.314482,
            "ap_weighted": 0.314482
          },
          {
            "accuracy": 0.545333,
            "f1": 0.497453,
            "f1_weighted": 0.560535,
            "ap": 0.297929,
            "ap_weighted": 0.297929
          },
          {
            "accuracy": 0.615333,
            "f1": 0.550413,
            "f1_weighted": 0.619889,
            "ap": 0.32334,
            "ap_weighted": 0.32334
          },
          {
            "accuracy": 0.609667,
            "f1": 0.549164,
            "f1_weighted": 0.616328,
            "ap": 0.323062,
            "ap_weighted": 0.323062
          },
          {
            "accuracy": 0.604,
            "f1": 0.571687,
            "f1_weighted": 0.619529,
            "ap": 0.345218,
            "ap_weighted": 0.345218
          },
          {
            "accuracy": 0.609,
            "f1": 0.5708,
            "f1_weighted": 0.622872,
            "ap": 0.342153,
            "ap_weighted": 0.342153
          },
          {
            "accuracy": 0.525333,
            "f1": 0.51339,
            "f1_weighted": 0.544392,
            "ap": 0.321849,
            "ap_weighted": 0.321849
          },
          {
            "accuracy": 0.556333,
            "f1": 0.53497,
            "f1_weighted": 0.575503,
            "ap": 0.327231,
            "ap_weighted": 0.327231
          },
          {
            "accuracy": 0.55,
            "f1": 0.534231,
            "f1_weighted": 0.569083,
            "ap": 0.33103,
            "ap_weighted": 0.33103
          }
        ],
        "main_score": 0.572433,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 205.99100065231323,
  "kg_co2_emissions": null
}