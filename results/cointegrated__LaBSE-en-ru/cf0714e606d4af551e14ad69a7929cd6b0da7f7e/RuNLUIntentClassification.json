{
  "dataset_revision": "424d0f767aaa5c411e3a529eec04658e5726a39e",
  "task_name": "RuNLUIntentClassification",
  "mteb_version": "1.38.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.61164,
        "f1": 0.549913,
        "f1_weighted": 0.598506,
        "scores_per_experiment": [
          {
            "accuracy": 0.6076,
            "f1": 0.54767,
            "f1_weighted": 0.597187
          },
          {
            "accuracy": 0.5968,
            "f1": 0.541504,
            "f1_weighted": 0.581928
          },
          {
            "accuracy": 0.618,
            "f1": 0.553091,
            "f1_weighted": 0.606849
          },
          {
            "accuracy": 0.5926,
            "f1": 0.528608,
            "f1_weighted": 0.57762
          },
          {
            "accuracy": 0.6118,
            "f1": 0.553993,
            "f1_weighted": 0.597585
          },
          {
            "accuracy": 0.6184,
            "f1": 0.546021,
            "f1_weighted": 0.606671
          },
          {
            "accuracy": 0.6448,
            "f1": 0.572056,
            "f1_weighted": 0.632213
          },
          {
            "accuracy": 0.619,
            "f1": 0.554663,
            "f1_weighted": 0.606681
          },
          {
            "accuracy": 0.6048,
            "f1": 0.553439,
            "f1_weighted": 0.592555
          },
          {
            "accuracy": 0.6026,
            "f1": 0.548083,
            "f1_weighted": 0.585772
          }
        ],
        "main_score": 0.61164,
        "hf_subset": "rus-eng",
        "languages": [
          "rus-Cyrl",
          "rus-Latn"
        ]
      },
      {
        "accuracy": 0.6027,
        "f1": 0.536807,
        "f1_weighted": 0.587707,
        "scores_per_experiment": [
          {
            "accuracy": 0.5848,
            "f1": 0.526639,
            "f1_weighted": 0.571317
          },
          {
            "accuracy": 0.593,
            "f1": 0.525897,
            "f1_weighted": 0.576289
          },
          {
            "accuracy": 0.5912,
            "f1": 0.523767,
            "f1_weighted": 0.575685
          },
          {
            "accuracy": 0.5926,
            "f1": 0.523214,
            "f1_weighted": 0.581286
          },
          {
            "accuracy": 0.5944,
            "f1": 0.533915,
            "f1_weighted": 0.577058
          },
          {
            "accuracy": 0.6124,
            "f1": 0.538406,
            "f1_weighted": 0.59521
          },
          {
            "accuracy": 0.6438,
            "f1": 0.565737,
            "f1_weighted": 0.632562
          },
          {
            "accuracy": 0.623,
            "f1": 0.558596,
            "f1_weighted": 0.60873
          },
          {
            "accuracy": 0.5888,
            "f1": 0.535232,
            "f1_weighted": 0.574616
          },
          {
            "accuracy": 0.603,
            "f1": 0.536667,
            "f1_weighted": 0.584312
          }
        ],
        "main_score": 0.6027,
        "hf_subset": "rus",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 476.48728609085083,
  "kg_co2_emissions": null
}