{
  "dataset_revision": "87b7a0d1c402dbb481db649569c556d9aa27ac05",
  "task_name": "TweetTopicSingleClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test_2021": [
      {
        "accuracy": 0.515535,
        "f1": 0.404471,
        "f1_weighted": 0.564743,
        "scores_per_experiment": [
          {
            "accuracy": 0.444182,
            "f1": 0.369313,
            "f1_weighted": 0.503082
          },
          {
            "accuracy": 0.529829,
            "f1": 0.444226,
            "f1_weighted": 0.569106
          },
          {
            "accuracy": 0.562315,
            "f1": 0.428103,
            "f1_weighted": 0.60675
          },
          {
            "accuracy": 0.496751,
            "f1": 0.410418,
            "f1_weighted": 0.543536
          },
          {
            "accuracy": 0.523922,
            "f1": 0.411194,
            "f1_weighted": 0.578149
          },
          {
            "accuracy": 0.520969,
            "f1": 0.377036,
            "f1_weighted": 0.569735
          },
          {
            "accuracy": 0.483757,
            "f1": 0.363795,
            "f1_weighted": 0.539635
          },
          {
            "accuracy": 0.582989,
            "f1": 0.468754,
            "f1_weighted": 0.619136
          },
          {
            "accuracy": 0.507974,
            "f1": 0.391586,
            "f1_weighted": 0.550341
          },
          {
            "accuracy": 0.502658,
            "f1": 0.380287,
            "f1_weighted": 0.567959
          }
        ],
        "main_score": 0.515535,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 7.528219223022461,
  "kg_co2_emissions": 0.0002591591629034062
}