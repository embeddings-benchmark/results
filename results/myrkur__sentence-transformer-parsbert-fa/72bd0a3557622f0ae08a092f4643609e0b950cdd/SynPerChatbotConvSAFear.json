{
  "dataset_revision": "3c22f7e6bf4e366c86d69293c9164bf9e9d80aac",
  "task_name": "SynPerChatbotConvSAFear",
  "mteb_version": "1.25.8",
  "scores": {
    "test": [
      {
        "accuracy": 0.669231,
        "f1": 0.659632,
        "f1_weighted": 0.670916,
        "ap": 0.706299,
        "ap_weighted": 0.706299,
        "scores_per_experiment": [
          {
            "accuracy": 0.675214,
            "f1": 0.666417,
            "f1_weighted": 0.677992,
            "ap": 0.708294,
            "ap_weighted": 0.708294
          },
          {
            "accuracy": 0.700855,
            "f1": 0.679803,
            "f1_weighted": 0.697346,
            "ap": 0.709376,
            "ap_weighted": 0.709376
          },
          {
            "accuracy": 0.735043,
            "f1": 0.718946,
            "f1_weighted": 0.733318,
            "ap": 0.738044,
            "ap_weighted": 0.738044
          },
          {
            "accuracy": 0.632479,
            "f1": 0.625475,
            "f1_weighted": 0.636418,
            "ap": 0.68224,
            "ap_weighted": 0.68224
          },
          {
            "accuracy": 0.632479,
            "f1": 0.632049,
            "f1_weighted": 0.634736,
            "ap": 0.701955,
            "ap_weighted": 0.701955
          },
          {
            "accuracy": 0.700855,
            "f1": 0.698653,
            "f1_weighted": 0.704157,
            "ap": 0.745868,
            "ap_weighted": 0.745868
          },
          {
            "accuracy": 0.641026,
            "f1": 0.635027,
            "f1_weighted": 0.645025,
            "ap": 0.689782,
            "ap_weighted": 0.689782
          },
          {
            "accuracy": 0.623932,
            "f1": 0.608815,
            "f1_weighted": 0.625246,
            "ap": 0.666881,
            "ap_weighted": 0.666881
          },
          {
            "accuracy": 0.641026,
            "f1": 0.626596,
            "f1_weighted": 0.64228,
            "ap": 0.678134,
            "ap_weighted": 0.678134
          },
          {
            "accuracy": 0.709402,
            "f1": 0.704545,
            "f1_weighted": 0.712639,
            "ap": 0.74241,
            "ap_weighted": 0.74241
          }
        ],
        "main_score": 0.669231,
        "hf_subset": "default",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 8.839233636856079,
  "kg_co2_emissions": null
}