{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "task_name": "WebLINXCandidatesReranking",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "map": 0.114199,
        "mrr": 0.096982,
        "nAUC_map_max": -0.135263,
        "nAUC_map_std": 0.004914,
        "nAUC_map_diff1": 0.015397,
        "nAUC_mrr_max": -0.135234,
        "nAUC_mrr_std": 0.010384,
        "nAUC_mrr_diff1": 0.024826,
        "main_score": 0.096982,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_iid": [
      {
        "map": 0.10829,
        "mrr": 0.09086,
        "nAUC_map_max": -0.065514,
        "nAUC_map_std": 0.078083,
        "nAUC_map_diff1": 0.106249,
        "nAUC_mrr_max": -0.073736,
        "nAUC_mrr_std": 0.07374,
        "nAUC_mrr_diff1": 0.107059,
        "main_score": 0.09086,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_cat": [
      {
        "map": 0.082464,
        "mrr": 0.06501,
        "nAUC_map_max": -0.215657,
        "nAUC_map_std": 0.323985,
        "nAUC_map_diff1": 0.187546,
        "nAUC_mrr_max": -0.204912,
        "nAUC_mrr_std": 0.300527,
        "nAUC_mrr_diff1": 0.190101,
        "main_score": 0.06501,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_geo": [
      {
        "map": 0.129124,
        "mrr": 0.11127,
        "nAUC_map_max": -0.066865,
        "nAUC_map_std": 0.073082,
        "nAUC_map_diff1": 0.177814,
        "nAUC_mrr_max": -0.057234,
        "nAUC_mrr_std": 0.065894,
        "nAUC_mrr_diff1": 0.176633,
        "main_score": 0.11127,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_vis": [
      {
        "map": 0.106637,
        "mrr": 0.088325,
        "nAUC_map_max": -0.062803,
        "nAUC_map_std": 0.075437,
        "nAUC_map_diff1": 0.082234,
        "nAUC_mrr_max": -0.059124,
        "nAUC_mrr_std": 0.055473,
        "nAUC_mrr_diff1": 0.078331,
        "main_score": 0.088325,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_web": [
      {
        "map": 0.083146,
        "mrr": 0.063622,
        "nAUC_map_max": -0.069048,
        "nAUC_map_std": 0.049634,
        "nAUC_map_diff1": 0.132461,
        "nAUC_mrr_max": -0.088172,
        "nAUC_mrr_std": 0.050784,
        "nAUC_mrr_diff1": 0.135544,
        "main_score": 0.063622,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 23945.49173808098,
  "kg_co2_emissions": 2.2498690118390186
}