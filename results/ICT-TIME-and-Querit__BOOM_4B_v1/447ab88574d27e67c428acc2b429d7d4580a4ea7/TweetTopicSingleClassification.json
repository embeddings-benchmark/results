{
  "dataset_revision": "b4280e921a2760ce34d2dd80a9e5dc8bcbf61785",
  "task_name": "TweetTopicSingleClassification",
  "mteb_version": "2.7.22",
  "scores": {
    "test_2021": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.780862,
            "f1": 0.601098,
            "f1_weighted": 0.792815,
            "precision": 0.59022,
            "precision_weighted": 0.830598,
            "recall": 0.676765,
            "recall_weighted": 0.780862,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.786769,
            "f1": 0.624235,
            "f1_weighted": 0.803853,
            "precision": 0.601579,
            "precision_weighted": 0.84479,
            "recall": 0.706336,
            "recall_weighted": 0.786769,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.748966,
            "f1": 0.593013,
            "f1_weighted": 0.775492,
            "precision": 0.578977,
            "precision_weighted": 0.839581,
            "recall": 0.695655,
            "recall_weighted": 0.748966,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.789722,
            "f1": 0.637243,
            "f1_weighted": 0.803872,
            "precision": 0.61246,
            "precision_weighted": 0.833386,
            "recall": 0.715227,
            "recall_weighted": 0.789722,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.789722,
            "f1": 0.623658,
            "f1_weighted": 0.805687,
            "precision": 0.607264,
            "precision_weighted": 0.83651,
            "recall": 0.689002,
            "recall_weighted": 0.789722,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.789132,
            "f1": 0.639686,
            "f1_weighted": 0.801376,
            "precision": 0.614988,
            "precision_weighted": 0.822293,
            "recall": 0.696265,
            "recall_weighted": 0.789132,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.764914,
            "f1": 0.562957,
            "f1_weighted": 0.777615,
            "precision": 0.554338,
            "precision_weighted": 0.809116,
            "recall": 0.643565,
            "recall_weighted": 0.764914,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.792085,
            "f1": 0.625165,
            "f1_weighted": 0.80981,
            "precision": 0.603857,
            "precision_weighted": 0.842364,
            "recall": 0.692016,
            "recall_weighted": 0.792085,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.75192,
            "f1": 0.577818,
            "f1_weighted": 0.769819,
            "precision": 0.562893,
            "precision_weighted": 0.803151,
            "recall": 0.646616,
            "recall_weighted": 0.75192,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.798582,
            "f1": 0.616671,
            "f1_weighted": 0.809909,
            "precision": 0.616433,
            "precision_weighted": 0.833668,
            "recall": 0.673448,
            "recall_weighted": 0.798582,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.779268,
        "f1": 0.610154,
        "f1_weighted": 0.795025,
        "precision": 0.594301,
        "precision_weighted": 0.829546,
        "recall": 0.683489,
        "recall_weighted": 0.779268,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.779268,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 23.38649272918701,
  "kg_co2_emissions": null
}