{
  "dataset_revision": "e60893b7a8d01c9b8c12fadfe8f0a06e9d548a63",
  "task_name": "SynPerChatbotConvSAHappiness",
  "mteb_version": "2.7.22",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.5,
            "f1": 0.472376,
            "f1_weighted": 0.515347,
            "precision": 0.481713,
            "precision_weighted": 0.545718,
            "recall": 0.479276,
            "recall_weighted": 0.5,
            "ap": 0.669136,
            "ap_weighted": 0.669136
          },
          {
            "accuracy": 0.584746,
            "f1": 0.571164,
            "f1_weighted": 0.598328,
            "precision": 0.584746,
            "precision_weighted": 0.64809,
            "recall": 0.597039,
            "recall_weighted": 0.584746,
            "ap": 0.725636,
            "ap_weighted": 0.725636
          },
          {
            "accuracy": 0.516949,
            "f1": 0.497872,
            "f1_weighted": 0.532708,
            "precision": 0.510929,
            "precision_weighted": 0.574141,
            "recall": 0.5125,
            "recall_weighted": 0.516949,
            "ap": 0.683509,
            "ap_weighted": 0.683509
          },
          {
            "accuracy": 0.525424,
            "f1": 0.501208,
            "f1_weighted": 0.540326,
            "precision": 0.510417,
            "precision_weighted": 0.573446,
            "recall": 0.511842,
            "recall_weighted": 0.525424,
            "ap": 0.68321,
            "ap_weighted": 0.68321
          },
          {
            "accuracy": 0.542373,
            "f1": 0.506047,
            "f1_weighted": 0.553725,
            "precision": 0.509524,
            "precision_weighted": 0.572236,
            "recall": 0.510526,
            "recall_weighted": 0.542373,
            "ap": 0.682615,
            "ap_weighted": 0.682615
          },
          {
            "accuracy": 0.542373,
            "f1": 0.531471,
            "f1_weighted": 0.556909,
            "precision": 0.551555,
            "precision_weighted": 0.615832,
            "recall": 0.558882,
            "recall_weighted": 0.542373,
            "ap": 0.705732,
            "ap_weighted": 0.705732
          },
          {
            "accuracy": 0.423729,
            "f1": 0.412935,
            "f1_weighted": 0.441268,
            "precision": 0.438368,
            "precision_weighted": 0.499853,
            "recall": 0.429934,
            "recall_weighted": 0.423729,
            "ap": 0.650388,
            "ap_weighted": 0.650388
          },
          {
            "accuracy": 0.466102,
            "f1": 0.426522,
            "f1_weighted": 0.480146,
            "precision": 0.434043,
            "precision_weighted": 0.501366,
            "recall": 0.426645,
            "recall_weighted": 0.466102,
            "ap": 0.648523,
            "ap_weighted": 0.648523
          },
          {
            "accuracy": 0.542373,
            "f1": 0.525893,
            "f1_weighted": 0.557355,
            "precision": 0.539368,
            "precision_weighted": 0.602474,
            "recall": 0.545066,
            "recall_weighted": 0.542373,
            "ap": 0.698768,
            "ap_weighted": 0.698768
          },
          {
            "accuracy": 0.601695,
            "f1": 0.5679,
            "f1_weighted": 0.610912,
            "precision": 0.568325,
            "precision_weighted": 0.626722,
            "recall": 0.575,
            "recall_weighted": 0.601695,
            "ap": 0.713344,
            "ap_weighted": 0.713344
          }
        ],
        "accuracy": 0.524576,
        "f1": 0.501339,
        "f1_weighted": 0.538702,
        "precision": 0.512899,
        "precision_weighted": 0.575988,
        "recall": 0.514671,
        "recall_weighted": 0.524576,
        "ap": 0.686086,
        "ap_weighted": 0.686086,
        "main_score": 0.524576,
        "hf_subset": "default",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 13.591943740844727,
  "kg_co2_emissions": null
}