{
  "dataset_revision": "f334d90a9f68cc3af78cc2a2ece6a3b69408124c",
  "task_name": "KurdishSentimentClassification",
  "mteb_version": "2.7.22",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.695521,
            "f1": 0.683321,
            "f1_weighted": 0.678848,
            "precision": 0.770633,
            "precision_weighted": 0.78227,
            "recall": 0.713341,
            "recall_weighted": 0.695521,
            "ap": 0.720574,
            "ap_weighted": 0.720574
          },
          {
            "accuracy": 0.771515,
            "f1": 0.771135,
            "f1_weighted": 0.771806,
            "precision": 0.771196,
            "precision_weighted": 0.773599,
            "recall": 0.772583,
            "recall_weighted": 0.771515,
            "ap": 0.739516,
            "ap_weighted": 0.739516
          },
          {
            "accuracy": 0.6769,
            "f1": 0.654018,
            "f1_weighted": 0.660421,
            "precision": 0.70015,
            "precision_weighted": 0.696368,
            "recall": 0.66182,
            "recall_weighted": 0.6769,
            "ap": 0.633235,
            "ap_weighted": 0.633235
          },
          {
            "accuracy": 0.763463,
            "f1": 0.761483,
            "f1_weighted": 0.75992,
            "precision": 0.791432,
            "precision_weighted": 0.799353,
            "recall": 0.774027,
            "recall_weighted": 0.763463,
            "ap": 0.765237,
            "ap_weighted": 0.765237
          },
          {
            "accuracy": 0.698541,
            "f1": 0.698334,
            "f1_weighted": 0.697766,
            "precision": 0.7062,
            "precision_weighted": 0.710634,
            "recall": 0.704071,
            "recall_weighted": 0.698541,
            "ap": 0.681396,
            "ap_weighted": 0.681396
          },
          {
            "accuracy": 0.726724,
            "f1": 0.722328,
            "f1_weighted": 0.719814,
            "precision": 0.76625,
            "precision_weighted": 0.77504,
            "recall": 0.739609,
            "recall_weighted": 0.726724,
            "ap": 0.733531,
            "ap_weighted": 0.733531
          },
          {
            "accuracy": 0.75692,
            "f1": 0.75692,
            "f1_weighted": 0.756902,
            "precision": 0.761011,
            "precision_weighted": 0.76499,
            "recall": 0.760934,
            "recall_weighted": 0.75692,
            "ap": 0.733656,
            "ap_weighted": 0.733656
          },
          {
            "accuracy": 0.72622,
            "f1": 0.724456,
            "f1_weighted": 0.726043,
            "precision": 0.724795,
            "precision_weighted": 0.72596,
            "recall": 0.724213,
            "recall_weighted": 0.72622,
            "ap": 0.690164,
            "ap_weighted": 0.690164
          },
          {
            "accuracy": 0.685959,
            "f1": 0.679917,
            "f1_weighted": 0.676752,
            "precision": 0.725557,
            "precision_weighted": 0.733775,
            "recall": 0.699469,
            "recall_weighted": 0.685959,
            "ap": 0.691434,
            "ap_weighted": 0.691434
          },
          {
            "accuracy": 0.715652,
            "f1": 0.707075,
            "f1_weighted": 0.703468,
            "precision": 0.7783,
            "precision_weighted": 0.789185,
            "recall": 0.731756,
            "recall_weighted": 0.715652,
            "ap": 0.73591,
            "ap_weighted": 0.73591
          }
        ],
        "accuracy": 0.721741,
        "f1": 0.715899,
        "f1_weighted": 0.715174,
        "precision": 0.749552,
        "precision_weighted": 0.755117,
        "recall": 0.728182,
        "recall_weighted": 0.721741,
        "ap": 0.712465,
        "ap_weighted": 0.712465,
        "main_score": 0.721741,
        "hf_subset": "default",
        "languages": [
          "kur-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 5.473206996917725,
  "kg_co2_emissions": null
}