{
  "dataset_revision": "1a5f2fa2914bfeff4fcdc6fff4194fa8ec8fa19e",
  "task_name": "GujaratiNewsClassification",
  "mteb_version": "2.7.22",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.906677,
            "f1": 0.890199,
            "f1_weighted": 0.908641,
            "precision": 0.881855,
            "precision_weighted": 0.914406,
            "recall": 0.904497,
            "recall_weighted": 0.906677,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.902883,
            "f1": 0.883643,
            "f1_weighted": 0.904677,
            "precision": 0.876507,
            "precision_weighted": 0.909258,
            "recall": 0.895128,
            "recall_weighted": 0.902883,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.901366,
            "f1": 0.882082,
            "f1_weighted": 0.902891,
            "precision": 0.878646,
            "precision_weighted": 0.916389,
            "recall": 0.901264,
            "recall_weighted": 0.901366,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.89909,
            "f1": 0.874762,
            "f1_weighted": 0.8995,
            "precision": 0.87202,
            "precision_weighted": 0.900316,
            "recall": 0.878073,
            "recall_weighted": 0.89909,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.88695,
            "f1": 0.864226,
            "f1_weighted": 0.88854,
            "precision": 0.862566,
            "precision_weighted": 0.89131,
            "recall": 0.866926,
            "recall_weighted": 0.88695,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.897572,
            "f1": 0.88373,
            "f1_weighted": 0.900308,
            "precision": 0.874173,
            "precision_weighted": 0.910194,
            "recall": 0.904316,
            "recall_weighted": 0.897572,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.885432,
            "f1": 0.865006,
            "f1_weighted": 0.888814,
            "precision": 0.858986,
            "precision_weighted": 0.902795,
            "recall": 0.886766,
            "recall_weighted": 0.885432,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.911988,
            "f1": 0.894366,
            "f1_weighted": 0.913261,
            "precision": 0.887388,
            "precision_weighted": 0.917186,
            "recall": 0.905397,
            "recall_weighted": 0.911988,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.899848,
            "f1": 0.876412,
            "f1_weighted": 0.900134,
            "precision": 0.884221,
            "precision_weighted": 0.903428,
            "recall": 0.871799,
            "recall_weighted": 0.899848,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.906677,
            "f1": 0.887564,
            "f1_weighted": 0.907766,
            "precision": 0.881376,
            "precision_weighted": 0.911526,
            "recall": 0.897657,
            "recall_weighted": 0.906677,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.899848,
        "f1": 0.880199,
        "f1_weighted": 0.901453,
        "precision": 0.875774,
        "precision_weighted": 0.907681,
        "recall": 0.891182,
        "recall_weighted": 0.899848,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.899848,
        "hf_subset": "default",
        "languages": [
          "guj-Gujr"
        ]
      }
    ]
  },
  "evaluation_time": 19.41394591331482,
  "kg_co2_emissions": null
}