{
  "dataset_revision": "0d5104ecaa109d2448afe1f40dbf860f5e4458a8",
  "task_name": "SynPerChatbotConvSAJealousy",
  "mteb_version": "2.7.22",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.62069,
            "f1": 0.618877,
            "f1_weighted": 0.612532,
            "precision": 0.694444,
            "precision_weighted": 0.741379,
            "recall": 0.676768,
            "recall_weighted": 0.62069,
            "ap": 0.739889,
            "ap_weighted": 0.739889
          },
          {
            "accuracy": 0.586207,
            "f1": 0.573529,
            "f1_weighted": 0.591278,
            "precision": 0.574519,
            "precision_weighted": 0.60179,
            "recall": 0.578283,
            "recall_weighted": 0.586207,
            "ap": 0.661518,
            "ap_weighted": 0.661518
          },
          {
            "accuracy": 0.62069,
            "f1": 0.603727,
            "f1_weighted": 0.623517,
            "precision": 0.602941,
            "precision_weighted": 0.627789,
            "recall": 0.606061,
            "recall_weighted": 0.62069,
            "ap": 0.677485,
            "ap_weighted": 0.677485
          },
          {
            "accuracy": 0.655172,
            "f1": 0.654762,
            "f1_weighted": 0.651888,
            "precision": 0.713158,
            "precision_weighted": 0.758258,
            "recall": 0.704545,
            "recall_weighted": 0.655172,
            "ap": 0.760345,
            "ap_weighted": 0.760345
          },
          {
            "accuracy": 0.62069,
            "f1": 0.62069,
            "f1_weighted": 0.62069,
            "precision": 0.659091,
            "precision_weighted": 0.697492,
            "recall": 0.659091,
            "recall_weighted": 0.62069,
            "ap": 0.719436,
            "ap_weighted": 0.719436
          },
          {
            "accuracy": 0.689655,
            "f1": 0.683636,
            "f1_weighted": 0.694169,
            "precision": 0.685714,
            "precision_weighted": 0.7133,
            "recall": 0.69697,
            "recall_weighted": 0.689655,
            "ap": 0.74023,
            "ap_weighted": 0.74023
          },
          {
            "accuracy": 0.62069,
            "f1": 0.618877,
            "f1_weighted": 0.612532,
            "precision": 0.694444,
            "precision_weighted": 0.741379,
            "recall": 0.676768,
            "recall_weighted": 0.62069,
            "ap": 0.739889,
            "ap_weighted": 0.739889
          },
          {
            "accuracy": 0.724138,
            "f1": 0.72381,
            "f1_weighted": 0.721511,
            "precision": 0.789474,
            "precision_weighted": 0.84029,
            "recall": 0.777778,
            "recall_weighted": 0.724138,
            "ap": 0.831418,
            "ap_weighted": 0.831418
          },
          {
            "accuracy": 0.586207,
            "f1": 0.581731,
            "f1_weighted": 0.592175,
            "precision": 0.590476,
            "precision_weighted": 0.620361,
            "recall": 0.59596,
            "recall_weighted": 0.586207,
            "ap": 0.672687,
            "ap_weighted": 0.672687
          },
          {
            "accuracy": 0.655172,
            "f1": 0.654762,
            "f1_weighted": 0.651888,
            "precision": 0.713158,
            "precision_weighted": 0.758258,
            "recall": 0.704545,
            "recall_weighted": 0.655172,
            "ap": 0.760345,
            "ap_weighted": 0.760345
          }
        ],
        "accuracy": 0.637931,
        "f1": 0.63344,
        "f1_weighted": 0.637218,
        "precision": 0.671742,
        "precision_weighted": 0.71003,
        "recall": 0.667677,
        "recall_weighted": 0.637931,
        "ap": 0.730324,
        "ap_weighted": 0.730324,
        "main_score": 0.637931,
        "hf_subset": "default",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 28.097981452941895,
  "kg_co2_emissions": null
}