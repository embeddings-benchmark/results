{
  "dataset_revision": "5cae68b7fc094cb2fa6890a464e4d836e8107f5e",
  "task_name": "SynPerChatbotConvSAAnger",
  "mteb_version": "2.7.22",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.743902,
            "f1": 0.723647,
            "f1_weighted": 0.749559,
            "precision": 0.718429,
            "precision_weighted": 0.763644,
            "recall": 0.738752,
            "recall_weighted": 0.743902,
            "ap": 0.805664,
            "ap_weighted": 0.805664
          },
          {
            "accuracy": 0.797561,
            "f1": 0.776816,
            "f1_weighted": 0.800382,
            "precision": 0.770731,
            "precision_weighted": 0.806008,
            "recall": 0.786286,
            "recall_weighted": 0.797561,
            "ap": 0.836461,
            "ap_weighted": 0.836461
          },
          {
            "accuracy": 0.782927,
            "f1": 0.774991,
            "f1_weighted": 0.789626,
            "precision": 0.778097,
            "precision_weighted": 0.835254,
            "recall": 0.815731,
            "recall_weighted": 0.782927,
            "ap": 0.867814,
            "ap_weighted": 0.867814
          },
          {
            "accuracy": 0.743902,
            "f1": 0.715765,
            "f1_weighted": 0.746738,
            "precision": 0.711867,
            "precision_weighted": 0.751083,
            "recall": 0.721474,
            "recall_weighted": 0.743902,
            "ap": 0.792618,
            "ap_weighted": 0.792618
          },
          {
            "accuracy": 0.726829,
            "f1": 0.710984,
            "f1_weighted": 0.734422,
            "precision": 0.708816,
            "precision_weighted": 0.759973,
            "recall": 0.73375,
            "recall_weighted": 0.726829,
            "ap": 0.804041,
            "ap_weighted": 0.804041
          },
          {
            "accuracy": 0.773171,
            "f1": 0.75523,
            "f1_weighted": 0.778181,
            "precision": 0.748856,
            "precision_weighted": 0.792015,
            "recall": 0.77201,
            "recall_weighted": 0.773171,
            "ap": 0.828475,
            "ap_weighted": 0.828475
          },
          {
            "accuracy": 0.809756,
            "f1": 0.789915,
            "f1_weighted": 0.812276,
            "precision": 0.783718,
            "precision_weighted": 0.817331,
            "recall": 0.799183,
            "recall_weighted": 0.809756,
            "ap": 0.845417,
            "ap_weighted": 0.845417
          },
          {
            "accuracy": 0.804878,
            "f1": 0.780755,
            "f1_weighted": 0.805942,
            "precision": 0.77791,
            "precision_weighted": 0.807367,
            "recall": 0.784042,
            "recall_weighted": 0.804878,
            "ap": 0.833394,
            "ap_weighted": 0.833394
          },
          {
            "accuracy": 0.75122,
            "f1": 0.737417,
            "f1_weighted": 0.758268,
            "precision": 0.734746,
            "precision_weighted": 0.7856,
            "recall": 0.763384,
            "recall_weighted": 0.75122,
            "ap": 0.824946,
            "ap_weighted": 0.824946
          },
          {
            "accuracy": 0.75122,
            "f1": 0.731158,
            "f1_weighted": 0.756593,
            "precision": 0.72561,
            "precision_weighted": 0.769958,
            "recall": 0.746106,
            "recall_weighted": 0.75122,
            "ap": 0.810498,
            "ap_weighted": 0.810498
          }
        ],
        "accuracy": 0.768537,
        "f1": 0.749668,
        "f1_weighted": 0.773199,
        "precision": 0.745878,
        "precision_weighted": 0.788823,
        "recall": 0.766072,
        "recall_weighted": 0.768537,
        "ap": 0.824933,
        "ap_weighted": 0.824933,
        "main_score": 0.768537,
        "hf_subset": "default",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 40.60434293746948,
  "kg_co2_emissions": null
}