{
  "dataset_revision": "24fcf066e6b96f9e0d743e8b79184e0c599f73c3",
  "task_name": "SwahiliNewsClassification",
  "mteb_version": "2.7.22",
  "scores": {
    "train": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.661621,
            "f1": 0.62459,
            "f1_weighted": 0.676739,
            "precision": 0.602685,
            "precision_weighted": 0.7742,
            "recall": 0.745151,
            "recall_weighted": 0.661621,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.714355,
            "f1": 0.672675,
            "f1_weighted": 0.732051,
            "precision": 0.645929,
            "precision_weighted": 0.803343,
            "recall": 0.778543,
            "recall_weighted": 0.714355,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.699219,
            "f1": 0.659841,
            "f1_weighted": 0.711162,
            "precision": 0.6237,
            "precision_weighted": 0.797432,
            "recall": 0.783385,
            "recall_weighted": 0.699219,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.667969,
            "f1": 0.638196,
            "f1_weighted": 0.678453,
            "precision": 0.618189,
            "precision_weighted": 0.799869,
            "recall": 0.774645,
            "recall_weighted": 0.667969,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.606934,
            "f1": 0.620164,
            "f1_weighted": 0.617775,
            "precision": 0.636428,
            "precision_weighted": 0.811684,
            "recall": 0.775633,
            "recall_weighted": 0.606934,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.650391,
            "f1": 0.626543,
            "f1_weighted": 0.662133,
            "precision": 0.607272,
            "precision_weighted": 0.791431,
            "recall": 0.766925,
            "recall_weighted": 0.650391,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.584961,
            "f1": 0.572599,
            "f1_weighted": 0.575252,
            "precision": 0.5644,
            "precision_weighted": 0.767824,
            "recall": 0.746913,
            "recall_weighted": 0.584961,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.69873,
            "f1": 0.65212,
            "f1_weighted": 0.719088,
            "precision": 0.625058,
            "precision_weighted": 0.795443,
            "recall": 0.76715,
            "recall_weighted": 0.69873,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.585938,
            "f1": 0.586484,
            "f1_weighted": 0.583334,
            "precision": 0.587879,
            "precision_weighted": 0.780557,
            "recall": 0.762645,
            "recall_weighted": 0.585938,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.718262,
            "f1": 0.673965,
            "f1_weighted": 0.734199,
            "precision": 0.646181,
            "precision_weighted": 0.788065,
            "recall": 0.763821,
            "recall_weighted": 0.718262,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.658838,
        "f1": 0.632718,
        "f1_weighted": 0.669019,
        "precision": 0.615772,
        "precision_weighted": 0.790985,
        "recall": 0.766481,
        "recall_weighted": 0.658838,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.658838,
        "hf_subset": "default",
        "languages": [
          "swa-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 48.58172845840454,
  "kg_co2_emissions": null
}