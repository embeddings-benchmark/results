{
  "dataset_revision": "23a20c659d868740ef9c54854de631fe19cd5c17",
  "task_name": "CSFDSKMovieReviewSentimentClassification",
  "mteb_version": "2.7.22",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.483398,
            "f1": 0.460429,
            "f1_weighted": 0.463139,
            "precision": 0.47505,
            "precision_weighted": 0.475812,
            "recall": 0.478197,
            "recall_weighted": 0.483398,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.478516,
            "f1": 0.461867,
            "f1_weighted": 0.464829,
            "precision": 0.475291,
            "precision_weighted": 0.475869,
            "recall": 0.472755,
            "recall_weighted": 0.478516,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.470215,
            "f1": 0.4321,
            "f1_weighted": 0.435966,
            "precision": 0.43827,
            "precision_weighted": 0.440393,
            "recall": 0.464666,
            "recall_weighted": 0.470215,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.494141,
            "f1": 0.483869,
            "f1_weighted": 0.486467,
            "precision": 0.492403,
            "precision_weighted": 0.493881,
            "recall": 0.490464,
            "recall_weighted": 0.494141,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.50293,
            "f1": 0.497597,
            "f1_weighted": 0.499775,
            "precision": 0.496857,
            "precision_weighted": 0.498687,
            "recall": 0.500381,
            "recall_weighted": 0.50293,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.477539,
            "f1": 0.462787,
            "f1_weighted": 0.466196,
            "precision": 0.480248,
            "precision_weighted": 0.481628,
            "recall": 0.472196,
            "recall_weighted": 0.477539,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.491699,
            "f1": 0.480364,
            "f1_weighted": 0.4833,
            "precision": 0.484257,
            "precision_weighted": 0.485873,
            "recall": 0.487302,
            "recall_weighted": 0.491699,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.506836,
            "f1": 0.504903,
            "f1_weighted": 0.507141,
            "precision": 0.512224,
            "precision_weighted": 0.514209,
            "recall": 0.504349,
            "recall_weighted": 0.506836,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.472656,
            "f1": 0.451376,
            "f1_weighted": 0.455216,
            "precision": 0.464741,
            "precision_weighted": 0.466363,
            "recall": 0.466474,
            "recall_weighted": 0.472656,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.486816,
            "f1": 0.448815,
            "f1_weighted": 0.450604,
            "precision": 0.490587,
            "precision_weighted": 0.492497,
            "recall": 0.484428,
            "recall_weighted": 0.486816,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.486475,
        "f1": 0.468411,
        "f1_weighted": 0.471263,
        "precision": 0.480993,
        "precision_weighted": 0.482521,
        "recall": 0.482121,
        "recall_weighted": 0.486475,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.486475,
        "hf_subset": "default",
        "languages": [
          "slk-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 28.58534550666809,
  "kg_co2_emissions": null
}