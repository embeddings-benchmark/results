{
  "dataset_revision": "a76d16fae880597b9c73047b50159220a441cb54",
  "task_name": "MTOPDomainClassification",
  "mteb_version": "2.7.22",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.971728,
            "f1": 0.969271,
            "f1_weighted": 0.971778,
            "precision": 0.970242,
            "precision_weighted": 0.97314,
            "recall": 0.96982,
            "recall_weighted": 0.971728,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.969448,
            "f1": 0.969056,
            "f1_weighted": 0.969574,
            "precision": 0.967771,
            "precision_weighted": 0.970579,
            "recall": 0.971203,
            "recall_weighted": 0.969448,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.967396,
            "f1": 0.965619,
            "f1_weighted": 0.967641,
            "precision": 0.964282,
            "precision_weighted": 0.969244,
            "recall": 0.968248,
            "recall_weighted": 0.967396,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.964204,
            "f1": 0.963167,
            "f1_weighted": 0.964425,
            "precision": 0.962496,
            "precision_weighted": 0.967156,
            "recall": 0.966231,
            "recall_weighted": 0.964204,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.96922,
            "f1": 0.966098,
            "f1_weighted": 0.96915,
            "precision": 0.966097,
            "precision_weighted": 0.96979,
            "recall": 0.96691,
            "recall_weighted": 0.96922,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.971272,
            "f1": 0.970206,
            "f1_weighted": 0.971385,
            "precision": 0.968215,
            "precision_weighted": 0.972703,
            "recall": 0.973307,
            "recall_weighted": 0.971272,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.975832,
            "f1": 0.972865,
            "f1_weighted": 0.975752,
            "precision": 0.974299,
            "precision_weighted": 0.975915,
            "recall": 0.971802,
            "recall_weighted": 0.975832,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.975604,
            "f1": 0.973987,
            "f1_weighted": 0.975643,
            "precision": 0.973147,
            "precision_weighted": 0.976084,
            "recall": 0.975276,
            "recall_weighted": 0.975604,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.962152,
            "f1": 0.960564,
            "f1_weighted": 0.962311,
            "precision": 0.960511,
            "precision_weighted": 0.964843,
            "recall": 0.962976,
            "recall_weighted": 0.962152,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.974692,
            "f1": 0.973258,
            "f1_weighted": 0.974719,
            "precision": 0.973309,
            "precision_weighted": 0.97525,
            "recall": 0.973758,
            "recall_weighted": 0.974692,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.970155,
        "f1": 0.968409,
        "f1_weighted": 0.970238,
        "precision": 0.968037,
        "precision_weighted": 0.97147,
        "recall": 0.969953,
        "recall_weighted": 0.970155,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.970155,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 34.37715792655945,
  "kg_co2_emissions": null
}