{
  "dataset_revision": "099ee143c7fdfa6bd7965be8c801cb161c313b29",
  "task_name": "TwitterHjerneRetrieval",
  "mteb_version": "2.7.12",
  "scores": {
    "train": [
      {
        "ndcg_at_1": 0.833333,
        "map_at_1": 0.833333,
        "recall_at_1": 0.272009,
        "precision_at_1": 0.833333,
        "mrr_at_1": 0.833333,
        "ndcg_at_3": 0.696016,
        "map_at_3": 0.62963,
        "recall_at_3": 0.545085,
        "precision_at_3": 0.602564,
        "mrr_at_3": 0.871795,
        "ndcg_at_5": 0.697975,
        "map_at_5": 0.616902,
        "recall_at_5": 0.665812,
        "precision_at_5": 0.453846,
        "mrr_at_5": 0.875,
        "ndcg_at_10": 0.739302,
        "map_at_10": 0.644643,
        "recall_at_10": 0.763034,
        "precision_at_10": 0.261538,
        "mrr_at_10": 0.880571,
        "ndcg_at_20": 0.760182,
        "map_at_20": 0.655595,
        "recall_at_20": 0.817735,
        "precision_at_20": 0.140385,
        "mrr_at_20": 0.881639,
        "ndcg_at_100": 0.794278,
        "map_at_100": 0.666378,
        "recall_at_100": 0.941026,
        "precision_at_100": 0.032179,
        "mrr_at_100": 0.882028,
        "main_score": 0.739302,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 6.176603078842163,
  "kg_co2_emissions": null
}