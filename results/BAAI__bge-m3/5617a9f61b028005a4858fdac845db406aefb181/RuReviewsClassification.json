{
  "dataset_revision": "f6d2c31f4dc6b88f468552750bfec05b4b41b05a",
  "evaluation_time": 8.048463106155396,
  "kg_co2_emissions": 0.001539607167518362,
  "mteb_version": "1.14.12",
  "scores": {
    "test": [
      {
        "accuracy": 0.669091796875,
        "f1": 0.6625761344425213,
        "f1_weighted": 0.6625913585749419,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.669091796875,
        "scores_per_experiment": [
          {
            "accuracy": 0.70703125,
            "f1": 0.7049156217886431,
            "f1_weighted": 0.7049335284834637
          },
          {
            "accuracy": 0.62109375,
            "f1": 0.6112108176660208,
            "f1_weighted": 0.6112475938683681
          },
          {
            "accuracy": 0.689453125,
            "f1": 0.6895494182937661,
            "f1_weighted": 0.6895558265694731
          },
          {
            "accuracy": 0.6865234375,
            "f1": 0.6842735812399544,
            "f1_weighted": 0.6842979040103332
          },
          {
            "accuracy": 0.69140625,
            "f1": 0.693331510670597,
            "f1_weighted": 0.6933440622386584
          },
          {
            "accuracy": 0.63671875,
            "f1": 0.6264906674232248,
            "f1_weighted": 0.6265143534662165
          },
          {
            "accuracy": 0.642578125,
            "f1": 0.6304175162470517,
            "f1_weighted": 0.6304311840401358
          },
          {
            "accuracy": 0.64892578125,
            "f1": 0.6391037688218956,
            "f1_weighted": 0.6391062647564465
          },
          {
            "accuracy": 0.67333984375,
            "f1": 0.6566568221610026,
            "f1_weighted": 0.656640896118893
          },
          {
            "accuracy": 0.69384765625,
            "f1": 0.6898116201130567,
            "f1_weighted": 0.6898419721974309
          }
        ]
      }
    ]
  },
  "task_name": "RuReviewsClassification"
}