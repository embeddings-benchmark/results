{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 14.915643692016602,
  "kg_co2_emissions": 0.0004767075376205489,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.37158203125,
        "f1": 0.34926660693456413,
        "f1_weighted": 0.3492731250107525,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.37158203125,
        "scores_per_experiment": [
          {
            "accuracy": 0.380859375,
            "f1": 0.3618558900334813,
            "f1_weighted": 0.3618573808612479
          },
          {
            "accuracy": 0.38427734375,
            "f1": 0.3574870904196795,
            "f1_weighted": 0.3574670487986624
          },
          {
            "accuracy": 0.3798828125,
            "f1": 0.34795106804302367,
            "f1_weighted": 0.3479519904128438
          },
          {
            "accuracy": 0.37353515625,
            "f1": 0.35694614057777996,
            "f1_weighted": 0.35693551761801445
          },
          {
            "accuracy": 0.361328125,
            "f1": 0.34617993399215063,
            "f1_weighted": 0.3461847144413739
          },
          {
            "accuracy": 0.369140625,
            "f1": 0.32593857734280895,
            "f1_weighted": 0.32593673266343515
          },
          {
            "accuracy": 0.36474609375,
            "f1": 0.36077612790672786,
            "f1_weighted": 0.36078628972136034
          },
          {
            "accuracy": 0.39501953125,
            "f1": 0.37287897562911104,
            "f1_weighted": 0.3729252181113907
          },
          {
            "accuracy": 0.37109375,
            "f1": 0.3520573652309705,
            "f1_weighted": 0.35203075651149757
          },
          {
            "accuracy": 0.3359375,
            "f1": 0.3105949001699077,
            "f1_weighted": 0.31065560096769934
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.363623046875,
        "f1": 0.34183033080818476,
        "f1_weighted": 0.3418280011475491,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.363623046875,
        "scores_per_experiment": [
          {
            "accuracy": 0.361328125,
            "f1": 0.339314148214097,
            "f1_weighted": 0.339293810341598
          },
          {
            "accuracy": 0.375,
            "f1": 0.3460920690676538,
            "f1_weighted": 0.34608264501701796
          },
          {
            "accuracy": 0.3720703125,
            "f1": 0.34331999913876776,
            "f1_weighted": 0.3433238723180113
          },
          {
            "accuracy": 0.373046875,
            "f1": 0.3588684557194646,
            "f1_weighted": 0.35885229250580813
          },
          {
            "accuracy": 0.35205078125,
            "f1": 0.3383692755559847,
            "f1_weighted": 0.33837276393637566
          },
          {
            "accuracy": 0.3662109375,
            "f1": 0.32248353930995916,
            "f1_weighted": 0.32248746957872737
          },
          {
            "accuracy": 0.3427734375,
            "f1": 0.33921148143550994,
            "f1_weighted": 0.33919625036018514
          },
          {
            "accuracy": 0.39453125,
            "f1": 0.3764665122065584,
            "f1_weighted": 0.3764774171823264
          },
          {
            "accuracy": 0.3564453125,
            "f1": 0.33781830081114206,
            "f1_weighted": 0.3377912972307301
          },
          {
            "accuracy": 0.3427734375,
            "f1": 0.31635952662271016,
            "f1_weighted": 0.31640219300471073
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}