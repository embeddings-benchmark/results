{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "task_name": "MTOPIntentClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.657741,
        "f1": 0.433268,
        "f1_weighted": 0.690047,
        "scores_per_experiment": [
          {
            "accuracy": 0.661157,
            "f1": 0.432011,
            "f1_weighted": 0.696916
          },
          {
            "accuracy": 0.678788,
            "f1": 0.466991,
            "f1_weighted": 0.714941
          },
          {
            "accuracy": 0.663361,
            "f1": 0.43114,
            "f1_weighted": 0.696194
          },
          {
            "accuracy": 0.649587,
            "f1": 0.432822,
            "f1_weighted": 0.674071
          },
          {
            "accuracy": 0.662259,
            "f1": 0.432395,
            "f1_weighted": 0.69183
          },
          {
            "accuracy": 0.627548,
            "f1": 0.427565,
            "f1_weighted": 0.660601
          },
          {
            "accuracy": 0.672727,
            "f1": 0.431111,
            "f1_weighted": 0.700139
          },
          {
            "accuracy": 0.660606,
            "f1": 0.442187,
            "f1_weighted": 0.696946
          },
          {
            "accuracy": 0.650689,
            "f1": 0.420515,
            "f1_weighted": 0.684475
          },
          {
            "accuracy": 0.650689,
            "f1": 0.415943,
            "f1_weighted": 0.684352
          }
        ],
        "main_score": 0.657741,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.68084,
        "f1": 0.461842,
        "f1_weighted": 0.711913,
        "scores_per_experiment": [
          {
            "accuracy": 0.686391,
            "f1": 0.470712,
            "f1_weighted": 0.717191
          },
          {
            "accuracy": 0.694844,
            "f1": 0.472763,
            "f1_weighted": 0.731372
          },
          {
            "accuracy": 0.686954,
            "f1": 0.459152,
            "f1_weighted": 0.720311
          },
          {
            "accuracy": 0.674556,
            "f1": 0.468172,
            "f1_weighted": 0.698181
          },
          {
            "accuracy": 0.684982,
            "f1": 0.464202,
            "f1_weighted": 0.7154
          },
          {
            "accuracy": 0.64328,
            "f1": 0.44399,
            "f1_weighted": 0.674767
          },
          {
            "accuracy": 0.691181,
            "f1": 0.460218,
            "f1_weighted": 0.717318
          },
          {
            "accuracy": 0.681037,
            "f1": 0.467666,
            "f1_weighted": 0.714447
          },
          {
            "accuracy": 0.680473,
            "f1": 0.453633,
            "f1_weighted": 0.713016
          },
          {
            "accuracy": 0.6847,
            "f1": 0.457912,
            "f1_weighted": 0.717126
          }
        ],
        "main_score": 0.68084,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 51.36896634101868,
  "kg_co2_emissions": null
}