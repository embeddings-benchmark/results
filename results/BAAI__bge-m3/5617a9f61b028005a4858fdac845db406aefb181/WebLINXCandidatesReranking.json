{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 1523.4912962913513,
  "kg_co2_emissions": 0.10974046221789953,
  "mteb_version": "1.12.75",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.06248294542536116,
        "map": 0.07949101768040351,
        "mrr": 0.06248294542536116,
        "nAUC_map_diff1": 0.16958633770711415,
        "nAUC_map_max": 0.010689466610172749,
        "nAUC_map_std": 0.34696337940858546,
        "nAUC_mrr_diff1": 0.1594609755495382,
        "nAUC_mrr_max": 0.01384501323930805,
        "nAUC_mrr_std": 0.3282232237289355
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09946489919537112,
        "map": 0.11711307935949335,
        "mrr": 0.09946489919537112,
        "nAUC_map_diff1": 0.039375113335015956,
        "nAUC_map_max": -0.010282019983076757,
        "nAUC_map_std": 0.03291614900837429,
        "nAUC_mrr_diff1": 0.03332158041302599,
        "nAUC_mrr_max": -0.009346914417713344,
        "nAUC_mrr_std": 0.02660154056026256
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.10093273285206525,
        "map": 0.11519239298491503,
        "mrr": 0.10093273285206525,
        "nAUC_map_diff1": 0.22032214366939928,
        "nAUC_map_max": 0.17866249580583662,
        "nAUC_map_std": 0.16900321803600618,
        "nAUC_mrr_diff1": 0.2168496609364459,
        "nAUC_mrr_max": 0.1766254355731984,
        "nAUC_mrr_std": 0.15250258489186286
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09238781331080312,
        "map": 0.10974791285119524,
        "mrr": 0.09238781331080312,
        "nAUC_map_diff1": 0.17975251280045185,
        "nAUC_map_max": 0.09347044755250594,
        "nAUC_map_std": 0.20777522473868096,
        "nAUC_mrr_diff1": 0.18373711772671175,
        "nAUC_mrr_max": 0.10867163131763254,
        "nAUC_mrr_std": 0.1926238517714905
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08024531483500948,
        "map": 0.09495535377087498,
        "mrr": 0.08024531483500948,
        "nAUC_map_diff1": 0.21251178220310346,
        "nAUC_map_max": 0.09144145636949064,
        "nAUC_map_std": 0.15369762399460624,
        "nAUC_mrr_diff1": 0.2371729787461611,
        "nAUC_mrr_max": 0.07370631047388483,
        "nAUC_mrr_std": 0.12921906557158602
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.1236570159706209,
        "map": 0.14072146002984653,
        "mrr": 0.1236570159706209,
        "nAUC_map_diff1": 0.25025673307175617,
        "nAUC_map_max": 0.18152334693908276,
        "nAUC_map_std": 0.03098428541743902,
        "nAUC_mrr_diff1": 0.25574726802677955,
        "nAUC_mrr_max": 0.18835958962287772,
        "nAUC_mrr_std": 0.030957651529616503
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}