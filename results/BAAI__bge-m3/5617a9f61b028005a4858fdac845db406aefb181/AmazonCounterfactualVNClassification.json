{
  "dataset_revision": "b48bc27d383cfca5b6a47135a52390fa5f66b253",
  "task_name": "AmazonCounterfactualVNClassification",
  "mteb_version": "1.38.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.587768,
        "f1": 0.53698,
        "f1_weighted": 0.632811,
        "ap": 0.243489,
        "ap_weighted": 0.243489,
        "scores_per_experiment": [
          {
            "accuracy": 0.542918,
            "f1": 0.517479,
            "f1_weighted": 0.587854,
            "ap": 0.254562,
            "ap_weighted": 0.254562
          },
          {
            "accuracy": 0.678112,
            "f1": 0.620175,
            "f1_weighted": 0.714402,
            "ap": 0.304012,
            "ap_weighted": 0.304012
          },
          {
            "accuracy": 0.624464,
            "f1": 0.564432,
            "f1_weighted": 0.667145,
            "ap": 0.252683,
            "ap_weighted": 0.252683
          },
          {
            "accuracy": 0.504292,
            "f1": 0.474475,
            "f1_weighted": 0.553987,
            "ap": 0.216851,
            "ap_weighted": 0.216851
          },
          {
            "accuracy": 0.594421,
            "f1": 0.532999,
            "f1_weighted": 0.640577,
            "ap": 0.228344,
            "ap_weighted": 0.228344
          },
          {
            "accuracy": 0.564378,
            "f1": 0.513272,
            "f1_weighted": 0.613453,
            "ap": 0.222641,
            "ap_weighted": 0.222641
          },
          {
            "accuracy": 0.658798,
            "f1": 0.596585,
            "f1_weighted": 0.697214,
            "ap": 0.277758,
            "ap_weighted": 0.277758
          },
          {
            "accuracy": 0.566524,
            "f1": 0.524855,
            "f1_weighted": 0.614232,
            "ap": 0.239367,
            "ap_weighted": 0.239367
          },
          {
            "accuracy": 0.564378,
            "f1": 0.521834,
            "f1_weighted": 0.612431,
            "ap": 0.23623,
            "ap_weighted": 0.23623
          },
          {
            "accuracy": 0.579399,
            "f1": 0.503695,
            "f1_weighted": 0.626818,
            "ap": 0.202437,
            "ap_weighted": 0.202437
          }
        ],
        "main_score": 0.587768,
        "hf_subset": "default",
        "languages": [
          "vie-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 1.7666997909545898,
  "kg_co2_emissions": null
}