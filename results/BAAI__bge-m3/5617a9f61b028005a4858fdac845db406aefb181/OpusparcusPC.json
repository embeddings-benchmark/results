{
  "dataset_revision": "9e9b1f8ef51616073f47f306f7f47dd91663f86a",
  "evaluation_time": 7.961075782775879,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test.full": [
      {
        "cosine_accuracy": 0.758578431372549,
        "cosine_accuracy_threshold": 0.6951028108596802,
        "cosine_ap": 0.896438306963133,
        "cosine_f1": 0.8320864505403158,
        "cosine_f1_threshold": 0.6403511166572571,
        "cosine_precision": 0.7481315396113603,
        "cosine_recall": 0.9372659176029963,
        "dot_accuracy": 0.758578431372549,
        "dot_accuracy_threshold": 0.6951029300689697,
        "dot_ap": 0.896438306963133,
        "dot_f1": 0.8320864505403158,
        "dot_f1_threshold": 0.6403510570526123,
        "dot_precision": 0.7481315396113603,
        "dot_recall": 0.9372659176029963,
        "euclidean_accuracy": 0.758578431372549,
        "euclidean_accuracy_threshold": 0.7808932065963745,
        "euclidean_ap": 0.896438306963133,
        "euclidean_f1": 0.8320864505403158,
        "euclidean_f1_threshold": 0.8481138944625854,
        "euclidean_precision": 0.7481315396113603,
        "euclidean_recall": 0.9372659176029963,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.896438306963133,
        "manhattan_accuracy": 0.7610294117647058,
        "manhattan_accuracy_threshold": 20.151119232177734,
        "manhattan_ap": 0.8959536202163157,
        "manhattan_f1": 0.8316008316008315,
        "manhattan_f1_threshold": 21.423490524291992,
        "manhattan_precision": 0.7479431563201197,
        "manhattan_recall": 0.9363295880149812,
        "max_ap": 0.896438306963133,
        "max_f1": 0.8320864505403158,
        "max_precision": 0.7481315396113603,
        "max_recall": 0.9372659176029963,
        "similarity_accuracy": 0.758578431372549,
        "similarity_accuracy_threshold": 0.6951029300689697,
        "similarity_ap": 0.896438306963133,
        "similarity_f1": 0.8320864505403158,
        "similarity_f1_threshold": 0.6403511762619019,
        "similarity_precision": 0.7481315396113603,
        "similarity_recall": 0.9372659176029963
      }
    ],
    "validation.full": [
      {
        "cosine_accuracy": 0.7715894868585732,
        "cosine_accuracy_threshold": 0.7194346189498901,
        "cosine_ap": 0.9015011646105067,
        "cosine_f1": 0.8305315765561109,
        "cosine_f1_threshold": 0.6906485557556152,
        "cosine_precision": 0.7739204064352244,
        "cosine_recall": 0.8960784313725491,
        "dot_accuracy": 0.7715894868585732,
        "dot_accuracy_threshold": 0.7194346189498901,
        "dot_ap": 0.9015011646105067,
        "dot_f1": 0.8305315765561109,
        "dot_f1_threshold": 0.69064861536026,
        "dot_precision": 0.7739204064352244,
        "dot_recall": 0.8960784313725491,
        "euclidean_accuracy": 0.7715894868585732,
        "euclidean_accuracy_threshold": 0.7490866184234619,
        "euclidean_ap": 0.9015011646105067,
        "euclidean_f1": 0.8305315765561109,
        "euclidean_f1_threshold": 0.7865766882896423,
        "euclidean_precision": 0.7739204064352244,
        "euclidean_recall": 0.8960784313725491,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.9015011646105067,
        "manhattan_accuracy": 0.7709637046307884,
        "manhattan_accuracy_threshold": 18.85064125061035,
        "manhattan_ap": 0.9014117978035419,
        "manhattan_f1": 0.8299957392415851,
        "manhattan_f1_threshold": 21.605838775634766,
        "manhattan_precision": 0.7339864355689525,
        "manhattan_recall": 0.9549019607843138,
        "max_ap": 0.9015011646105067,
        "max_f1": 0.8305315765561109,
        "max_precision": 0.7739204064352244,
        "max_recall": 0.9549019607843138,
        "similarity_accuracy": 0.7715894868585732,
        "similarity_accuracy_threshold": 0.7194346189498901,
        "similarity_ap": 0.9015011646105067,
        "similarity_f1": 0.8305315765561109,
        "similarity_f1_threshold": 0.6906485557556152,
        "similarity_precision": 0.7739204064352244,
        "similarity_recall": 0.8960784313725491
      }
    ]
  },
  "task_name": "OpusparcusPC"
}