{
  "dataset_revision": "673a610d6d3dd91a547a0d57ae1b56f37ebbf6a1",
  "evaluation_time": 58.1754310131073,
  "kg_co2_emissions": 0.011656587213374882,
  "mteb_version": "1.14.12",
  "scores": {
    "test": [
      {
        "accuracy": 0.558056640625,
        "f1": 0.5477248967630249,
        "f1_weighted": 0.5478425388854311,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.558056640625,
        "scores_per_experiment": [
          {
            "accuracy": 0.5703125,
            "f1": 0.5566846267851252,
            "f1_weighted": 0.5567654569401991
          },
          {
            "accuracy": 0.5556640625,
            "f1": 0.5468626848818289,
            "f1_weighted": 0.5469850312602725
          },
          {
            "accuracy": 0.54052734375,
            "f1": 0.5322878872866024,
            "f1_weighted": 0.5324538475177301
          },
          {
            "accuracy": 0.5771484375,
            "f1": 0.5700910882863337,
            "f1_weighted": 0.5702090576754779
          },
          {
            "accuracy": 0.57470703125,
            "f1": 0.5712659921179167,
            "f1_weighted": 0.571263243438395
          },
          {
            "accuracy": 0.51806640625,
            "f1": 0.5042019377264696,
            "f1_weighted": 0.5042898547142672
          },
          {
            "accuracy": 0.560546875,
            "f1": 0.5472005015725384,
            "f1_weighted": 0.5474018877680656
          },
          {
            "accuracy": 0.55859375,
            "f1": 0.5471838645117639,
            "f1_weighted": 0.5473316965889516
          },
          {
            "accuracy": 0.5625,
            "f1": 0.5523120325126569,
            "f1_weighted": 0.5524304562873782
          },
          {
            "accuracy": 0.5625,
            "f1": 0.5491583519490137,
            "f1_weighted": 0.5492948566635738
          }
        ]
      }
    ]
  },
  "task_name": "RuSciBenchGRNTIClassification"
}