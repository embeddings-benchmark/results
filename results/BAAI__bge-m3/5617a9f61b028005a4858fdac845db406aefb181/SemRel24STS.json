{
  "dataset_revision": "ef5c383d1b87eb8feccde3dfb7f95e42b1b050dd",
  "evaluation_time": 8.910184144973755,
  "kg_co2_emissions": 0.000343832612576076,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "cosine_pearson": 0.8136258268770212,
        "cosine_spearman": 0.8015908635016942,
        "euclidean_pearson": 0.7915119066882547,
        "euclidean_spearman": 0.8015282299361068,
        "hf_subset": "afr",
        "languages": [
          "afr-Latn"
        ],
        "main_score": 0.8015908635016942,
        "manhattan_pearson": 0.7920914734060621,
        "manhattan_spearman": 0.8025866233153227,
        "pearson": 0.8136258268770212,
        "spearman": 0.8015908635016942
      },
      {
        "cosine_pearson": 0.8330538820386887,
        "cosine_spearman": 0.8259255215702297,
        "euclidean_pearson": 0.8280215700872198,
        "euclidean_spearman": 0.8259147182855052,
        "hf_subset": "amh",
        "languages": [
          "amh-Ethi"
        ],
        "main_score": 0.8259255215702297,
        "manhattan_pearson": 0.8278912704898198,
        "manhattan_spearman": 0.8271943073428778,
        "pearson": 0.8330538820386887,
        "spearman": 0.8259255215702297
      },
      {
        "cosine_pearson": 0.5894046178061041,
        "cosine_spearman": 0.5904461483411833,
        "euclidean_pearson": 0.5844237899566262,
        "euclidean_spearman": 0.5904493966377754,
        "hf_subset": "arb",
        "languages": [
          "arb-Arab"
        ],
        "main_score": 0.5904461483411833,
        "manhattan_pearson": 0.5829146058038424,
        "manhattan_spearman": 0.5881561277375641,
        "pearson": 0.5894046178061041,
        "spearman": 0.5904461483411833
      },
      {
        "cosine_pearson": 0.5098987997545484,
        "cosine_spearman": 0.47266328433856497,
        "euclidean_pearson": 0.5145036827777877,
        "euclidean_spearman": 0.47281674259458567,
        "hf_subset": "arq",
        "languages": [
          "arq-Arab"
        ],
        "main_score": 0.47266328433856497,
        "manhattan_pearson": 0.5104003640951773,
        "manhattan_spearman": 0.4703496456685107,
        "pearson": 0.5098987997545484,
        "spearman": 0.47266328433856497
      },
      {
        "cosine_pearson": 0.33210686611646056,
        "cosine_spearman": 0.30277673654771947,
        "euclidean_pearson": 0.30607111862188685,
        "euclidean_spearman": 0.3028227040234337,
        "hf_subset": "ary",
        "languages": [
          "ary-Arab"
        ],
        "main_score": 0.30277673654771947,
        "manhattan_pearson": 0.3133333432507359,
        "manhattan_spearman": 0.3117730063724801,
        "pearson": 0.33210686611646056,
        "spearman": 0.30277673654771947
      },
      {
        "cosine_pearson": 0.8044389475084356,
        "cosine_spearman": 0.78820353462058,
        "euclidean_pearson": 0.8060079829216226,
        "euclidean_spearman": 0.788188200649991,
        "hf_subset": "eng",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.78820353462058,
        "manhattan_pearson": 0.8053911849062667,
        "manhattan_spearman": 0.7880159087957387,
        "pearson": 0.8044389475084356,
        "spearman": 0.78820353462058
      },
      {
        "cosine_pearson": 0.5705031823685561,
        "cosine_spearman": 0.5654028024447897,
        "euclidean_pearson": 0.5636421767676618,
        "euclidean_spearman": 0.5652948913883937,
        "hf_subset": "hau",
        "languages": [
          "hau-Latn"
        ],
        "main_score": 0.5654028024447897,
        "manhattan_pearson": 0.562434774853758,
        "manhattan_spearman": 0.5635247993114618,
        "pearson": 0.5705031823685561,
        "spearman": 0.5654028024447897
      },
      {
        "cosine_pearson": 0.770645420946441,
        "cosine_spearman": 0.7776032565406992,
        "euclidean_pearson": 0.7479225317014939,
        "euclidean_spearman": 0.7776584267305146,
        "hf_subset": "hin",
        "languages": [
          "hin-Deva"
        ],
        "main_score": 0.7776032565406992,
        "manhattan_pearson": 0.7437561145742538,
        "manhattan_spearman": 0.7704465138878781,
        "pearson": 0.770645420946441,
        "spearman": 0.7776032565406992
      },
      {
        "cosine_pearson": 0.5102984509629801,
        "cosine_spearman": 0.5222593897070684,
        "euclidean_pearson": 0.5318393566864362,
        "euclidean_spearman": 0.5222977601075948,
        "hf_subset": "ind",
        "languages": [
          "ind-Latn"
        ],
        "main_score": 0.5222593897070684,
        "manhattan_pearson": 0.53280037406159,
        "manhattan_spearman": 0.5251477183102631,
        "pearson": 0.5102984509629801,
        "spearman": 0.5222593897070684
      },
      {
        "cosine_pearson": 0.5592196034812021,
        "cosine_spearman": 0.5525406863593173,
        "euclidean_pearson": 0.560050199042698,
        "euclidean_spearman": 0.5522075650506583,
        "hf_subset": "kin",
        "languages": [
          "kin-Latn"
        ],
        "main_score": 0.5525406863593173,
        "manhattan_pearson": 0.558393590705421,
        "manhattan_spearman": 0.5533166888806414,
        "pearson": 0.5592196034812021,
        "spearman": 0.5525406863593173
      },
      {
        "cosine_pearson": 0.8325644861057275,
        "cosine_spearman": 0.8146251697688253,
        "euclidean_pearson": 0.8166955715252879,
        "euclidean_spearman": 0.8146644080230797,
        "hf_subset": "mar",
        "languages": [
          "mar-Deva"
        ],
        "main_score": 0.8146251697688253,
        "manhattan_pearson": 0.8176321498207093,
        "manhattan_spearman": 0.815258198483418,
        "pearson": 0.8325644861057275,
        "spearman": 0.8146251697688253
      },
      {
        "cosine_pearson": 0.8431098802589719,
        "cosine_spearman": 0.8316637214184077,
        "euclidean_pearson": 0.8153643205272897,
        "euclidean_spearman": 0.831574838157386,
        "hf_subset": "tel",
        "languages": [
          "tel-Telu"
        ],
        "main_score": 0.8316637214184077,
        "manhattan_pearson": 0.8142571256433494,
        "manhattan_spearman": 0.8277219320565484,
        "pearson": 0.8431098802589719,
        "spearman": 0.8316637214184077
      }
    ]
  },
  "task_name": "SemRel24STS"
}