{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 35.180290937423706,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test": [
      {
        "accuracy": 0.6875252185608607,
        "f1": 0.6632599103999904,
        "f1_weighted": 0.6771718960627099,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6875252185608607,
        "scores_per_experiment": [
          {
            "accuracy": 0.7131809011432414,
            "f1": 0.6899825706015636,
            "f1_weighted": 0.7052446040375291
          },
          {
            "accuracy": 0.7168796234028245,
            "f1": 0.6787576448239125,
            "f1_weighted": 0.7112954105708726
          },
          {
            "accuracy": 0.6822461331540014,
            "f1": 0.6554945268074037,
            "f1_weighted": 0.6741763520511138
          },
          {
            "accuracy": 0.7094821788836584,
            "f1": 0.6757448988776131,
            "f1_weighted": 0.7047893149862097
          },
          {
            "accuracy": 0.6815736381977135,
            "f1": 0.6604450768536088,
            "f1_weighted": 0.6657613279277044
          },
          {
            "accuracy": 0.6543375924680565,
            "f1": 0.6402426698181969,
            "f1_weighted": 0.642490120439858
          },
          {
            "accuracy": 0.6755211835911231,
            "f1": 0.6617237679300249,
            "f1_weighted": 0.6616491216235292
          },
          {
            "accuracy": 0.67182246133154,
            "f1": 0.6387329560472155,
            "f1_weighted": 0.6636021996235716
          },
          {
            "accuracy": 0.6627437794216543,
            "f1": 0.6492719643288475,
            "f1_weighted": 0.6407075938983933
          },
          {
            "accuracy": 0.7074646940147948,
            "f1": 0.6822030279115181,
            "f1_weighted": 0.7020029154683164
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6843580914904083,
        "f1": 0.6427547051594557,
        "f1_weighted": 0.6724473440242258,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6843580914904083,
        "scores_per_experiment": [
          {
            "accuracy": 0.6940482046237088,
            "f1": 0.657342785112008,
            "f1_weighted": 0.688533720132422
          },
          {
            "accuracy": 0.7088047220855878,
            "f1": 0.6552615990995528,
            "f1_weighted": 0.7017485025093317
          },
          {
            "accuracy": 0.7038858829316281,
            "f1": 0.6568897364702162,
            "f1_weighted": 0.6929666736986214
          },
          {
            "accuracy": 0.706837186424004,
            "f1": 0.6529168232911243,
            "f1_weighted": 0.6986077738148917
          },
          {
            "accuracy": 0.6773241515002459,
            "f1": 0.6274171029175878,
            "f1_weighted": 0.6600541394422532
          },
          {
            "accuracy": 0.6566650270536154,
            "f1": 0.6235194197036874,
            "f1_weighted": 0.6433031709823057
          },
          {
            "accuracy": 0.6679783571077226,
            "f1": 0.6354548405195527,
            "f1_weighted": 0.650374919884686
          },
          {
            "accuracy": 0.661091982292179,
            "f1": 0.6263475319927034,
            "f1_weighted": 0.649550945246375
          },
          {
            "accuracy": 0.6581406787998032,
            "f1": 0.6333155274594738,
            "f1_weighted": 0.635707142600472
          },
          {
            "accuracy": 0.7088047220855878,
            "f1": 0.6590816850286499,
            "f1_weighted": 0.7036264519309001
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}