{
    "dataset_revision": "9080400076fbadbb4c4dcb136ff4eddc40b42553",
    "task_name": "Tatoeba",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "sqi-eng",
                "languages": [
                    "sqi-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.19400000000000003,
                "f1": 0.15386076064970075,
                "precision": 0.14253878834615677,
                "recall": 0.19400000000000003,
                "main_score": 0.15386076064970075
            },
            {
                "hf_subset": "fry-eng",
                "languages": [
                    "fry-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.42196531791907516,
                "f1": 0.37726396917148364,
                "precision": 0.3614643545279384,
                "recall": 0.42196531791907516,
                "main_score": 0.37726396917148364
            },
            {
                "hf_subset": "kur-eng",
                "languages": [
                    "kur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.18536585365853658,
                "f1": 0.135120103473762,
                "precision": 0.12034068912117693,
                "recall": 0.18536585365853658,
                "main_score": 0.135120103473762
            },
            {
                "hf_subset": "tur-eng",
                "languages": [
                    "tur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8169999999999998,
                "f1": 0.7737888888888889,
                "precision": 0.7549583333333332,
                "recall": 0.8169999999999998,
                "main_score": 0.7737888888888889
            },
            {
                "hf_subset": "deu-eng",
                "languages": [
                    "deu-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9739999999999999,
                "f1": 0.9656666666666667,
                "precision": 0.9616666666666667,
                "recall": 0.9739999999999999,
                "main_score": 0.9656666666666667
            },
            {
                "hf_subset": "nld-eng",
                "languages": [
                    "nld-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9,
                "f1": 0.8722333333333333,
                "precision": 0.8589166666666667,
                "recall": 0.9,
                "main_score": 0.8722333333333333
            },
            {
                "hf_subset": "ron-eng",
                "languages": [
                    "ron-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.647,
                "f1": 0.5910904761904763,
                "precision": 0.5691968253968254,
                "recall": 0.647,
                "main_score": 0.5910904761904763
            },
            {
                "hf_subset": "ang-eng",
                "languages": [
                    "ang-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.3880597014925373,
                "f1": 0.30890784174366265,
                "precision": 0.28327114427860695,
                "recall": 0.3880597014925373,
                "main_score": 0.30890784174366265
            },
            {
                "hf_subset": "ido-eng",
                "languages": [
                    "ido-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.539,
                "f1": 0.48294138583638585,
                "precision": 0.4633349567099567,
                "recall": 0.539,
                "main_score": 0.48294138583638585
            },
            {
                "hf_subset": "jav-eng",
                "languages": [
                    "jav-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.11707317073170734,
                "f1": 0.08999999999999998,
                "precision": 0.08175377468060395,
                "recall": 0.11707317073170734,
                "main_score": 0.08999999999999998
            },
            {
                "hf_subset": "isl-eng",
                "languages": [
                    "isl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.159,
                "f1": 0.12451226269430603,
                "precision": 0.11404807799760325,
                "recall": 0.159,
                "main_score": 0.12451226269430603
            },
            {
                "hf_subset": "slv-eng",
                "languages": [
                    "slv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.41919805589307413,
                "f1": 0.35880619060297064,
                "precision": 0.33776823082412394,
                "recall": 0.41919805589307413,
                "main_score": 0.35880619060297064
            },
            {
                "hf_subset": "cym-eng",
                "languages": [
                    "cym-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.10956521739130434,
                "f1": 0.09098715976676995,
                "precision": 0.08659935858401333,
                "recall": 0.10956521739130434,
                "main_score": 0.09098715976676995
            },
            {
                "hf_subset": "kaz-eng",
                "languages": [
                    "kaz-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.11652173913043479,
                "f1": 0.09154324883225136,
                "precision": 0.08505898125360802,
                "recall": 0.11652173913043479,
                "main_score": 0.09154324883225136
            },
            {
                "hf_subset": "est-eng",
                "languages": [
                    "est-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.09700000000000002,
                "f1": 0.07431679431679432,
                "precision": 0.06799925118740907,
                "recall": 0.09700000000000002,
                "main_score": 0.07431679431679432
            },
            {
                "hf_subset": "heb-eng",
                "languages": [
                    "heb-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.775,
                "f1": 0.7239999999999999,
                "precision": 0.7013444444444444,
                "recall": 0.775,
                "main_score": 0.7239999999999999
            },
            {
                "hf_subset": "gla-eng",
                "languages": [
                    "gla-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.05548854041013269,
                "f1": 0.042331554653629436,
                "precision": 0.03948150869646547,
                "recall": 0.05548854041013269,
                "main_score": 0.042331554653629436
            },
            {
                "hf_subset": "mar-eng",
                "languages": [
                    "mar-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.735,
                "f1": 0.6735333333333332,
                "precision": 0.6463666666666665,
                "recall": 0.735,
                "main_score": 0.6735333333333332
            },
            {
                "hf_subset": "lat-eng",
                "languages": [
                    "lat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.277,
                "f1": 0.21152765495941964,
                "precision": 0.1927832403707404,
                "recall": 0.277,
                "main_score": 0.21152765495941964
            },
            {
                "hf_subset": "bel-eng",
                "languages": [
                    "bel-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.48100000000000004,
                "f1": 0.4121001443001443,
                "precision": 0.3862849567099568,
                "recall": 0.48100000000000004,
                "main_score": 0.4121001443001443
            },
            {
                "hf_subset": "pms-eng",
                "languages": [
                    "pms-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.4,
                "f1": 0.3432060003488575,
                "precision": 0.3232134353741497,
                "recall": 0.4,
                "main_score": 0.3432060003488575
            },
            {
                "hf_subset": "gle-eng",
                "languages": [
                    "gle-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.068,
                "f1": 0.043954389450190465,
                "precision": 0.03893838027469606,
                "recall": 0.068,
                "main_score": 0.043954389450190465
            },
            {
                "hf_subset": "pes-eng",
                "languages": [
                    "pes-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.518,
                "f1": 0.45042229437229436,
                "precision": 0.4254198412698413,
                "recall": 0.518,
                "main_score": 0.45042229437229436
            },
            {
                "hf_subset": "nob-eng",
                "languages": [
                    "nob-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.831,
                "f1": 0.7920675324675324,
                "precision": 0.7744944444444444,
                "recall": 0.831,
                "main_score": 0.7920675324675324
            },
            {
                "hf_subset": "bul-eng",
                "languages": [
                    "bul-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.6679999999999999,
                "f1": 0.6025746031746031,
                "precision": 0.5755250000000001,
                "recall": 0.6679999999999999,
                "main_score": 0.6025746031746031
            },
            {
                "hf_subset": "cbk-eng",
                "languages": [
                    "cbk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.636,
                "f1": 0.5673421356421356,
                "precision": 0.5402218253968254,
                "recall": 0.636,
                "main_score": 0.5673421356421356
            },
            {
                "hf_subset": "hun-eng",
                "languages": [
                    "hun-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.176,
                "f1": 0.1317699134199134,
                "precision": 0.11774448051948051,
                "recall": 0.176,
                "main_score": 0.1317699134199134
            },
            {
                "hf_subset": "uig-eng",
                "languages": [
                    "uig-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.02,
                "f1": 0.013126923076923078,
                "precision": 0.01104952380952381,
                "recall": 0.02,
                "main_score": 0.013126923076923078
            },
            {
                "hf_subset": "rus-eng",
                "languages": [
                    "rus-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.883,
                "f1": 0.8496333333333334,
                "precision": 0.8338333333333332,
                "recall": 0.883,
                "main_score": 0.8496333333333334
            },
            {
                "hf_subset": "spa-eng",
                "languages": [
                    "spa-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9469999999999998,
                "f1": 0.9312333333333334,
                "precision": 0.92375,
                "recall": 0.9469999999999998,
                "main_score": 0.9312333333333334
            },
            {
                "hf_subset": "hye-eng",
                "languages": [
                    "hye-Armn",
                    "eng-Latn"
                ],
                "accuracy": 0.006738544474393532,
                "f1": 0.0036908495662913944,
                "precision": 0.003305452159899599,
                "recall": 0.006738544474393532,
                "main_score": 0.0036908495662913944
            },
            {
                "hf_subset": "tel-eng",
                "languages": [
                    "tel-Telu",
                    "eng-Latn"
                ],
                "accuracy": 0.717948717948718,
                "f1": 0.6537037037037037,
                "precision": 0.6246438746438747,
                "recall": 0.717948717948718,
                "main_score": 0.6537037037037037
            },
            {
                "hf_subset": "afr-eng",
                "languages": [
                    "afr-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.567,
                "f1": 0.5058054945054945,
                "precision": 0.48313047619047617,
                "recall": 0.567,
                "main_score": 0.5058054945054945
            },
            {
                "hf_subset": "mon-eng",
                "languages": [
                    "mon-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.13863636363636364,
                "f1": 0.10948429096156369,
                "precision": 0.10227287994137522,
                "recall": 0.13863636363636364,
                "main_score": 0.10948429096156369
            },
            {
                "hf_subset": "arz-eng",
                "languages": [
                    "arz-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.6247379454926625,
                "f1": 0.5604172906059699,
                "precision": 0.5326694619147448,
                "recall": 0.6247379454926625,
                "main_score": 0.5604172906059699
            },
            {
                "hf_subset": "hrv-eng",
                "languages": [
                    "hrv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.4,
                "f1": 0.3462948179271708,
                "precision": 0.3269903091060986,
                "recall": 0.4,
                "main_score": 0.3462948179271708
            },
            {
                "hf_subset": "nov-eng",
                "languages": [
                    "nov-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.603112840466926,
                "f1": 0.5406182447038479,
                "precision": 0.5175792106725959,
                "recall": 0.603112840466926,
                "main_score": 0.5406182447038479
            },
            {
                "hf_subset": "gsw-eng",
                "languages": [
                    "gsw-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.4358974358974359,
                "f1": 0.37042359350051657,
                "precision": 0.3475783475783476,
                "recall": 0.4358974358974359,
                "main_score": 0.37042359350051657
            },
            {
                "hf_subset": "nds-eng",
                "languages": [
                    "nds-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.565,
                "f1": 0.49471269841269844,
                "precision": 0.4674218253968255,
                "recall": 0.565,
                "main_score": 0.49471269841269844
            },
            {
                "hf_subset": "ukr-eng",
                "languages": [
                    "ukr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.715,
                "f1": 0.6532880952380951,
                "precision": 0.6271261904761904,
                "recall": 0.715,
                "main_score": 0.6532880952380951
            },
            {
                "hf_subset": "uzb-eng",
                "languages": [
                    "uzb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.11448598130841121,
                "f1": 0.07861361294691689,
                "precision": 0.06961045509526818,
                "recall": 0.11448598130841121,
                "main_score": 0.07861361294691689
            },
            {
                "hf_subset": "lit-eng",
                "languages": [
                    "lit-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.135,
                "f1": 0.10448586132968155,
                "precision": 0.09624691955878398,
                "recall": 0.135,
                "main_score": 0.10448586132968155
            },
            {
                "hf_subset": "ina-eng",
                "languages": [
                    "ina-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8219999999999998,
                "f1": 0.7825366946778712,
                "precision": 0.7654291666666667,
                "recall": 0.8219999999999998,
                "main_score": 0.7825366946778712
            },
            {
                "hf_subset": "lfn-eng",
                "languages": [
                    "lfn-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.535,
                "f1": 0.4748505411255411,
                "precision": 0.45298015873015873,
                "recall": 0.535,
                "main_score": 0.4748505411255411
            },
            {
                "hf_subset": "zsm-eng",
                "languages": [
                    "zsm-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.611,
                "f1": 0.5460758056758057,
                "precision": 0.5216455433455434,
                "recall": 0.611,
                "main_score": 0.5460758056758057
            },
            {
                "hf_subset": "ita-eng",
                "languages": [
                    "ita-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.851,
                "f1": 0.8198506715506716,
                "precision": 0.8064754901960783,
                "recall": 0.851,
                "main_score": 0.8198506715506716
            },
            {
                "hf_subset": "cmn-eng",
                "languages": [
                    "cmn-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.892,
                "f1": 0.8613333333333333,
                "precision": 0.8465,
                "recall": 0.892,
                "main_score": 0.8613333333333333
            },
            {
                "hf_subset": "lvs-eng",
                "languages": [
                    "lvs-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.136,
                "f1": 0.10721816580317724,
                "precision": 0.0997922024538847,
                "recall": 0.136,
                "main_score": 0.10721816580317724
            },
            {
                "hf_subset": "glg-eng",
                "languages": [
                    "glg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.79,
                "f1": 0.7426523809523811,
                "precision": 0.7218690476190476,
                "recall": 0.79,
                "main_score": 0.7426523809523811
            },
            {
                "hf_subset": "ceb-eng",
                "languages": [
                    "ceb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.12833333333333333,
                "f1": 0.1045993265993266,
                "precision": 0.09849548907882243,
                "recall": 0.12833333333333333,
                "main_score": 0.1045993265993266
            },
            {
                "hf_subset": "bre-eng",
                "languages": [
                    "bre-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.083,
                "f1": 0.054573113716921764,
                "precision": 0.04846694150814859,
                "recall": 0.083,
                "main_score": 0.054573113716921764
            },
            {
                "hf_subset": "ben-eng",
                "languages": [
                    "ben-Beng",
                    "eng-Latn"
                ],
                "accuracy": 0.263,
                "f1": 0.20851341154819417,
                "precision": 0.19117361794552198,
                "recall": 0.263,
                "main_score": 0.20851341154819417
            },
            {
                "hf_subset": "swg-eng",
                "languages": [
                    "swg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.41964285714285715,
                "f1": 0.3638605442176871,
                "precision": 0.34523809523809523,
                "recall": 0.41964285714285715,
                "main_score": 0.3638605442176871
            },
            {
                "hf_subset": "arq-eng",
                "languages": [
                    "arq-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.2645444566410538,
                "f1": 0.20676927658266842,
                "precision": 0.18684070229075714,
                "recall": 0.2645444566410538,
                "main_score": 0.20676927658266842
            },
            {
                "hf_subset": "kab-eng",
                "languages": [
                    "kab-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.028000000000000004,
                "f1": 0.019487240537240536,
                "precision": 0.017766582325720256,
                "recall": 0.028000000000000004,
                "main_score": 0.019487240537240536
            },
            {
                "hf_subset": "fra-eng",
                "languages": [
                    "fra-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.915,
                "f1": 0.8939,
                "precision": 0.88425,
                "recall": 0.915,
                "main_score": 0.8939
            },
            {
                "hf_subset": "por-eng",
                "languages": [
                    "por-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.915,
                "f1": 0.8938333333333333,
                "precision": 0.8836666666666667,
                "recall": 0.915,
                "main_score": 0.8938333333333333
            },
            {
                "hf_subset": "tat-eng",
                "languages": [
                    "tat-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.092,
                "f1": 0.06672282438325199,
                "precision": 0.06046073589145276,
                "recall": 0.092,
                "main_score": 0.06672282438325199
            },
            {
                "hf_subset": "oci-eng",
                "languages": [
                    "oci-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.452,
                "f1": 0.39120952380952384,
                "precision": 0.3682095238095238,
                "recall": 0.452,
                "main_score": 0.39120952380952384
            },
            {
                "hf_subset": "pol-eng",
                "languages": [
                    "pol-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.868,
                "f1": 0.8335000000000001,
                "precision": 0.81825,
                "recall": 0.868,
                "main_score": 0.8335000000000001
            },
            {
                "hf_subset": "war-eng",
                "languages": [
                    "war-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.135,
                "f1": 0.1066862856136998,
                "precision": 0.09845928551928551,
                "recall": 0.135,
                "main_score": 0.1066862856136998
            },
            {
                "hf_subset": "aze-eng",
                "languages": [
                    "aze-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.33399999999999996,
                "f1": 0.2778153389993659,
                "precision": 0.25778055555555557,
                "recall": 0.33399999999999996,
                "main_score": 0.2778153389993659
            },
            {
                "hf_subset": "vie-eng",
                "languages": [
                    "vie-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.577,
                "f1": 0.5044071428571428,
                "precision": 0.47643968253968244,
                "recall": 0.577,
                "main_score": 0.5044071428571428
            },
            {
                "hf_subset": "nno-eng",
                "languages": [
                    "nno-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.622,
                "f1": 0.560098625351257,
                "precision": 0.5369191409897291,
                "recall": 0.622,
                "main_score": 0.560098625351257
            },
            {
                "hf_subset": "cha-eng",
                "languages": [
                    "cha-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.27007299270072993,
                "f1": 0.22798053527980536,
                "precision": 0.21107055961070556,
                "recall": 0.27007299270072993,
                "main_score": 0.22798053527980536
            },
            {
                "hf_subset": "mhr-eng",
                "languages": [
                    "mhr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.062,
                "f1": 0.04295544090473964,
                "precision": 0.03913153952193392,
                "recall": 0.062,
                "main_score": 0.04295544090473964
            },
            {
                "hf_subset": "dan-eng",
                "languages": [
                    "dan-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7710000000000001,
                "f1": 0.7249333333333334,
                "precision": 0.7053368637110018,
                "recall": 0.7710000000000001,
                "main_score": 0.7249333333333334
            },
            {
                "hf_subset": "ell-eng",
                "languages": [
                    "ell-Grek",
                    "eng-Latn"
                ],
                "accuracy": 0.152,
                "f1": 0.10429591693330824,
                "precision": 0.09145801926831337,
                "recall": 0.152,
                "main_score": 0.10429591693330824
            },
            {
                "hf_subset": "amh-eng",
                "languages": [
                    "amh-Ethi",
                    "eng-Latn"
                ],
                "accuracy": 0.017857142857142856,
                "f1": 0.0036352040816326533,
                "precision": 0.00205026455026455,
                "recall": 0.017857142857142856,
                "main_score": 0.0036352040816326533
            },
            {
                "hf_subset": "pam-eng",
                "languages": [
                    "pam-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.064,
                "f1": 0.048412763053939524,
                "precision": 0.04444087810337809,
                "recall": 0.064,
                "main_score": 0.048412763053939524
            },
            {
                "hf_subset": "hsb-eng",
                "languages": [
                    "hsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.43478260869565216,
                "f1": 0.3713266949291794,
                "precision": 0.34655332590115195,
                "recall": 0.43478260869565216,
                "main_score": 0.3713266949291794
            },
            {
                "hf_subset": "srp-eng",
                "languages": [
                    "srp-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.42,
                "f1": 0.35412229437229437,
                "precision": 0.3290753968253968,
                "recall": 0.42,
                "main_score": 0.35412229437229437
            },
            {
                "hf_subset": "epo-eng",
                "languages": [
                    "epo-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.36,
                "f1": 0.3053874458874459,
                "precision": 0.2871119240838281,
                "recall": 0.36,
                "main_score": 0.3053874458874459
            },
            {
                "hf_subset": "kzj-eng",
                "languages": [
                    "kzj-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.079,
                "f1": 0.0580190114561213,
                "precision": 0.052985275318363556,
                "recall": 0.079,
                "main_score": 0.0580190114561213
            },
            {
                "hf_subset": "awa-eng",
                "languages": [
                    "awa-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.4935064935064935,
                "f1": 0.4157805638325119,
                "precision": 0.38874458874458867,
                "recall": 0.4935064935064935,
                "main_score": 0.4157805638325119
            },
            {
                "hf_subset": "fao-eng",
                "languages": [
                    "fao-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.25572519083969464,
                "f1": 0.21338006776938073,
                "precision": 0.20194474736459467,
                "recall": 0.25572519083969464,
                "main_score": 0.21338006776938073
            },
            {
                "hf_subset": "mal-eng",
                "languages": [
                    "mal-Mlym",
                    "eng-Latn"
                ],
                "accuracy": 0.7962154294032024,
                "f1": 0.7447355652595827,
                "precision": 0.722076661814653,
                "recall": 0.7962154294032024,
                "main_score": 0.7447355652595827
            },
            {
                "hf_subset": "ile-eng",
                "languages": [
                    "ile-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.68,
                "f1": 0.6180859649122807,
                "precision": 0.5930381381381381,
                "recall": 0.68,
                "main_score": 0.6180859649122807
            },
            {
                "hf_subset": "bos-eng",
                "languages": [
                    "bos-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.4293785310734463,
                "f1": 0.36726172013061353,
                "precision": 0.3472641059505466,
                "recall": 0.4293785310734463,
                "main_score": 0.36726172013061353
            },
            {
                "hf_subset": "cor-eng",
                "languages": [
                    "cor-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.055,
                "f1": 0.03865165898617511,
                "precision": 0.034432814407814406,
                "recall": 0.055,
                "main_score": 0.03865165898617511
            },
            {
                "hf_subset": "cat-eng",
                "languages": [
                    "cat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.6919999999999998,
                "f1": 0.6341880952380953,
                "precision": 0.6107913419913419,
                "recall": 0.6919999999999998,
                "main_score": 0.6341880952380953
            },
            {
                "hf_subset": "eus-eng",
                "languages": [
                    "eus-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.154,
                "f1": 0.11672122577122575,
                "precision": 0.1059919974661354,
                "recall": 0.154,
                "main_score": 0.11672122577122575
            },
            {
                "hf_subset": "yue-eng",
                "languages": [
                    "yue-Hant",
                    "eng-Latn"
                ],
                "accuracy": 0.585,
                "f1": 0.5131880452880453,
                "precision": 0.48605501253132827,
                "recall": 0.585,
                "main_score": 0.5131880452880453
            },
            {
                "hf_subset": "swe-eng",
                "languages": [
                    "swe-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.893,
                "f1": 0.8632666666666666,
                "precision": 0.8498333333333333,
                "recall": 0.893,
                "main_score": 0.8632666666666666
            },
            {
                "hf_subset": "dtp-eng",
                "languages": [
                    "dtp-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.057,
                "f1": 0.038739805216757545,
                "precision": 0.034734608954367016,
                "recall": 0.057,
                "main_score": 0.038739805216757545
            },
            {
                "hf_subset": "kat-eng",
                "languages": [
                    "kat-Geor",
                    "eng-Latn"
                ],
                "accuracy": 0.00804289544235925,
                "f1": 0.007596067917783735,
                "precision": 0.007372654155495978,
                "recall": 0.00804289544235925,
                "main_score": 0.007596067917783735
            },
            {
                "hf_subset": "jpn-eng",
                "languages": [
                    "jpn-Jpan",
                    "eng-Latn"
                ],
                "accuracy": 0.897,
                "f1": 0.8692333333333333,
                "precision": 0.8564166666666666,
                "recall": 0.897,
                "main_score": 0.8692333333333333
            },
            {
                "hf_subset": "csb-eng",
                "languages": [
                    "csb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.2608695652173913,
                "f1": 0.20517863778733342,
                "precision": 0.18901098901098898,
                "recall": 0.2608695652173913,
                "main_score": 0.20517863778733342
            },
            {
                "hf_subset": "xho-eng",
                "languages": [
                    "xho-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.1267605633802817,
                "f1": 0.09526324614352782,
                "precision": 0.09006292657908235,
                "recall": 0.1267605633802817,
                "main_score": 0.09526324614352782
            },
            {
                "hf_subset": "orv-eng",
                "languages": [
                    "orv-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.24910179640718563,
                "f1": 0.19645099411566475,
                "precision": 0.17676076418591385,
                "recall": 0.24910179640718563,
                "main_score": 0.19645099411566475
            },
            {
                "hf_subset": "ind-eng",
                "languages": [
                    "ind-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.614,
                "f1": 0.5464269841269841,
                "precision": 0.5198107142857142,
                "recall": 0.614,
                "main_score": 0.5464269841269841
            },
            {
                "hf_subset": "tuk-eng",
                "languages": [
                    "tuk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.11330049261083744,
                "f1": 0.09610016420361248,
                "precision": 0.09123781574258465,
                "recall": 0.11330049261083744,
                "main_score": 0.09610016420361248
            },
            {
                "hf_subset": "max-eng",
                "languages": [
                    "max-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.27816901408450706,
                "f1": 0.22519253451744953,
                "precision": 0.2110468365750056,
                "recall": 0.27816901408450706,
                "main_score": 0.22519253451744953
            },
            {
                "hf_subset": "swh-eng",
                "languages": [
                    "swh-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.11282051282051282,
                "f1": 0.0777716709723783,
                "precision": 0.07050109879436801,
                "recall": 0.11282051282051282,
                "main_score": 0.0777716709723783
            },
            {
                "hf_subset": "hin-eng",
                "languages": [
                    "hin-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.86,
                "f1": 0.8205857142857143,
                "precision": 0.8025,
                "recall": 0.86,
                "main_score": 0.8205857142857143
            },
            {
                "hf_subset": "dsb-eng",
                "languages": [
                    "dsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.34446764091858045,
                "f1": 0.2829651721509759,
                "precision": 0.2616624956236465,
                "recall": 0.34446764091858045,
                "main_score": 0.2829651721509759
            },
            {
                "hf_subset": "ber-eng",
                "languages": [
                    "ber-Tfng",
                    "eng-Latn"
                ],
                "accuracy": 0.072,
                "f1": 0.0550005163193804,
                "precision": 0.051644115104244416,
                "recall": 0.072,
                "main_score": 0.0550005163193804
            },
            {
                "hf_subset": "tam-eng",
                "languages": [
                    "tam-Taml",
                    "eng-Latn"
                ],
                "accuracy": 0.719869706840391,
                "f1": 0.6579339227547696,
                "precision": 0.6316503800217155,
                "recall": 0.719869706840391,
                "main_score": 0.6579339227547696
            },
            {
                "hf_subset": "slk-eng",
                "languages": [
                    "slk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.709,
                "f1": 0.6541523809523809,
                "precision": 0.6310666666666666,
                "recall": 0.709,
                "main_score": 0.6541523809523809
            },
            {
                "hf_subset": "tgl-eng",
                "languages": [
                    "tgl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.21,
                "f1": 0.17864381976446492,
                "precision": 0.1684469948469949,
                "recall": 0.21,
                "main_score": 0.17864381976446492
            },
            {
                "hf_subset": "ast-eng",
                "languages": [
                    "ast-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.6220472440944882,
                "f1": 0.5581364829396325,
                "precision": 0.5326209223847019,
                "recall": 0.6220472440944882,
                "main_score": 0.5581364829396325
            },
            {
                "hf_subset": "mkd-eng",
                "languages": [
                    "mkd-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.418,
                "f1": 0.34724603174603175,
                "precision": 0.32040277777777776,
                "recall": 0.418,
                "main_score": 0.34724603174603175
            },
            {
                "hf_subset": "khm-eng",
                "languages": [
                    "khm-Khmr",
                    "eng-Latn"
                ],
                "accuracy": 0.004155124653739612,
                "f1": 0.0034626038781163434,
                "precision": 0.0032317636195752534,
                "recall": 0.004155124653739612,
                "main_score": 0.0034626038781163434
            },
            {
                "hf_subset": "ces-eng",
                "languages": [
                    "ces-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.856,
                "f1": 0.8181333333333334,
                "precision": 0.8008333333333334,
                "recall": 0.856,
                "main_score": 0.8181333333333334
            },
            {
                "hf_subset": "tzl-eng",
                "languages": [
                    "tzl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.3173076923076923,
                "f1": 0.26097374847374843,
                "precision": 0.2431891025641026,
                "recall": 0.3173076923076923,
                "main_score": 0.26097374847374843
            },
            {
                "hf_subset": "urd-eng",
                "languages": [
                    "urd-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.096,
                "f1": 0.06598392371412456,
                "precision": 0.05855494356434758,
                "recall": 0.096,
                "main_score": 0.06598392371412456
            },
            {
                "hf_subset": "ara-eng",
                "languages": [
                    "ara-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.835,
                "f1": 0.7965190476190476,
                "precision": 0.77875,
                "recall": 0.835,
                "main_score": 0.7965190476190476
            },
            {
                "hf_subset": "kor-eng",
                "languages": [
                    "kor-Hang",
                    "eng-Latn"
                ],
                "accuracy": 0.805,
                "f1": 0.7575999999999999,
                "precision": 0.7360333333333332,
                "recall": 0.805,
                "main_score": 0.7575999999999999
            },
            {
                "hf_subset": "yid-eng",
                "languages": [
                    "yid-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.021226415094339625,
                "f1": 0.014622641509433962,
                "precision": 0.012637578616352204,
                "recall": 0.021226415094339625,
                "main_score": 0.014622641509433962
            },
            {
                "hf_subset": "fin-eng",
                "languages": [
                    "fin-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.23,
                "f1": 0.18111780719280715,
                "precision": 0.16497738095238096,
                "recall": 0.23,
                "main_score": 0.18111780719280715
            },
            {
                "hf_subset": "tha-eng",
                "languages": [
                    "tha-Thai",
                    "eng-Latn"
                ],
                "accuracy": 0.04562043795620438,
                "f1": 0.03163211990766736,
                "precision": 0.028806772100567722,
                "recall": 0.04562043795620438,
                "main_score": 0.03163211990766736
            },
            {
                "hf_subset": "wuu-eng",
                "languages": [
                    "wuu-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.759,
                "f1": 0.7057690476190476,
                "precision": 0.6819761904761904,
                "recall": 0.759,
                "main_score": 0.7057690476190476
            }
        ]
    }
}