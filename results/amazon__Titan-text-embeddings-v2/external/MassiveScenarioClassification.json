{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 0.5806993947545394,
                "f1": 0.5504073616892449,
                "main_score": 0.5806993947545394
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 0.3821452589105581,
                "f1": 0.3642184260742777,
                "main_score": 0.3821452589105581
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 0.5747141896435777,
                "f1": 0.5722453431938479,
                "main_score": 0.5747141896435777
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 0.5437121721587089,
                "f1": 0.5300497608712014,
                "main_score": 0.5437121721587089
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 0.5271687962340283,
                "f1": 0.5114015134234164,
                "main_score": 0.5271687962340283
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 0.49502353732347004,
                "f1": 0.45746047539698476,
                "main_score": 0.49502353732347004
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 0.6425689307330196,
                "f1": 0.6225355539317913,
                "main_score": 0.6425689307330196
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 0.7127774041694689,
                "f1": 0.7026880477280841,
                "main_score": 0.7127774041694689
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 0.5242098184263619,
                "f1": 0.5082454736621357,
                "main_score": 0.5242098184263619
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 0.7411230665770006,
                "f1": 0.7300723710263364,
                "main_score": 0.7411230665770006
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 0.6704102219233355,
                "f1": 0.667904194512351,
                "main_score": 0.6704102219233355
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 0.601714862138534,
                "f1": 0.587812089338461,
                "main_score": 0.601714862138534
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 0.5404841963685272,
                "f1": 0.5118500714832854,
                "main_score": 0.5404841963685272
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 0.6976462676529926,
                "f1": 0.6885227238388135,
                "main_score": 0.6976462676529926
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 0.6284801613987895,
                "f1": 0.6118395865529196,
                "main_score": 0.6284801613987895
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 0.6217888365837256,
                "f1": 0.6040570575783402,
                "main_score": 0.6217888365837256
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 0.5352051109616678,
                "f1": 0.5121069627855201,
                "main_score": 0.5352051109616678
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 0.4594821788836584,
                "f1": 0.4365062337089374,
                "main_score": 0.4594821788836584
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 0.6033288500336248,
                "f1": 0.5950436947982156,
                "main_score": 0.6033288500336248
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 0.5009751176866174,
                "f1": 0.47293838685239004,
                "main_score": 0.5009751176866174
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 0.6649293880295898,
                "f1": 0.6596586462307134,
                "main_score": 0.6649293880295898
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 0.6835911230665769,
                "f1": 0.6777840431764355,
                "main_score": 0.6835911230665769
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 0.5058507061197042,
                "f1": 0.47957277125670295,
                "main_score": 0.5058507061197042
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 0.4276059179556153,
                "f1": 0.4044632736132556,
                "main_score": 0.4276059179556153
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 0.40648957632817756,
                "f1": 0.37231284508608276,
                "main_score": 0.40648957632817756
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 0.5724613315400134,
                "f1": 0.5514523425690653,
                "main_score": 0.5724613315400134
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 0.638399462004035,
                "f1": 0.626239063060589,
                "main_score": 0.638399462004035
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 0.5314391392064559,
                "f1": 0.5008744471966442,
                "main_score": 0.5314391392064559
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 0.588399462004035,
                "f1": 0.5758699111774079,
                "main_score": 0.588399462004035
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 0.4481842636180229,
                "f1": 0.42828139750846556,
                "main_score": 0.4481842636180229
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 0.5890047074646939,
                "f1": 0.5664050313474571,
                "main_score": 0.5890047074646939
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 0.38520511096166776,
                "f1": 0.36504553927569455,
                "main_score": 0.38520511096166776
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 0.6463685272360458,
                "f1": 0.6288129994502907,
                "main_score": 0.6463685272360458
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 0.6754203093476798,
                "f1": 0.6602745142287086,
                "main_score": 0.6754203093476798
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 0.6400470746469402,
                "f1": 0.6291845058355313,
                "main_score": 0.6400470746469402
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 0.6569939475453934,
                "f1": 0.6537413822081011,
                "main_score": 0.6569939475453934
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 0.5719905850706121,
                "f1": 0.5508271383695852,
                "main_score": 0.5719905850706121
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 0.6542367182246135,
                "f1": 0.6461962307022019,
                "main_score": 0.6542367182246135
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 0.5514794889038332,
                "f1": 0.532933851469903,
                "main_score": 0.5514794889038332
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 0.5567921990585072,
                "f1": 0.5280159603468007,
                "main_score": 0.5567921990585072
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 0.6942165433759245,
                "f1": 0.6799984081248609,
                "main_score": 0.6942165433759245
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 0.5230329522528581,
                "f1": 0.5010810382364662,
                "main_score": 0.5230329522528581
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 0.5618695359784802,
                "f1": 0.5551656586643505,
                "main_score": 0.5618695359784802
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 0.5801950235373236,
                "f1": 0.5626072658635873,
                "main_score": 0.5801950235373236
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 0.5255548083389374,
                "f1": 0.5113971226436271,
                "main_score": 0.5255548083389374
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 0.5743443174176194,
                "f1": 0.5576244076715635,
                "main_score": 0.5743443174176194
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 0.6155346334902488,
                "f1": 0.6125819823057803,
                "main_score": 0.6155346334902488
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 0.47114996637525214,
                "f1": 0.45204281695469734,
                "main_score": 0.47114996637525214
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 0.5683254875588434,
                "f1": 0.5600919757601416,
                "main_score": 0.5683254875588434
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 0.6957969065232013,
                "f1": 0.6917378512156805,
                "main_score": 0.6957969065232013
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 0.6402488231338264,
                "f1": 0.6409790488949964,
                "main_score": 0.6402488231338264
            }
        ]
    }
}