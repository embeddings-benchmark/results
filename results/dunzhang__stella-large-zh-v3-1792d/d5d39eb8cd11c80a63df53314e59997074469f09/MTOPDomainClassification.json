{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "task_name": "MTOPDomainClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.629146,
        "f1": 0.612862,
        "f1_weighted": 0.63282,
        "scores_per_experiment": [
          {
            "accuracy": 0.641873,
            "f1": 0.626335,
            "f1_weighted": 0.644819
          },
          {
            "accuracy": 0.596143,
            "f1": 0.585147,
            "f1_weighted": 0.599996
          },
          {
            "accuracy": 0.614325,
            "f1": 0.59543,
            "f1_weighted": 0.617777
          },
          {
            "accuracy": 0.648485,
            "f1": 0.631013,
            "f1_weighted": 0.652629
          },
          {
            "accuracy": 0.646832,
            "f1": 0.62623,
            "f1_weighted": 0.649756
          },
          {
            "accuracy": 0.573003,
            "f1": 0.552107,
            "f1_weighted": 0.571105
          },
          {
            "accuracy": 0.644628,
            "f1": 0.626818,
            "f1_weighted": 0.651698
          },
          {
            "accuracy": 0.64022,
            "f1": 0.632312,
            "f1_weighted": 0.648354
          },
          {
            "accuracy": 0.659504,
            "f1": 0.643146,
            "f1_weighted": 0.663263
          },
          {
            "accuracy": 0.626446,
            "f1": 0.610079,
            "f1_weighted": 0.628804
          }
        ],
        "main_score": 0.629146,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.639279,
        "f1": 0.617693,
        "f1_weighted": 0.642886,
        "scores_per_experiment": [
          {
            "accuracy": 0.661595,
            "f1": 0.644243,
            "f1_weighted": 0.665596
          },
          {
            "accuracy": 0.584954,
            "f1": 0.568615,
            "f1_weighted": 0.589426
          },
          {
            "accuracy": 0.629473,
            "f1": 0.607643,
            "f1_weighted": 0.632906
          },
          {
            "accuracy": 0.659341,
            "f1": 0.63334,
            "f1_weighted": 0.66441
          },
          {
            "accuracy": 0.653423,
            "f1": 0.629062,
            "f1_weighted": 0.655911
          },
          {
            "accuracy": 0.605804,
            "f1": 0.581681,
            "f1_weighted": 0.603163
          },
          {
            "accuracy": 0.657932,
            "f1": 0.632984,
            "f1_weighted": 0.665025
          },
          {
            "accuracy": 0.648352,
            "f1": 0.633099,
            "f1_weighted": 0.656492
          },
          {
            "accuracy": 0.670048,
            "f1": 0.647544,
            "f1_weighted": 0.673709
          },
          {
            "accuracy": 0.621865,
            "f1": 0.59872,
            "f1_weighted": 0.622226
          }
        ],
        "main_score": 0.639279,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 30.005934715270996,
  "kg_co2_emissions": null
}