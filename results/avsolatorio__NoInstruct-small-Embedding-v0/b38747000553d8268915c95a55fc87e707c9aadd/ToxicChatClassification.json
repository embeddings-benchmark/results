{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.800773,
        "f1": 0.680039,
        "f1_weighted": 0.826615,
        "ap": 0.302272,
        "ap_weighted": 0.302272,
        "scores_per_experiment": [
          {
            "accuracy": 0.716495,
            "f1": 0.595833,
            "f1_weighted": 0.761269,
            "ap": 0.216111,
            "ap_weighted": 0.216111
          },
          {
            "accuracy": 0.849656,
            "f1": 0.714424,
            "f1_weighted": 0.861643,
            "ap": 0.32357,
            "ap_weighted": 0.32357
          },
          {
            "accuracy": 0.851375,
            "f1": 0.738366,
            "f1_weighted": 0.867181,
            "ap": 0.374038,
            "ap_weighted": 0.374038
          },
          {
            "accuracy": 0.804124,
            "f1": 0.685964,
            "f1_weighted": 0.830271,
            "ap": 0.307669,
            "ap_weighted": 0.307669
          },
          {
            "accuracy": 0.773196,
            "f1": 0.631368,
            "f1_weighted": 0.802661,
            "ap": 0.232258,
            "ap_weighted": 0.232258
          },
          {
            "accuracy": 0.809278,
            "f1": 0.687845,
            "f1_weighted": 0.833699,
            "ap": 0.306243,
            "ap_weighted": 0.306243
          },
          {
            "accuracy": 0.863402,
            "f1": 0.724165,
            "f1_weighted": 0.870978,
            "ap": 0.333018,
            "ap_weighted": 0.333018
          },
          {
            "accuracy": 0.736254,
            "f1": 0.639511,
            "f1_weighted": 0.779412,
            "ap": 0.282522,
            "ap_weighted": 0.282522
          },
          {
            "accuracy": 0.764605,
            "f1": 0.65504,
            "f1_weighted": 0.800681,
            "ap": 0.28392,
            "ap_weighted": 0.28392
          },
          {
            "accuracy": 0.839347,
            "f1": 0.727871,
            "f1_weighted": 0.85835,
            "ap": 0.363372,
            "ap_weighted": 0.363372
          }
        ],
        "main_score": 0.800773,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 8.49859619140625,
  "kg_co2_emissions": 0.0002867769726482026
}