{
  "dataset_revision": "edfaf9da55d3dd50d43143d90c1ac476895ae6de",
  "task_name": "ToxicConversationsClassification",
  "mteb_version": "1.36.5",
  "scores": {
    "test": [
      {
        "accuracy": 0.866455,
        "f1": 0.725879,
        "f1_weighted": 0.891036,
        "ap": 0.352051,
        "ap_weighted": 0.352051,
        "scores_per_experiment": [
          {
            "accuracy": 0.856445,
            "f1": 0.713742,
            "f1_weighted": 0.883881,
            "ap": 0.336622,
            "ap_weighted": 0.336622
          },
          {
            "accuracy": 0.891113,
            "f1": 0.753992,
            "f1_weighted": 0.908601,
            "ap": 0.385575,
            "ap_weighted": 0.385575
          },
          {
            "accuracy": 0.87207,
            "f1": 0.733023,
            "f1_weighted": 0.895214,
            "ap": 0.362718,
            "ap_weighted": 0.362718
          },
          {
            "accuracy": 0.850098,
            "f1": 0.70631,
            "f1_weighted": 0.879297,
            "ap": 0.327074,
            "ap_weighted": 0.327074
          },
          {
            "accuracy": 0.879395,
            "f1": 0.740227,
            "f1_weighted": 0.900284,
            "ap": 0.369058,
            "ap_weighted": 0.369058
          },
          {
            "accuracy": 0.828613,
            "f1": 0.683401,
            "f1_weighted": 0.863895,
            "ap": 0.300617,
            "ap_weighted": 0.300617
          },
          {
            "accuracy": 0.89502,
            "f1": 0.759642,
            "f1_weighted": 0.91149,
            "ap": 0.394257,
            "ap_weighted": 0.394257
          },
          {
            "accuracy": 0.836914,
            "f1": 0.691526,
            "f1_weighted": 0.869797,
            "ap": 0.308896,
            "ap_weighted": 0.308896
          },
          {
            "accuracy": 0.880371,
            "f1": 0.741534,
            "f1_weighted": 0.900998,
            "ap": 0.370947,
            "ap_weighted": 0.370947
          },
          {
            "accuracy": 0.874512,
            "f1": 0.735387,
            "f1_weighted": 0.896903,
            "ap": 0.364744,
            "ap_weighted": 0.364744
          }
        ],
        "main_score": 0.866455,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 17.649029970169067,
  "kg_co2_emissions": null
}