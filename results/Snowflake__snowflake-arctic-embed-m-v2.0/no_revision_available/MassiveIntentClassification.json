{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.21.3",
  "scores": {
    "test": [
      {
        "accuracy": 0.680128,
        "f1": 0.644011,
        "f1_weighted": 0.674705,
        "scores_per_experiment": [
          {
            "accuracy": 0.697041,
            "f1": 0.650161,
            "f1_weighted": 0.695325
          },
          {
            "accuracy": 0.718561,
            "f1": 0.681218,
            "f1_weighted": 0.716402
          },
          {
            "accuracy": 0.66577,
            "f1": 0.626798,
            "f1_weighted": 0.659956
          },
          {
            "accuracy": 0.675521,
            "f1": 0.632618,
            "f1_weighted": 0.677702
          },
          {
            "accuracy": 0.682246,
            "f1": 0.637262,
            "f1_weighted": 0.668867
          },
          {
            "accuracy": 0.654001,
            "f1": 0.634717,
            "f1_weighted": 0.644399
          },
          {
            "accuracy": 0.682919,
            "f1": 0.647568,
            "f1_weighted": 0.676705
          },
          {
            "accuracy": 0.683927,
            "f1": 0.647697,
            "f1_weighted": 0.679293
          },
          {
            "accuracy": 0.653329,
            "f1": 0.635605,
            "f1_weighted": 0.641909
          },
          {
            "accuracy": 0.687962,
            "f1": 0.646466,
            "f1_weighted": 0.686489
          }
        ],
        "main_score": 0.680128,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 18.145219087600708,
  "kg_co2_emissions": null
}