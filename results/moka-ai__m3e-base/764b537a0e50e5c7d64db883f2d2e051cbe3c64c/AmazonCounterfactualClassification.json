{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "task_name": "AmazonCounterfactualClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.662876,
        "f1": 0.639024,
        "f1_weighted": 0.675158,
        "ap": 0.781621,
        "ap_weighted": 0.781621,
        "scores_per_experiment": [
          {
            "accuracy": 0.628755,
            "f1": 0.60468,
            "f1_weighted": 0.643201,
            "ap": 0.759849,
            "ap_weighted": 0.759849
          },
          {
            "accuracy": 0.686695,
            "f1": 0.652594,
            "f1_weighted": 0.695571,
            "ap": 0.779457,
            "ap_weighted": 0.779457
          },
          {
            "accuracy": 0.699571,
            "f1": 0.684453,
            "f1_weighted": 0.711724,
            "ap": 0.818077,
            "ap_weighted": 0.818077
          },
          {
            "accuracy": 0.660944,
            "f1": 0.644577,
            "f1_weighted": 0.674693,
            "ap": 0.790981,
            "ap_weighted": 0.790981
          },
          {
            "accuracy": 0.641631,
            "f1": 0.623224,
            "f1_weighted": 0.656106,
            "ap": 0.775565,
            "ap_weighted": 0.775565
          },
          {
            "accuracy": 0.641631,
            "f1": 0.627423,
            "f1_weighted": 0.656151,
            "ap": 0.78328,
            "ap_weighted": 0.78328
          },
          {
            "accuracy": 0.684549,
            "f1": 0.654705,
            "f1_weighted": 0.694788,
            "ap": 0.783517,
            "ap_weighted": 0.783517
          },
          {
            "accuracy": 0.643777,
            "f1": 0.601365,
            "f1_weighted": 0.652706,
            "ap": 0.748818,
            "ap_weighted": 0.748818
          },
          {
            "accuracy": 0.624464,
            "f1": 0.599198,
            "f1_weighted": 0.638932,
            "ap": 0.755972,
            "ap_weighted": 0.755972
          },
          {
            "accuracy": 0.716738,
            "f1": 0.698027,
            "f1_weighted": 0.727707,
            "ap": 0.820698,
            "ap_weighted": 0.820698
          }
        ],
        "main_score": 0.662876,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.64379,
        "f1": 0.623809,
        "f1_weighted": 0.657334,
        "ap": 0.774071,
        "ap_weighted": 0.774071,
        "scores_per_experiment": [
          {
            "accuracy": 0.6606,
            "f1": 0.634632,
            "f1_weighted": 0.672801,
            "ap": 0.773506,
            "ap_weighted": 0.773506
          },
          {
            "accuracy": 0.657388,
            "f1": 0.630026,
            "f1_weighted": 0.669453,
            "ap": 0.769885,
            "ap_weighted": 0.769885
          },
          {
            "accuracy": 0.648822,
            "f1": 0.634643,
            "f1_weighted": 0.662848,
            "ap": 0.785922,
            "ap_weighted": 0.785922
          },
          {
            "accuracy": 0.655246,
            "f1": 0.639365,
            "f1_weighted": 0.669021,
            "ap": 0.786512,
            "ap_weighted": 0.786512
          },
          {
            "accuracy": 0.626338,
            "f1": 0.613691,
            "f1_weighted": 0.641082,
            "ap": 0.774984,
            "ap_weighted": 0.774984
          },
          {
            "accuracy": 0.619914,
            "f1": 0.608275,
            "f1_weighted": 0.634735,
            "ap": 0.773079,
            "ap_weighted": 0.773079
          },
          {
            "accuracy": 0.652034,
            "f1": 0.622068,
            "f1_weighted": 0.66377,
            "ap": 0.763775,
            "ap_weighted": 0.763775
          },
          {
            "accuracy": 0.599572,
            "f1": 0.571766,
            "f1_weighted": 0.614526,
            "ap": 0.737941,
            "ap_weighted": 0.737941
          },
          {
            "accuracy": 0.617773,
            "f1": 0.598428,
            "f1_weighted": 0.632966,
            "ap": 0.758287,
            "ap_weighted": 0.758287
          },
          {
            "accuracy": 0.700214,
            "f1": 0.685196,
            "f1_weighted": 0.71214,
            "ap": 0.816822,
            "ap_weighted": 0.816822
          }
        ],
        "main_score": 0.64379,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 44.98429250717163,
  "kg_co2_emissions": null
}