{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.440384,
        "f1": 0.429884,
        "f1_weighted": 0.439987,
        "scores_per_experiment": [
          {
            "accuracy": 0.424004,
            "f1": 0.411872,
            "f1_weighted": 0.423963
          },
          {
            "accuracy": 0.430398,
            "f1": 0.416,
            "f1_weighted": 0.433174
          },
          {
            "accuracy": 0.424988,
            "f1": 0.415856,
            "f1_weighted": 0.421379
          },
          {
            "accuracy": 0.453025,
            "f1": 0.445046,
            "f1_weighted": 0.450772
          },
          {
            "accuracy": 0.473684,
            "f1": 0.453541,
            "f1_weighted": 0.475065
          },
          {
            "accuracy": 0.451549,
            "f1": 0.424932,
            "f1_weighted": 0.457897
          },
          {
            "accuracy": 0.41515,
            "f1": 0.415674,
            "f1_weighted": 0.40991
          },
          {
            "accuracy": 0.445155,
            "f1": 0.4354,
            "f1_weighted": 0.442416
          },
          {
            "accuracy": 0.42548,
            "f1": 0.430272,
            "f1_weighted": 0.423387
          },
          {
            "accuracy": 0.460403,
            "f1": 0.450245,
            "f1_weighted": 0.461904
          }
        ],
        "main_score": 0.440384,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.439913,
        "f1": 0.42617,
        "f1_weighted": 0.441195,
        "scores_per_experiment": [
          {
            "accuracy": 0.424681,
            "f1": 0.428707,
            "f1_weighted": 0.42855
          },
          {
            "accuracy": 0.444855,
            "f1": 0.417493,
            "f1_weighted": 0.44954
          },
          {
            "accuracy": 0.446537,
            "f1": 0.433914,
            "f1_weighted": 0.445481
          },
          {
            "accuracy": 0.442165,
            "f1": 0.425065,
            "f1_weighted": 0.441785
          },
          {
            "accuracy": 0.450572,
            "f1": 0.429739,
            "f1_weighted": 0.448609
          },
          {
            "accuracy": 0.4462,
            "f1": 0.429245,
            "f1_weighted": 0.451084
          },
          {
            "accuracy": 0.419637,
            "f1": 0.411991,
            "f1_weighted": 0.420124
          },
          {
            "accuracy": 0.453598,
            "f1": 0.44105,
            "f1_weighted": 0.450769
          },
          {
            "accuracy": 0.423336,
            "f1": 0.407491,
            "f1_weighted": 0.423919
          },
          {
            "accuracy": 0.447545,
            "f1": 0.437006,
            "f1_weighted": 0.452084
          }
        ],
        "main_score": 0.439913,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 26.750417470932007,
  "kg_co2_emissions": null
}