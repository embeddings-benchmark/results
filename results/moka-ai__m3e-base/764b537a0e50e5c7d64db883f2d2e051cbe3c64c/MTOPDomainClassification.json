{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "task_name": "MTOPDomainClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.642314,
        "f1": 0.621615,
        "f1_weighted": 0.643138,
        "scores_per_experiment": [
          {
            "accuracy": 0.647934,
            "f1": 0.634955,
            "f1_weighted": 0.651184
          },
          {
            "accuracy": 0.624242,
            "f1": 0.607107,
            "f1_weighted": 0.624271
          },
          {
            "accuracy": 0.614325,
            "f1": 0.58404,
            "f1_weighted": 0.611794
          },
          {
            "accuracy": 0.636915,
            "f1": 0.621164,
            "f1_weighted": 0.639929
          },
          {
            "accuracy": 0.641322,
            "f1": 0.620171,
            "f1_weighted": 0.644302
          },
          {
            "accuracy": 0.653444,
            "f1": 0.627958,
            "f1_weighted": 0.652329
          },
          {
            "accuracy": 0.650138,
            "f1": 0.622592,
            "f1_weighted": 0.647742
          },
          {
            "accuracy": 0.658402,
            "f1": 0.642049,
            "f1_weighted": 0.662414
          },
          {
            "accuracy": 0.649587,
            "f1": 0.634199,
            "f1_weighted": 0.651682
          },
          {
            "accuracy": 0.646832,
            "f1": 0.62192,
            "f1_weighted": 0.645736
          }
        ],
        "main_score": 0.642314,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.655593,
        "f1": 0.63129,
        "f1_weighted": 0.657527,
        "scores_per_experiment": [
          {
            "accuracy": 0.665258,
            "f1": 0.649677,
            "f1_weighted": 0.667956
          },
          {
            "accuracy": 0.625528,
            "f1": 0.603412,
            "f1_weighted": 0.62735
          },
          {
            "accuracy": 0.632009,
            "f1": 0.598873,
            "f1_weighted": 0.633525
          },
          {
            "accuracy": 0.657368,
            "f1": 0.633593,
            "f1_weighted": 0.660806
          },
          {
            "accuracy": 0.66075,
            "f1": 0.634123,
            "f1_weighted": 0.663655
          },
          {
            "accuracy": 0.674838,
            "f1": 0.64948,
            "f1_weighted": 0.674197
          },
          {
            "accuracy": 0.673147,
            "f1": 0.639776,
            "f1_weighted": 0.672976
          },
          {
            "accuracy": 0.653987,
            "f1": 0.635076,
            "f1_weighted": 0.65675
          },
          {
            "accuracy": 0.659904,
            "f1": 0.643588,
            "f1_weighted": 0.663539
          },
          {
            "accuracy": 0.653142,
            "f1": 0.625303,
            "f1_weighted": 0.654513
          }
        ],
        "main_score": 0.655593,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 27.206826210021973,
  "kg_co2_emissions": null
}