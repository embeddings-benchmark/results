{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "task_name": "WebLINXCandidatesReranking",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "map": 0.130993,
        "mrr": 0.112425,
        "nAUC_map_max": -0.01911,
        "nAUC_map_std": -0.027866,
        "nAUC_map_diff1": 0.225498,
        "nAUC_mrr_max": -0.011717,
        "nAUC_mrr_std": -0.038584,
        "nAUC_mrr_diff1": 0.2249,
        "main_score": 0.112425,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_iid": [
      {
        "map": 0.100566,
        "mrr": 0.080884,
        "nAUC_map_max": 0.013244,
        "nAUC_map_std": 0.109635,
        "nAUC_map_diff1": 0.196065,
        "nAUC_mrr_max": 0.030667,
        "nAUC_mrr_std": 0.09833,
        "nAUC_mrr_diff1": 0.198102,
        "main_score": 0.080884,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_cat": [
      {
        "map": 0.081776,
        "mrr": 0.064448,
        "nAUC_map_max": 0.001472,
        "nAUC_map_std": 0.205024,
        "nAUC_map_diff1": 0.052081,
        "nAUC_mrr_max": 0.002988,
        "nAUC_mrr_std": 0.186675,
        "nAUC_mrr_diff1": 0.049619,
        "main_score": 0.064448,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_geo": [
      {
        "map": 0.108626,
        "mrr": 0.089645,
        "nAUC_map_max": -0.1338,
        "nAUC_map_std": 0.033997,
        "nAUC_map_diff1": 0.112161,
        "nAUC_mrr_max": -0.129815,
        "nAUC_mrr_std": 0.026518,
        "nAUC_mrr_diff1": 0.118183,
        "main_score": 0.089645,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_vis": [
      {
        "map": 0.104035,
        "mrr": 0.084677,
        "nAUC_map_max": 0.022369,
        "nAUC_map_std": 0.083076,
        "nAUC_map_diff1": 0.056726,
        "nAUC_mrr_max": 0.020122,
        "nAUC_mrr_std": 0.065469,
        "nAUC_mrr_diff1": 0.052898,
        "main_score": 0.084677,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_web": [
      {
        "map": 0.071872,
        "mrr": 0.051665,
        "nAUC_map_max": -0.084528,
        "nAUC_map_std": 0.152101,
        "nAUC_map_diff1": 0.060437,
        "nAUC_mrr_max": -0.098582,
        "nAUC_mrr_std": 0.153879,
        "nAUC_mrr_diff1": 0.047693,
        "main_score": 0.051665,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 898.8922784328461,
  "kg_co2_emissions": 0.04562012218144429
}