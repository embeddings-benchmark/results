{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.78677,
        "f1": 0.657181,
        "f1_weighted": 0.81418,
        "ap": 0.272053,
        "ap_weighted": 0.272053,
        "scores_per_experiment": [
          {
            "accuracy": 0.662371,
            "f1": 0.541855,
            "f1_weighted": 0.717885,
            "ap": 0.173829,
            "ap_weighted": 0.173829
          },
          {
            "accuracy": 0.851375,
            "f1": 0.698389,
            "f1_weighted": 0.85931,
            "ap": 0.293611,
            "ap_weighted": 0.293611
          },
          {
            "accuracy": 0.844502,
            "f1": 0.72182,
            "f1_weighted": 0.860214,
            "ap": 0.343443,
            "ap_weighted": 0.343443
          },
          {
            "accuracy": 0.770619,
            "f1": 0.653358,
            "f1_weighted": 0.804394,
            "ap": 0.273968,
            "ap_weighted": 0.273968
          },
          {
            "accuracy": 0.786082,
            "f1": 0.620402,
            "f1_weighted": 0.808274,
            "ap": 0.209301,
            "ap_weighted": 0.209301
          },
          {
            "accuracy": 0.738832,
            "f1": 0.623622,
            "f1_weighted": 0.77962,
            "ap": 0.246257,
            "ap_weighted": 0.246257
          },
          {
            "accuracy": 0.853093,
            "f1": 0.695803,
            "f1_weighted": 0.85967,
            "ap": 0.288647,
            "ap_weighted": 0.288647
          },
          {
            "accuracy": 0.797251,
            "f1": 0.690314,
            "f1_weighted": 0.826643,
            "ap": 0.325844,
            "ap_weighted": 0.325844
          },
          {
            "accuracy": 0.773196,
            "f1": 0.653571,
            "f1_weighted": 0.806075,
            "ap": 0.271839,
            "ap_weighted": 0.271839
          },
          {
            "accuracy": 0.790378,
            "f1": 0.672676,
            "f1_weighted": 0.819719,
            "ap": 0.293789,
            "ap_weighted": 0.293789
          }
        ],
        "main_score": 0.78677,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 7.757477760314941,
  "kg_co2_emissions": 0.00023540685817585903
}