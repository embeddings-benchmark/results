{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.255273,
        "f1": 0.251063,
        "f1_weighted": 0.251063,
        "scores_per_experiment": [
          {
            "accuracy": 0.262695,
            "f1": 0.255428,
            "f1_weighted": 0.255447
          },
          {
            "accuracy": 0.240234,
            "f1": 0.232685,
            "f1_weighted": 0.232677
          },
          {
            "accuracy": 0.244141,
            "f1": 0.24345,
            "f1_weighted": 0.243427
          },
          {
            "accuracy": 0.288574,
            "f1": 0.285744,
            "f1_weighted": 0.285741
          },
          {
            "accuracy": 0.240234,
            "f1": 0.238003,
            "f1_weighted": 0.237988
          },
          {
            "accuracy": 0.250977,
            "f1": 0.240957,
            "f1_weighted": 0.240976
          },
          {
            "accuracy": 0.224609,
            "f1": 0.225345,
            "f1_weighted": 0.225312
          },
          {
            "accuracy": 0.276855,
            "f1": 0.270133,
            "f1_weighted": 0.270173
          },
          {
            "accuracy": 0.280273,
            "f1": 0.275889,
            "f1_weighted": 0.275877
          },
          {
            "accuracy": 0.244141,
            "f1": 0.243001,
            "f1_weighted": 0.243014
          }
        ],
        "main_score": 0.255273,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.253809,
        "f1": 0.249221,
        "f1_weighted": 0.249226,
        "scores_per_experiment": [
          {
            "accuracy": 0.256836,
            "f1": 0.252141,
            "f1_weighted": 0.252161
          },
          {
            "accuracy": 0.254395,
            "f1": 0.244244,
            "f1_weighted": 0.244254
          },
          {
            "accuracy": 0.231445,
            "f1": 0.231364,
            "f1_weighted": 0.231364
          },
          {
            "accuracy": 0.272949,
            "f1": 0.269578,
            "f1_weighted": 0.269591
          },
          {
            "accuracy": 0.254883,
            "f1": 0.251575,
            "f1_weighted": 0.251552
          },
          {
            "accuracy": 0.237305,
            "f1": 0.224182,
            "f1_weighted": 0.224197
          },
          {
            "accuracy": 0.232422,
            "f1": 0.232248,
            "f1_weighted": 0.232217
          },
          {
            "accuracy": 0.273926,
            "f1": 0.26736,
            "f1_weighted": 0.267403
          },
          {
            "accuracy": 0.288574,
            "f1": 0.283682,
            "f1_weighted": 0.283668
          },
          {
            "accuracy": 0.235352,
            "f1": 0.235836,
            "f1_weighted": 0.235855
          }
        ],
        "main_score": 0.253809,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 14.9563889503479,
  "kg_co2_emissions": 0.00042910610175568315
}