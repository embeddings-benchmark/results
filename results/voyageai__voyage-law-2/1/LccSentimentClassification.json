{
  "dataset_revision": "de7ba3406ee55ea2cc52a0a41408fa6aede6d3c6",
  "task_name": "LccSentimentClassification",
  "mteb_version": "2.3.2",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.58,
            "f1": 0.571318,
            "f1_weighted": 0.585636,
            "precision": 0.566277,
            "precision_weighted": 0.63039,
            "recall": 0.615914,
            "recall_weighted": 0.58,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.533333,
            "f1": 0.525026,
            "f1_weighted": 0.532496,
            "precision": 0.512615,
            "precision_weighted": 0.545109,
            "recall": 0.556085,
            "recall_weighted": 0.533333,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.593333,
            "f1": 0.585169,
            "f1_weighted": 0.601462,
            "precision": 0.599016,
            "precision_weighted": 0.675389,
            "recall": 0.646439,
            "recall_weighted": 0.593333,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.546667,
            "f1": 0.548005,
            "f1_weighted": 0.546169,
            "precision": 0.559125,
            "precision_weighted": 0.629747,
            "recall": 0.617315,
            "recall_weighted": 0.546667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.553333,
            "f1": 0.532727,
            "f1_weighted": 0.559883,
            "precision": 0.524762,
            "precision_weighted": 0.57983,
            "recall": 0.554702,
            "recall_weighted": 0.553333,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.58,
            "f1": 0.560345,
            "f1_weighted": 0.585781,
            "precision": 0.551084,
            "precision_weighted": 0.604097,
            "recall": 0.583186,
            "recall_weighted": 0.58,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.56,
            "f1": 0.54114,
            "f1_weighted": 0.565601,
            "precision": 0.542127,
            "precision_weighted": 0.59797,
            "recall": 0.57668,
            "recall_weighted": 0.56,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.633333,
            "f1": 0.608878,
            "f1_weighted": 0.635955,
            "precision": 0.608292,
            "precision_weighted": 0.651827,
            "recall": 0.629919,
            "recall_weighted": 0.633333,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.606667,
            "f1": 0.592251,
            "f1_weighted": 0.610307,
            "precision": 0.619279,
            "precision_weighted": 0.684743,
            "recall": 0.660014,
            "recall_weighted": 0.606667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.613333,
            "f1": 0.588983,
            "f1_weighted": 0.624909,
            "precision": 0.594554,
            "precision_weighted": 0.656729,
            "recall": 0.611155,
            "recall_weighted": 0.613333,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.58,
        "f1": 0.565384,
        "f1_weighted": 0.58482,
        "precision": 0.567713,
        "precision_weighted": 0.625583,
        "recall": 0.605141,
        "recall_weighted": 0.58,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.58,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 30.98514413833618,
  "kg_co2_emissions": null
}