{
  "dataset_revision": "6f183d3e509464fd9d92516d4eff91e11b8ec622",
  "task_name": "HindiDiscourseClassification",
  "mteb_version": "2.1.17",
  "scores": {
    "train": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.430664,
            "f1": 0.32156,
            "f1_weighted": 0.431805,
            "precision": 0.348526,
            "precision_weighted": 0.565071,
            "recall": 0.584989,
            "recall_weighted": 0.430664,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.348145,
            "f1": 0.330566,
            "f1_weighted": 0.378121,
            "precision": 0.321157,
            "precision_weighted": 0.434695,
            "recall": 0.55774,
            "recall_weighted": 0.348145,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.425781,
            "f1": 0.35058,
            "f1_weighted": 0.451176,
            "precision": 0.341425,
            "precision_weighted": 0.516878,
            "recall": 0.566719,
            "recall_weighted": 0.425781,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.350098,
            "f1": 0.30639,
            "f1_weighted": 0.357507,
            "precision": 0.285605,
            "precision_weighted": 0.398826,
            "recall": 0.536802,
            "recall_weighted": 0.350098,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.431641,
            "f1": 0.344035,
            "f1_weighted": 0.470804,
            "precision": 0.349979,
            "precision_weighted": 0.561333,
            "recall": 0.594171,
            "recall_weighted": 0.431641,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.37207,
            "f1": 0.295763,
            "f1_weighted": 0.417316,
            "precision": 0.307504,
            "precision_weighted": 0.506276,
            "recall": 0.585453,
            "recall_weighted": 0.37207,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.385742,
            "f1": 0.306081,
            "f1_weighted": 0.409946,
            "precision": 0.29951,
            "precision_weighted": 0.475911,
            "recall": 0.554782,
            "recall_weighted": 0.385742,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.421387,
            "f1": 0.334287,
            "f1_weighted": 0.466503,
            "precision": 0.353142,
            "precision_weighted": 0.571016,
            "recall": 0.558264,
            "recall_weighted": 0.421387,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.422363,
            "f1": 0.354797,
            "f1_weighted": 0.477216,
            "precision": 0.372074,
            "precision_weighted": 0.589896,
            "recall": 0.606294,
            "recall_weighted": 0.422363,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.35791,
            "f1": 0.308586,
            "f1_weighted": 0.384579,
            "precision": 0.329045,
            "precision_weighted": 0.50271,
            "recall": 0.562317,
            "recall_weighted": 0.35791,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.39458,
        "f1": 0.325264,
        "f1_weighted": 0.424497,
        "precision": 0.330797,
        "precision_weighted": 0.512261,
        "recall": 0.570753,
        "recall_weighted": 0.39458,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.39458,
        "hf_subset": "default",
        "languages": [
          "hin-Deva"
        ]
      }
    ]
  },
  "evaluation_time": 30.54915142059326,
  "kg_co2_emissions": null
}