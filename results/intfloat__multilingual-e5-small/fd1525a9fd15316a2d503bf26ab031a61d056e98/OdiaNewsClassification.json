{
  "dataset_revision": "ffb8a34c9637fb20256e8c7be02504d16af4bd6b",
  "task_name": "OdiaNewsClassification",
  "mteb_version": "2.1.17",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.796387,
            "f1": 0.796904,
            "f1_weighted": 0.795942,
            "precision": 0.795374,
            "precision_weighted": 0.822678,
            "recall": 0.825174,
            "recall_weighted": 0.796387,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.799316,
            "f1": 0.801139,
            "f1_weighted": 0.799964,
            "precision": 0.813869,
            "precision_weighted": 0.803482,
            "recall": 0.791939,
            "recall_weighted": 0.799316,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.765625,
            "f1": 0.768504,
            "f1_weighted": 0.763195,
            "precision": 0.770283,
            "precision_weighted": 0.795479,
            "recall": 0.797949,
            "recall_weighted": 0.765625,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.790527,
            "f1": 0.787548,
            "f1_weighted": 0.790089,
            "precision": 0.786055,
            "precision_weighted": 0.793672,
            "recall": 0.794141,
            "recall_weighted": 0.790527,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.768066,
            "f1": 0.767872,
            "f1_weighted": 0.76807,
            "precision": 0.762171,
            "precision_weighted": 0.779419,
            "recall": 0.786095,
            "recall_weighted": 0.768066,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.817871,
            "f1": 0.821501,
            "f1_weighted": 0.817498,
            "precision": 0.819991,
            "precision_weighted": 0.817309,
            "recall": 0.823247,
            "recall_weighted": 0.817871,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.722656,
            "f1": 0.737304,
            "f1_weighted": 0.724199,
            "precision": 0.736232,
            "precision_weighted": 0.730605,
            "recall": 0.742764,
            "recall_weighted": 0.722656,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.799805,
            "f1": 0.80142,
            "f1_weighted": 0.799614,
            "precision": 0.794712,
            "precision_weighted": 0.802664,
            "recall": 0.811489,
            "recall_weighted": 0.799805,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.796387,
            "f1": 0.798598,
            "f1_weighted": 0.796603,
            "precision": 0.794326,
            "precision_weighted": 0.799773,
            "recall": 0.805546,
            "recall_weighted": 0.796387,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.78418,
            "f1": 0.784712,
            "f1_weighted": 0.783845,
            "precision": 0.780425,
            "precision_weighted": 0.803472,
            "recall": 0.808797,
            "recall_weighted": 0.78418,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.784082,
        "f1": 0.78655,
        "f1_weighted": 0.783902,
        "precision": 0.785344,
        "precision_weighted": 0.794855,
        "recall": 0.798714,
        "recall_weighted": 0.784082,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.78655,
        "hf_subset": "default",
        "languages": [
          "ory-Orya"
        ]
      }
    ]
  },
  "evaluation_time": 55.83564758300781,
  "kg_co2_emissions": null
}