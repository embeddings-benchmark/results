{
  "dataset_revision": "7fb2f514ea683c5282dfec0a9672ece8de90ac50",
  "task_name": "SinhalaNewsClassification",
  "mteb_version": "2.1.17",
  "scores": {
    "train": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.657227,
            "f1": 0.63178,
            "f1_weighted": 0.654388,
            "precision": 0.654225,
            "precision_weighted": 0.698045,
            "recall": 0.664,
            "recall_weighted": 0.657227,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.654785,
            "f1": 0.616937,
            "f1_weighted": 0.649699,
            "precision": 0.616722,
            "precision_weighted": 0.663922,
            "recall": 0.637989,
            "recall_weighted": 0.654785,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.643555,
            "f1": 0.616613,
            "f1_weighted": 0.623575,
            "precision": 0.631056,
            "precision_weighted": 0.653538,
            "recall": 0.648462,
            "recall_weighted": 0.643555,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.663086,
            "f1": 0.632346,
            "f1_weighted": 0.654202,
            "precision": 0.635348,
            "precision_weighted": 0.673337,
            "recall": 0.657527,
            "recall_weighted": 0.663086,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.680664,
            "f1": 0.663023,
            "f1_weighted": 0.67733,
            "precision": 0.668657,
            "precision_weighted": 0.695549,
            "recall": 0.682916,
            "recall_weighted": 0.680664,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.666504,
            "f1": 0.643872,
            "f1_weighted": 0.669997,
            "precision": 0.652609,
            "precision_weighted": 0.70387,
            "recall": 0.66709,
            "recall_weighted": 0.666504,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.682129,
            "f1": 0.661316,
            "f1_weighted": 0.675794,
            "precision": 0.665354,
            "precision_weighted": 0.693081,
            "recall": 0.685094,
            "recall_weighted": 0.682129,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.706055,
            "f1": 0.68533,
            "f1_weighted": 0.701171,
            "precision": 0.683341,
            "precision_weighted": 0.714312,
            "recall": 0.706464,
            "recall_weighted": 0.706055,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.709473,
            "f1": 0.688497,
            "f1_weighted": 0.708093,
            "precision": 0.682252,
            "precision_weighted": 0.710905,
            "recall": 0.699501,
            "recall_weighted": 0.709473,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.688477,
            "f1": 0.666895,
            "f1_weighted": 0.690542,
            "precision": 0.680321,
            "precision_weighted": 0.717613,
            "recall": 0.685277,
            "recall_weighted": 0.688477,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.675195,
        "f1": 0.650661,
        "f1_weighted": 0.670479,
        "precision": 0.656988,
        "precision_weighted": 0.692417,
        "recall": 0.673432,
        "recall_weighted": 0.675195,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.675195,
        "hf_subset": "default",
        "languages": [
          "sin-Sinh"
        ]
      }
    ]
  },
  "evaluation_time": 51.78352451324463,
  "kg_co2_emissions": null
}