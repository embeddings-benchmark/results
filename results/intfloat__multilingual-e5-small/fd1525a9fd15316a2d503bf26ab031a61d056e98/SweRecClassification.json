{
  "dataset_revision": "b07c6ce548f6a7ac8d546e1bbe197a0086409190",
  "task_name": "SweRecClassification",
  "mteb_version": "2.1.17",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.723633,
            "f1": 0.6076,
            "f1_weighted": 0.71964,
            "precision": 0.60915,
            "precision_weighted": 0.718398,
            "recall": 0.608706,
            "recall_weighted": 0.723633,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.73584,
            "f1": 0.653248,
            "f1_weighted": 0.753456,
            "precision": 0.657262,
            "precision_weighted": 0.778411,
            "recall": 0.66333,
            "recall_weighted": 0.73584,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.734375,
            "f1": 0.65078,
            "f1_weighted": 0.745729,
            "precision": 0.659672,
            "precision_weighted": 0.780617,
            "recall": 0.665173,
            "recall_weighted": 0.734375,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.730957,
            "f1": 0.624227,
            "f1_weighted": 0.739919,
            "precision": 0.625832,
            "precision_weighted": 0.75022,
            "recall": 0.625224,
            "recall_weighted": 0.730957,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.684082,
            "f1": 0.561638,
            "f1_weighted": 0.679897,
            "precision": 0.575575,
            "precision_weighted": 0.700445,
            "recall": 0.567004,
            "recall_weighted": 0.684082,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.757324,
            "f1": 0.653074,
            "f1_weighted": 0.759682,
            "precision": 0.652591,
            "precision_weighted": 0.762206,
            "recall": 0.653877,
            "recall_weighted": 0.757324,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.744141,
            "f1": 0.646159,
            "f1_weighted": 0.748247,
            "precision": 0.647611,
            "precision_weighted": 0.757862,
            "recall": 0.64917,
            "recall_weighted": 0.744141,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.75293,
            "f1": 0.676954,
            "f1_weighted": 0.770016,
            "precision": 0.679642,
            "precision_weighted": 0.795735,
            "recall": 0.691426,
            "recall_weighted": 0.75293,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.669434,
            "f1": 0.601604,
            "f1_weighted": 0.686198,
            "precision": 0.639542,
            "precision_weighted": 0.771373,
            "recall": 0.632742,
            "recall_weighted": 0.669434,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.741211,
            "f1": 0.622037,
            "f1_weighted": 0.739412,
            "precision": 0.62234,
            "precision_weighted": 0.737702,
            "recall": 0.621898,
            "recall_weighted": 0.741211,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.727393,
        "f1": 0.629732,
        "f1_weighted": 0.73422,
        "precision": 0.636922,
        "precision_weighted": 0.755297,
        "recall": 0.637855,
        "recall_weighted": 0.727393,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.727393,
        "hf_subset": "default",
        "languages": [
          "swe-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 23.67826271057129,
  "kg_co2_emissions": null
}