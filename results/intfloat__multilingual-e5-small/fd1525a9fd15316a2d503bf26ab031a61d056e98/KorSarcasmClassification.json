{
  "dataset_revision": "6701f384372c04aa8c64b10582e72eb84135a1d4",
  "task_name": "KorSarcasmClassification",
  "mteb_version": "2.1.17",
  "scores": {
    "train": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.614746,
            "f1": 0.608286,
            "f1_weighted": 0.608385,
            "precision": 0.622452,
            "precision_weighted": 0.622393,
            "recall": 0.614498,
            "recall_weighted": 0.614746,
            "ap": 0.5739,
            "ap_weighted": 0.5739
          },
          {
            "accuracy": 0.525391,
            "f1": 0.522549,
            "f1_weighted": 0.522621,
            "precision": 0.525842,
            "precision_weighted": 0.525836,
            "recall": 0.525242,
            "recall_weighted": 0.525391,
            "ap": 0.512397,
            "ap_weighted": 0.512397
          },
          {
            "accuracy": 0.589844,
            "f1": 0.576159,
            "f1_weighted": 0.576308,
            "precision": 0.602602,
            "precision_weighted": 0.602532,
            "recall": 0.589495,
            "recall_weighted": 0.589844,
            "ap": 0.55626,
            "ap_weighted": 0.55626
          },
          {
            "accuracy": 0.535156,
            "f1": 0.534833,
            "f1_weighted": 0.534857,
            "precision": 0.535197,
            "precision_weighted": 0.535196,
            "recall": 0.535107,
            "recall_weighted": 0.535156,
            "ap": 0.517878,
            "ap_weighted": 0.517878
          },
          {
            "accuracy": 0.513672,
            "f1": 0.503721,
            "f1_weighted": 0.503858,
            "precision": 0.514548,
            "precision_weighted": 0.514542,
            "recall": 0.513397,
            "recall_weighted": 0.513672,
            "ap": 0.505972,
            "ap_weighted": 0.505972
          },
          {
            "accuracy": 0.557617,
            "f1": 0.549434,
            "f1_weighted": 0.549553,
            "precision": 0.561779,
            "precision_weighted": 0.561749,
            "recall": 0.557356,
            "recall_weighted": 0.557617,
            "ap": 0.532202,
            "ap_weighted": 0.532202
          },
          {
            "accuracy": 0.571777,
            "f1": 0.571003,
            "f1_weighted": 0.570968,
            "precision": 0.572409,
            "precision_weighted": 0.572423,
            "recall": 0.571862,
            "recall_weighted": 0.571777,
            "ap": 0.539715,
            "ap_weighted": 0.539715
          },
          {
            "accuracy": 0.566406,
            "f1": 0.564312,
            "f1_weighted": 0.564253,
            "precision": 0.567886,
            "precision_weighted": 0.567907,
            "recall": 0.566544,
            "recall_weighted": 0.566406,
            "ap": 0.536185,
            "ap_weighted": 0.536185
          },
          {
            "accuracy": 0.499512,
            "f1": 0.495304,
            "f1_weighted": 0.495214,
            "precision": 0.499681,
            "precision_weighted": 0.499683,
            "recall": 0.499692,
            "recall_weighted": 0.499512,
            "ap": 0.498869,
            "ap_weighted": 0.498869
          },
          {
            "accuracy": 0.533691,
            "f1": 0.532848,
            "f1_weighted": 0.532887,
            "precision": 0.533844,
            "precision_weighted": 0.53384,
            "recall": 0.53361,
            "recall_weighted": 0.533691,
            "ap": 0.517063,
            "ap_weighted": 0.517063
          }
        ],
        "accuracy": 0.550781,
        "f1": 0.545845,
        "f1_weighted": 0.54589,
        "precision": 0.553624,
        "precision_weighted": 0.55361,
        "recall": 0.55068,
        "recall_weighted": 0.550781,
        "ap": 0.529044,
        "ap_weighted": 0.529044,
        "main_score": 0.550781,
        "hf_subset": "default",
        "languages": [
          "kor-Hang"
        ]
      }
    ]
  },
  "evaluation_time": 28.42043423652649,
  "kg_co2_emissions": null
}