{
  "dataset_revision": "07dcc23c08a2540ba37ebe1e487da9dc497cc15c",
  "task_name": "IndicGenBenchFloresBitextMining",
  "mteb_version": "2.1.17",
  "scores": {
    "validation": [
      {
        "precision": 0.998495,
        "recall": 0.998997,
        "f1": 0.998663,
        "accuracy": 0.998997,
        "main_score": 0.998663,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.985289,
        "recall": 0.98997,
        "f1": 0.986794,
        "accuracy": 0.98997,
        "main_score": 0.986794,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ]
      },
      {
        "precision": 0.992477,
        "recall": 0.994985,
        "f1": 0.993313,
        "accuracy": 0.994985,
        "main_score": 0.993313,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.988298,
        "recall": 0.991976,
        "f1": 0.989468,
        "accuracy": 0.991976,
        "main_score": 0.989468,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ]
      },
      {
        "precision": 0.998495,
        "recall": 0.998997,
        "f1": 0.998663,
        "accuracy": 0.998997,
        "main_score": 0.998663,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.998495,
        "recall": 0.998997,
        "f1": 0.998663,
        "accuracy": 0.998997,
        "main_score": 0.998663,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ]
      },
      {
        "precision": 0.995821,
        "recall": 0.996991,
        "f1": 0.996155,
        "accuracy": 0.996991,
        "main_score": 0.996155,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.993982,
        "recall": 0.995988,
        "f1": 0.994651,
        "accuracy": 0.995988,
        "main_score": 0.994651,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ]
      },
      {
        "precision": 0.993982,
        "recall": 0.995988,
        "f1": 0.994651,
        "accuracy": 0.995988,
        "main_score": 0.994651,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.995486,
        "recall": 0.996991,
        "f1": 0.995988,
        "accuracy": 0.996991,
        "main_score": 0.995988,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ]
      },
      {
        "precision": 0.996991,
        "recall": 0.997994,
        "f1": 0.997325,
        "accuracy": 0.997994,
        "main_score": 0.997325,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.989468,
        "recall": 0.992979,
        "f1": 0.990639,
        "accuracy": 0.992979,
        "main_score": 0.990639,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ]
      },
      {
        "precision": 0.996991,
        "recall": 0.997994,
        "f1": 0.997325,
        "accuracy": 0.997994,
        "main_score": 0.997325,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.986459,
        "recall": 0.990973,
        "f1": 0.987964,
        "accuracy": 0.990973,
        "main_score": 0.987964,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ]
      },
      {
        "precision": 0.995486,
        "recall": 0.996991,
        "f1": 0.995988,
        "accuracy": 0.996991,
        "main_score": 0.995988,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.989468,
        "recall": 0.992979,
        "f1": 0.990639,
        "accuracy": 0.992979,
        "main_score": 0.990639,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ]
      },
      {
        "precision": 0.987964,
        "recall": 0.991976,
        "f1": 0.989301,
        "accuracy": 0.991976,
        "main_score": 0.989301,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.992477,
        "recall": 0.994985,
        "f1": 0.993313,
        "accuracy": 0.994985,
        "main_score": 0.993313,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ]
      },
      {
        "precision": 0.962721,
        "recall": 0.974925,
        "f1": 0.966734,
        "accuracy": 0.974925,
        "main_score": 0.966734,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.954865,
        "recall": 0.968907,
        "f1": 0.959345,
        "accuracy": 0.968907,
        "main_score": 0.959345,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ]
      },
      {
        "precision": 0.977014,
        "recall": 0.983952,
        "f1": 0.979171,
        "accuracy": 0.983952,
        "main_score": 0.979171,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.969157,
        "recall": 0.978937,
        "f1": 0.972317,
        "accuracy": 0.978937,
        "main_score": 0.972317,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ]
      },
      {
        "precision": 0.985456,
        "recall": 0.98997,
        "f1": 0.986961,
        "accuracy": 0.98997,
        "main_score": 0.986961,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.97877,
        "recall": 0.984955,
        "f1": 0.980776,
        "accuracy": 0.984955,
        "main_score": 0.980776,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ]
      },
      {
        "precision": 0.992477,
        "recall": 0.994985,
        "f1": 0.993313,
        "accuracy": 0.994985,
        "main_score": 0.993313,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.977767,
        "recall": 0.984955,
        "f1": 0.980107,
        "accuracy": 0.984955,
        "main_score": 0.980107,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ]
      },
      {
        "precision": 0.992477,
        "recall": 0.994985,
        "f1": 0.993313,
        "accuracy": 0.994985,
        "main_score": 0.993313,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.992477,
        "recall": 0.994985,
        "f1": 0.993313,
        "accuracy": 0.994985,
        "main_score": 0.993313,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ]
      },
      {
        "precision": 0.960164,
        "recall": 0.971916,
        "f1": 0.963791,
        "accuracy": 0.971916,
        "main_score": 0.963791,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.970411,
        "recall": 0.97994,
        "f1": 0.973587,
        "accuracy": 0.97994,
        "main_score": 0.973587,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ]
      },
      {
        "precision": 0.922935,
        "recall": 0.944835,
        "f1": 0.929742,
        "accuracy": 0.944835,
        "main_score": 0.929742,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.875543,
        "recall": 0.911735,
        "f1": 0.886727,
        "accuracy": 0.911735,
        "main_score": 0.886727,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ]
      },
      {
        "precision": 0.988967,
        "recall": 0.991976,
        "f1": 0.98997,
        "accuracy": 0.991976,
        "main_score": 0.98997,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.980274,
        "recall": 0.985958,
        "f1": 0.982113,
        "accuracy": 0.985958,
        "main_score": 0.982113,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ]
      },
      {
        "precision": 0.985624,
        "recall": 0.98997,
        "f1": 0.986961,
        "accuracy": 0.98997,
        "main_score": 0.986961,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.975928,
        "recall": 0.983952,
        "f1": 0.978602,
        "accuracy": 0.983952,
        "main_score": 0.978602,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ]
      },
      {
        "precision": 0.038067,
        "recall": 0.051153,
        "f1": 0.040557,
        "accuracy": 0.051153,
        "main_score": 0.040557,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.042636,
        "recall": 0.080241,
        "f1": 0.047609,
        "accuracy": 0.080241,
        "main_score": 0.047609,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ]
      },
      {
        "precision": 0.196709,
        "recall": 0.225677,
        "f1": 0.203291,
        "accuracy": 0.225677,
        "main_score": 0.203291,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.248047,
        "recall": 0.323972,
        "f1": 0.266575,
        "accuracy": 0.323972,
        "main_score": 0.266575,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ]
      },
      {
        "precision": 0.977432,
        "recall": 0.984955,
        "f1": 0.97994,
        "accuracy": 0.984955,
        "main_score": 0.97994,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.970578,
        "recall": 0.97994,
        "f1": 0.973587,
        "accuracy": 0.97994,
        "main_score": 0.973587,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ]
      },
      {
        "precision": 0.875784,
        "recall": 0.904714,
        "f1": 0.884397,
        "accuracy": 0.904714,
        "main_score": 0.884397,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.838633,
        "recall": 0.883651,
        "f1": 0.852257,
        "accuracy": 0.883651,
        "main_score": 0.852257,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ]
      },
      {
        "precision": 0.988298,
        "recall": 0.991976,
        "f1": 0.989468,
        "accuracy": 0.991976,
        "main_score": 0.989468,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.986961,
        "recall": 0.990973,
        "f1": 0.988298,
        "accuracy": 0.990973,
        "main_score": 0.988298,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ]
      },
      {
        "precision": 0.972919,
        "recall": 0.981946,
        "f1": 0.975928,
        "accuracy": 0.981946,
        "main_score": 0.975928,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.970996,
        "recall": 0.97994,
        "f1": 0.973821,
        "accuracy": 0.97994,
        "main_score": 0.973821,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ]
      },
      {
        "precision": 0.995486,
        "recall": 0.996991,
        "f1": 0.995988,
        "accuracy": 0.996991,
        "main_score": 0.995988,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.989468,
        "recall": 0.992979,
        "f1": 0.990639,
        "accuracy": 0.992979,
        "main_score": 0.990639,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ]
      },
      {
        "precision": 0.462931,
        "recall": 0.539619,
        "f1": 0.480912,
        "accuracy": 0.539619,
        "main_score": 0.480912,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.622403,
        "recall": 0.694082,
        "f1": 0.642841,
        "accuracy": 0.694082,
        "main_score": 0.642841,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ]
      },
      {
        "precision": 0.986459,
        "recall": 0.990973,
        "f1": 0.987964,
        "accuracy": 0.990973,
        "main_score": 0.987964,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.962387,
        "recall": 0.974925,
        "f1": 0.966566,
        "accuracy": 0.974925,
        "main_score": 0.966566,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ]
      },
      {
        "precision": 0.987964,
        "recall": 0.991976,
        "f1": 0.989301,
        "accuracy": 0.991976,
        "main_score": 0.989301,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.964226,
        "recall": 0.975928,
        "f1": 0.968071,
        "accuracy": 0.975928,
        "main_score": 0.968071,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ]
      },
      {
        "precision": 0.011527,
        "recall": 0.02006,
        "f1": 0.012334,
        "accuracy": 0.02006,
        "main_score": 0.012334,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.010714,
        "recall": 0.028084,
        "f1": 0.012901,
        "accuracy": 0.028084,
        "main_score": 0.012901,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ]
      }
    ],
    "test": [
      {
        "precision": 0.997036,
        "recall": 0.998024,
        "f1": 0.997365,
        "accuracy": 0.998024,
        "main_score": 0.997365,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.994071,
        "recall": 0.996047,
        "f1": 0.99473,
        "accuracy": 0.996047,
        "main_score": 0.99473,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ]
      },
      {
        "precision": 0.994071,
        "recall": 0.996047,
        "f1": 0.99473,
        "accuracy": 0.996047,
        "main_score": 0.99473,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.997036,
        "recall": 0.998024,
        "f1": 0.997365,
        "accuracy": 0.998024,
        "main_score": 0.997365,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ]
      },
      {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "accuracy": 1.0,
        "main_score": 1.0,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.995553,
        "recall": 0.997036,
        "f1": 0.996047,
        "accuracy": 0.997036,
        "main_score": 0.996047,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ]
      },
      {
        "precision": 0.995553,
        "recall": 0.997036,
        "f1": 0.996047,
        "accuracy": 0.997036,
        "main_score": 0.996047,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.994071,
        "recall": 0.996047,
        "f1": 0.99473,
        "accuracy": 0.996047,
        "main_score": 0.99473,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ]
      },
      {
        "precision": 0.997036,
        "recall": 0.998024,
        "f1": 0.997365,
        "accuracy": 0.998024,
        "main_score": 0.997365,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.994071,
        "recall": 0.996047,
        "f1": 0.99473,
        "accuracy": 0.996047,
        "main_score": 0.99473,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ]
      },
      {
        "precision": 0.994071,
        "recall": 0.996047,
        "f1": 0.99473,
        "accuracy": 0.996047,
        "main_score": 0.99473,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.984025,
        "recall": 0.98913,
        "f1": 0.985672,
        "accuracy": 0.98913,
        "main_score": 0.985672,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ]
      },
      {
        "precision": 0.998518,
        "recall": 0.999012,
        "f1": 0.998682,
        "accuracy": 0.999012,
        "main_score": 0.998682,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.988142,
        "recall": 0.992095,
        "f1": 0.98946,
        "accuracy": 0.992095,
        "main_score": 0.98946,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ]
      },
      {
        "precision": 0.995553,
        "recall": 0.997036,
        "f1": 0.996047,
        "accuracy": 0.997036,
        "main_score": 0.996047,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.991107,
        "recall": 0.994071,
        "f1": 0.992095,
        "accuracy": 0.994071,
        "main_score": 0.992095,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ]
      },
      {
        "precision": 0.988636,
        "recall": 0.992095,
        "f1": 0.989789,
        "accuracy": 0.992095,
        "main_score": 0.989789,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.991107,
        "recall": 0.994071,
        "f1": 0.992095,
        "accuracy": 0.994071,
        "main_score": 0.992095,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ]
      },
      {
        "precision": 0.972167,
        "recall": 0.981225,
        "f1": 0.975132,
        "accuracy": 0.981225,
        "main_score": 0.975132,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.963636,
        "recall": 0.974308,
        "f1": 0.966897,
        "accuracy": 0.974308,
        "main_score": 0.966897,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ]
      },
      {
        "precision": 0.975132,
        "recall": 0.983202,
        "f1": 0.977767,
        "accuracy": 0.983202,
        "main_score": 0.977767,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.982543,
        "recall": 0.988142,
        "f1": 0.984354,
        "accuracy": 0.988142,
        "main_score": 0.984354,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ]
      },
      {
        "precision": 0.991601,
        "recall": 0.994071,
        "f1": 0.992424,
        "accuracy": 0.994071,
        "main_score": 0.992424,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.982708,
        "recall": 0.988142,
        "f1": 0.984519,
        "accuracy": 0.988142,
        "main_score": 0.984519,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ]
      },
      {
        "precision": 0.992918,
        "recall": 0.995059,
        "f1": 0.993577,
        "accuracy": 0.995059,
        "main_score": 0.993577,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.990366,
        "recall": 0.993083,
        "f1": 0.991173,
        "accuracy": 0.993083,
        "main_score": 0.991173,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ]
      },
      {
        "precision": 0.991107,
        "recall": 0.994071,
        "f1": 0.992095,
        "accuracy": 0.994071,
        "main_score": 0.992095,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.992589,
        "recall": 0.995059,
        "f1": 0.993412,
        "accuracy": 0.995059,
        "main_score": 0.993412,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ]
      },
      {
        "precision": 0.972661,
        "recall": 0.981225,
        "f1": 0.975461,
        "accuracy": 0.981225,
        "main_score": 0.975461,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.968709,
        "recall": 0.978261,
        "f1": 0.971673,
        "accuracy": 0.978261,
        "main_score": 0.971673,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ]
      },
      {
        "precision": 0.937739,
        "recall": 0.953557,
        "f1": 0.942216,
        "accuracy": 0.953557,
        "main_score": 0.942216,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.887516,
        "recall": 0.922925,
        "f1": 0.899045,
        "accuracy": 0.922925,
        "main_score": 0.899045,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ]
      },
      {
        "precision": 0.981719,
        "recall": 0.987154,
        "f1": 0.983531,
        "accuracy": 0.987154,
        "main_score": 0.983531,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.963109,
        "recall": 0.974308,
        "f1": 0.966733,
        "accuracy": 0.974308,
        "main_score": 0.966733,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ]
      },
      {
        "precision": 0.982543,
        "recall": 0.988142,
        "f1": 0.984354,
        "accuracy": 0.988142,
        "main_score": 0.984354,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.98666,
        "recall": 0.991107,
        "f1": 0.988142,
        "accuracy": 0.991107,
        "main_score": 0.988142,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ]
      },
      {
        "precision": 0.032907,
        "recall": 0.04249,
        "f1": 0.034899,
        "accuracy": 0.04249,
        "main_score": 0.034899,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.036855,
        "recall": 0.066206,
        "f1": 0.041977,
        "accuracy": 0.066206,
        "main_score": 0.041977,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ]
      },
      {
        "precision": 0.220695,
        "recall": 0.264822,
        "f1": 0.228913,
        "accuracy": 0.264822,
        "main_score": 0.228913,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.211684,
        "recall": 0.284585,
        "f1": 0.227986,
        "accuracy": 0.284585,
        "main_score": 0.227986,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ]
      },
      {
        "precision": 0.976285,
        "recall": 0.98419,
        "f1": 0.97892,
        "accuracy": 0.98419,
        "main_score": 0.97892,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.975132,
        "recall": 0.983202,
        "f1": 0.977767,
        "accuracy": 0.983202,
        "main_score": 0.977767,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ]
      },
      {
        "precision": 0.886375,
        "recall": 0.91502,
        "f1": 0.894669,
        "accuracy": 0.91502,
        "main_score": 0.894669,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.849967,
        "recall": 0.893281,
        "f1": 0.863076,
        "accuracy": 0.893281,
        "main_score": 0.863076,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ]
      },
      {
        "precision": 0.988142,
        "recall": 0.992095,
        "f1": 0.98946,
        "accuracy": 0.992095,
        "main_score": 0.98946,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.991107,
        "recall": 0.994071,
        "f1": 0.992095,
        "accuracy": 0.994071,
        "main_score": 0.992095,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ]
      },
      {
        "precision": 0.96805,
        "recall": 0.978261,
        "f1": 0.971344,
        "accuracy": 0.978261,
        "main_score": 0.971344,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.965909,
        "recall": 0.977273,
        "f1": 0.969697,
        "accuracy": 0.977273,
        "main_score": 0.969697,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ]
      },
      {
        "precision": 0.994071,
        "recall": 0.996047,
        "f1": 0.99473,
        "accuracy": 0.996047,
        "main_score": 0.99473,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.98666,
        "recall": 0.991107,
        "f1": 0.988142,
        "accuracy": 0.991107,
        "main_score": 0.988142,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ]
      },
      {
        "precision": 0.438097,
        "recall": 0.504941,
        "f1": 0.45384,
        "accuracy": 0.504941,
        "main_score": 0.45384,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.589044,
        "recall": 0.664032,
        "f1": 0.610089,
        "accuracy": 0.664032,
        "main_score": 0.610089,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ]
      },
      {
        "precision": 0.986989,
        "recall": 0.991107,
        "f1": 0.988307,
        "accuracy": 0.991107,
        "main_score": 0.988307,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.960804,
        "recall": 0.97332,
        "f1": 0.964921,
        "accuracy": 0.97332,
        "main_score": 0.964921,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ]
      },
      {
        "precision": 0.976943,
        "recall": 0.98419,
        "f1": 0.979249,
        "accuracy": 0.98419,
        "main_score": 0.979249,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.965744,
        "recall": 0.976285,
        "f1": 0.969203,
        "accuracy": 0.976285,
        "main_score": 0.969203,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ]
      },
      {
        "precision": 0.011984,
        "recall": 0.019763,
        "f1": 0.013492,
        "accuracy": 0.019763,
        "main_score": 0.013492,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.017261,
        "recall": 0.039526,
        "f1": 0.020753,
        "accuracy": 0.039526,
        "main_score": 0.020753,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ]
      }
    ]
  },
  "evaluation_time": 269.4974915981293,
  "kg_co2_emissions": null
}