{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "task_name": "MTOPDomainClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.860661,
        "f1": 0.84838,
        "f1_weighted": 0.860317,
        "scores_per_experiment": [
          {
            "accuracy": 0.873829,
            "f1": 0.862083,
            "f1_weighted": 0.873636
          },
          {
            "accuracy": 0.839669,
            "f1": 0.829691,
            "f1_weighted": 0.839844
          },
          {
            "accuracy": 0.854545,
            "f1": 0.837422,
            "f1_weighted": 0.854302
          },
          {
            "accuracy": 0.869972,
            "f1": 0.857268,
            "f1_weighted": 0.868732
          },
          {
            "accuracy": 0.872176,
            "f1": 0.85866,
            "f1_weighted": 0.871064
          },
          {
            "accuracy": 0.857851,
            "f1": 0.838354,
            "f1_weighted": 0.855244
          },
          {
            "accuracy": 0.863912,
            "f1": 0.854162,
            "f1_weighted": 0.863789
          },
          {
            "accuracy": 0.842975,
            "f1": 0.835944,
            "f1_weighted": 0.844849
          },
          {
            "accuracy": 0.860055,
            "f1": 0.851354,
            "f1_weighted": 0.860699
          },
          {
            "accuracy": 0.871625,
            "f1": 0.858866,
            "f1_weighted": 0.871011
          }
        ],
        "main_score": 0.860661,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.862468,
        "f1": 0.847967,
        "f1_weighted": 0.861675,
        "scores_per_experiment": [
          {
            "accuracy": 0.873204,
            "f1": 0.8603,
            "f1_weighted": 0.872062
          },
          {
            "accuracy": 0.840518,
            "f1": 0.826207,
            "f1_weighted": 0.840088
          },
          {
            "accuracy": 0.861651,
            "f1": 0.839463,
            "f1_weighted": 0.860128
          },
          {
            "accuracy": 0.872077,
            "f1": 0.859522,
            "f1_weighted": 0.871429
          },
          {
            "accuracy": 0.871231,
            "f1": 0.856149,
            "f1_weighted": 0.869944
          },
          {
            "accuracy": 0.857706,
            "f1": 0.835122,
            "f1_weighted": 0.854029
          },
          {
            "accuracy": 0.86785,
            "f1": 0.8532,
            "f1_weighted": 0.867412
          },
          {
            "accuracy": 0.84869,
            "f1": 0.840454,
            "f1_weighted": 0.850568
          },
          {
            "accuracy": 0.857988,
            "f1": 0.848332,
            "f1_weighted": 0.85862
          },
          {
            "accuracy": 0.873767,
            "f1": 0.860924,
            "f1_weighted": 0.872476
          }
        ],
        "main_score": 0.862468,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 21.148167371749878,
  "kg_co2_emissions": null
}