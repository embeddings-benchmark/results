{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "task_name": "AmazonCounterfactualClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.705794,
        "f1": 0.68743,
        "f1_weighted": 0.716737,
        "ap": 0.814942,
        "ap_weighted": 0.814942,
        "scores_per_experiment": [
          {
            "accuracy": 0.738197,
            "f1": 0.722147,
            "f1_weighted": 0.748515,
            "ap": 0.840371,
            "ap_weighted": 0.840371
          },
          {
            "accuracy": 0.736052,
            "f1": 0.710274,
            "f1_weighted": 0.744397,
            "ap": 0.81851,
            "ap_weighted": 0.81851
          },
          {
            "accuracy": 0.759657,
            "f1": 0.731876,
            "f1_weighted": 0.765954,
            "ap": 0.828119,
            "ap_weighted": 0.828119
          },
          {
            "accuracy": 0.654506,
            "f1": 0.642616,
            "f1_weighted": 0.668355,
            "ap": 0.796669,
            "ap_weighted": 0.796669
          },
          {
            "accuracy": 0.718884,
            "f1": 0.705028,
            "f1_weighted": 0.730271,
            "ap": 0.83386,
            "ap_weighted": 0.83386
          },
          {
            "accuracy": 0.688841,
            "f1": 0.670125,
            "f1_weighted": 0.70115,
            "ap": 0.803576,
            "ap_weighted": 0.803576
          },
          {
            "accuracy": 0.708155,
            "f1": 0.688159,
            "f1_weighted": 0.719338,
            "ap": 0.81283,
            "ap_weighted": 0.81283
          },
          {
            "accuracy": 0.639485,
            "f1": 0.627965,
            "f1_weighted": 0.653814,
            "ap": 0.787802,
            "ap_weighted": 0.787802
          },
          {
            "accuracy": 0.751073,
            "f1": 0.726376,
            "f1_weighted": 0.758835,
            "ap": 0.828942,
            "ap_weighted": 0.828942
          },
          {
            "accuracy": 0.66309,
            "f1": 0.649733,
            "f1_weighted": 0.676741,
            "ap": 0.798744,
            "ap_weighted": 0.798744
          }
        ],
        "main_score": 0.705794,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.698501,
        "f1": 0.68184,
        "f1_weighted": 0.709785,
        "ap": 0.812608,
        "ap_weighted": 0.812608,
        "scores_per_experiment": [
          {
            "accuracy": 0.702355,
            "f1": 0.686188,
            "f1_weighted": 0.7141,
            "ap": 0.815494,
            "ap_weighted": 0.815494
          },
          {
            "accuracy": 0.711991,
            "f1": 0.688399,
            "f1_weighted": 0.721998,
            "ap": 0.806709,
            "ap_weighted": 0.806709
          },
          {
            "accuracy": 0.760171,
            "f1": 0.734972,
            "f1_weighted": 0.766996,
            "ap": 0.831388,
            "ap_weighted": 0.831388
          },
          {
            "accuracy": 0.692719,
            "f1": 0.681273,
            "f1_weighted": 0.704942,
            "ap": 0.821292,
            "ap_weighted": 0.821292
          },
          {
            "accuracy": 0.705567,
            "f1": 0.694079,
            "f1_weighted": 0.71731,
            "ap": 0.829876,
            "ap_weighted": 0.829876
          },
          {
            "accuracy": 0.718415,
            "f1": 0.701393,
            "f1_weighted": 0.729331,
            "ap": 0.823653,
            "ap_weighted": 0.823653
          },
          {
            "accuracy": 0.665953,
            "f1": 0.647078,
            "f1_weighted": 0.679061,
            "ap": 0.787511,
            "ap_weighted": 0.787511
          },
          {
            "accuracy": 0.642398,
            "f1": 0.632931,
            "f1_weighted": 0.656032,
            "ap": 0.792926,
            "ap_weighted": 0.792926
          },
          {
            "accuracy": 0.731263,
            "f1": 0.708501,
            "f1_weighted": 0.740421,
            "ap": 0.819467,
            "ap_weighted": 0.819467
          },
          {
            "accuracy": 0.654176,
            "f1": 0.643585,
            "f1_weighted": 0.66766,
            "ap": 0.797767,
            "ap_weighted": 0.797767
          }
        ],
        "main_score": 0.698501,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 85.73426079750061,
  "kg_co2_emissions": null
}