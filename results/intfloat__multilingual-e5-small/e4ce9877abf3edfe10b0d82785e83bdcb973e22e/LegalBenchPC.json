{
  "dataset_revision": "12ca3b695563788fead87a982ad1a068284413f4",
  "evaluation_time": 1.1380748748779297,
  "kg_co2_emissions": 0.00018292794558317878,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.67236328125,
          "accuracy_threshold": 0.7890610694885254,
          "ap": 0.7312524758763079,
          "f1": 0.7612387612387613,
          "f1_threshold": 0.7667849063873291,
          "precision": 0.6414141414141414,
          "recall": 0.9361179361179361
        },
        "dot": {
          "accuracy": 0.67236328125,
          "accuracy_threshold": 0.7890612483024597,
          "ap": 0.7312512202291989,
          "f1": 0.7612387612387613,
          "f1_threshold": 0.7667847275733948,
          "precision": 0.6414141414141414,
          "recall": 0.9361179361179361
        },
        "euclidean": {
          "accuracy": 0.67236328125,
          "accuracy_threshold": 0.6495212316513062,
          "ap": 0.7312524758763079,
          "f1": 0.7612387612387613,
          "f1_threshold": 0.6829569339752197,
          "precision": 0.6414141414141414,
          "recall": 0.9361179361179361
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.67626953125,
        "manhattan": {
          "accuracy": 0.67626953125,
          "accuracy_threshold": 10.396942138671875,
          "ap": 0.7328028562077238,
          "f1": 0.7622803872355682,
          "f1_threshold": 10.396942138671875,
          "precision": 0.6779336734693877,
          "recall": 0.8705978705978706
        },
        "max": {
          "accuracy": 0.67626953125,
          "ap": 0.7328028562077238,
          "f1": 0.7622803872355682
        },
        "similarity": {
          "accuracy": 0.67236328125,
          "accuracy_threshold": 0.7890610694885254,
          "ap": 0.7312524758763079,
          "f1": 0.7612387612387613,
          "f1_threshold": 0.7667849063873291,
          "precision": 0.6414141414141414,
          "recall": 0.9361179361179361
        }
      }
    ]
  },
  "task_name": "LegalBenchPC"
}