{
  "dataset_revision": "416b34a802308eac30e4192afc0ff99bb8dcc7f2",
  "task_name": "SensitiveTopicsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.181152,
        "f1": 0.036015,
        "lrap": 0.263763,
        "scores_per_experiment": [
          {
            "accuracy": 0.183105,
            "f1": 0.028237,
            "lrap": 0.26024
          },
          {
            "accuracy": 0.184082,
            "f1": 0.057646,
            "lrap": 0.272881
          },
          {
            "accuracy": 0.178711,
            "f1": 0.032179,
            "lrap": 0.262736
          },
          {
            "accuracy": 0.178711,
            "f1": 0.030136,
            "lrap": 0.260525
          },
          {
            "accuracy": 0.185547,
            "f1": 0.031572,
            "lrap": 0.262112
          },
          {
            "accuracy": 0.17334,
            "f1": 0.039733,
            "lrap": 0.26983
          },
          {
            "accuracy": 0.180664,
            "f1": 0.022319,
            "lrap": 0.261244
          },
          {
            "accuracy": 0.181152,
            "f1": 0.026282,
            "lrap": 0.260281
          },
          {
            "accuracy": 0.180176,
            "f1": 0.033349,
            "lrap": 0.258965
          },
          {
            "accuracy": 0.186035,
            "f1": 0.058693,
            "lrap": 0.268812
          }
        ],
        "main_score": 0.181152,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 6.453489780426025,
  "kg_co2_emissions": 0.0003108242419703704
}