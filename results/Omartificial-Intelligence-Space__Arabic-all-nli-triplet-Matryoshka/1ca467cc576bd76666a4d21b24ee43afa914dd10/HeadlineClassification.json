{
  "dataset_revision": "2fe05ee6b5832cda29f2ef7aaad7b7fe6a3609eb",
  "task_name": "HeadlineClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.661182,
        "f1": 0.661328,
        "f1_weighted": 0.66134,
        "scores_per_experiment": [
          {
            "accuracy": 0.636719,
            "f1": 0.640276,
            "f1_weighted": 0.640305
          },
          {
            "accuracy": 0.651367,
            "f1": 0.651942,
            "f1_weighted": 0.651954
          },
          {
            "accuracy": 0.666504,
            "f1": 0.666016,
            "f1_weighted": 0.666051
          },
          {
            "accuracy": 0.69873,
            "f1": 0.699384,
            "f1_weighted": 0.699354
          },
          {
            "accuracy": 0.67627,
            "f1": 0.676689,
            "f1_weighted": 0.676677
          },
          {
            "accuracy": 0.708496,
            "f1": 0.707289,
            "f1_weighted": 0.70731
          },
          {
            "accuracy": 0.614746,
            "f1": 0.61574,
            "f1_weighted": 0.615736
          },
          {
            "accuracy": 0.655762,
            "f1": 0.655314,
            "f1_weighted": 0.655336
          },
          {
            "accuracy": 0.612793,
            "f1": 0.610742,
            "f1_weighted": 0.610807
          },
          {
            "accuracy": 0.69043,
            "f1": 0.689883,
            "f1_weighted": 0.689875
          }
        ],
        "main_score": 0.661182,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 8.047564029693604,
  "kg_co2_emissions": 0.00025820938614153473
}