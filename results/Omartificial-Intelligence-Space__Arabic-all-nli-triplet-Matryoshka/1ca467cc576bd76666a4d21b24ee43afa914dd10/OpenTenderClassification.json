{
  "dataset_revision": "9af5657575a669dc18c7f897a67287ff7d1a0c65",
  "task_name": "OpenTenderClassification",
  "mteb_version": "2.1.8",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.410256,
            "f1": 0.407718,
            "f1_weighted": 0.407763,
            "precision": 0.412229,
            "precision_weighted": 0.412313,
            "recall": 0.410249,
            "recall_weighted": 0.410256,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.419844,
            "f1": 0.414635,
            "f1_weighted": 0.414611,
            "precision": 0.41966,
            "precision_weighted": 0.419648,
            "recall": 0.419872,
            "recall_weighted": 0.419844,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.412263,
            "f1": 0.407454,
            "f1_weighted": 0.407429,
            "precision": 0.407498,
            "precision_weighted": 0.407443,
            "recall": 0.412255,
            "recall_weighted": 0.412263,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.405128,
            "f1": 0.402883,
            "f1_weighted": 0.402844,
            "precision": 0.413323,
            "precision_weighted": 0.413284,
            "recall": 0.405176,
            "recall_weighted": 0.405128,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.383501,
            "f1": 0.383623,
            "f1_weighted": 0.3836,
            "precision": 0.393667,
            "precision_weighted": 0.393671,
            "recall": 0.383543,
            "recall_weighted": 0.383501,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.395095,
            "f1": 0.395751,
            "f1_weighted": 0.395668,
            "precision": 0.404828,
            "precision_weighted": 0.404772,
            "recall": 0.395206,
            "recall_weighted": 0.395095,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.397324,
            "f1": 0.394677,
            "f1_weighted": 0.394675,
            "precision": 0.402603,
            "precision_weighted": 0.402593,
            "recall": 0.397294,
            "recall_weighted": 0.397324,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.394203,
            "f1": 0.392684,
            "f1_weighted": 0.392657,
            "precision": 0.398248,
            "precision_weighted": 0.398208,
            "recall": 0.394218,
            "recall_weighted": 0.394203,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.377035,
            "f1": 0.372194,
            "f1_weighted": 0.372033,
            "precision": 0.373043,
            "precision_weighted": 0.372869,
            "recall": 0.377182,
            "recall_weighted": 0.377035,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.394203,
            "f1": 0.393281,
            "f1_weighted": 0.393251,
            "precision": 0.399327,
            "precision_weighted": 0.399284,
            "recall": 0.394212,
            "recall_weighted": 0.394203,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.398885,
        "f1": 0.39649,
        "f1_weighted": 0.396453,
        "precision": 0.402443,
        "precision_weighted": 0.402408,
        "recall": 0.398921,
        "recall_weighted": 0.398885,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.39649,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 63.2989981174469,
  "kg_co2_emissions": null
}