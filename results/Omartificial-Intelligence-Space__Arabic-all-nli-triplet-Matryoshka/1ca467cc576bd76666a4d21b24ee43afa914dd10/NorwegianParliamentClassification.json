{
  "dataset_revision": "f7393532774c66312378d30b197610b43d751972",
  "task_name": "NorwegianParliamentClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.546417,
        "f1": 0.543608,
        "f1_weighted": 0.543608,
        "ap": 0.525877,
        "ap_weighted": 0.525877,
        "scores_per_experiment": [
          {
            "accuracy": 0.549167,
            "f1": 0.545554,
            "f1_weighted": 0.545554,
            "ap": 0.527525,
            "ap_weighted": 0.527525
          },
          {
            "accuracy": 0.5275,
            "f1": 0.525117,
            "f1_weighted": 0.525117,
            "ap": 0.514412,
            "ap_weighted": 0.514412
          },
          {
            "accuracy": 0.561667,
            "f1": 0.55961,
            "f1_weighted": 0.55961,
            "ap": 0.535238,
            "ap_weighted": 0.535238
          },
          {
            "accuracy": 0.549167,
            "f1": 0.549054,
            "f1_weighted": 0.549054,
            "ap": 0.526926,
            "ap_weighted": 0.526926
          },
          {
            "accuracy": 0.521667,
            "f1": 0.521475,
            "f1_weighted": 0.521475,
            "ap": 0.511285,
            "ap_weighted": 0.511285
          },
          {
            "accuracy": 0.529167,
            "f1": 0.526792,
            "f1_weighted": 0.526792,
            "ap": 0.515574,
            "ap_weighted": 0.515574
          },
          {
            "accuracy": 0.553333,
            "f1": 0.543052,
            "f1_weighted": 0.543052,
            "ap": 0.528855,
            "ap_weighted": 0.528855
          },
          {
            "accuracy": 0.525833,
            "f1": 0.524438,
            "f1_weighted": 0.524438,
            "ap": 0.513665,
            "ap_weighted": 0.513665
          },
          {
            "accuracy": 0.569167,
            "f1": 0.563644,
            "f1_weighted": 0.563644,
            "ap": 0.540756,
            "ap_weighted": 0.540756
          },
          {
            "accuracy": 0.5775,
            "f1": 0.577345,
            "f1_weighted": 0.577345,
            "ap": 0.544535,
            "ap_weighted": 0.544535
          }
        ],
        "main_score": 0.546417,
        "hf_subset": "default",
        "languages": [
          "nob-Latn"
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.537583,
        "f1": 0.534701,
        "f1_weighted": 0.534701,
        "ap": 0.521491,
        "ap_weighted": 0.521491,
        "scores_per_experiment": [
          {
            "accuracy": 0.553333,
            "f1": 0.548354,
            "f1_weighted": 0.548354,
            "ap": 0.530267,
            "ap_weighted": 0.530267
          },
          {
            "accuracy": 0.461667,
            "f1": 0.458199,
            "f1_weighted": 0.458199,
            "ap": 0.4821,
            "ap_weighted": 0.4821
          },
          {
            "accuracy": 0.560833,
            "f1": 0.557574,
            "f1_weighted": 0.557574,
            "ap": 0.534884,
            "ap_weighted": 0.534884
          },
          {
            "accuracy": 0.5525,
            "f1": 0.552363,
            "f1_weighted": 0.552363,
            "ap": 0.528913,
            "ap_weighted": 0.528913
          },
          {
            "accuracy": 0.534167,
            "f1": 0.534112,
            "f1_weighted": 0.534112,
            "ap": 0.518226,
            "ap_weighted": 0.518226
          },
          {
            "accuracy": 0.500833,
            "f1": 0.498072,
            "f1_weighted": 0.498072,
            "ap": 0.500417,
            "ap_weighted": 0.500417
          },
          {
            "accuracy": 0.546667,
            "f1": 0.537158,
            "f1_weighted": 0.537158,
            "ap": 0.525026,
            "ap_weighted": 0.525026
          },
          {
            "accuracy": 0.5425,
            "f1": 0.541236,
            "f1_weighted": 0.541236,
            "ap": 0.523268,
            "ap_weighted": 0.523268
          },
          {
            "accuracy": 0.585,
            "f1": 0.581859,
            "f1_weighted": 0.581859,
            "ap": 0.55124,
            "ap_weighted": 0.55124
          },
          {
            "accuracy": 0.538333,
            "f1": 0.538082,
            "f1_weighted": 0.538082,
            "ap": 0.520571,
            "ap_weighted": 0.520571
          }
        ],
        "main_score": 0.537583,
        "hf_subset": "default",
        "languages": [
          "nob-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 20.475869178771973,
  "kg_co2_emissions": 0.0006806912744575478
}