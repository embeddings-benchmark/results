{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.331396,
        "f1": 0.324078,
        "f1_weighted": 0.32406,
        "scores_per_experiment": [
          {
            "accuracy": 0.335449,
            "f1": 0.325894,
            "f1_weighted": 0.325876
          },
          {
            "accuracy": 0.321289,
            "f1": 0.320481,
            "f1_weighted": 0.320476
          },
          {
            "accuracy": 0.348145,
            "f1": 0.337664,
            "f1_weighted": 0.337587
          },
          {
            "accuracy": 0.36377,
            "f1": 0.352945,
            "f1_weighted": 0.352915
          },
          {
            "accuracy": 0.29248,
            "f1": 0.289136,
            "f1_weighted": 0.289144
          },
          {
            "accuracy": 0.337402,
            "f1": 0.325139,
            "f1_weighted": 0.32514
          },
          {
            "accuracy": 0.293945,
            "f1": 0.296265,
            "f1_weighted": 0.296246
          },
          {
            "accuracy": 0.371094,
            "f1": 0.364675,
            "f1_weighted": 0.364644
          },
          {
            "accuracy": 0.334961,
            "f1": 0.323591,
            "f1_weighted": 0.323541
          },
          {
            "accuracy": 0.31543,
            "f1": 0.304986,
            "f1_weighted": 0.305034
          }
        ],
        "main_score": 0.331396,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.331299,
        "f1": 0.324792,
        "f1_weighted": 0.324786,
        "scores_per_experiment": [
          {
            "accuracy": 0.333008,
            "f1": 0.325358,
            "f1_weighted": 0.325372
          },
          {
            "accuracy": 0.32959,
            "f1": 0.327919,
            "f1_weighted": 0.327914
          },
          {
            "accuracy": 0.318359,
            "f1": 0.308159,
            "f1_weighted": 0.308102
          },
          {
            "accuracy": 0.348145,
            "f1": 0.337801,
            "f1_weighted": 0.337788
          },
          {
            "accuracy": 0.295898,
            "f1": 0.293601,
            "f1_weighted": 0.293618
          },
          {
            "accuracy": 0.334473,
            "f1": 0.32258,
            "f1_weighted": 0.322574
          },
          {
            "accuracy": 0.337891,
            "f1": 0.341067,
            "f1_weighted": 0.341041
          },
          {
            "accuracy": 0.369629,
            "f1": 0.364681,
            "f1_weighted": 0.364666
          },
          {
            "accuracy": 0.344727,
            "f1": 0.33709,
            "f1_weighted": 0.337048
          },
          {
            "accuracy": 0.30127,
            "f1": 0.289667,
            "f1_weighted": 0.289732
          }
        ],
        "main_score": 0.331299,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 15.840120792388916,
  "kg_co2_emissions": 0.0005597289226842901
}