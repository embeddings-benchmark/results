{
  "dataset_revision": "7fb2f514ea683c5282dfec0a9672ece8de90ac50",
  "task_name": "SinhalaNewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "train": [
      {
        "accuracy": 0.683691,
        "f1": 0.664208,
        "f1_weighted": 0.683757,
        "scores_per_experiment": [
          {
            "accuracy": 0.661133,
            "f1": 0.638218,
            "f1_weighted": 0.656928
          },
          {
            "accuracy": 0.701172,
            "f1": 0.677905,
            "f1_weighted": 0.702134
          },
          {
            "accuracy": 0.700195,
            "f1": 0.685339,
            "f1_weighted": 0.699527
          },
          {
            "accuracy": 0.66748,
            "f1": 0.656134,
            "f1_weighted": 0.67502
          },
          {
            "accuracy": 0.644531,
            "f1": 0.626446,
            "f1_weighted": 0.6421
          },
          {
            "accuracy": 0.669922,
            "f1": 0.646462,
            "f1_weighted": 0.66996
          },
          {
            "accuracy": 0.665039,
            "f1": 0.646402,
            "f1_weighted": 0.662504
          },
          {
            "accuracy": 0.718262,
            "f1": 0.699735,
            "f1_weighted": 0.718941
          },
          {
            "accuracy": 0.709473,
            "f1": 0.690965,
            "f1_weighted": 0.712468
          },
          {
            "accuracy": 0.699707,
            "f1": 0.674477,
            "f1_weighted": 0.697984
          }
        ],
        "main_score": 0.683691,
        "hf_subset": "default",
        "languages": [
          "sin-Sinh"
        ]
      }
    ]
  },
  "evaluation_time": 9.084379196166992,
  "kg_co2_emissions": 0.0003042906602572823
}