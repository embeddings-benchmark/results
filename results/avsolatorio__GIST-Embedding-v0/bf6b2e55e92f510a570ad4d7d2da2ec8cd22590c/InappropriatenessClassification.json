{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "evaluation_time": 12.102399587631226,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.512744140625,
        "ap": 0.5068530710416295,
        "ap_weighted": 0.5068530710416295,
        "f1": 0.5121831976581946,
        "f1_weighted": 0.5121831976581946,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.512744140625,
        "scores_per_experiment": [
          {
            "accuracy": 0.494140625,
            "ap": 0.49710663094008267,
            "ap_weighted": 0.49710663094008267,
            "f1": 0.493762120726251,
            "f1_weighted": 0.493762120726251
          },
          {
            "accuracy": 0.51806640625,
            "ap": 0.5093725214149746,
            "ap_weighted": 0.5093725214149746,
            "f1": 0.517891577026524,
            "f1_weighted": 0.517891577026524
          },
          {
            "accuracy": 0.5263671875,
            "ap": 0.5139393411624205,
            "ap_weighted": 0.5139393411624205,
            "f1": 0.5256066749769557,
            "f1_weighted": 0.5256066749769557
          },
          {
            "accuracy": 0.48974609375,
            "ap": 0.4949722782258064,
            "ap_weighted": 0.4949722782258064,
            "f1": 0.4892930172245723,
            "f1_weighted": 0.4892930172245723
          },
          {
            "accuracy": 0.54931640625,
            "ap": 0.5271611966080403,
            "ap_weighted": 0.5271611966080403,
            "f1": 0.5492260215483002,
            "f1_weighted": 0.5492260215483002
          },
          {
            "accuracy": 0.525390625,
            "ap": 0.5133902138157895,
            "ap_weighted": 0.5133902138157895,
            "f1": 0.5247701743781316,
            "f1_weighted": 0.5247701743781316
          },
          {
            "accuracy": 0.51513671875,
            "ap": 0.5078027441308691,
            "ap_weighted": 0.5078027441308691,
            "f1": 0.5150755584169394,
            "f1_weighted": 0.5150755584169394
          },
          {
            "accuracy": 0.5009765625,
            "ap": 0.5004891771788991,
            "ap_weighted": 0.5004891771788991,
            "f1": 0.5004577622443047,
            "f1_weighted": 0.5004577622443047
          },
          {
            "accuracy": 0.51220703125,
            "ap": 0.5062340441884089,
            "ap_weighted": 0.5062340441884089,
            "f1": 0.5097495278892209,
            "f1_weighted": 0.5097495278892209
          },
          {
            "accuracy": 0.49609375,
            "ap": 0.498062562751004,
            "ap_weighted": 0.498062562751004,
            "f1": 0.4959995421507469,
            "f1_weighted": 0.4959995421507469
          }
        ]
      }
    ]
  },
  "task_name": "InappropriatenessClassification"
}