{
    "dataset_revision": "9080400076fbadbb4c4dcb136ff4eddc40b42553",
    "task_name": "Tatoeba",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "sqi-eng",
                "languages": [
                    "sqi-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.08,
                "f1": 0.06298401229470593,
                "precision": 0.05916991709050532,
                "recall": 0.08,
                "main_score": 0.06298401229470593
            },
            {
                "hf_subset": "fry-eng",
                "languages": [
                    "fry-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.17341040462427745,
                "f1": 0.14621650026274302,
                "precision": 0.139250609139035,
                "recall": 0.17341040462427745,
                "main_score": 0.14621650026274302
            },
            {
                "hf_subset": "kur-eng",
                "languages": [
                    "kur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.08536585365853659,
                "f1": 0.0630972482801751,
                "precision": 0.05796517326875397,
                "recall": 0.08536585365853659,
                "main_score": 0.0630972482801751
            },
            {
                "hf_subset": "tur-eng",
                "languages": [
                    "tur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.064,
                "f1": 0.042211267436267436,
                "precision": 0.03822815143403898,
                "recall": 0.064,
                "main_score": 0.042211267436267436
            },
            {
                "hf_subset": "deu-eng",
                "languages": [
                    "deu-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.198,
                "f1": 0.1813768093781855,
                "precision": 0.1754646004378763,
                "recall": 0.198,
                "main_score": 0.1813768093781855
            },
            {
                "hf_subset": "nld-eng",
                "languages": [
                    "nld-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.137,
                "f1": 0.12367662337662337,
                "precision": 0.11934237966189185,
                "recall": 0.137,
                "main_score": 0.12367662337662337
            },
            {
                "hf_subset": "ron-eng",
                "languages": [
                    "ron-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.143,
                "f1": 0.10942180289268338,
                "precision": 0.10153968847262192,
                "recall": 0.143,
                "main_score": 0.10942180289268338
            },
            {
                "hf_subset": "ang-eng",
                "languages": [
                    "ang-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.22388059701492538,
                "f1": 0.17001577336604332,
                "precision": 0.15650551589876702,
                "recall": 0.22388059701492538,
                "main_score": 0.17001577336604332
            },
            {
                "hf_subset": "ido-eng",
                "languages": [
                    "ido-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.22,
                "f1": 0.174576947358322,
                "precision": 0.16261363669827777,
                "recall": 0.22,
                "main_score": 0.174576947358322
            },
            {
                "hf_subset": "jav-eng",
                "languages": [
                    "jav-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.08292682926829269,
                "f1": 0.055440484560056236,
                "precision": 0.05009506603002538,
                "recall": 0.08292682926829269,
                "main_score": 0.055440484560056236
            },
            {
                "hf_subset": "isl-eng",
                "languages": [
                    "isl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.054000000000000006,
                "f1": 0.04148897174789229,
                "precision": 0.038622172594495636,
                "recall": 0.054000000000000006,
                "main_score": 0.04148897174789229
            },
            {
                "hf_subset": "slv-eng",
                "languages": [
                    "slv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.05589307411907654,
                "f1": 0.04375041810373159,
                "precision": 0.04181207113088141,
                "recall": 0.05589307411907654,
                "main_score": 0.04375041810373159
            },
            {
                "hf_subset": "cym-eng",
                "languages": [
                    "cym-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.08173913043478259,
                "f1": 0.06448011891490153,
                "precision": 0.059719116632160105,
                "recall": 0.08173913043478259,
                "main_score": 0.06448011891490153
            },
            {
                "hf_subset": "kaz-eng",
                "languages": [
                    "kaz-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.008695652173913044,
                "f1": 0.00582815734989648,
                "precision": 0.0055808852330591465,
                "recall": 0.008695652173913044,
                "main_score": 0.00582815734989648
            },
            {
                "hf_subset": "est-eng",
                "languages": [
                    "est-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.051,
                "f1": 0.035000615825615825,
                "precision": 0.03207352357799471,
                "recall": 0.051,
                "main_score": 0.035000615825615825
            },
            {
                "hf_subset": "heb-eng",
                "languages": [
                    "heb-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.003,
                "f1": 0.0010109884927372195,
                "precision": 0.0010055127118392896,
                "recall": 0.003,
                "main_score": 0.0010109884927372195
            },
            {
                "hf_subset": "gla-eng",
                "languages": [
                    "gla-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.038600723763570564,
                "f1": 0.028177402725050493,
                "precision": 0.02566268781969921,
                "recall": 0.038600723763570564,
                "main_score": 0.028177402725050493
            },
            {
                "hf_subset": "mar-eng",
                "languages": [
                    "mar-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.0,
                "f1": 0.0,
                "precision": 0.0,
                "recall": 0.0,
                "main_score": 0.0
            },
            {
                "hf_subset": "lat-eng",
                "languages": [
                    "lat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.153,
                "f1": 0.11377964359824291,
                "precision": 0.10361140908892764,
                "recall": 0.153,
                "main_score": 0.11377964359824291
            },
            {
                "hf_subset": "bel-eng",
                "languages": [
                    "bel-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.013000000000000001,
                "f1": 0.00960082023239918,
                "precision": 0.009151648856810397,
                "recall": 0.013000000000000001,
                "main_score": 0.00960082023239918
            },
            {
                "hf_subset": "pms-eng",
                "languages": [
                    "pms-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.14095238095238094,
                "f1": 0.11400815418190441,
                "precision": 0.10645867976820358,
                "recall": 0.14095238095238094,
                "main_score": 0.11400815418190441
            },
            {
                "hf_subset": "gle-eng",
                "languages": [
                    "gle-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.04,
                "f1": 0.02380070450196343,
                "precision": 0.020919368034607456,
                "recall": 0.04,
                "main_score": 0.02380070450196343
            },
            {
                "hf_subset": "pes-eng",
                "languages": [
                    "pes-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.003,
                "f1": 0.002002053388090349,
                "precision": 0.002001027749229188,
                "recall": 0.003,
                "main_score": 0.002002053388090349
            },
            {
                "hf_subset": "nob-eng",
                "languages": [
                    "nob-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.117,
                "f1": 0.1029755634495992,
                "precision": 0.09876637220292393,
                "recall": 0.117,
                "main_score": 0.1029755634495992
            },
            {
                "hf_subset": "bul-eng",
                "languages": [
                    "bul-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.017,
                "f1": 0.00985815849620051,
                "precision": 0.00888468992248062,
                "recall": 0.017,
                "main_score": 0.00985815849620051
            },
            {
                "hf_subset": "cbk-eng",
                "languages": [
                    "cbk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.176,
                "f1": 0.14086312656126182,
                "precision": 0.13192360560816124,
                "recall": 0.176,
                "main_score": 0.14086312656126182
            },
            {
                "hf_subset": "hun-eng",
                "languages": [
                    "hun-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.061,
                "f1": 0.046837957291730875,
                "precision": 0.0431687579027912,
                "recall": 0.061,
                "main_score": 0.046837957291730875
            },
            {
                "hf_subset": "uig-eng",
                "languages": [
                    "uig-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.004,
                "f1": 0.002096666666666667,
                "precision": 0.0020500700280112047,
                "recall": 0.004,
                "main_score": 0.002096666666666667
            },
            {
                "hf_subset": "rus-eng",
                "languages": [
                    "rus-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.006,
                "f1": 0.002454665118079752,
                "precision": 0.0022551251679916178,
                "recall": 0.006,
                "main_score": 0.002454665118079752
            },
            {
                "hf_subset": "spa-eng",
                "languages": [
                    "spa-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.21,
                "f1": 0.1896590124206602,
                "precision": 0.18381437375171,
                "recall": 0.21,
                "main_score": 0.1896590124206602
            },
            {
                "hf_subset": "hye-eng",
                "languages": [
                    "hye-Armn",
                    "eng-Latn"
                ],
                "accuracy": 0.005390835579514825,
                "f1": 0.0040488984572051925,
                "precision": 0.004046018763809678,
                "recall": 0.005390835579514825,
                "main_score": 0.0040488984572051925
            },
            {
                "hf_subset": "tel-eng",
                "languages": [
                    "tel-Telu",
                    "eng-Latn"
                ],
                "accuracy": 0.01282051282051282,
                "f1": 0.005098554872310529,
                "precision": 0.004715099715099715,
                "recall": 0.01282051282051282,
                "main_score": 0.005098554872310529
            },
            {
                "hf_subset": "afr-eng",
                "languages": [
                    "afr-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.107,
                "f1": 0.08045120643200705,
                "precision": 0.07387598023074453,
                "recall": 0.107,
                "main_score": 0.08045120643200705
            },
            {
                "hf_subset": "mon-eng",
                "languages": [
                    "mon-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.022727272727272728,
                "f1": 0.0144184724004356,
                "precision": 0.014082306862044766,
                "recall": 0.022727272727272728,
                "main_score": 0.0144184724004356
            },
            {
                "hf_subset": "arz-eng",
                "languages": [
                    "arz-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.0020964360587002098,
                "f1": 1.335309591528796e-05,
                "precision": 6.697878781789807e-06,
                "recall": 0.0020964360587002098,
                "main_score": 1.335309591528796e-05
            },
            {
                "hf_subset": "hrv-eng",
                "languages": [
                    "hrv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.071,
                "f1": 0.05522254020507502,
                "precision": 0.050818494267239035,
                "recall": 0.071,
                "main_score": 0.05522254020507502
            },
            {
                "hf_subset": "nov-eng",
                "languages": [
                    "nov-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.3657587548638132,
                "f1": 0.30325515383881146,
                "precision": 0.2859255854392041,
                "recall": 0.3657587548638132,
                "main_score": 0.30325515383881146
            },
            {
                "hf_subset": "gsw-eng",
                "languages": [
                    "gsw-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.16239316239316237,
                "f1": 0.13548783761549718,
                "precision": 0.130472896359184,
                "recall": 0.16239316239316237,
                "main_score": 0.13548783761549718
            },
            {
                "hf_subset": "nds-eng",
                "languages": [
                    "nds-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.163,
                "f1": 0.133418584934734,
                "precision": 0.12506853047473757,
                "recall": 0.163,
                "main_score": 0.133418584934734
            },
            {
                "hf_subset": "ukr-eng",
                "languages": [
                    "ukr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.01,
                "f1": 0.007764001197963461,
                "precision": 0.007551049317943337,
                "recall": 0.01,
                "main_score": 0.007764001197963461
            },
            {
                "hf_subset": "uzb-eng",
                "languages": [
                    "uzb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.0397196261682243,
                "f1": 0.03190729401654313,
                "precision": 0.03001159168296747,
                "recall": 0.0397196261682243,
                "main_score": 0.03190729401654313
            },
            {
                "hf_subset": "lit-eng",
                "languages": [
                    "lit-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.034,
                "f1": 0.024847456001574654,
                "precision": 0.02308739271803959,
                "recall": 0.034,
                "main_score": 0.024847456001574654
            },
            {
                "hf_subset": "ina-eng",
                "languages": [
                    "ina-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.369,
                "f1": 0.313904079550637,
                "precision": 0.29631294298308614,
                "recall": 0.369,
                "main_score": 0.313904079550637
            },
            {
                "hf_subset": "lfn-eng",
                "languages": [
                    "lfn-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.142,
                "f1": 0.12551591810861895,
                "precision": 0.12100586917562724,
                "recall": 0.142,
                "main_score": 0.12551591810861895
            },
            {
                "hf_subset": "zsm-eng",
                "languages": [
                    "zsm-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.092,
                "f1": 0.07556189564821143,
                "precision": 0.07177371101110253,
                "recall": 0.092,
                "main_score": 0.07556189564821143
            },
            {
                "hf_subset": "ita-eng",
                "languages": [
                    "ita-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.212,
                "f1": 0.18498268429117876,
                "precision": 0.17693915156965356,
                "recall": 0.212,
                "main_score": 0.18498268429117876
            },
            {
                "hf_subset": "cmn-eng",
                "languages": [
                    "cmn-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.042,
                "f1": 0.028865727825309363,
                "precision": 0.025806792595351916,
                "recall": 0.042,
                "main_score": 0.028865727825309363
            },
            {
                "hf_subset": "lvs-eng",
                "languages": [
                    "lvs-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.068,
                "f1": 0.04881091920308238,
                "precision": 0.04436731163345769,
                "recall": 0.068,
                "main_score": 0.04881091920308238
            },
            {
                "hf_subset": "glg-eng",
                "languages": [
                    "glg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.221,
                "f1": 0.18493832677140737,
                "precision": 0.1752055858924503,
                "recall": 0.221,
                "main_score": 0.18493832677140737
            },
            {
                "hf_subset": "ceb-eng",
                "languages": [
                    "ceb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.06,
                "f1": 0.0458716840215435,
                "precision": 0.04303119297298687,
                "recall": 0.06,
                "main_score": 0.0458716840215435
            },
            {
                "hf_subset": "bre-eng",
                "languages": [
                    "bre-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.055,
                "f1": 0.03813678559437776,
                "precision": 0.0352375763382276,
                "recall": 0.055,
                "main_score": 0.03813678559437776
            },
            {
                "hf_subset": "ben-eng",
                "languages": [
                    "ben-Beng",
                    "eng-Latn"
                ],
                "accuracy": 0.002,
                "f1": 0.0006701509872241579,
                "precision": 0.0005017452006980803,
                "recall": 0.002,
                "main_score": 0.0006701509872241579
            },
            {
                "hf_subset": "swg-eng",
                "languages": [
                    "swg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.125,
                "f1": 0.09325396825396826,
                "precision": 0.08681972789115645,
                "recall": 0.125,
                "main_score": 0.09325396825396826
            },
            {
                "hf_subset": "arq-eng",
                "languages": [
                    "arq-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.0043907793633369925,
                "f1": 0.0026369680618309756,
                "precision": 0.002471065039358055,
                "recall": 0.0043907793633369925,
                "main_score": 0.0026369680618309756
            },
            {
                "hf_subset": "kab-eng",
                "languages": [
                    "kab-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.017,
                "f1": 0.010240727731562104,
                "precision": 0.009379457073996874,
                "recall": 0.017,
                "main_score": 0.010240727731562104
            },
            {
                "hf_subset": "fra-eng",
                "languages": [
                    "fra-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.24600000000000002,
                "f1": 0.21527732683982684,
                "precision": 0.20460911398969853,
                "recall": 0.24600000000000002,
                "main_score": 0.21527732683982684
            },
            {
                "hf_subset": "por-eng",
                "languages": [
                    "por-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.234,
                "f1": 0.18861948871033607,
                "precision": 0.17469730524988158,
                "recall": 0.234,
                "main_score": 0.18861948871033607
            },
            {
                "hf_subset": "tat-eng",
                "languages": [
                    "tat-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.013000000000000001,
                "f1": 0.008081609699284276,
                "precision": 0.008041232161030669,
                "recall": 0.013000000000000001,
                "main_score": 0.008081609699284276
            },
            {
                "hf_subset": "oci-eng",
                "languages": [
                    "oci-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.144,
                "f1": 0.11982642360594899,
                "precision": 0.11423911681034546,
                "recall": 0.144,
                "main_score": 0.11982642360594899
            },
            {
                "hf_subset": "pol-eng",
                "languages": [
                    "pol-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.087,
                "f1": 0.06565099922088448,
                "precision": 0.060099608063946305,
                "recall": 0.087,
                "main_score": 0.06565099922088448
            },
            {
                "hf_subset": "war-eng",
                "languages": [
                    "war-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.071,
                "f1": 0.054832441160532853,
                "precision": 0.050803667581084205,
                "recall": 0.071,
                "main_score": 0.054832441160532853
            },
            {
                "hf_subset": "aze-eng",
                "languages": [
                    "aze-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.044,
                "f1": 0.032643948695904146,
                "precision": 0.03031506651474311,
                "recall": 0.044,
                "main_score": 0.032643948695904146
            },
            {
                "hf_subset": "vie-eng",
                "languages": [
                    "vie-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.071,
                "f1": 0.05278776676539834,
                "precision": 0.04883891459552525,
                "recall": 0.071,
                "main_score": 0.05278776676539834
            },
            {
                "hf_subset": "nno-eng",
                "languages": [
                    "nno-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.085,
                "f1": 0.07022436974789914,
                "precision": 0.06517919923571304,
                "recall": 0.085,
                "main_score": 0.07022436974789914
            },
            {
                "hf_subset": "cha-eng",
                "languages": [
                    "cha-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.17518248175182483,
                "f1": 0.14159211038143835,
                "precision": 0.13419131771033424,
                "recall": 0.17518248175182483,
                "main_score": 0.14159211038143835
            },
            {
                "hf_subset": "mhr-eng",
                "languages": [
                    "mhr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.003,
                "f1": 0.001008802791411487,
                "precision": 0.0010044111373948112,
                "recall": 0.003,
                "main_score": 0.001008802791411487
            },
            {
                "hf_subset": "dan-eng",
                "languages": [
                    "dan-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.113,
                "f1": 0.10064263107889398,
                "precision": 0.09714481189937882,
                "recall": 0.113,
                "main_score": 0.10064263107889398
            },
            {
                "hf_subset": "ell-eng",
                "languages": [
                    "ell-Grek",
                    "eng-Latn"
                ],
                "accuracy": 0.007000000000000001,
                "f1": 0.005023625310859353,
                "precision": 0.005011883541295306,
                "recall": 0.007000000000000001,
                "main_score": 0.005023625310859353
            },
            {
                "hf_subset": "amh-eng",
                "languages": [
                    "amh-Ethi",
                    "eng-Latn"
                ],
                "accuracy": 0.017857142857142856,
                "f1": 0.0067315005472387635,
                "precision": 0.006364087301587301,
                "recall": 0.017857142857142856,
                "main_score": 0.0067315005472387635
            },
            {
                "hf_subset": "pam-eng",
                "languages": [
                    "pam-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.07,
                "f1": 0.04850226809905071,
                "precision": 0.04354967218806848,
                "recall": 0.07,
                "main_score": 0.04850226809905071
            },
            {
                "hf_subset": "hsb-eng",
                "languages": [
                    "hsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.05383022774327122,
                "f1": 0.04080351427081423,
                "precision": 0.0374317711274233,
                "recall": 0.05383022774327122,
                "main_score": 0.04080351427081423
            },
            {
                "hf_subset": "srp-eng",
                "languages": [
                    "srp-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.039,
                "f1": 0.02975065835065835,
                "precision": 0.027082951373488765,
                "recall": 0.039,
                "main_score": 0.02975065835065835
            },
            {
                "hf_subset": "epo-eng",
                "languages": [
                    "epo-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.138,
                "f1": 0.10976459812917616,
                "precision": 0.10214566903851945,
                "recall": 0.138,
                "main_score": 0.10976459812917616
            },
            {
                "hf_subset": "kzj-eng",
                "languages": [
                    "kzj-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.049,
                "f1": 0.03599811209980933,
                "precision": 0.03391430386128988,
                "recall": 0.049,
                "main_score": 0.03599811209980933
            },
            {
                "hf_subset": "awa-eng",
                "languages": [
                    "awa-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.021645021645021644,
                "f1": 0.002896920567403394,
                "precision": 0.0016489313769797239,
                "recall": 0.021645021645021644,
                "main_score": 0.002896920567403394
            },
            {
                "hf_subset": "fao-eng",
                "languages": [
                    "fao-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.09541984732824428,
                "f1": 0.08129327179123026,
                "precision": 0.07860730567672362,
                "recall": 0.09541984732824428,
                "main_score": 0.08129327179123026
            },
            {
                "hf_subset": "mal-eng",
                "languages": [
                    "mal-Mlym",
                    "eng-Latn"
                ],
                "accuracy": 0.005822416302765648,
                "f1": 0.003960292169899156,
                "precision": 0.0036794436357755135,
                "recall": 0.005822416302765648,
                "main_score": 0.003960292169899156
            },
            {
                "hf_subset": "ile-eng",
                "languages": [
                    "ile-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.259,
                "f1": 0.20981622737697278,
                "precision": 0.19591031936732237,
                "recall": 0.259,
                "main_score": 0.20981622737697278
            },
            {
                "hf_subset": "bos-eng",
                "languages": [
                    "bos-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.09322033898305085,
                "f1": 0.07176463221173916,
                "precision": 0.06547619047619047,
                "recall": 0.09322033898305085,
                "main_score": 0.07176463221173916
            },
            {
                "hf_subset": "cor-eng",
                "languages": [
                    "cor-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.044,
                "f1": 0.030484795026022214,
                "precision": 0.028132647991077685,
                "recall": 0.044,
                "main_score": 0.030484795026022214
            },
            {
                "hf_subset": "cat-eng",
                "languages": [
                    "cat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.188,
                "f1": 0.1552276497119774,
                "precision": 0.1463296284434154,
                "recall": 0.188,
                "main_score": 0.1552276497119774
            },
            {
                "hf_subset": "eus-eng",
                "languages": [
                    "eus-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.1,
                "f1": 0.0735190130573739,
                "precision": 0.06759061952118554,
                "recall": 0.1,
                "main_score": 0.0735190130573739
            },
            {
                "hf_subset": "yue-eng",
                "languages": [
                    "yue-Hant",
                    "eng-Latn"
                ],
                "accuracy": 0.031,
                "f1": 0.02152743764172335,
                "precision": 0.020008336640383418,
                "recall": 0.031,
                "main_score": 0.02152743764172335
            },
            {
                "hf_subset": "swe-eng",
                "languages": [
                    "swe-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.106,
                "f1": 0.08471815215313618,
                "precision": 0.07942319409218233,
                "recall": 0.106,
                "main_score": 0.08471815215313618
            },
            {
                "hf_subset": "dtp-eng",
                "languages": [
                    "dtp-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.043,
                "f1": 0.027338036427188243,
                "precision": 0.02549226138483905,
                "recall": 0.043,
                "main_score": 0.027338036427188243
            },
            {
                "hf_subset": "kat-eng",
                "languages": [
                    "kat-Geor",
                    "eng-Latn"
                ],
                "accuracy": 0.004021447721179625,
                "f1": 0.0028150134048257375,
                "precision": 0.002751516861859743,
                "recall": 0.004021447721179625,
                "main_score": 0.0028150134048257375
            },
            {
                "hf_subset": "jpn-eng",
                "languages": [
                    "jpn-Jpan",
                    "eng-Latn"
                ],
                "accuracy": 0.03,
                "f1": 0.015834901411814404,
                "precision": 0.013894010894944847,
                "recall": 0.03,
                "main_score": 0.015834901411814404
            },
            {
                "hf_subset": "csb-eng",
                "languages": [
                    "csb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.07905138339920949,
                "f1": 0.06639704798109673,
                "precision": 0.0632664437012263,
                "recall": 0.07905138339920949,
                "main_score": 0.06639704798109673
            },
            {
                "hf_subset": "xho-eng",
                "languages": [
                    "xho-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.035211267605633804,
                "f1": 0.02173419196807775,
                "precision": 0.021438889748748902,
                "recall": 0.035211267605633804,
                "main_score": 0.02173419196807775
            },
            {
                "hf_subset": "orv-eng",
                "languages": [
                    "orv-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.0023952095808383233,
                "f1": 1.262128032547595e-05,
                "precision": 6.327654461278806e-06,
                "recall": 0.0023952095808383233,
                "main_score": 1.262128032547595e-05
            },
            {
                "hf_subset": "ind-eng",
                "languages": [
                    "ind-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.10400000000000001,
                "f1": 0.08370422351826372,
                "precision": 0.07943809523809522,
                "recall": 0.10400000000000001,
                "main_score": 0.08370422351826372
            },
            {
                "hf_subset": "tuk-eng",
                "languages": [
                    "tuk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.054187192118226604,
                "f1": 0.034763895108722694,
                "precision": 0.03133184624688218,
                "recall": 0.054187192118226604,
                "main_score": 0.034763895108722694
            },
            {
                "hf_subset": "max-eng",
                "languages": [
                    "max-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.0915492957746479,
                "f1": 0.07267458920187794,
                "precision": 0.06893803787858965,
                "recall": 0.0915492957746479,
                "main_score": 0.07267458920187794
            },
            {
                "hf_subset": "swh-eng",
                "languages": [
                    "swh-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.09487179487179487,
                "f1": 0.06902767160316073,
                "precision": 0.06450346503818517,
                "recall": 0.09487179487179487,
                "main_score": 0.06902767160316073
            },
            {
                "hf_subset": "hin-eng",
                "languages": [
                    "hin-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.001,
                "f1": 2.042900919305414e-06,
                "precision": 1.0224948875255625e-06,
                "recall": 0.001,
                "main_score": 2.042900919305414e-06
            },
            {
                "hf_subset": "dsb-eng",
                "languages": [
                    "dsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.05010438413361169,
                "f1": 0.03811664721450528,
                "precision": 0.035454644309619636,
                "recall": 0.05010438413361169,
                "main_score": 0.03811664721450528
            },
            {
                "hf_subset": "ber-eng",
                "languages": [
                    "ber-Tfng",
                    "eng-Latn"
                ],
                "accuracy": 0.062,
                "f1": 0.052131589154338694,
                "precision": 0.05080398110661268,
                "recall": 0.062,
                "main_score": 0.052131589154338694
            },
            {
                "hf_subset": "tam-eng",
                "languages": [
                    "tam-Taml",
                    "eng-Latn"
                ],
                "accuracy": 0.009771986970684038,
                "f1": 0.0050613881232773736,
                "precision": 0.004343105320304017,
                "recall": 0.009771986970684038,
                "main_score": 0.0050613881232773736
            },
            {
                "hf_subset": "slk-eng",
                "languages": [
                    "slk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.073,
                "f1": 0.05631318092102775,
                "precision": 0.05303887400540395,
                "recall": 0.073,
                "main_score": 0.05631318092102775
            },
            {
                "hf_subset": "tgl-eng",
                "languages": [
                    "tgl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.036,
                "f1": 0.032180089485458606,
                "precision": 0.03100675675675675,
                "recall": 0.036,
                "main_score": 0.032180089485458606
            },
            {
                "hf_subset": "ast-eng",
                "languages": [
                    "ast-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.2204724409448819,
                "f1": 0.1792525934258218,
                "precision": 0.1648251629836593,
                "recall": 0.2204724409448819,
                "main_score": 0.1792525934258218
            },
            {
                "hf_subset": "mkd-eng",
                "languages": [
                    "mkd-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.005,
                "f1": 0.0015437431862324142,
                "precision": 0.001355493357217495,
                "recall": 0.005,
                "main_score": 0.0015437431862324142
            },
            {
                "hf_subset": "khm-eng",
                "languages": [
                    "khm-Khmr",
                    "eng-Latn"
                ],
                "accuracy": 0.008310249307479225,
                "f1": 0.005102255597841558,
                "precision": 0.004859595744731704,
                "recall": 0.008310249307479225,
                "main_score": 0.005102255597841558
            },
            {
                "hf_subset": "ces-eng",
                "languages": [
                    "ces-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.069,
                "f1": 0.047258390633390636,
                "precision": 0.042883665702752795,
                "recall": 0.069,
                "main_score": 0.047258390633390636
            },
            {
                "hf_subset": "tzl-eng",
                "languages": [
                    "tzl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.17307692307692307,
                "f1": 0.14763313609467454,
                "precision": 0.14129273504273504,
                "recall": 0.17307692307692307,
                "main_score": 0.14763313609467454
            },
            {
                "hf_subset": "urd-eng",
                "languages": [
                    "urd-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.003,
                "f1": 2.2196828248667186e-05,
                "precision": 1.1148527298850575e-05,
                "recall": 0.003,
                "main_score": 2.2196828248667186e-05
            },
            {
                "hf_subset": "ara-eng",
                "languages": [
                    "ara-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.003,
                "f1": 0.003,
                "precision": 0.003,
                "recall": 0.003,
                "main_score": 0.003
            },
            {
                "hf_subset": "kor-eng",
                "languages": [
                    "kor-Hang",
                    "eng-Latn"
                ],
                "accuracy": 0.006,
                "f1": 0.00500206611570248,
                "precision": 0.005001034126163392,
                "recall": 0.006,
                "main_score": 0.00500206611570248
            },
            {
                "hf_subset": "yid-eng",
                "languages": [
                    "yid-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.0047169811320754715,
                "f1": 0.0029533776954177894,
                "precision": 0.0027542104596682284,
                "recall": 0.0047169811320754715,
                "main_score": 0.0029533776954177894
            },
            {
                "hf_subset": "fin-eng",
                "languages": [
                    "fin-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.044,
                "f1": 0.036228414442700155,
                "precision": 0.03431823899371069,
                "recall": 0.044,
                "main_score": 0.036228414442700155
            },
            {
                "hf_subset": "tha-eng",
                "languages": [
                    "tha-Thai",
                    "eng-Latn"
                ],
                "accuracy": 0.012773722627737228,
                "f1": 0.010043318098096731,
                "precision": 0.009735777358593729,
                "recall": 0.012773722627737228,
                "main_score": 0.010043318098096731
            },
            {
                "hf_subset": "wuu-eng",
                "languages": [
                    "wuu-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.039,
                "f1": 0.026164533097276226,
                "precision": 0.023558186153594084,
                "recall": 0.039,
                "main_score": 0.026164533097276226
            }
        ]
    }
}