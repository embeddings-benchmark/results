{
  "dataset_revision": "2fe05ee6b5832cda29f2ef7aaad7b7fe6a3609eb",
  "task_name": "HeadlineClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.753857,
        "f1": 0.753436,
        "f1_weighted": 0.753445,
        "scores_per_experiment": [
          {
            "accuracy": 0.74707,
            "f1": 0.748379,
            "f1_weighted": 0.748393
          },
          {
            "accuracy": 0.737793,
            "f1": 0.737243,
            "f1_weighted": 0.73726
          },
          {
            "accuracy": 0.757812,
            "f1": 0.757964,
            "f1_weighted": 0.757962
          },
          {
            "accuracy": 0.756836,
            "f1": 0.756595,
            "f1_weighted": 0.756601
          },
          {
            "accuracy": 0.777832,
            "f1": 0.779054,
            "f1_weighted": 0.779056
          },
          {
            "accuracy": 0.759277,
            "f1": 0.758702,
            "f1_weighted": 0.75871
          },
          {
            "accuracy": 0.759766,
            "f1": 0.75935,
            "f1_weighted": 0.759331
          },
          {
            "accuracy": 0.737305,
            "f1": 0.736951,
            "f1_weighted": 0.736967
          },
          {
            "accuracy": 0.723145,
            "f1": 0.718922,
            "f1_weighted": 0.718963
          },
          {
            "accuracy": 0.781738,
            "f1": 0.781197,
            "f1_weighted": 0.78121
          }
        ],
        "main_score": 0.753857,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 8.801976442337036,
  "kg_co2_emissions": 0.0003125310876736363
}