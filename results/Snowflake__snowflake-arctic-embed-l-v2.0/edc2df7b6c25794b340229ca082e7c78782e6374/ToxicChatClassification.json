{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.775258,
        "f1": 0.658052,
        "f1_weighted": 0.80673,
        "ap": 0.28329,
        "ap_weighted": 0.28329,
        "scores_per_experiment": [
          {
            "accuracy": 0.639175,
            "f1": 0.544803,
            "f1_weighted": 0.700072,
            "ap": 0.194197,
            "ap_weighted": 0.194197
          },
          {
            "accuracy": 0.868557,
            "f1": 0.735875,
            "f1_weighted": 0.876116,
            "ap": 0.352474,
            "ap_weighted": 0.352474
          },
          {
            "accuracy": 0.843643,
            "f1": 0.724207,
            "f1_weighted": 0.86017,
            "ap": 0.349671,
            "ap_weighted": 0.349671
          },
          {
            "accuracy": 0.805842,
            "f1": 0.694854,
            "f1_weighted": 0.832719,
            "ap": 0.325858,
            "ap_weighted": 0.325858
          },
          {
            "accuracy": 0.686426,
            "f1": 0.564992,
            "f1_weighted": 0.737172,
            "ap": 0.190244,
            "ap_weighted": 0.190244
          },
          {
            "accuracy": 0.831615,
            "f1": 0.70417,
            "f1_weighted": 0.849631,
            "ap": 0.318353,
            "ap_weighted": 0.318353
          },
          {
            "accuracy": 0.812715,
            "f1": 0.680972,
            "f1_weighted": 0.834554,
            "ap": 0.289008,
            "ap_weighted": 0.289008
          },
          {
            "accuracy": 0.730241,
            "f1": 0.634387,
            "f1_weighted": 0.774629,
            "ap": 0.278106,
            "ap_weighted": 0.278106
          },
          {
            "accuracy": 0.790378,
            "f1": 0.659295,
            "f1_weighted": 0.817611,
            "ap": 0.267169,
            "ap_weighted": 0.267169
          },
          {
            "accuracy": 0.743986,
            "f1": 0.636963,
            "f1_weighted": 0.784628,
            "ap": 0.267816,
            "ap_weighted": 0.267816
          }
        ],
        "main_score": 0.775258,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 12.992612361907959,
  "kg_co2_emissions": 0.0005274996857000259
}