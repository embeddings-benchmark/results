{
  "dataset_revision": "9e9b1f8ef51616073f47f306f7f47dd91663f86a",
  "evaluation_time": 2.8873653411865234,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test.full": [
      {
        "cosine_accuracy": 0.6997549019607843,
        "cosine_accuracy_threshold": 0.6995484828948975,
        "cosine_ap": 0.8151905187474621,
        "cosine_f1": 0.8015748031496063,
        "cosine_f1_threshold": 0.5972793102264404,
        "cosine_precision": 0.6915760869565217,
        "cosine_recall": 0.9531835205992509,
        "dot_accuracy": 0.696078431372549,
        "dot_accuracy_threshold": 273.5108947753906,
        "dot_ap": 0.7940879116167047,
        "dot_f1": 0.8020069471246623,
        "dot_f1_threshold": 212.98458862304688,
        "dot_precision": 0.6822061720288903,
        "dot_recall": 0.9728464419475655,
        "euclidean_accuracy": 0.7003676470588235,
        "euclidean_accuracy_threshold": 15.098949432373047,
        "euclidean_ap": 0.8173675160336831,
        "euclidean_f1": 0.8031809145129225,
        "euclidean_f1_threshold": 16.961246490478516,
        "euclidean_precision": 0.6979958534899793,
        "euclidean_recall": 0.9456928838951311,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.817635400644728,
        "manhattan_accuracy": 0.7003676470588235,
        "manhattan_accuracy_threshold": 326.52447509765625,
        "manhattan_ap": 0.817635400644728,
        "manhattan_f1": 0.8023856858846918,
        "manhattan_f1_threshold": 370.8173828125,
        "manhattan_precision": 0.6973047684865239,
        "manhattan_recall": 0.9447565543071161,
        "max_ap": 0.817635400644728,
        "max_f1": 0.8031809145129225,
        "max_precision": 0.6979958534899793,
        "max_recall": 0.9728464419475655,
        "similarity_accuracy": 0.6997549019607843,
        "similarity_accuracy_threshold": 0.6995484828948975,
        "similarity_ap": 0.8151902265266875,
        "similarity_f1": 0.8015748031496063,
        "similarity_f1_threshold": 0.5972793102264404,
        "similarity_precision": 0.6915760869565217,
        "similarity_recall": 0.9531835205992509
      }
    ],
    "validation.full": [
      {
        "cosine_accuracy": 0.7158948685857321,
        "cosine_accuracy_threshold": 0.7145564556121826,
        "cosine_ap": 0.8119802920716188,
        "cosine_f1": 0.7991836734693878,
        "cosine_f1_threshold": 0.612188458442688,
        "cosine_precision": 0.6846153846153846,
        "cosine_recall": 0.9598039215686275,
        "dot_accuracy": 0.7077596996245307,
        "dot_accuracy_threshold": 274.74334716796875,
        "dot_ap": 0.7902172049583858,
        "dot_f1": 0.7983505154639174,
        "dot_f1_threshold": 236.37034606933594,
        "dot_precision": 0.6889679715302491,
        "dot_recall": 0.9490196078431372,
        "euclidean_accuracy": 0.7152690863579474,
        "euclidean_accuracy_threshold": 13.534857749938965,
        "euclidean_ap": 0.8137567272521331,
        "euclidean_f1": 0.7977381470204437,
        "euclidean_f1_threshold": 15.376288414001465,
        "euclidean_precision": 0.7169663799843627,
        "euclidean_recall": 0.8990196078431373,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.8137567272521331,
        "manhattan_accuracy": 0.7177722152690864,
        "manhattan_accuracy_threshold": 313.6429748535156,
        "manhattan_ap": 0.8131344582395055,
        "manhattan_f1": 0.7973962571196095,
        "manhattan_f1_threshold": 373.08160400390625,
        "manhattan_precision": 0.6815020862308763,
        "manhattan_recall": 0.9607843137254902,
        "max_ap": 0.8137567272521331,
        "max_f1": 0.7991836734693878,
        "max_precision": 0.7169663799843627,
        "max_recall": 0.9607843137254902,
        "similarity_accuracy": 0.7158948685857321,
        "similarity_accuracy_threshold": 0.7145565748214722,
        "similarity_ap": 0.8119802920716188,
        "similarity_f1": 0.7991836734693878,
        "similarity_f1_threshold": 0.6121885776519775,
        "similarity_precision": 0.6846153846153846,
        "similarity_recall": 0.9598039215686275
      }
    ]
  },
  "task_name": "OpusparcusPC"
}