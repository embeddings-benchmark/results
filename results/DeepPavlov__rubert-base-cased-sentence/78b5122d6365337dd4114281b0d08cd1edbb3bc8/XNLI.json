{
  "dataset_revision": "09698e0180d87dc247ca447d3a1248b931ac0cdb",
  "evaluation_time": 3.5486960411071777,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.7318681318681318,
        "cosine_accuracy_threshold": 0.721663236618042,
        "cosine_ap": 0.7978780229673998,
        "cosine_f1": 0.7514450867052023,
        "cosine_f1_threshold": 0.6079367399215698,
        "cosine_precision": 0.6685714285714286,
        "cosine_recall": 0.8577712609970675,
        "dot_accuracy": 0.7274725274725274,
        "dot_accuracy_threshold": 257.8810729980469,
        "dot_ap": 0.7919675418856027,
        "dot_f1": 0.7442472057856674,
        "dot_f1_threshold": 224.49407958984375,
        "dot_precision": 0.6746126340882003,
        "dot_recall": 0.8299120234604106,
        "euclidean_accuracy": 0.7355311355311356,
        "euclidean_accuracy_threshold": 15.526878356933594,
        "euclidean_ap": 0.798237659858158,
        "euclidean_f1": 0.7551282051282051,
        "euclidean_f1_threshold": 16.866817474365234,
        "euclidean_precision": 0.6708428246013668,
        "euclidean_recall": 0.8636363636363636,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.798237659858158,
        "manhattan_accuracy": 0.7347985347985349,
        "manhattan_accuracy_threshold": 335.6731872558594,
        "manhattan_ap": 0.7982375560821562,
        "manhattan_f1": 0.7529706066291431,
        "manhattan_f1_threshold": 373.5480651855469,
        "manhattan_precision": 0.6564885496183206,
        "manhattan_recall": 0.8826979472140762,
        "max_ap": 0.798237659858158,
        "max_f1": 0.7551282051282051,
        "max_precision": 0.6746126340882003,
        "max_recall": 0.8826979472140762,
        "similarity_accuracy": 0.7318681318681318,
        "similarity_accuracy_threshold": 0.7216631770133972,
        "similarity_ap": 0.7978780229673998,
        "similarity_f1": 0.7514450867052023,
        "similarity_f1_threshold": 0.607936680316925,
        "similarity_precision": 0.6685714285714286,
        "similarity_recall": 0.8577712609970675
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.8153846153846154,
        "cosine_accuracy_threshold": 0.7090477347373962,
        "cosine_ap": 0.901069744070188,
        "cosine_f1": 0.8199445983379501,
        "cosine_f1_threshold": 0.649490237236023,
        "cosine_precision": 0.7769028871391076,
        "cosine_recall": 0.8680351906158358,
        "dot_accuracy": 0.8124542124542125,
        "dot_accuracy_threshold": 258.4656982421875,
        "dot_ap": 0.8942880974984475,
        "dot_f1": 0.8176870748299319,
        "dot_f1_threshold": 232.702880859375,
        "dot_precision": 0.7626903553299492,
        "dot_recall": 0.8812316715542522,
        "euclidean_accuracy": 0.8168498168498168,
        "euclidean_accuracy_threshold": 14.808027267456055,
        "euclidean_ap": 0.9011770543184173,
        "euclidean_f1": 0.8198324022346368,
        "euclidean_f1_threshold": 15.798379898071289,
        "euclidean_precision": 0.7826666666666666,
        "euclidean_recall": 0.8607038123167156,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.901355839534422,
        "manhattan_accuracy": 0.8153846153846154,
        "manhattan_accuracy_threshold": 308.67633056640625,
        "manhattan_ap": 0.901355839534422,
        "manhattan_f1": 0.8184357541899442,
        "manhattan_f1_threshold": 342.5804443359375,
        "manhattan_precision": 0.7813333333333333,
        "manhattan_recall": 0.8592375366568915,
        "max_ap": 0.901355839534422,
        "max_f1": 0.8199445983379501,
        "max_precision": 0.7826666666666666,
        "max_recall": 0.8812316715542522,
        "similarity_accuracy": 0.8153846153846154,
        "similarity_accuracy_threshold": 0.709047794342041,
        "similarity_ap": 0.9010700063735261,
        "similarity_f1": 0.8199445983379501,
        "similarity_f1_threshold": 0.6494902968406677,
        "similarity_precision": 0.7769028871391076,
        "similarity_recall": 0.8680351906158358
      }
    ]
  },
  "task_name": "XNLI"
}