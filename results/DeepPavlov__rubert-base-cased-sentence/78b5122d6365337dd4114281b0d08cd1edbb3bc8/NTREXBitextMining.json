{
  "dataset_revision": "ed9a4403ed4adbfaf4aab56d5b2709e9f6c3ba33",
  "evaluation_time": 130.51930284500122,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test": [
      {
        "accuracy": 0.0025037556334501754,
        "f1": 0.0013179943074785336,
        "hf_subset": "arb_Arab-rus_Cyrl",
        "languages": [
          "arb-Arab",
          "rus-Cyrl"
        ],
        "main_score": 0.0013179943074785336,
        "precision": 0.0011883838830140637,
        "recall": 0.0025037556334501754
      },
      {
        "accuracy": 0.11617426139208813,
        "f1": 0.08560409687098947,
        "hf_subset": "bel_Cyrl-rus_Cyrl",
        "languages": [
          "bel-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.08560409687098947,
        "precision": 0.07868385634076365,
        "recall": 0.11617426139208813
      },
      {
        "accuracy": 0.00100150225338007,
        "f1": 0.00013512331990048565,
        "hf_subset": "ben_Beng-rus_Cyrl",
        "languages": [
          "ben-Beng",
          "rus-Cyrl"
        ],
        "main_score": 0.00013512331990048565,
        "precision": 7.480733295064548e-05,
        "recall": 0.00100150225338007
      },
      {
        "accuracy": 0.04156234351527291,
        "f1": 0.026042707966073483,
        "hf_subset": "bos_Latn-rus_Cyrl",
        "languages": [
          "bos-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.026042707966073483,
        "precision": 0.023634005736267658,
        "recall": 0.04156234351527291
      },
      {
        "accuracy": 0.24236354531797696,
        "f1": 0.19352488762888995,
        "hf_subset": "bul_Cyrl-rus_Cyrl",
        "languages": [
          "bul-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.19352488762888995,
        "precision": 0.17981636471245885,
        "recall": 0.24236354531797696
      },
      {
        "accuracy": 0.04306459689534301,
        "f1": 0.026315002137538828,
        "hf_subset": "ces_Latn-rus_Cyrl",
        "languages": [
          "ces-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.026315002137538828,
        "precision": 0.023326609610116737,
        "recall": 0.04306459689534301
      },
      {
        "accuracy": 0.07461191787681522,
        "f1": 0.05267352271905816,
        "hf_subset": "deu_Latn-rus_Cyrl",
        "languages": [
          "deu-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.05267352271905816,
        "precision": 0.047339580683969776,
        "recall": 0.07461191787681522
      },
      {
        "accuracy": 0.010015022533800702,
        "f1": 0.00595433668464267,
        "hf_subset": "ell_Grek-rus_Cyrl",
        "languages": [
          "ell-Grek",
          "rus-Cyrl"
        ],
        "main_score": 0.00595433668464267,
        "precision": 0.005329230812750209,
        "recall": 0.010015022533800702
      },
      {
        "accuracy": 0.3189784677015523,
        "f1": 0.2594288185525041,
        "hf_subset": "eng_Latn-rus_Cyrl",
        "languages": [
          "eng-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.2594288185525041,
        "precision": 0.2403787899994166,
        "recall": 0.3189784677015523
      },
      {
        "accuracy": 0.00400600901352028,
        "f1": 0.003314495552853089,
        "hf_subset": "fas_Arab-rus_Cyrl",
        "languages": [
          "fas-Arab",
          "rus-Cyrl"
        ],
        "main_score": 0.003314495552853089,
        "precision": 0.003087965281255216,
        "recall": 0.00400600901352028
      },
      {
        "accuracy": 0.01902854281422133,
        "f1": 0.01073918334236548,
        "hf_subset": "fin_Latn-rus_Cyrl",
        "languages": [
          "fin-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.01073918334236548,
        "precision": 0.009615193255619313,
        "recall": 0.01902854281422133
      },
      {
        "accuracy": 0.08412618928392589,
        "f1": 0.05651789273647316,
        "hf_subset": "fra_Latn-rus_Cyrl",
        "languages": [
          "fra-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.05651789273647316,
        "precision": 0.05067755004737493,
        "recall": 0.08412618928392589
      },
      {
        "accuracy": 0.0030045067601402104,
        "f1": 0.0013866842010046916,
        "hf_subset": "heb_Hebr-rus_Cyrl",
        "languages": [
          "heb-Hebr",
          "rus-Cyrl"
        ],
        "main_score": 0.0013866842010046916,
        "precision": 0.0011357701648046828,
        "recall": 0.0030045067601402104
      },
      {
        "accuracy": 0.006509764646970456,
        "f1": 0.003169089196377903,
        "hf_subset": "hin_Deva-rus_Cyrl",
        "languages": [
          "hin-Deva",
          "rus-Cyrl"
        ],
        "main_score": 0.003169089196377903,
        "precision": 0.002806223369835332,
        "recall": 0.006509764646970456
      },
      {
        "accuracy": 0.03905858788182273,
        "f1": 0.02254767583481242,
        "hf_subset": "hrv_Latn-rus_Cyrl",
        "languages": [
          "hrv-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.02254767583481242,
        "precision": 0.0197625710411184,
        "recall": 0.03905858788182273
      },
      {
        "accuracy": 0.01902854281422133,
        "f1": 0.01231615429750857,
        "hf_subset": "hun_Latn-rus_Cyrl",
        "languages": [
          "hun-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.01231615429750857,
        "precision": 0.011038653289529602,
        "recall": 0.01902854281422133
      },
      {
        "accuracy": 0.0385578367551327,
        "f1": 0.023111488254823627,
        "hf_subset": "ind_Latn-rus_Cyrl",
        "languages": [
          "ind-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.023111488254823627,
        "precision": 0.02002716368798571,
        "recall": 0.0385578367551327
      },
      {
        "accuracy": 0.04006009013520281,
        "f1": 0.024338323227546196,
        "hf_subset": "jpn_Jpan-rus_Cyrl",
        "languages": [
          "jpn-Jpan",
          "rus-Cyrl"
        ],
        "main_score": 0.024338323227546196,
        "precision": 0.02119984017444684,
        "recall": 0.04006009013520281
      },
      {
        "accuracy": 0.008512769153730596,
        "f1": 0.003704932534183714,
        "hf_subset": "kor_Hang-rus_Cyrl",
        "languages": [
          "kor-Hang",
          "rus-Cyrl"
        ],
        "main_score": 0.003704932534183714,
        "precision": 0.003313118203775344,
        "recall": 0.008512769153730596
      },
      {
        "accuracy": 0.02503755633450175,
        "f1": 0.01340588727805347,
        "hf_subset": "lit_Latn-rus_Cyrl",
        "languages": [
          "lit-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.01340588727805347,
        "precision": 0.011325396839796156,
        "recall": 0.02503755633450175
      },
      {
        "accuracy": 0.15022533800701052,
        "f1": 0.11105230574904901,
        "hf_subset": "mkd_Cyrl-rus_Cyrl",
        "languages": [
          "mkd-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.11105230574904901,
        "precision": 0.1020490083059729,
        "recall": 0.15022533800701052
      },
      {
        "accuracy": 0.05207811717576365,
        "f1": 0.03414812083855493,
        "hf_subset": "nld_Latn-rus_Cyrl",
        "languages": [
          "nld-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.03414812083855493,
        "precision": 0.030634282863147313,
        "recall": 0.05207811717576365
      },
      {
        "accuracy": 0.04006009013520281,
        "f1": 0.025115099389010253,
        "hf_subset": "pol_Latn-rus_Cyrl",
        "languages": [
          "pol-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.025115099389010253,
        "precision": 0.022406128115088107,
        "recall": 0.04006009013520281
      },
      {
        "accuracy": 0.05658487731597396,
        "f1": 0.034320080273267016,
        "hf_subset": "por_Latn-rus_Cyrl",
        "languages": [
          "por-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.034320080273267016,
        "precision": 0.029924178471268582,
        "recall": 0.05658487731597396
      },
      {
        "accuracy": 0.007010515773660491,
        "f1": 0.0016508925408614233,
        "hf_subset": "rus_Cyrl-arb_Arab",
        "languages": [
          "rus-Cyrl",
          "arb-Arab"
        ],
        "main_score": 0.0016508925408614233,
        "precision": 0.001268129656880305,
        "recall": 0.007010515773660491
      },
      {
        "accuracy": 0.1412118177265899,
        "f1": 0.08619823450363859,
        "hf_subset": "rus_Cyrl-bel_Cyrl",
        "languages": [
          "rus-Cyrl",
          "bel-Cyrl"
        ],
        "main_score": 0.08619823450363859,
        "precision": 0.07387894161804903,
        "recall": 0.1412118177265899
      },
      {
        "accuracy": 0.005007511266900351,
        "f1": 0.0007030706523654395,
        "hf_subset": "rus_Cyrl-ben_Beng",
        "languages": [
          "rus-Cyrl",
          "ben-Beng"
        ],
        "main_score": 0.0007030706523654395,
        "precision": 0.0004539112674077901,
        "recall": 0.005007511266900351
      },
      {
        "accuracy": 0.05157736604907361,
        "f1": 0.0277148682264356,
        "hf_subset": "rus_Cyrl-bos_Latn",
        "languages": [
          "rus-Cyrl",
          "bos-Latn"
        ],
        "main_score": 0.0277148682264356,
        "precision": 0.023922610297449635,
        "recall": 0.05157736604907361
      },
      {
        "accuracy": 0.22834251377065598,
        "f1": 0.16019156292663012,
        "hf_subset": "rus_Cyrl-bul_Cyrl",
        "languages": [
          "rus-Cyrl",
          "bul-Cyrl"
        ],
        "main_score": 0.16019156292663012,
        "precision": 0.14223646370616827,
        "recall": 0.22834251377065598
      },
      {
        "accuracy": 0.057085628442663995,
        "f1": 0.029824882365527838,
        "hf_subset": "rus_Cyrl-ces_Latn",
        "languages": [
          "rus-Cyrl",
          "ces-Latn"
        ],
        "main_score": 0.029824882365527838,
        "precision": 0.02501079679755718,
        "recall": 0.057085628442663995
      },
      {
        "accuracy": 0.09364046069103656,
        "f1": 0.05587311642090479,
        "hf_subset": "rus_Cyrl-deu_Latn",
        "languages": [
          "rus-Cyrl",
          "deu-Latn"
        ],
        "main_score": 0.05587311642090479,
        "precision": 0.04801838227784372,
        "recall": 0.09364046069103656
      },
      {
        "accuracy": 0.020530796194291438,
        "f1": 0.004992323560510147,
        "hf_subset": "rus_Cyrl-ell_Grek",
        "languages": [
          "rus-Cyrl",
          "ell-Grek"
        ],
        "main_score": 0.004992323560510147,
        "precision": 0.0033546159035919053,
        "recall": 0.020530796194291438
      },
      {
        "accuracy": 0.2969454181271908,
        "f1": 0.22689721163682103,
        "hf_subset": "rus_Cyrl-eng_Latn",
        "languages": [
          "rus-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.22689721163682103,
        "precision": 0.20586545014622487,
        "recall": 0.2969454181271908
      },
      {
        "accuracy": 0.012518778167250876,
        "f1": 0.002587801589414046,
        "hf_subset": "rus_Cyrl-fas_Arab",
        "languages": [
          "rus-Cyrl",
          "fas-Arab"
        ],
        "main_score": 0.002587801589414046,
        "precision": 0.0018272209271721154,
        "recall": 0.012518778167250876
      },
      {
        "accuracy": 0.033049574361542315,
        "f1": 0.013364336233417172,
        "hf_subset": "rus_Cyrl-fin_Latn",
        "languages": [
          "rus-Cyrl",
          "fin-Latn"
        ],
        "main_score": 0.013364336233417172,
        "precision": 0.010586946903758531,
        "recall": 0.033049574361542315
      },
      {
        "accuracy": 0.11266900350525788,
        "f1": 0.07538826960323335,
        "hf_subset": "rus_Cyrl-fra_Latn",
        "languages": [
          "rus-Cyrl",
          "fra-Latn"
        ],
        "main_score": 0.07538826960323335,
        "precision": 0.06671414452866653,
        "recall": 0.11266900350525788
      },
      {
        "accuracy": 0.010515773660490736,
        "f1": 0.002122564592489759,
        "hf_subset": "rus_Cyrl-heb_Hebr",
        "languages": [
          "rus-Cyrl",
          "heb-Hebr"
        ],
        "main_score": 0.002122564592489759,
        "precision": 0.0015139203253668244,
        "recall": 0.010515773660490736
      },
      {
        "accuracy": 0.013520280420630946,
        "f1": 0.003010605358702607,
        "hf_subset": "rus_Cyrl-hin_Deva",
        "languages": [
          "rus-Cyrl",
          "hin-Deva"
        ],
        "main_score": 0.003010605358702607,
        "precision": 0.0019649484762071334,
        "recall": 0.013520280420630946
      },
      {
        "accuracy": 0.04807210816224337,
        "f1": 0.02422196655709608,
        "hf_subset": "rus_Cyrl-hrv_Latn",
        "languages": [
          "rus-Cyrl",
          "hrv-Latn"
        ],
        "main_score": 0.02422196655709608,
        "precision": 0.020004267597236555,
        "recall": 0.04807210816224337
      },
      {
        "accuracy": 0.02704056084126189,
        "f1": 0.010559393823594719,
        "hf_subset": "rus_Cyrl-hun_Latn",
        "languages": [
          "rus-Cyrl",
          "hun-Latn"
        ],
        "main_score": 0.010559393823594719,
        "precision": 0.008172091327218285,
        "recall": 0.02704056084126189
      },
      {
        "accuracy": 0.0385578367551327,
        "f1": 0.019185568417353367,
        "hf_subset": "rus_Cyrl-ind_Latn",
        "languages": [
          "rus-Cyrl",
          "ind-Latn"
        ],
        "main_score": 0.019185568417353367,
        "precision": 0.015617126721705248,
        "recall": 0.0385578367551327
      },
      {
        "accuracy": 0.05508262393590386,
        "f1": 0.03246761634480878,
        "hf_subset": "rus_Cyrl-jpn_Jpan",
        "languages": [
          "rus-Cyrl",
          "jpn-Jpan"
        ],
        "main_score": 0.03246761634480878,
        "precision": 0.028326843717563853,
        "recall": 0.05508262393590386
      },
      {
        "accuracy": 0.017526289434151226,
        "f1": 0.005212887785128176,
        "hf_subset": "rus_Cyrl-kor_Hang",
        "languages": [
          "rus-Cyrl",
          "kor-Hang"
        ],
        "main_score": 0.005212887785128176,
        "precision": 0.004002849459814809,
        "recall": 0.017526289434151226
      },
      {
        "accuracy": 0.03655483224837256,
        "f1": 0.014191439362027925,
        "hf_subset": "rus_Cyrl-lit_Latn",
        "languages": [
          "rus-Cyrl",
          "lit-Latn"
        ],
        "main_score": 0.014191439362027925,
        "precision": 0.010837365499540215,
        "recall": 0.03655483224837256
      },
      {
        "accuracy": 0.15923885828743115,
        "f1": 0.10379270887946994,
        "hf_subset": "rus_Cyrl-mkd_Cyrl",
        "languages": [
          "rus-Cyrl",
          "mkd-Cyrl"
        ],
        "main_score": 0.10379270887946994,
        "precision": 0.09096190061322727,
        "recall": 0.15923885828743115
      },
      {
        "accuracy": 0.06509764646970456,
        "f1": 0.033954160962565566,
        "hf_subset": "rus_Cyrl-nld_Latn",
        "languages": [
          "rus-Cyrl",
          "nld-Latn"
        ],
        "main_score": 0.033954160962565566,
        "precision": 0.028302775969701183,
        "recall": 0.06509764646970456
      },
      {
        "accuracy": 0.06209313970956434,
        "f1": 0.03207502471462325,
        "hf_subset": "rus_Cyrl-pol_Latn",
        "languages": [
          "rus-Cyrl",
          "pol-Latn"
        ],
        "main_score": 0.03207502471462325,
        "precision": 0.026957542134610747,
        "recall": 0.06209313970956434
      },
      {
        "accuracy": 0.07060590886329494,
        "f1": 0.03887893071214089,
        "hf_subset": "rus_Cyrl-por_Latn",
        "languages": [
          "rus-Cyrl",
          "por-Latn"
        ],
        "main_score": 0.03887893071214089,
        "precision": 0.032855820138022566,
        "recall": 0.07060590886329494
      },
      {
        "accuracy": 0.04957436154231347,
        "f1": 0.02608873189544506,
        "hf_subset": "rus_Cyrl-slk_Latn",
        "languages": [
          "rus-Cyrl",
          "slk-Latn"
        ],
        "main_score": 0.02608873189544506,
        "precision": 0.021534252993881956,
        "recall": 0.04957436154231347
      },
      {
        "accuracy": 0.04707060590886329,
        "f1": 0.02558142424801797,
        "hf_subset": "rus_Cyrl-slv_Latn",
        "languages": [
          "rus-Cyrl",
          "slv-Latn"
        ],
        "main_score": 0.02558142424801797,
        "precision": 0.021651063944062174,
        "recall": 0.04707060590886329
      },
      {
        "accuracy": 0.0971457185778668,
        "f1": 0.06053947838103267,
        "hf_subset": "rus_Cyrl-spa_Latn",
        "languages": [
          "rus-Cyrl",
          "spa-Latn"
        ],
        "main_score": 0.06053947838103267,
        "precision": 0.053356616756771405,
        "recall": 0.0971457185778668
      },
      {
        "accuracy": 0.11767651477215824,
        "f1": 0.06904093766446831,
        "hf_subset": "rus_Cyrl-srp_Cyrl",
        "languages": [
          "rus-Cyrl",
          "srp-Cyrl"
        ],
        "main_score": 0.06904093766446831,
        "precision": 0.05915033085959034,
        "recall": 0.11767651477215824
      },
      {
        "accuracy": 0.037055583375062595,
        "f1": 0.020213380264170737,
        "hf_subset": "rus_Cyrl-srp_Latn",
        "languages": [
          "rus-Cyrl",
          "srp-Latn"
        ],
        "main_score": 0.020213380264170737,
        "precision": 0.017117177549751627,
        "recall": 0.037055583375062595
      },
      {
        "accuracy": 0.026539809714571858,
        "f1": 0.009335225821349078,
        "hf_subset": "rus_Cyrl-swa_Latn",
        "languages": [
          "rus-Cyrl",
          "swa-Latn"
        ],
        "main_score": 0.009335225821349078,
        "precision": 0.006905683272892863,
        "recall": 0.026539809714571858
      },
      {
        "accuracy": 0.06159238858287431,
        "f1": 0.03426743445804571,
        "hf_subset": "rus_Cyrl-swe_Latn",
        "languages": [
          "rus-Cyrl",
          "swe-Latn"
        ],
        "main_score": 0.03426743445804571,
        "precision": 0.028778232640443546,
        "recall": 0.06159238858287431
      },
      {
        "accuracy": 0.016524787180771158,
        "f1": 0.005192999927308365,
        "hf_subset": "rus_Cyrl-tam_Taml",
        "languages": [
          "rus-Cyrl",
          "tam-Taml"
        ],
        "main_score": 0.005192999927308365,
        "precision": 0.003894477782180062,
        "recall": 0.016524787180771158
      },
      {
        "accuracy": 0.031547320981472206,
        "f1": 0.012945178560953936,
        "hf_subset": "rus_Cyrl-tur_Latn",
        "languages": [
          "rus-Cyrl",
          "tur-Latn"
        ],
        "main_score": 0.012945178560953936,
        "precision": 0.010364234755704662,
        "recall": 0.031547320981472206
      },
      {
        "accuracy": 0.6049073610415624,
        "f1": 0.5362651842121047,
        "hf_subset": "rus_Cyrl-ukr_Cyrl",
        "languages": [
          "rus-Cyrl",
          "ukr-Cyrl"
        ],
        "main_score": 0.5362651842121047,
        "precision": 0.509820882116826,
        "recall": 0.6049073610415624
      },
      {
        "accuracy": 0.03755633450175263,
        "f1": 0.01622737627353417,
        "hf_subset": "rus_Cyrl-vie_Latn",
        "languages": [
          "rus-Cyrl",
          "vie-Latn"
        ],
        "main_score": 0.01622737627353417,
        "precision": 0.013095737377621664,
        "recall": 0.03755633450175263
      },
      {
        "accuracy": 0.12418627941912869,
        "f1": 0.08264722610936151,
        "hf_subset": "rus_Cyrl-zho_Hant",
        "languages": [
          "rus-Cyrl",
          "zho-Hant"
        ],
        "main_score": 0.08264722610936151,
        "precision": 0.0736382432900652,
        "recall": 0.12418627941912869
      },
      {
        "accuracy": 0.03405107661492238,
        "f1": 0.01637901368046474,
        "hf_subset": "rus_Cyrl-zul_Latn",
        "languages": [
          "rus-Cyrl",
          "zul-Latn"
        ],
        "main_score": 0.01637901368046474,
        "precision": 0.01328731946317104,
        "recall": 0.03405107661492238
      },
      {
        "accuracy": 0.03455182774161242,
        "f1": 0.021798902095658535,
        "hf_subset": "slk_Latn-rus_Cyrl",
        "languages": [
          "slk-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.021798902095658535,
        "precision": 0.019476724843869784,
        "recall": 0.03455182774161242
      },
      {
        "accuracy": 0.030545818728092138,
        "f1": 0.016263799500898398,
        "hf_subset": "slv_Latn-rus_Cyrl",
        "languages": [
          "slv-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.016263799500898398,
        "precision": 0.014321109342225531,
        "recall": 0.030545818728092138
      },
      {
        "accuracy": 0.0886329494241362,
        "f1": 0.05937973370022338,
        "hf_subset": "spa_Latn-rus_Cyrl",
        "languages": [
          "spa-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.05937973370022338,
        "precision": 0.05291017942771644,
        "recall": 0.0886329494241362
      },
      {
        "accuracy": 0.10465698547821732,
        "f1": 0.07764329517192557,
        "hf_subset": "srp_Cyrl-rus_Cyrl",
        "languages": [
          "srp-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.07764329517192557,
        "precision": 0.0711047291605629,
        "recall": 0.10465698547821732
      },
      {
        "accuracy": 0.026039058587881823,
        "f1": 0.015276894778106865,
        "hf_subset": "srp_Latn-rus_Cyrl",
        "languages": [
          "srp-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.015276894778106865,
        "precision": 0.013340202041391646,
        "recall": 0.026039058587881823
      },
      {
        "accuracy": 0.018527791687531298,
        "f1": 0.010424824796998461,
        "hf_subset": "swa_Latn-rus_Cyrl",
        "languages": [
          "swa-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.010424824796998461,
        "precision": 0.008764434853728933,
        "recall": 0.018527791687531298
      },
      {
        "accuracy": 0.05107661492238358,
        "f1": 0.03445151761066035,
        "hf_subset": "swe_Latn-rus_Cyrl",
        "languages": [
          "swe-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.03445151761066035,
        "precision": 0.030949922500436666,
        "recall": 0.05107661492238358
      },
      {
        "accuracy": 0.007010515773660491,
        "f1": 0.004528641229670691,
        "hf_subset": "tam_Taml-rus_Cyrl",
        "languages": [
          "tam-Taml",
          "rus-Cyrl"
        ],
        "main_score": 0.004528641229670691,
        "precision": 0.004085004471178118,
        "recall": 0.007010515773660491
      },
      {
        "accuracy": 0.020030045067601403,
        "f1": 0.01193193394521012,
        "hf_subset": "tur_Latn-rus_Cyrl",
        "languages": [
          "tur-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.01193193394521012,
        "precision": 0.01073118374584777,
        "recall": 0.020030045067601403
      },
      {
        "accuracy": 0.6349524286429644,
        "f1": 0.5705767506732111,
        "hf_subset": "ukr_Cyrl-rus_Cyrl",
        "languages": [
          "ukr-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.5705767506732111,
        "precision": 0.5451937827375984,
        "recall": 0.6349524286429644
      },
      {
        "accuracy": 0.022033049574361543,
        "f1": 0.014375460352092645,
        "hf_subset": "vie_Latn-rus_Cyrl",
        "languages": [
          "vie-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.014375460352092645,
        "precision": 0.01303634634515968,
        "recall": 0.022033049574361543
      },
      {
        "accuracy": 0.08913370055082624,
        "f1": 0.059478752343175886,
        "hf_subset": "zho_Hant-rus_Cyrl",
        "languages": [
          "zho-Hant",
          "rus-Cyrl"
        ],
        "main_score": 0.059478752343175886,
        "precision": 0.052991261068200464,
        "recall": 0.08913370055082624
      },
      {
        "accuracy": 0.030045067601402103,
        "f1": 0.02109277958972296,
        "hf_subset": "zul_Latn-rus_Cyrl",
        "languages": [
          "zul-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.02109277958972296,
        "precision": 0.01920007883953803,
        "recall": 0.030045067601402103
      }
    ]
  },
  "task_name": "NTREXBitextMining"
}