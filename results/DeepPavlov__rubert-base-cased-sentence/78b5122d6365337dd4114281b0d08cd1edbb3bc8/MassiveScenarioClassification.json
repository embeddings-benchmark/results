{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 13.361588478088379,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test": [
      {
        "accuracy": 0.5190652320107599,
        "f1": 0.5124325958490579,
        "f1_weighted": 0.5203918734888038,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.5190652320107599,
        "scores_per_experiment": [
          {
            "accuracy": 0.519502353732347,
            "f1": 0.5147438705932976,
            "f1_weighted": 0.5195715374761367
          },
          {
            "accuracy": 0.5474108944182918,
            "f1": 0.5345774195453905,
            "f1_weighted": 0.5497780182247273
          },
          {
            "accuracy": 0.4976462676529926,
            "f1": 0.4977707216833553,
            "f1_weighted": 0.49726319062870655
          },
          {
            "accuracy": 0.5144586415601883,
            "f1": 0.5003204965892647,
            "f1_weighted": 0.5157486692151518
          },
          {
            "accuracy": 0.5248823133826497,
            "f1": 0.5167344716223681,
            "f1_weighted": 0.5216721297810833
          },
          {
            "accuracy": 0.48957632817753866,
            "f1": 0.46978994953171405,
            "f1_weighted": 0.4870444066778294
          },
          {
            "accuracy": 0.5060524546065904,
            "f1": 0.5008643145058153,
            "f1_weighted": 0.5142671565079042
          },
          {
            "accuracy": 0.5184936112979153,
            "f1": 0.5237872622662593,
            "f1_weighted": 0.5182080765248135
          },
          {
            "accuracy": 0.5400134498991258,
            "f1": 0.5406240119840698,
            "f1_weighted": 0.5416281039645217
          },
          {
            "accuracy": 0.5326160053799597,
            "f1": 0.5251134401690432,
            "f1_weighted": 0.538737445887162
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5260698475159862,
        "f1": 0.5197008019442414,
        "f1_weighted": 0.5270474459680135,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.5260698475159862,
        "scores_per_experiment": [
          {
            "accuracy": 0.5376291195277915,
            "f1": 0.5303431106603894,
            "f1_weighted": 0.5356601544742582
          },
          {
            "accuracy": 0.5459911460895229,
            "f1": 0.5379725256776263,
            "f1_weighted": 0.5444766576786717
          },
          {
            "accuracy": 0.5125430398425972,
            "f1": 0.5089372338902659,
            "f1_weighted": 0.514470221445771
          },
          {
            "accuracy": 0.5204131824889326,
            "f1": 0.5035624055002148,
            "f1_weighted": 0.5229518625483328
          },
          {
            "accuracy": 0.5322183964584358,
            "f1": 0.5194798111190124,
            "f1_weighted": 0.5321658184938843
          },
          {
            "accuracy": 0.4928676832267585,
            "f1": 0.47965451930563385,
            "f1_weighted": 0.4929540679783836
          },
          {
            "accuracy": 0.5095917363502214,
            "f1": 0.5056929931135439,
            "f1_weighted": 0.5156263470552426
          },
          {
            "accuracy": 0.5204131824889326,
            "f1": 0.5254799091764499,
            "f1_weighted": 0.5208940709396926
          },
          {
            "accuracy": 0.5612395474667978,
            "f1": 0.5675637373310845,
            "f1_weighted": 0.5594419806082843
          },
          {
            "accuracy": 0.5277914412198721,
            "f1": 0.5183217736681927,
            "f1_weighted": 0.5318332784576133
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}