{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 23.62216067314148,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test": [
      {
        "accuracy": 0.4910221923335575,
        "f1": 0.4757738382038261,
        "f1_weighted": 0.4963015896774592,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.4910221923335575,
        "scores_per_experiment": [
          {
            "accuracy": 0.49394754539340957,
            "f1": 0.4797811667768657,
            "f1_weighted": 0.5019102822541397
          },
          {
            "accuracy": 0.50773369199731,
            "f1": 0.48073509676193604,
            "f1_weighted": 0.5143478095986224
          },
          {
            "accuracy": 0.49428379287155344,
            "f1": 0.4754002912312793,
            "f1_weighted": 0.49563686039774213
          },
          {
            "accuracy": 0.5100874243443174,
            "f1": 0.49007840229423544,
            "f1_weighted": 0.5159520206265407
          },
          {
            "accuracy": 0.4979825151311365,
            "f1": 0.4809556312849506,
            "f1_weighted": 0.5040966729557732
          },
          {
            "accuracy": 0.47679892400806995,
            "f1": 0.4660998330451964,
            "f1_weighted": 0.480007088768336
          },
          {
            "accuracy": 0.47579018157363817,
            "f1": 0.4687863126876313,
            "f1_weighted": 0.48517528788552633
          },
          {
            "accuracy": 0.4909213180901143,
            "f1": 0.4800171686696272,
            "f1_weighted": 0.4940106148184169
          },
          {
            "accuracy": 0.476462676529926,
            "f1": 0.4570473502562547,
            "f1_weighted": 0.4782889668493272
          },
          {
            "accuracy": 0.4862138533960995,
            "f1": 0.4788371290302845,
            "f1_weighted": 0.493590292620167
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5012297097884899,
        "f1": 0.4824159949934167,
        "f1_weighted": 0.5076378712308479,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.5012297097884899,
        "scores_per_experiment": [
          {
            "accuracy": 0.5007378258730939,
            "f1": 0.4862714692365137,
            "f1_weighted": 0.5109437147207612
          },
          {
            "accuracy": 0.5332021642892277,
            "f1": 0.5086319598837451,
            "f1_weighted": 0.5417362049647108
          },
          {
            "accuracy": 0.5159862272503689,
            "f1": 0.4912776886153672,
            "f1_weighted": 0.5210772414364236
          },
          {
            "accuracy": 0.5041810132808657,
            "f1": 0.48076447262981714,
            "f1_weighted": 0.5119722028654214
          },
          {
            "accuracy": 0.5027053615346778,
            "f1": 0.4786066083347024,
            "f1_weighted": 0.5104750353428377
          },
          {
            "accuracy": 0.4864731923266109,
            "f1": 0.46548189722711303,
            "f1_weighted": 0.4931208493092742
          },
          {
            "accuracy": 0.48007870142646336,
            "f1": 0.47152606074488174,
            "f1_weighted": 0.48728794039895595
          },
          {
            "accuracy": 0.48598130841121495,
            "f1": 0.46798544164049627,
            "f1_weighted": 0.4879220856254766
          },
          {
            "accuracy": 0.5066404328578455,
            "f1": 0.48505097437456407,
            "f1_weighted": 0.5072923612776746
          },
          {
            "accuracy": 0.4963108706345303,
            "f1": 0.4885633772469665,
            "f1_weighted": 0.5045510763669427
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}