{
  "dataset_revision": "5b7d477a8c62cdd18e2fed7e015497c20b4371ad",
  "evaluation_time": 1.7136945724487305,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.7487179487179487,
        "cosine_accuracy_threshold": 0.6759661436080933,
        "cosine_ap": 0.8013015622593761,
        "cosine_f1": 0.7613082811412666,
        "cosine_f1_threshold": 0.6676629781723022,
        "cosine_precision": 0.7245033112582782,
        "cosine_recall": 0.8020527859237536,
        "dot_accuracy": 0.736996336996337,
        "dot_accuracy_threshold": 242.19842529296875,
        "dot_ap": 0.7964635857915079,
        "dot_f1": 0.7572293207800942,
        "dot_f1_threshold": 233.89016723632812,
        "dot_precision": 0.6993788819875777,
        "dot_recall": 0.8255131964809385,
        "euclidean_accuracy": 0.7494505494505495,
        "euclidean_accuracy_threshold": 15.405463218688965,
        "euclidean_ap": 0.8016480657544596,
        "euclidean_f1": 0.7598314606741573,
        "euclidean_f1_threshold": 15.424232482910156,
        "euclidean_precision": 0.72911051212938,
        "euclidean_recall": 0.7932551319648093,
        "hf_subset": "russian",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.8016480657544596,
        "manhattan_accuracy": 0.7487179487179487,
        "manhattan_accuracy_threshold": 330.5122375488281,
        "manhattan_ap": 0.8011495500070853,
        "manhattan_f1": 0.7610497237569059,
        "manhattan_f1_threshold": 339.3514404296875,
        "manhattan_precision": 0.7193211488250653,
        "manhattan_recall": 0.8079178885630498,
        "max_ap": 0.8016480657544596,
        "max_f1": 0.7613082811412666,
        "max_precision": 0.72911051212938,
        "max_recall": 0.8255131964809385,
        "similarity_accuracy": 0.7487179487179487,
        "similarity_accuracy_threshold": 0.6759662628173828,
        "similarity_ap": 0.8013015622593761,
        "similarity_f1": 0.7613082811412666,
        "similarity_f1_threshold": 0.6676629185676575,
        "similarity_precision": 0.7245033112582782,
        "similarity_recall": 0.8020527859237536
      }
    ]
  },
  "task_name": "XNLIV2"
}