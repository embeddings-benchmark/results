{
  "dataset_revision": "701984d6c6efea0e14a1c7850ef70e464c5577c0",
  "task_name": "BulgarianStoreReviewSentimentClassfication",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.493407,
        "f1": 0.38747,
        "f1_weighted": 0.557925,
        "scores_per_experiment": [
          {
            "accuracy": 0.483516,
            "f1": 0.391303,
            "f1_weighted": 0.551237
          },
          {
            "accuracy": 0.532967,
            "f1": 0.410707,
            "f1_weighted": 0.612229
          },
          {
            "accuracy": 0.423077,
            "f1": 0.346136,
            "f1_weighted": 0.487307
          },
          {
            "accuracy": 0.467033,
            "f1": 0.362429,
            "f1_weighted": 0.544538
          },
          {
            "accuracy": 0.582418,
            "f1": 0.423464,
            "f1_weighted": 0.622341
          },
          {
            "accuracy": 0.401099,
            "f1": 0.354942,
            "f1_weighted": 0.470805
          },
          {
            "accuracy": 0.532967,
            "f1": 0.418741,
            "f1_weighted": 0.596265
          },
          {
            "accuracy": 0.505495,
            "f1": 0.398144,
            "f1_weighted": 0.553158
          },
          {
            "accuracy": 0.43956,
            "f1": 0.330047,
            "f1_weighted": 0.517131
          },
          {
            "accuracy": 0.565934,
            "f1": 0.438785,
            "f1_weighted": 0.624242
          }
        ],
        "main_score": 0.493407,
        "hf_subset": "default",
        "languages": [
          "bul-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 7.257461071014404,
  "kg_co2_emissions": 0.0002078047028302331
}