{
  "dataset_revision": "2fe05ee6b5832cda29f2ef7aaad7b7fe6a3609eb",
  "task_name": "HeadlineClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.630908,
        "f1": 0.631262,
        "f1_weighted": 0.631279,
        "scores_per_experiment": [
          {
            "accuracy": 0.60791,
            "f1": 0.611303,
            "f1_weighted": 0.611323
          },
          {
            "accuracy": 0.629883,
            "f1": 0.629694,
            "f1_weighted": 0.629707
          },
          {
            "accuracy": 0.643066,
            "f1": 0.641909,
            "f1_weighted": 0.641958
          },
          {
            "accuracy": 0.656738,
            "f1": 0.657001,
            "f1_weighted": 0.656991
          },
          {
            "accuracy": 0.642578,
            "f1": 0.642942,
            "f1_weighted": 0.642944
          },
          {
            "accuracy": 0.691406,
            "f1": 0.691681,
            "f1_weighted": 0.691698
          },
          {
            "accuracy": 0.594238,
            "f1": 0.595978,
            "f1_weighted": 0.595967
          },
          {
            "accuracy": 0.614258,
            "f1": 0.614537,
            "f1_weighted": 0.614585
          },
          {
            "accuracy": 0.578125,
            "f1": 0.576667,
            "f1_weighted": 0.576727
          },
          {
            "accuracy": 0.650879,
            "f1": 0.650911,
            "f1_weighted": 0.650891
          }
        ],
        "main_score": 0.630908,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 7.801445722579956,
  "kg_co2_emissions": 0.00022000956526229442
}