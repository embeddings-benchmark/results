{
  "dataset_revision": "edfaf9da55d3dd50d43143d90c1ac476895ae6de",
  "task_name": "ToxicConversationsClassification",
  "mteb_version": "1.38.54",
  "scores": {
    "test": [
      {
        "accuracy": 0.903613,
        "f1": 0.767962,
        "f1_weighted": 0.917199,
        "ap": 0.3998,
        "ap_weighted": 0.3998,
        "scores_per_experiment": [
          {
            "accuracy": 0.903809,
            "f1": 0.765429,
            "f1_weighted": 0.917092,
            "ap": 0.39227,
            "ap_weighted": 0.39227
          },
          {
            "accuracy": 0.922363,
            "f1": 0.788642,
            "f1_weighted": 0.930162,
            "ap": 0.421543,
            "ap_weighted": 0.421543
          },
          {
            "accuracy": 0.912109,
            "f1": 0.777906,
            "f1_weighted": 0.923237,
            "ap": 0.411568,
            "ap_weighted": 0.411568
          },
          {
            "accuracy": 0.916504,
            "f1": 0.785294,
            "f1_weighted": 0.926584,
            "ap": 0.424194,
            "ap_weighted": 0.424194
          },
          {
            "accuracy": 0.927734,
            "f1": 0.803679,
            "f1_weighted": 0.93505,
            "ap": 0.455226,
            "ap_weighted": 0.455226
          },
          {
            "accuracy": 0.844238,
            "f1": 0.695645,
            "f1_weighted": 0.874663,
            "ap": 0.307542,
            "ap_weighted": 0.307542
          },
          {
            "accuracy": 0.898438,
            "f1": 0.761434,
            "f1_weighted": 0.913621,
            "ap": 0.392023,
            "ap_weighted": 0.392023
          },
          {
            "accuracy": 0.888672,
            "f1": 0.749712,
            "f1_weighted": 0.906702,
            "ap": 0.377849,
            "ap_weighted": 0.377849
          },
          {
            "accuracy": 0.906738,
            "f1": 0.772573,
            "f1_weighted": 0.919617,
            "ap": 0.407431,
            "ap_weighted": 0.407431
          },
          {
            "accuracy": 0.915527,
            "f1": 0.779308,
            "f1_weighted": 0.925263,
            "ap": 0.408353,
            "ap_weighted": 0.408353
          }
        ],
        "main_score": 0.903613,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 60.10746955871582,
  "kg_co2_emissions": null
}