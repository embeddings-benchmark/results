{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "task_name": "AmazonCounterfactualClassification",
  "mteb_version": "1.38.54",
  "scores": {
    "test": [
      {
        "accuracy": 0.948351,
        "f1": 0.877166,
        "f1_weighted": 0.951187,
        "ap": 0.634986,
        "ap_weighted": 0.634986,
        "scores_per_experiment": [
          {
            "accuracy": 0.951274,
            "f1": 0.881045,
            "f1_weighted": 0.953398,
            "ap": 0.642621,
            "ap_weighted": 0.642621
          },
          {
            "accuracy": 0.952774,
            "f1": 0.885985,
            "f1_weighted": 0.955063,
            "ap": 0.65653,
            "ap_weighted": 0.65653
          },
          {
            "accuracy": 0.948276,
            "f1": 0.877162,
            "f1_weighted": 0.951148,
            "ap": 0.634995,
            "ap_weighted": 0.634995
          },
          {
            "accuracy": 0.944528,
            "f1": 0.871383,
            "f1_weighted": 0.948163,
            "ap": 0.622942,
            "ap_weighted": 0.622942
          },
          {
            "accuracy": 0.949775,
            "f1": 0.880073,
            "f1_weighted": 0.952448,
            "ap": 0.642012,
            "ap_weighted": 0.642012
          },
          {
            "accuracy": 0.949775,
            "f1": 0.880073,
            "f1_weighted": 0.952448,
            "ap": 0.642012,
            "ap_weighted": 0.642012
          },
          {
            "accuracy": 0.943028,
            "f1": 0.867907,
            "f1_weighted": 0.946762,
            "ap": 0.614062,
            "ap_weighted": 0.614062
          },
          {
            "accuracy": 0.948276,
            "f1": 0.875814,
            "f1_weighted": 0.950907,
            "ap": 0.630413,
            "ap_weighted": 0.630413
          },
          {
            "accuracy": 0.946777,
            "f1": 0.873602,
            "f1_weighted": 0.949732,
            "ap": 0.625793,
            "ap_weighted": 0.625793
          },
          {
            "accuracy": 0.949025,
            "f1": 0.878614,
            "f1_weighted": 0.951798,
            "ap": 0.638484,
            "ap_weighted": 0.638484
          }
        ],
        "main_score": 0.948351,
        "hf_subset": "en-ext",
        "languages": [
          "eng-Latn"
        ]
      },
      {
        "accuracy": 0.930597,
        "f1": 0.89975,
        "f1_weighted": 0.933609,
        "ap": 0.731458,
        "ap_weighted": 0.731458,
        "scores_per_experiment": [
          {
            "accuracy": 0.925373,
            "f1": 0.893134,
            "f1_weighted": 0.928877,
            "ap": 0.716181,
            "ap_weighted": 0.716181
          },
          {
            "accuracy": 0.949254,
            "f1": 0.924813,
            "f1_weighted": 0.950917,
            "ap": 0.790188,
            "ap_weighted": 0.790188
          },
          {
            "accuracy": 0.928358,
            "f1": 0.89643,
            "f1_weighted": 0.931448,
            "ap": 0.723176,
            "ap_weighted": 0.723176
          },
          {
            "accuracy": 0.919403,
            "f1": 0.884584,
            "f1_weighted": 0.923188,
            "ap": 0.696846,
            "ap_weighted": 0.696846
          },
          {
            "accuracy": 0.934328,
            "f1": 0.904138,
            "f1_weighted": 0.936898,
            "ap": 0.740703,
            "ap_weighted": 0.740703
          },
          {
            "accuracy": 0.932836,
            "f1": 0.902198,
            "f1_weighted": 0.935532,
            "ap": 0.736241,
            "ap_weighted": 0.736241
          },
          {
            "accuracy": 0.926866,
            "f1": 0.894525,
            "f1_weighted": 0.930091,
            "ap": 0.718924,
            "ap_weighted": 0.718924
          },
          {
            "accuracy": 0.943284,
            "f1": 0.91554,
            "f1_weighted": 0.945018,
            "ap": 0.7677,
            "ap_weighted": 0.7677
          },
          {
            "accuracy": 0.923881,
            "f1": 0.892253,
            "f1_weighted": 0.927801,
            "ap": 0.715095,
            "ap_weighted": 0.715095
          },
          {
            "accuracy": 0.922388,
            "f1": 0.889888,
            "f1_weighted": 0.926317,
            "ap": 0.709525,
            "ap_weighted": 0.709525
          }
        ],
        "main_score": 0.930597,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 29.955069303512573,
  "kg_co2_emissions": null
}