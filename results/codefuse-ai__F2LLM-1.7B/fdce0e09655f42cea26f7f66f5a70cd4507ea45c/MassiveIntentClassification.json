{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.38.54",
  "scores": {
    "test": [
      {
        "accuracy": 0.861163,
        "f1": 0.830629,
        "f1_weighted": 0.855922,
        "scores_per_experiment": [
          {
            "accuracy": 0.865837,
            "f1": 0.833453,
            "f1_weighted": 0.861021
          },
          {
            "accuracy": 0.859112,
            "f1": 0.831291,
            "f1_weighted": 0.855166
          },
          {
            "accuracy": 0.853732,
            "f1": 0.821265,
            "f1_weighted": 0.845016
          },
          {
            "accuracy": 0.865165,
            "f1": 0.834729,
            "f1_weighted": 0.860536
          },
          {
            "accuracy": 0.853396,
            "f1": 0.825393,
            "f1_weighted": 0.847489
          },
          {
            "accuracy": 0.864492,
            "f1": 0.831904,
            "f1_weighted": 0.860937
          },
          {
            "accuracy": 0.856086,
            "f1": 0.826681,
            "f1_weighted": 0.848927
          },
          {
            "accuracy": 0.865165,
            "f1": 0.834366,
            "f1_weighted": 0.857276
          },
          {
            "accuracy": 0.859449,
            "f1": 0.826871,
            "f1_weighted": 0.854928
          },
          {
            "accuracy": 0.8692,
            "f1": 0.840331,
            "f1_weighted": 0.867924
          }
        ],
        "main_score": 0.861163,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 30.790899991989136,
  "kg_co2_emissions": null
}