{
  "dataset_revision": "ed9a4403ed4adbfaf4aab56d5b2709e9f6c3ba33",
  "evaluation_time": 407.27165150642395,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test": [
      {
        "accuracy": 0.05808713069604406,
        "f1": 0.04463779084951882,
        "hf_subset": "arb_Arab-rus_Cyrl",
        "languages": [
          "arb-Arab",
          "rus-Cyrl"
        ],
        "main_score": 0.04463779084951882,
        "precision": 0.04109262541351851,
        "recall": 0.05808713069604406
      },
      {
        "accuracy": 0.9694541812719079,
        "f1": 0.9607744950759471,
        "hf_subset": "bel_Cyrl-rus_Cyrl",
        "languages": [
          "bel-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.9607744950759471,
        "precision": 0.956601569020197,
        "recall": 0.9694541812719079
      },
      {
        "accuracy": 0.03355032548823235,
        "f1": 0.026341987092305178,
        "hf_subset": "ben_Beng-rus_Cyrl",
        "languages": [
          "ben-Beng",
          "rus-Cyrl"
        ],
        "main_score": 0.026341987092305178,
        "precision": 0.024801204264550653,
        "recall": 0.03355032548823235
      },
      {
        "accuracy": 0.971957936905358,
        "f1": 0.9636120847938575,
        "hf_subset": "bos_Latn-rus_Cyrl",
        "languages": [
          "bos-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9636120847938575,
        "precision": 0.9596895343014522,
        "recall": 0.971957936905358
      },
      {
        "accuracy": 0.9809714571857787,
        "f1": 0.9751293607077283,
        "hf_subset": "bul_Cyrl-rus_Cyrl",
        "languages": [
          "bul-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.9751293607077283,
        "precision": 0.972291770989818,
        "recall": 0.9809714571857787
      },
      {
        "accuracy": 0.9629444166249375,
        "f1": 0.9530724658416195,
        "hf_subset": "ces_Latn-rus_Cyrl",
        "languages": [
          "ces-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9530724658416195,
        "precision": 0.9483391754298114,
        "recall": 0.9629444166249375
      },
      {
        "accuracy": 0.971457185778668,
        "f1": 0.9637790018360876,
        "hf_subset": "deu_Latn-rus_Cyrl",
        "languages": [
          "deu-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9637790018360876,
        "precision": 0.9601068269070273,
        "recall": 0.971457185778668
      },
      {
        "accuracy": 0.17526289434151227,
        "f1": 0.14414235840984785,
        "hf_subset": "ell_Grek-rus_Cyrl",
        "languages": [
          "ell-Grek",
          "rus-Cyrl"
        ],
        "main_score": 0.14414235840984785,
        "precision": 0.13505161449578076,
        "recall": 0.17526289434151227
      },
      {
        "accuracy": 0.9864797195793691,
        "f1": 0.9821398764813887,
        "hf_subset": "eng_Latn-rus_Cyrl",
        "languages": [
          "eng-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9821398764813887,
        "precision": 0.9800534134535135,
        "recall": 0.9864797195793691
      },
      {
        "accuracy": 0.15222834251377065,
        "f1": 0.13462311609766844,
        "hf_subset": "fas_Arab-rus_Cyrl",
        "languages": [
          "fas-Arab",
          "rus-Cyrl"
        ],
        "main_score": 0.13462311609766844,
        "precision": 0.12812571026521702,
        "recall": 0.15222834251377065
      },
      {
        "accuracy": 0.9474211316975463,
        "f1": 0.9331497245868803,
        "hf_subset": "fin_Latn-rus_Cyrl",
        "languages": [
          "fin-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9331497245868803,
        "precision": 0.9264897346019029,
        "recall": 0.9474211316975463
      },
      {
        "accuracy": 0.9809714571857787,
        "f1": 0.9751627441161743,
        "hf_subset": "fra_Latn-rus_Cyrl",
        "languages": [
          "fra-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9751627441161743,
        "precision": 0.9725004172926055,
        "recall": 0.9809714571857787
      },
      {
        "accuracy": 0.08763144717075613,
        "f1": 0.06842343520623428,
        "hf_subset": "heb_Hebr-rus_Cyrl",
        "languages": [
          "heb-Hebr",
          "rus-Cyrl"
        ],
        "main_score": 0.06842343520623428,
        "precision": 0.06368932475528467,
        "recall": 0.08763144717075613
      },
      {
        "accuracy": 0.0871306960440661,
        "f1": 0.06843692098311183,
        "hf_subset": "hin_Deva-rus_Cyrl",
        "languages": [
          "hin-Deva",
          "rus-Cyrl"
        ],
        "main_score": 0.06843692098311183,
        "precision": 0.06407466749893499,
        "recall": 0.0871306960440661
      },
      {
        "accuracy": 0.9759639459188784,
        "f1": 0.9685361375396428,
        "hf_subset": "hrv_Latn-rus_Cyrl",
        "languages": [
          "hrv-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9685361375396428,
        "precision": 0.9650475713570354,
        "recall": 0.9759639459188784
      },
      {
        "accuracy": 0.8447671507260891,
        "f1": 0.8125978650515456,
        "hf_subset": "hun_Latn-rus_Cyrl",
        "languages": [
          "hun-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.8125978650515456,
        "precision": 0.798732272077584,
        "recall": 0.8447671507260891
      },
      {
        "accuracy": 0.9649474211316975,
        "f1": 0.9547654815556669,
        "hf_subset": "ind_Latn-rus_Cyrl",
        "languages": [
          "ind-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9547654815556669,
        "precision": 0.9500083458521116,
        "recall": 0.9649474211316975
      },
      {
        "accuracy": 0.07310966449674512,
        "f1": 0.05742328676092847,
        "hf_subset": "jpn_Jpan-rus_Cyrl",
        "languages": [
          "jpn-Jpan",
          "rus-Cyrl"
        ],
        "main_score": 0.05742328676092847,
        "precision": 0.05399407070778064,
        "recall": 0.07310966449674512
      },
      {
        "accuracy": 0.1457185778668002,
        "f1": 0.12380398356118902,
        "hf_subset": "kor_Hang-rus_Cyrl",
        "languages": [
          "kor-Hang",
          "rus-Cyrl"
        ],
        "main_score": 0.12380398356118902,
        "precision": 0.117989602668687,
        "recall": 0.1457185778668002
      },
      {
        "accuracy": 0.8522784176264396,
        "f1": 0.8195054486491643,
        "hf_subset": "lit_Latn-rus_Cyrl",
        "languages": [
          "lit-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.8195054486491643,
        "precision": 0.8050683167608556,
        "recall": 0.8522784176264396
      },
      {
        "accuracy": 0.9789684526790186,
        "f1": 0.9727090635953931,
        "hf_subset": "mkd_Cyrl-rus_Cyrl",
        "languages": [
          "mkd-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.9727090635953931,
        "precision": 0.9697045568352529,
        "recall": 0.9789684526790186
      },
      {
        "accuracy": 0.9664496745117677,
        "f1": 0.9567780241791258,
        "hf_subset": "nld_Latn-rus_Cyrl",
        "languages": [
          "nld-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9567780241791258,
        "precision": 0.9523034551827742,
        "recall": 0.9664496745117677
      },
      {
        "accuracy": 0.9489233850776164,
        "f1": 0.9338173927558003,
        "hf_subset": "pol_Latn-rus_Cyrl",
        "languages": [
          "pol-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9338173927558003,
        "precision": 0.9264730428976798,
        "recall": 0.9489233850776164
      },
      {
        "accuracy": 0.9739609414121182,
        "f1": 0.9661992989484226,
        "hf_subset": "por_Latn-rus_Cyrl",
        "languages": [
          "por-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9661992989484226,
        "precision": 0.9625438157235854,
        "recall": 0.9739609414121182
      },
      {
        "accuracy": 0.08662994491737606,
        "f1": 0.06312156757880587,
        "hf_subset": "rus_Cyrl-arb_Arab",
        "languages": [
          "rus-Cyrl",
          "arb-Arab"
        ],
        "main_score": 0.06312156757880587,
        "precision": 0.05863203884731444,
        "recall": 0.08662994491737606
      },
      {
        "accuracy": 0.9579369053580371,
        "f1": 0.9458115745046141,
        "hf_subset": "rus_Cyrl-bel_Cyrl",
        "languages": [
          "rus-Cyrl",
          "bel-Cyrl"
        ],
        "main_score": 0.9458115745046141,
        "precision": 0.9399933233183108,
        "recall": 0.9579369053580371
      },
      {
        "accuracy": 0.06209313970956434,
        "f1": 0.03567931593500681,
        "hf_subset": "rus_Cyrl-ben_Beng",
        "languages": [
          "rus-Cyrl",
          "ben-Beng"
        ],
        "main_score": 0.03567931593500681,
        "precision": 0.030303439825236953,
        "recall": 0.06209313970956434
      },
      {
        "accuracy": 0.957436154231347,
        "f1": 0.9445835419796361,
        "hf_subset": "rus_Cyrl-bos_Latn",
        "languages": [
          "rus-Cyrl",
          "bos-Latn"
        ],
        "main_score": 0.9445835419796361,
        "precision": 0.9384076114171257,
        "recall": 0.957436154231347
      },
      {
        "accuracy": 0.9809714571857787,
        "f1": 0.9747955266232683,
        "hf_subset": "rus_Cyrl-bul_Cyrl",
        "languages": [
          "rus-Cyrl",
          "bul-Cyrl"
        ],
        "main_score": 0.9747955266232683,
        "precision": 0.971791019863128,
        "recall": 0.9809714571857787
      },
      {
        "accuracy": 0.9544316474712068,
        "f1": 0.9408279085294609,
        "hf_subset": "rus_Cyrl-ces_Latn",
        "languages": [
          "rus-Cyrl",
          "ces-Latn"
        ],
        "main_score": 0.9408279085294609,
        "precision": 0.9344016024036054,
        "recall": 0.9544316474712068
      },
      {
        "accuracy": 0.9584376564847271,
        "f1": 0.9464196294441662,
        "hf_subset": "rus_Cyrl-deu_Latn",
        "languages": [
          "rus-Cyrl",
          "deu-Latn"
        ],
        "main_score": 0.9464196294441662,
        "precision": 0.9405942246703388,
        "recall": 0.9584376564847271
      },
      {
        "accuracy": 0.21081622433650477,
        "f1": 0.1555864609689077,
        "hf_subset": "rus_Cyrl-ell_Grek",
        "languages": [
          "rus-Cyrl",
          "ell-Grek"
        ],
        "main_score": 0.1555864609689077,
        "precision": 0.1414499713931214,
        "recall": 0.21081622433650477
      },
      {
        "accuracy": 0.9869804707060591,
        "f1": 0.9826406276080789,
        "hf_subset": "rus_Cyrl-eng_Latn",
        "languages": [
          "rus-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.9826406276080789,
        "precision": 0.9804707060590886,
        "recall": 0.9869804707060591
      },
      {
        "accuracy": 0.18577866800200302,
        "f1": 0.12655361285348646,
        "hf_subset": "rus_Cyrl-fas_Arab",
        "languages": [
          "rus-Cyrl",
          "fas-Arab"
        ],
        "main_score": 0.12655361285348646,
        "precision": 0.11217397558227961,
        "recall": 0.18577866800200302
      },
      {
        "accuracy": 0.9424136204306459,
        "f1": 0.9247537973627106,
        "hf_subset": "rus_Cyrl-fin_Latn",
        "languages": [
          "rus-Cyrl",
          "fin-Latn"
        ],
        "main_score": 0.9247537973627106,
        "precision": 0.9164162911033217,
        "recall": 0.9424136204306459
      },
      {
        "accuracy": 0.970956434651978,
        "f1": 0.9622266733433483,
        "hf_subset": "rus_Cyrl-fra_Latn",
        "languages": [
          "rus-Cyrl",
          "fra-Latn"
        ],
        "main_score": 0.9622266733433483,
        "precision": 0.9580620931397096,
        "recall": 0.970956434651978
      },
      {
        "accuracy": 0.13370055082623936,
        "f1": 0.08754464181488648,
        "hf_subset": "rus_Cyrl-heb_Hebr",
        "languages": [
          "rus-Cyrl",
          "heb-Hebr"
        ],
        "main_score": 0.08754464181488648,
        "precision": 0.07822346852058519,
        "recall": 0.13370055082623936
      },
      {
        "accuracy": 0.11817726589884828,
        "f1": 0.08219644451515201,
        "hf_subset": "rus_Cyrl-hin_Deva",
        "languages": [
          "rus-Cyrl",
          "hin-Deva"
        ],
        "main_score": 0.08219644451515201,
        "precision": 0.07494083126567538,
        "recall": 0.11817726589884828
      },
      {
        "accuracy": 0.9634451677516275,
        "f1": 0.9520447337673176,
        "hf_subset": "rus_Cyrl-hrv_Latn",
        "languages": [
          "rus-Cyrl",
          "hrv-Latn"
        ],
        "main_score": 0.9520447337673176,
        "precision": 0.9464613587047238,
        "recall": 0.9634451677516275
      },
      {
        "accuracy": 0.8137205808713069,
        "f1": 0.7695428062729014,
        "hf_subset": "rus_Cyrl-hun_Latn",
        "languages": [
          "rus-Cyrl",
          "hun-Latn"
        ],
        "main_score": 0.7695428062729014,
        "precision": 0.7507851062307747,
        "recall": 0.8137205808713069
      },
      {
        "accuracy": 0.956935403104657,
        "f1": 0.944516775162744,
        "hf_subset": "rus_Cyrl-ind_Latn",
        "languages": [
          "rus-Cyrl",
          "ind-Latn"
        ],
        "main_score": 0.944516775162744,
        "precision": 0.9387831747621432,
        "recall": 0.956935403104657
      },
      {
        "accuracy": 0.11467200801201803,
        "f1": 0.07280048486266794,
        "hf_subset": "rus_Cyrl-jpn_Jpan",
        "languages": [
          "rus-Cyrl",
          "jpn-Jpan"
        ],
        "main_score": 0.07280048486266794,
        "precision": 0.06481392293787133,
        "recall": 0.11467200801201803
      },
      {
        "accuracy": 0.16624937406109164,
        "f1": 0.11402328062546077,
        "hf_subset": "rus_Cyrl-kor_Hang",
        "languages": [
          "rus-Cyrl",
          "kor-Hang"
        ],
        "main_score": 0.11402328062546077,
        "precision": 0.1020409601477825,
        "recall": 0.16624937406109164
      },
      {
        "accuracy": 0.8112168252378568,
        "f1": 0.7642904833440638,
        "hf_subset": "rus_Cyrl-lit_Latn",
        "languages": [
          "rus-Cyrl",
          "lit-Latn"
        ],
        "main_score": 0.7642904833440638,
        "precision": 0.7445644657462384,
        "recall": 0.8112168252378568
      },
      {
        "accuracy": 0.9789684526790186,
        "f1": 0.972408612919379,
        "hf_subset": "rus_Cyrl-mkd_Cyrl",
        "languages": [
          "rus-Cyrl",
          "mkd-Cyrl"
        ],
        "main_score": 0.972408612919379,
        "precision": 0.9693289934902354,
        "recall": 0.9789684526790186
      },
      {
        "accuracy": 0.9619429143715573,
        "f1": 0.9506760140210315,
        "hf_subset": "rus_Cyrl-nld_Latn",
        "languages": [
          "rus-Cyrl",
          "nld-Latn"
        ],
        "main_score": 0.9506760140210315,
        "precision": 0.9451844433316642,
        "recall": 0.9619429143715573
      },
      {
        "accuracy": 0.9414121181772659,
        "f1": 0.9255883825738609,
        "hf_subset": "rus_Cyrl-pol_Latn",
        "languages": [
          "rus-Cyrl",
          "pol-Latn"
        ],
        "main_score": 0.9255883825738609,
        "precision": 0.9183525287931897,
        "recall": 0.9414121181772659
      },
      {
        "accuracy": 0.9689534301452178,
        "f1": 0.9593557002169921,
        "hf_subset": "rus_Cyrl-por_Latn",
        "languages": [
          "rus-Cyrl",
          "por-Latn"
        ],
        "main_score": 0.9593557002169921,
        "precision": 0.9547654815556669,
        "recall": 0.9689534301452178
      },
      {
        "accuracy": 0.9594391587381071,
        "f1": 0.9476715072608913,
        "hf_subset": "rus_Cyrl-slk_Latn",
        "languages": [
          "rus-Cyrl",
          "slk-Latn"
        ],
        "main_score": 0.9476715072608913,
        "precision": 0.942079786346186,
        "recall": 0.9594391587381071
      },
      {
        "accuracy": 0.9474211316975463,
        "f1": 0.9327157402770823,
        "hf_subset": "rus_Cyrl-slv_Latn",
        "languages": [
          "rus-Cyrl",
          "slv-Latn"
        ],
        "main_score": 0.9327157402770823,
        "precision": 0.9258888332498748,
        "recall": 0.9474211316975463
      },
      {
        "accuracy": 0.9699549323985979,
        "f1": 0.9610582540477381,
        "hf_subset": "rus_Cyrl-spa_Latn",
        "languages": [
          "rus-Cyrl",
          "spa-Latn"
        ],
        "main_score": 0.9610582540477381,
        "precision": 0.9567267568018696,
        "recall": 0.9699549323985979
      },
      {
        "accuracy": 0.9544316474712068,
        "f1": 0.9410282089801367,
        "hf_subset": "rus_Cyrl-srp_Cyrl",
        "languages": [
          "rus-Cyrl",
          "srp-Cyrl"
        ],
        "main_score": 0.9410282089801367,
        "precision": 0.9348606242697379,
        "recall": 0.9544316474712068
      },
      {
        "accuracy": 0.9649474211316975,
        "f1": 0.9544650308796527,
        "hf_subset": "rus_Cyrl-srp_Latn",
        "languages": [
          "rus-Cyrl",
          "srp-Latn"
        ],
        "main_score": 0.9544650308796527,
        "precision": 0.949465865464864,
        "recall": 0.9649474211316975
      },
      {
        "accuracy": 0.8813219829744617,
        "f1": 0.8509470555038908,
        "hf_subset": "rus_Cyrl-swa_Latn",
        "languages": [
          "rus-Cyrl",
          "swa-Latn"
        ],
        "main_score": 0.8509470555038908,
        "precision": 0.8377107327658154,
        "recall": 0.8813219829744617
      },
      {
        "accuracy": 0.9664496745117677,
        "f1": 0.9562677349357369,
        "hf_subset": "rus_Cyrl-swe_Latn",
        "languages": [
          "rus-Cyrl",
          "swe-Latn"
        ],
        "main_score": 0.9562677349357369,
        "precision": 0.9513436821899517,
        "recall": 0.9664496745117677
      },
      {
        "accuracy": 0.1186780170255383,
        "f1": 0.0787038410301353,
        "hf_subset": "rus_Cyrl-tam_Taml",
        "languages": [
          "rus-Cyrl",
          "tam-Taml"
        ],
        "main_score": 0.0787038410301353,
        "precision": 0.07039829742982286,
        "recall": 0.1186780170255383
      },
      {
        "accuracy": 0.9173760640961443,
        "f1": 0.8933066266065766,
        "hf_subset": "rus_Cyrl-tur_Latn",
        "languages": [
          "rus-Cyrl",
          "tur-Latn"
        ],
        "main_score": 0.8933066266065766,
        "precision": 0.8822400267067269,
        "recall": 0.9173760640961443
      },
      {
        "accuracy": 0.9834752128192289,
        "f1": 0.9783842430312134,
        "hf_subset": "rus_Cyrl-ukr_Cyrl",
        "languages": [
          "rus-Cyrl",
          "ukr-Cyrl"
        ],
        "main_score": 0.9783842430312134,
        "precision": 0.9759639459188784,
        "recall": 0.9834752128192289
      },
      {
        "accuracy": 0.48272408612919376,
        "f1": 0.415057799819943,
        "hf_subset": "rus_Cyrl-vie_Latn",
        "languages": [
          "rus-Cyrl",
          "vie-Latn"
        ],
        "main_score": 0.415057799819943,
        "precision": 0.3914216669474556,
        "recall": 0.48272408612919376
      },
      {
        "accuracy": 0.29844767150726087,
        "f1": 0.2274809468678437,
        "hf_subset": "rus_Cyrl-zho_Hant",
        "languages": [
          "rus-Cyrl",
          "zho-Hant"
        ],
        "main_score": 0.2274809468678437,
        "precision": 0.20867913898470325,
        "recall": 0.29844767150726087
      },
      {
        "accuracy": 0.7636454682023035,
        "f1": 0.712357363884655,
        "hf_subset": "rus_Cyrl-zul_Latn",
        "languages": [
          "rus-Cyrl",
          "zul-Latn"
        ],
        "main_score": 0.712357363884655,
        "precision": 0.6910866299449173,
        "recall": 0.7636454682023035
      },
      {
        "accuracy": 0.9704556835252879,
        "f1": 0.9620359110093712,
        "hf_subset": "slk_Latn-rus_Cyrl",
        "languages": [
          "slk-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9620359110093712,
        "precision": 0.9582290101819396,
        "recall": 0.9704556835252879
      },
      {
        "accuracy": 0.9604406609914873,
        "f1": 0.9501251877816725,
        "hf_subset": "slv_Latn-rus_Cyrl",
        "languages": [
          "slv-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9501251877816725,
        "precision": 0.9454932398597897,
        "recall": 0.9604406609914873
      },
      {
        "accuracy": 0.9744616925388082,
        "f1": 0.9673176431313636,
        "hf_subset": "spa_Latn-rus_Cyrl",
        "languages": [
          "spa-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9673176431313636,
        "precision": 0.9640043398430981,
        "recall": 0.9744616925388082
      },
      {
        "accuracy": 0.9674511767651477,
        "f1": 0.9584710398931731,
        "hf_subset": "srp_Cyrl-rus_Cyrl",
        "languages": [
          "srp-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.9584710398931731,
        "precision": 0.9541562343515273,
        "recall": 0.9674511767651477
      },
      {
        "accuracy": 0.9704556835252879,
        "f1": 0.9623935903855783,
        "hf_subset": "srp_Latn-rus_Cyrl",
        "languages": [
          "srp-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9623935903855783,
        "precision": 0.9586463027875146,
        "recall": 0.9704556835252879
      },
      {
        "accuracy": 0.8968452679018528,
        "f1": 0.8703054581872809,
        "hf_subset": "swa_Latn-rus_Cyrl",
        "languages": [
          "swa-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.8703054581872809,
        "precision": 0.8583764535692429,
        "recall": 0.8968452679018528
      },
      {
        "accuracy": 0.972458688032048,
        "f1": 0.9644299783007845,
        "hf_subset": "swe_Latn-rus_Cyrl",
        "languages": [
          "swe-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9644299783007845,
        "precision": 0.9606075780337172,
        "recall": 0.972458688032048
      },
      {
        "accuracy": 0.08412618928392589,
        "f1": 0.06624315036292348,
        "hf_subset": "tam_Taml-rus_Cyrl",
        "languages": [
          "tam-Taml",
          "rus-Cyrl"
        ],
        "main_score": 0.06624315036292348,
        "precision": 0.06204652509464568,
        "recall": 0.08412618928392589
      },
      {
        "accuracy": 0.9298948422633951,
        "f1": 0.9104823902520447,
        "hf_subset": "tur_Latn-rus_Cyrl",
        "languages": [
          "tur-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9104823902520447,
        "precision": 0.9014939075279585,
        "recall": 0.9298948422633951
      },
      {
        "accuracy": 0.9784677015523285,
        "f1": 0.9727090635953931,
        "hf_subset": "ukr_Cyrl-rus_Cyrl",
        "languages": [
          "ukr-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.9727090635953931,
        "precision": 0.9699549323985979,
        "recall": 0.9784677015523285
      },
      {
        "accuracy": 0.48122183274912367,
        "f1": 0.43781613145659215,
        "hf_subset": "vie_Latn-rus_Cyrl",
        "languages": [
          "vie-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.43781613145659215,
        "precision": 0.4227858828951257,
        "recall": 0.48122183274912367
      },
      {
        "accuracy": 0.2358537806710065,
        "f1": 0.2099949542371913,
        "hf_subset": "zho_Hant-rus_Cyrl",
        "languages": [
          "zho-Hant",
          "rus-Cyrl"
        ],
        "main_score": 0.2099949542371913,
        "precision": 0.2014560973599303,
        "recall": 0.2358537806710065
      },
      {
        "accuracy": 0.7666499749624437,
        "f1": 0.7223822927753824,
        "hf_subset": "zul_Latn-rus_Cyrl",
        "languages": [
          "zul-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.7223822927753824,
        "precision": 0.7043450276424739,
        "recall": 0.7666499749624437
      }
    ]
  },
  "task_name": "NTREXBitextMining"
}