{
  "dataset_revision": "13722b7320ef4b6a471f9e8b379f3f49167d0517",
  "task_name": "RuToxicOKMLCUPClassification",
  "mteb_version": "1.38.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.8271,
        "f1": 0.826442,
        "f1_weighted": 0.826442,
        "ap": 0.766108,
        "ap_weighted": 0.766108,
        "scores_per_experiment": [
          {
            "accuracy": 0.8345,
            "f1": 0.833697,
            "f1_weighted": 0.833697,
            "ap": 0.765486,
            "ap_weighted": 0.765486
          },
          {
            "accuracy": 0.8575,
            "f1": 0.857498,
            "f1_weighted": 0.857498,
            "ap": 0.807457,
            "ap_weighted": 0.807457
          },
          {
            "accuracy": 0.817,
            "f1": 0.816749,
            "f1_weighted": 0.816749,
            "ap": 0.767019,
            "ap_weighted": 0.767019
          },
          {
            "accuracy": 0.8175,
            "f1": 0.815925,
            "f1_weighted": 0.815925,
            "ap": 0.743819,
            "ap_weighted": 0.743819
          },
          {
            "accuracy": 0.816,
            "f1": 0.814395,
            "f1_weighted": 0.814395,
            "ap": 0.742196,
            "ap_weighted": 0.742196
          },
          {
            "accuracy": 0.8145,
            "f1": 0.813749,
            "f1_weighted": 0.813749,
            "ap": 0.745014,
            "ap_weighted": 0.745014
          },
          {
            "accuracy": 0.8075,
            "f1": 0.806861,
            "f1_weighted": 0.806861,
            "ap": 0.760593,
            "ap_weighted": 0.760593
          },
          {
            "accuracy": 0.829,
            "f1": 0.828901,
            "f1_weighted": 0.828901,
            "ap": 0.778199,
            "ap_weighted": 0.778199
          },
          {
            "accuracy": 0.84,
            "f1": 0.839977,
            "f1_weighted": 0.839977,
            "ap": 0.782891,
            "ap_weighted": 0.782891
          },
          {
            "accuracy": 0.8375,
            "f1": 0.836665,
            "f1_weighted": 0.836665,
            "ap": 0.768406,
            "ap_weighted": 0.768406
          }
        ],
        "main_score": 0.8271,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 415.5862648487091,
  "kg_co2_emissions": null
}