{
  "dataset_revision": "f6d2c31f4dc6b88f468552750bfec05b4b41b05a",
  "evaluation_time": 16.16543960571289,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test": [
      {
        "accuracy": 0.685205078125,
        "f1": 0.6765250022666258,
        "f1_weighted": 0.6765325275803843,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.685205078125,
        "scores_per_experiment": [
          {
            "accuracy": 0.708984375,
            "f1": 0.7061912139596283,
            "f1_weighted": 0.7062033991859833
          },
          {
            "accuracy": 0.65625,
            "f1": 0.6487048685184802,
            "f1_weighted": 0.6487273231292121
          },
          {
            "accuracy": 0.69873046875,
            "f1": 0.694677801419183,
            "f1_weighted": 0.6946789586990227
          },
          {
            "accuracy": 0.7001953125,
            "f1": 0.6961673011723749,
            "f1_weighted": 0.6961871440746121
          },
          {
            "accuracy": 0.69482421875,
            "f1": 0.6940857041899919,
            "f1_weighted": 0.6940925276463119
          },
          {
            "accuracy": 0.65625,
            "f1": 0.6478199629840541,
            "f1_weighted": 0.6478378067333668
          },
          {
            "accuracy": 0.6591796875,
            "f1": 0.640207048326704,
            "f1_weighted": 0.6402048365133355
          },
          {
            "accuracy": 0.6904296875,
            "f1": 0.6794524938128698,
            "f1_weighted": 0.679447631704187
          },
          {
            "accuracy": 0.69287109375,
            "f1": 0.6689425800084554,
            "f1_weighted": 0.6689163853855261
          },
          {
            "accuracy": 0.6943359375,
            "f1": 0.6890010482745157,
            "f1_weighted": 0.689029262732284
          }
        ]
      }
    ]
  },
  "task_name": "RuReviewsClassification"
}