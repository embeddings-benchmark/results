{
  "dataset_revision": "424d0f767aaa5c411e3a529eec04658e5726a39e",
  "task_name": "RuNLUIntentClassification",
  "mteb_version": "1.38.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.67526,
        "f1": 0.622438,
        "f1_weighted": 0.661377,
        "scores_per_experiment": [
          {
            "accuracy": 0.6668,
            "f1": 0.61536,
            "f1_weighted": 0.654751
          },
          {
            "accuracy": 0.6674,
            "f1": 0.612932,
            "f1_weighted": 0.654284
          },
          {
            "accuracy": 0.6704,
            "f1": 0.622815,
            "f1_weighted": 0.656435
          },
          {
            "accuracy": 0.6688,
            "f1": 0.617273,
            "f1_weighted": 0.654483
          },
          {
            "accuracy": 0.6614,
            "f1": 0.612942,
            "f1_weighted": 0.644385
          },
          {
            "accuracy": 0.6862,
            "f1": 0.632888,
            "f1_weighted": 0.676347
          },
          {
            "accuracy": 0.7052,
            "f1": 0.642786,
            "f1_weighted": 0.692953
          },
          {
            "accuracy": 0.6932,
            "f1": 0.633756,
            "f1_weighted": 0.679369
          },
          {
            "accuracy": 0.6652,
            "f1": 0.61899,
            "f1_weighted": 0.65066
          },
          {
            "accuracy": 0.668,
            "f1": 0.614636,
            "f1_weighted": 0.650105
          }
        ],
        "main_score": 0.67526,
        "hf_subset": "rus-eng",
        "languages": [
          "rus-Cyrl",
          "rus-Latn"
        ]
      },
      {
        "accuracy": 0.66502,
        "f1": 0.614131,
        "f1_weighted": 0.649639,
        "scores_per_experiment": [
          {
            "accuracy": 0.6586,
            "f1": 0.609867,
            "f1_weighted": 0.645076
          },
          {
            "accuracy": 0.651,
            "f1": 0.602708,
            "f1_weighted": 0.633525
          },
          {
            "accuracy": 0.6524,
            "f1": 0.606122,
            "f1_weighted": 0.634037
          },
          {
            "accuracy": 0.6678,
            "f1": 0.610612,
            "f1_weighted": 0.65469
          },
          {
            "accuracy": 0.6546,
            "f1": 0.609156,
            "f1_weighted": 0.63422
          },
          {
            "accuracy": 0.6688,
            "f1": 0.616936,
            "f1_weighted": 0.657347
          },
          {
            "accuracy": 0.6986,
            "f1": 0.631111,
            "f1_weighted": 0.688726
          },
          {
            "accuracy": 0.6876,
            "f1": 0.625106,
            "f1_weighted": 0.675906
          },
          {
            "accuracy": 0.6494,
            "f1": 0.612241,
            "f1_weighted": 0.630208
          },
          {
            "accuracy": 0.6614,
            "f1": 0.617452,
            "f1_weighted": 0.642658
          }
        ],
        "main_score": 0.66502,
        "hf_subset": "rus",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 1643.1368434429169,
  "kg_co2_emissions": null
}