{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.774656,
        "f1": 0.649658,
        "f1_weighted": 0.805434,
        "ap": 0.266673,
        "ap_weighted": 0.266673,
        "scores_per_experiment": [
          {
            "accuracy": 0.677835,
            "f1": 0.566954,
            "f1_weighted": 0.731111,
            "ap": 0.199177,
            "ap_weighted": 0.199177
          },
          {
            "accuracy": 0.841924,
            "f1": 0.701743,
            "f1_weighted": 0.854924,
            "ap": 0.305065,
            "ap_weighted": 0.305065
          },
          {
            "accuracy": 0.831615,
            "f1": 0.712083,
            "f1_weighted": 0.851059,
            "ap": 0.335431,
            "ap_weighted": 0.335431
          },
          {
            "accuracy": 0.829038,
            "f1": 0.707122,
            "f1_weighted": 0.848681,
            "ap": 0.327205,
            "ap_weighted": 0.327205
          },
          {
            "accuracy": 0.775773,
            "f1": 0.622985,
            "f1_weighted": 0.802784,
            "ap": 0.218074,
            "ap_weighted": 0.218074
          },
          {
            "accuracy": 0.746564,
            "f1": 0.628074,
            "f1_weighted": 0.785339,
            "ap": 0.247547,
            "ap_weighted": 0.247547
          },
          {
            "accuracy": 0.80756,
            "f1": 0.656434,
            "f1_weighted": 0.827136,
            "ap": 0.249723,
            "ap_weighted": 0.249723
          },
          {
            "accuracy": 0.699313,
            "f1": 0.612071,
            "f1_weighted": 0.749888,
            "ap": 0.265121,
            "ap_weighted": 0.265121
          },
          {
            "accuracy": 0.770619,
            "f1": 0.636525,
            "f1_weighted": 0.801913,
            "ap": 0.242577,
            "ap_weighted": 0.242577
          },
          {
            "accuracy": 0.766323,
            "f1": 0.652591,
            "f1_weighted": 0.801501,
            "ap": 0.276813,
            "ap_weighted": 0.276813
          }
        ],
        "main_score": 0.774656,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 9.306007146835327,
  "kg_co2_emissions": 0.0004232324205475083
}