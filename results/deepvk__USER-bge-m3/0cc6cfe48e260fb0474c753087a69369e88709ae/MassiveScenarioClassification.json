{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 22.130501747131348,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test": [
      {
        "accuracy": 0.728950907868191,
        "f1": 0.7216709922550637,
        "f1_weighted": 0.7238982412173287,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.728950907868191,
        "scores_per_experiment": [
          {
            "accuracy": 0.7417619367854741,
            "f1": 0.7360646811420946,
            "f1_weighted": 0.7372972814802542
          },
          {
            "accuracy": 0.7236045729657028,
            "f1": 0.7193558565444408,
            "f1_weighted": 0.7189909346672049
          },
          {
            "accuracy": 0.7424344317417619,
            "f1": 0.7328390882752787,
            "f1_weighted": 0.7383573141620485
          },
          {
            "accuracy": 0.7262945527908541,
            "f1": 0.720650772103308,
            "f1_weighted": 0.7255541783535994
          },
          {
            "accuracy": 0.7246133154001345,
            "f1": 0.7098231975913576,
            "f1_weighted": 0.7138062566454304
          },
          {
            "accuracy": 0.707128446536651,
            "f1": 0.6937803934244622,
            "f1_weighted": 0.6978292836566484
          },
          {
            "accuracy": 0.715198386012105,
            "f1": 0.7060209122856834,
            "f1_weighted": 0.7115800593081811
          },
          {
            "accuracy": 0.7051109616677875,
            "f1": 0.705712113846243,
            "f1_weighted": 0.700748044408774
          },
          {
            "accuracy": 0.7525218560860794,
            "f1": 0.7467402633346115,
            "f1_weighted": 0.7506814990197227
          },
          {
            "accuracy": 0.7508406186953598,
            "f1": 0.7457226440031572,
            "f1_weighted": 0.744137560471424
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.7311362518445648,
        "f1": 0.7197091572567068,
        "f1_weighted": 0.7268562931504184,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.7311362518445648,
        "scores_per_experiment": [
          {
            "accuracy": 0.7579931136251845,
            "f1": 0.7488969653444232,
            "f1_weighted": 0.7553791444059834
          },
          {
            "accuracy": 0.7191342843089031,
            "f1": 0.7135783367076445,
            "f1_weighted": 0.7160328762172499
          },
          {
            "accuracy": 0.7466797835710772,
            "f1": 0.7334295083438825,
            "f1_weighted": 0.7446153560833414
          },
          {
            "accuracy": 0.7142154451549434,
            "f1": 0.7054643944428227,
            "f1_weighted": 0.7100575377524024
          },
          {
            "accuracy": 0.7353664535169699,
            "f1": 0.7190002684700832,
            "f1_weighted": 0.7298351949996098
          },
          {
            "accuracy": 0.7063453025086079,
            "f1": 0.6907032609025298,
            "f1_weighted": 0.6975793763622358
          },
          {
            "accuracy": 0.7043777668470241,
            "f1": 0.6892436495355865,
            "f1_weighted": 0.7004595831001759
          },
          {
            "accuracy": 0.7147073290703394,
            "f1": 0.7073938596102799,
            "f1_weighted": 0.7115174810619932
          },
          {
            "accuracy": 0.7589768814559764,
            "f1": 0.7508574053520456,
            "f1_weighted": 0.7559559992360925
          },
          {
            "accuracy": 0.7535661583866208,
            "f1": 0.7385239238577697,
            "f1_weighted": 0.7471303822850999
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}