{
  "dataset_revision": "673a610d6d3dd91a547a0d57ae1b56f37ebbf6a1",
  "evaluation_time": 93.39899754524231,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test": [
      {
        "accuracy": 0.576708984375,
        "f1": 0.5657868599950736,
        "f1_weighted": 0.5658960268318218,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.576708984375,
        "scores_per_experiment": [
          {
            "accuracy": 0.59619140625,
            "f1": 0.5841364346826692,
            "f1_weighted": 0.5841979049609682
          },
          {
            "accuracy": 0.57421875,
            "f1": 0.5657371878044087,
            "f1_weighted": 0.5658708287930391
          },
          {
            "accuracy": 0.55029296875,
            "f1": 0.5399819606011311,
            "f1_weighted": 0.5401344122583075
          },
          {
            "accuracy": 0.5888671875,
            "f1": 0.5828154360039094,
            "f1_weighted": 0.5829435425713042
          },
          {
            "accuracy": 0.59521484375,
            "f1": 0.589199576858571,
            "f1_weighted": 0.5892325486293716
          },
          {
            "accuracy": 0.55078125,
            "f1": 0.5371830233908842,
            "f1_weighted": 0.5372512628992487
          },
          {
            "accuracy": 0.578125,
            "f1": 0.5638915476195631,
            "f1_weighted": 0.5640739853448604
          },
          {
            "accuracy": 0.57421875,
            "f1": 0.5620592090004173,
            "f1_weighted": 0.5621820723805379
          },
          {
            "accuracy": 0.58837890625,
            "f1": 0.5764541803561742,
            "f1_weighted": 0.5765604383780258
          },
          {
            "accuracy": 0.57080078125,
            "f1": 0.5564100436330073,
            "f1_weighted": 0.5565132721025547
          }
        ]
      }
    ]
  },
  "task_name": "RuSciBenchGRNTIClassification"
}