{
  "dataset_revision": "de7ba3406ee55ea2cc52a0a41408fa6aede6d3c6",
  "task_name": "LccSentimentClassification",
  "mteb_version": "2.3.9",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.553333,
            "f1": 0.519424,
            "f1_weighted": 0.567299,
            "precision": 0.521384,
            "precision_weighted": 0.606924,
            "recall": 0.540098,
            "recall_weighted": 0.553333,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.44,
            "f1": 0.419955,
            "f1_weighted": 0.455984,
            "precision": 0.43523,
            "precision_weighted": 0.505101,
            "recall": 0.443466,
            "recall_weighted": 0.44,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.466667,
            "f1": 0.471263,
            "f1_weighted": 0.487176,
            "precision": 0.511667,
            "precision_weighted": 0.574367,
            "recall": 0.505851,
            "recall_weighted": 0.466667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.46,
            "f1": 0.43427,
            "f1_weighted": 0.47522,
            "precision": 0.445961,
            "precision_weighted": 0.517726,
            "recall": 0.455514,
            "recall_weighted": 0.46,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.46,
            "f1": 0.437125,
            "f1_weighted": 0.470973,
            "precision": 0.437778,
            "precision_weighted": 0.505244,
            "recall": 0.457861,
            "recall_weighted": 0.46,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.52,
            "f1": 0.470601,
            "f1_weighted": 0.532521,
            "precision": 0.470741,
            "precision_weighted": 0.554137,
            "recall": 0.480412,
            "recall_weighted": 0.52,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.466667,
            "f1": 0.448032,
            "f1_weighted": 0.490426,
            "precision": 0.476545,
            "precision_weighted": 0.579149,
            "recall": 0.477654,
            "recall_weighted": 0.466667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.56,
            "f1": 0.517802,
            "f1_weighted": 0.572688,
            "precision": 0.517312,
            "precision_weighted": 0.596481,
            "recall": 0.530521,
            "recall_weighted": 0.56,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.473333,
            "f1": 0.447559,
            "f1_weighted": 0.496552,
            "precision": 0.466611,
            "precision_weighted": 0.562907,
            "recall": 0.469251,
            "recall_weighted": 0.473333,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.473333,
            "f1": 0.455041,
            "f1_weighted": 0.48672,
            "precision": 0.461408,
            "precision_weighted": 0.537524,
            "recall": 0.48167,
            "recall_weighted": 0.473333,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.487333,
        "f1": 0.462107,
        "f1_weighted": 0.503556,
        "precision": 0.474464,
        "precision_weighted": 0.553956,
        "recall": 0.48423,
        "recall_weighted": 0.487333,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.487333,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 5.166321516036987,
  "kg_co2_emissions": null
}