{
  "dataset_revision": "de7ba3406ee55ea2cc52a0a41408fa6aede6d3c6",
  "task_name": "LccSentimentClassification",
  "mteb_version": "2.3.9",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.553333,
            "f1": 0.519424,
            "f1_weighted": 0.567299,
            "precision": 0.521384,
            "precision_weighted": 0.606924,
            "recall": 0.540098,
            "recall_weighted": 0.553333,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.44,
            "f1": 0.419955,
            "f1_weighted": 0.455984,
            "precision": 0.43523,
            "precision_weighted": 0.505101,
            "recall": 0.443466,
            "recall_weighted": 0.44,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.466667,
            "f1": 0.471263,
            "f1_weighted": 0.487176,
            "precision": 0.511667,
            "precision_weighted": 0.574367,
            "recall": 0.505851,
            "recall_weighted": 0.466667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.46,
            "f1": 0.43427,
            "f1_weighted": 0.47522,
            "precision": 0.445961,
            "precision_weighted": 0.517726,
            "recall": 0.455514,
            "recall_weighted": 0.46,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.46,
            "f1": 0.437125,
            "f1_weighted": 0.470973,
            "precision": 0.437778,
            "precision_weighted": 0.505244,
            "recall": 0.457861,
            "recall_weighted": 0.46,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.52,
            "f1": 0.470601,
            "f1_weighted": 0.532521,
            "precision": 0.470741,
            "precision_weighted": 0.554137,
            "recall": 0.480412,
            "recall_weighted": 0.52,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.466667,
            "f1": 0.448032,
            "f1_weighted": 0.490426,
            "precision": 0.476545,
            "precision_weighted": 0.579149,
            "recall": 0.477654,
            "recall_weighted": 0.466667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.56,
            "f1": 0.517802,
            "f1_weighted": 0.572688,
            "precision": 0.517312,
            "precision_weighted": 0.596481,
            "recall": 0.530521,
            "recall_weighted": 0.56,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.48,
            "f1": 0.452742,
            "f1_weighted": 0.503605,
            "precision": 0.471353,
            "precision_weighted": 0.567989,
            "recall": 0.473267,
            "recall_weighted": 0.48,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.473333,
            "f1": 0.45571,
            "f1_weighted": 0.485641,
            "precision": 0.460141,
            "precision_weighted": 0.532291,
            "recall": 0.48167,
            "recall_weighted": 0.473333,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.488,
        "f1": 0.462692,
        "f1_weighted": 0.504153,
        "precision": 0.474811,
        "precision_weighted": 0.553941,
        "recall": 0.484632,
        "recall_weighted": 0.488,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.488,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 5.399604320526123,
  "kg_co2_emissions": null
}