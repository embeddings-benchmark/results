{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 28.32297396659851,
  "kg_co2_emissions": 0.0012304869072344185,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.28857421875,
        "f1": 0.2777094290524803,
        "f1_weighted": 0.27771768472301495,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.28857421875,
        "scores_per_experiment": [
          {
            "accuracy": 0.2900390625,
            "f1": 0.28977149262244,
            "f1_weighted": 0.2897721555409261
          },
          {
            "accuracy": 0.291015625,
            "f1": 0.2817874385376544,
            "f1_weighted": 0.2818367516127174
          },
          {
            "accuracy": 0.30224609375,
            "f1": 0.2916960209910383,
            "f1_weighted": 0.29170423132969586
          },
          {
            "accuracy": 0.3046875,
            "f1": 0.29861922888174985,
            "f1_weighted": 0.29860039190709037
          },
          {
            "accuracy": 0.26953125,
            "f1": 0.26553314405455314,
            "f1_weighted": 0.2654943986709275
          },
          {
            "accuracy": 0.26708984375,
            "f1": 0.23833815454741525,
            "f1_weighted": 0.2383506361321339
          },
          {
            "accuracy": 0.25244140625,
            "f1": 0.2516337042811191,
            "f1_weighted": 0.251642258866782
          },
          {
            "accuracy": 0.3310546875,
            "f1": 0.32436346821905016,
            "f1_weighted": 0.32438291142672504
          },
          {
            "accuracy": 0.31689453125,
            "f1": 0.30195118962810363,
            "f1_weighted": 0.30196723022275235
          },
          {
            "accuracy": 0.2607421875,
            "f1": 0.23340044876167884,
            "f1_weighted": 0.2334258815203988
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.286474609375,
        "f1": 0.274497048942113,
        "f1_weighted": 0.27449990402526414,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.286474609375,
        "scores_per_experiment": [
          {
            "accuracy": 0.32177734375,
            "f1": 0.32051770906614563,
            "f1_weighted": 0.3205127996076228
          },
          {
            "accuracy": 0.2607421875,
            "f1": 0.25360191189872106,
            "f1_weighted": 0.2536452079373307
          },
          {
            "accuracy": 0.29150390625,
            "f1": 0.2776436746044651,
            "f1_weighted": 0.2776317438989692
          },
          {
            "accuracy": 0.3173828125,
            "f1": 0.3086081531224159,
            "f1_weighted": 0.3085837218940255
          },
          {
            "accuracy": 0.27490234375,
            "f1": 0.26888553469415594,
            "f1_weighted": 0.2688551107633614
          },
          {
            "accuracy": 0.27734375,
            "f1": 0.24894858511585033,
            "f1_weighted": 0.2489744969495032
          },
          {
            "accuracy": 0.23193359375,
            "f1": 0.22732087605874715,
            "f1_weighted": 0.22733638749669632
          },
          {
            "accuracy": 0.3125,
            "f1": 0.306972599193736,
            "f1_weighted": 0.30697585268353356
          },
          {
            "accuracy": 0.3076171875,
            "f1": 0.29079292219547576,
            "f1_weighted": 0.29078554820439617
          },
          {
            "accuracy": 0.26904296875,
            "f1": 0.241678523471417,
            "f1_weighted": 0.24169817081720318
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}