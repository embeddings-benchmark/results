{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 63.202149629592896,
  "kg_co2_emissions": 0.0027043852244068066,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.997950495049505,
        "cosine_accuracy_threshold": 0.8713340163230896,
        "cosine_ap": 0.9560759476086738,
        "cosine_f1": 0.8958228485153499,
        "cosine_f1_threshold": 0.8713340163230896,
        "cosine_precision": 0.9017223910840932,
        "cosine_recall": 0.89,
        "dot_accuracy": 0.997950495049505,
        "dot_accuracy_threshold": 0.8713338971138,
        "dot_ap": 0.956075961598125,
        "dot_f1": 0.8958228485153499,
        "dot_f1_threshold": 0.8713338971138,
        "dot_precision": 0.9017223910840932,
        "dot_recall": 0.89,
        "euclidean_accuracy": 0.997950495049505,
        "euclidean_accuracy_threshold": 0.5072789192199707,
        "euclidean_ap": 0.9560759476086738,
        "euclidean_f1": 0.8958228485153499,
        "euclidean_f1_threshold": 0.5072789192199707,
        "euclidean_precision": 0.9017223910840932,
        "euclidean_recall": 0.89,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.956075961598125,
        "manhattan_accuracy": 0.9979405940594059,
        "manhattan_accuracy_threshold": 12.61263656616211,
        "manhattan_ap": 0.956040065499925,
        "manhattan_f1": 0.8951048951048951,
        "manhattan_f1_threshold": 13.037997245788574,
        "manhattan_precision": 0.8942115768463074,
        "manhattan_recall": 0.896,
        "max_accuracy": 0.997950495049505,
        "max_ap": 0.956075961598125,
        "max_f1": 0.8958228485153499,
        "max_precision": 0.9017223910840932,
        "max_recall": 0.896,
        "similarity_accuracy": 0.997950495049505,
        "similarity_accuracy_threshold": 0.8713340163230896,
        "similarity_ap": 0.9560759476086738,
        "similarity_f1": 0.8958228485153499,
        "similarity_f1_threshold": 0.8713340163230896,
        "similarity_precision": 0.9017223910840932,
        "similarity_recall": 0.89
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.9983168316831683,
        "cosine_accuracy_threshold": 0.8727831244468689,
        "cosine_ap": 0.9659886382108079,
        "cosine_f1": 0.9129989764585466,
        "cosine_f1_threshold": 0.8727831244468689,
        "cosine_precision": 0.9350104821802935,
        "cosine_recall": 0.892,
        "dot_accuracy": 0.9983168316831683,
        "dot_accuracy_threshold": 0.8727830648422241,
        "dot_ap": 0.9659879694548892,
        "dot_f1": 0.9129989764585466,
        "dot_f1_threshold": 0.8727830648422241,
        "dot_precision": 0.9350104821802935,
        "dot_recall": 0.892,
        "euclidean_accuracy": 0.9983168316831683,
        "euclidean_accuracy_threshold": 0.5044142007827759,
        "euclidean_ap": 0.9659886382108079,
        "euclidean_f1": 0.9129989764585466,
        "euclidean_f1_threshold": 0.5044142007827759,
        "euclidean_precision": 0.9350104821802935,
        "euclidean_recall": 0.892,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.9662073597361461,
        "manhattan_accuracy": 0.9982871287128713,
        "manhattan_accuracy_threshold": 12.742788314819336,
        "manhattan_ap": 0.9662073597361461,
        "manhattan_f1": 0.9129340714645193,
        "manhattan_f1_threshold": 13.138975143432617,
        "manhattan_precision": 0.9189463019250254,
        "manhattan_recall": 0.907,
        "max_accuracy": 0.9983168316831683,
        "max_ap": 0.9662073597361461,
        "max_f1": 0.9129989764585466,
        "max_precision": 0.9350104821802935,
        "max_recall": 0.907,
        "similarity_accuracy": 0.9983168316831683,
        "similarity_accuracy_threshold": 0.8727831244468689,
        "similarity_ap": 0.9659886382108079,
        "similarity_f1": 0.9129989764585466,
        "similarity_f1_threshold": 0.8727831244468689,
        "similarity_precision": 0.9350104821802935,
        "similarity_recall": 0.892
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}