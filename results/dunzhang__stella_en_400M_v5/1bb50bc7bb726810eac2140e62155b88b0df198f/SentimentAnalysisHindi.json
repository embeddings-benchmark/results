{
  "dataset_revision": "1beac1b941da76a9c51e3e5b39d230fde9a80983",
  "evaluation_time": 13.81373119354248,
  "kg_co2_emissions": 0.0005923235956012479,
  "mteb_version": "1.18.0",
  "scores": {
    "train": [
      {
        "accuracy": 0.4560546875,
        "f1": 0.4390504994014181,
        "f1_weighted": 0.4598265026144769,
        "hf_subset": "default",
        "languages": [
          "hin-Deva"
        ],
        "main_score": 0.4390504994014181,
        "scores_per_experiment": [
          {
            "accuracy": 0.4697265625,
            "f1": 0.46063213827275495,
            "f1_weighted": 0.48902575180819446
          },
          {
            "accuracy": 0.29345703125,
            "f1": 0.29398777135423865,
            "f1_weighted": 0.2877732682067993
          },
          {
            "accuracy": 0.3896484375,
            "f1": 0.38539827684360334,
            "f1_weighted": 0.39497408301330045
          },
          {
            "accuracy": 0.48583984375,
            "f1": 0.47145351992562406,
            "f1_weighted": 0.49731746318083864
          },
          {
            "accuracy": 0.4140625,
            "f1": 0.3966164235891639,
            "f1_weighted": 0.40805695921874635
          },
          {
            "accuracy": 0.462890625,
            "f1": 0.4500789641954717,
            "f1_weighted": 0.4734425367723948
          },
          {
            "accuracy": 0.5380859375,
            "f1": 0.5104873375364377,
            "f1_weighted": 0.540033961057313
          },
          {
            "accuracy": 0.55126953125,
            "f1": 0.5113512573249601,
            "f1_weighted": 0.5492660546051922
          },
          {
            "accuracy": 0.43408203125,
            "f1": 0.4180128565299373,
            "f1_weighted": 0.43660261357861524
          },
          {
            "accuracy": 0.521484375,
            "f1": 0.49248644844199013,
            "f1_weighted": 0.521772334703374
          }
        ]
      }
    ]
  },
  "task_name": "SentimentAnalysisHindi"
}