{
    "dataset_revision": "ad9991cb51e31e31e430383c75ffb2885547b5f0",
    "task_name": "CQADupstackEnglishRetrieval",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "default",
                "languages": [
                    "eng-Latn"
                ],
                "map_at_1": 0.37488999999999995,
                "map_at_10": 0.50708,
                "map_at_100": 0.52101,
                "map_at_1000": 0.5222,
                "map_at_20": 0.51514,
                "map_at_3": 0.46915,
                "map_at_5": 0.49185,
                "mrr_at_1": 0.47643,
                "mrr_at_10": 0.56806,
                "mrr_at_100": 0.57369,
                "mrr_at_1000": 0.57399,
                "mrr_at_20": 0.57141,
                "mrr_at_3": 0.54437,
                "mrr_at_5": 0.55956,
                "ndcg_at_1": 0.47643,
                "ndcg_at_10": 0.56989,
                "ndcg_at_100": 0.60996,
                "ndcg_at_1000": 0.62668,
                "ndcg_at_20": 0.58637,
                "ndcg_at_3": 0.52265,
                "ndcg_at_5": 0.54685,
                "precision_at_1": 0.47643,
                "precision_at_10": 0.10879,
                "precision_at_100": 0.01632,
                "precision_at_1000": 0.00211,
                "precision_at_20": 0.06338,
                "precision_at_3": 0.2552,
                "precision_at_5": 0.18228999999999998,
                "recall_at_1": 0.37488999999999995,
                "recall_at_10": 0.6810300000000001,
                "recall_at_100": 0.84497,
                "recall_at_1000": 0.94402,
                "recall_at_20": 0.7384900000000001,
                "recall_at_3": 0.53925,
                "recall_at_5": 0.60878,
                "main_score": 0.56989
            }
        ]
    }
}