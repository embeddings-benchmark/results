{
  "dataset_revision": "9af5657575a669dc18c7f897a67287ff7d1a0c65",
  "task_name": "OpenTenderClassification",
  "mteb_version": "2.1.17",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.451282,
            "f1": 0.453086,
            "f1_weighted": 0.453094,
            "precision": 0.464189,
            "precision_weighted": 0.464202,
            "recall": 0.451269,
            "recall_weighted": 0.451282,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.449944,
            "f1": 0.449425,
            "f1_weighted": 0.449382,
            "precision": 0.460232,
            "precision_weighted": 0.460203,
            "recall": 0.449988,
            "recall_weighted": 0.449944,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.448384,
            "f1": 0.443616,
            "f1_weighted": 0.443647,
            "precision": 0.449422,
            "precision_weighted": 0.449427,
            "recall": 0.448331,
            "recall_weighted": 0.448384,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.444147,
            "f1": 0.443753,
            "f1_weighted": 0.443686,
            "precision": 0.451763,
            "precision_weighted": 0.451729,
            "recall": 0.44424,
            "recall_weighted": 0.444147,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.424972,
            "f1": 0.42527,
            "f1_weighted": 0.425166,
            "precision": 0.433074,
            "precision_weighted": 0.432997,
            "recall": 0.425105,
            "recall_weighted": 0.424972,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.433222,
            "f1": 0.434437,
            "f1_weighted": 0.43428,
            "precision": 0.445045,
            "precision_weighted": 0.444906,
            "recall": 0.433396,
            "recall_weighted": 0.433222,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.432107,
            "f1": 0.434453,
            "f1_weighted": 0.43444,
            "precision": 0.453077,
            "precision_weighted": 0.453027,
            "recall": 0.432066,
            "recall_weighted": 0.432107,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.439242,
            "f1": 0.433969,
            "f1_weighted": 0.433885,
            "precision": 0.441515,
            "precision_weighted": 0.441472,
            "recall": 0.439381,
            "recall_weighted": 0.439242,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.425641,
            "f1": 0.424048,
            "f1_weighted": 0.423884,
            "precision": 0.427512,
            "precision_weighted": 0.427324,
            "recall": 0.425778,
            "recall_weighted": 0.425641,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.438127,
            "f1": 0.440514,
            "f1_weighted": 0.440495,
            "precision": 0.45068,
            "precision_weighted": 0.45063,
            "recall": 0.438112,
            "recall_weighted": 0.438127,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.438707,
        "f1": 0.438257,
        "f1_weighted": 0.438196,
        "precision": 0.447651,
        "precision_weighted": 0.447592,
        "recall": 0.438767,
        "recall_weighted": 0.438707,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.438257,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 50.714277267456055,
  "kg_co2_emissions": null
}