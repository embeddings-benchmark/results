{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "task_name": "InappropriatenessClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.600488,
        "f1": 0.597237,
        "f1_weighted": 0.597237,
        "ap": 0.56417,
        "ap_weighted": 0.56417,
        "scores_per_experiment": [
          {
            "accuracy": 0.634766,
            "f1": 0.633785,
            "f1_weighted": 0.633785,
            "ap": 0.583841,
            "ap_weighted": 0.583841
          },
          {
            "accuracy": 0.647949,
            "f1": 0.647637,
            "f1_weighted": 0.647637,
            "ap": 0.594633,
            "ap_weighted": 0.594633
          },
          {
            "accuracy": 0.660645,
            "f1": 0.650851,
            "f1_weighted": 0.650851,
            "ap": 0.619127,
            "ap_weighted": 0.619127
          },
          {
            "accuracy": 0.570801,
            "f1": 0.569536,
            "f1_weighted": 0.569536,
            "ap": 0.539923,
            "ap_weighted": 0.539923
          },
          {
            "accuracy": 0.603516,
            "f1": 0.594218,
            "f1_weighted": 0.594218,
            "ap": 0.567126,
            "ap_weighted": 0.567126
          },
          {
            "accuracy": 0.501465,
            "f1": 0.499022,
            "f1_weighted": 0.499022,
            "ap": 0.500735,
            "ap_weighted": 0.500735
          },
          {
            "accuracy": 0.658203,
            "f1": 0.658164,
            "f1_weighted": 0.658164,
            "ap": 0.603603,
            "ap_weighted": 0.603603
          },
          {
            "accuracy": 0.57959,
            "f1": 0.573785,
            "f1_weighted": 0.573785,
            "ap": 0.544931,
            "ap_weighted": 0.544931
          },
          {
            "accuracy": 0.532715,
            "f1": 0.530228,
            "f1_weighted": 0.530228,
            "ap": 0.517292,
            "ap_weighted": 0.517292
          },
          {
            "accuracy": 0.615234,
            "f1": 0.61514,
            "f1_weighted": 0.61514,
            "ap": 0.570494,
            "ap_weighted": 0.570494
          }
        ],
        "main_score": 0.600488,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 10.029211521148682,
  "kg_co2_emissions": 0.00039372495677554846
}