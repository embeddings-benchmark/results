{
  "dataset_revision": "bd27d0058bea2ad52470d9072a3b5da6b97c1ac3",
  "task_name": "VaccinChatNLClassification",
  "mteb_version": "2.1.17",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.552991,
            "f1": 0.572616,
            "f1_weighted": 0.544117,
            "precision": 0.556273,
            "precision_weighted": 0.621858,
            "recall": 0.677958,
            "recall_weighted": 0.552991,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.523932,
            "f1": 0.544925,
            "f1_weighted": 0.51578,
            "precision": 0.531879,
            "precision_weighted": 0.618423,
            "recall": 0.650488,
            "recall_weighted": 0.523932,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.536752,
            "f1": 0.546766,
            "f1_weighted": 0.530288,
            "precision": 0.523366,
            "precision_weighted": 0.624354,
            "recall": 0.679061,
            "recall_weighted": 0.536752,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.526496,
            "f1": 0.554261,
            "f1_weighted": 0.52441,
            "precision": 0.5494,
            "precision_weighted": 0.605059,
            "recall": 0.663041,
            "recall_weighted": 0.526496,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.55812,
            "f1": 0.566823,
            "f1_weighted": 0.551435,
            "precision": 0.54373,
            "precision_weighted": 0.619793,
            "recall": 0.678124,
            "recall_weighted": 0.55812,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.534188,
            "f1": 0.545222,
            "f1_weighted": 0.533588,
            "precision": 0.531749,
            "precision_weighted": 0.642062,
            "recall": 0.667328,
            "recall_weighted": 0.534188,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.552137,
            "f1": 0.55917,
            "f1_weighted": 0.540101,
            "precision": 0.542147,
            "precision_weighted": 0.615816,
            "recall": 0.663069,
            "recall_weighted": 0.552137,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.528205,
            "f1": 0.538752,
            "f1_weighted": 0.518789,
            "precision": 0.509474,
            "precision_weighted": 0.59822,
            "recall": 0.660286,
            "recall_weighted": 0.528205,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.52735,
            "f1": 0.547602,
            "f1_weighted": 0.513222,
            "precision": 0.536841,
            "precision_weighted": 0.583637,
            "recall": 0.656049,
            "recall_weighted": 0.52735,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.530769,
            "f1": 0.526971,
            "f1_weighted": 0.520171,
            "precision": 0.510064,
            "precision_weighted": 0.593055,
            "recall": 0.639196,
            "recall_weighted": 0.530769,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.537094,
        "f1": 0.550311,
        "f1_weighted": 0.52919,
        "precision": 0.533492,
        "precision_weighted": 0.612228,
        "recall": 0.66346,
        "recall_weighted": 0.537094,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.550311,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 43.85198497772217,
  "kg_co2_emissions": null
}