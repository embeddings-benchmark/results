{
  "dataset_revision": "9af5657575a669dc18c7f897a67287ff7d1a0c65",
  "task_name": "OpenTenderClassification",
  "mteb_version": "2.3.0",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.463099,
            "f1": 0.442673,
            "f1_weighted": 0.442681,
            "precision": 0.458572,
            "precision_weighted": 0.458457,
            "recall": 0.463048,
            "recall_weighted": 0.463099,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.454181,
            "f1": 0.435649,
            "f1_weighted": 0.435529,
            "precision": 0.445258,
            "precision_weighted": 0.445181,
            "recall": 0.454319,
            "recall_weighted": 0.454181,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.471572,
            "f1": 0.451791,
            "f1_weighted": 0.451738,
            "precision": 0.467746,
            "precision_weighted": 0.467735,
            "recall": 0.471647,
            "recall_weighted": 0.471572,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.471572,
            "f1": 0.451974,
            "f1_weighted": 0.451928,
            "precision": 0.464034,
            "precision_weighted": 0.464001,
            "recall": 0.471643,
            "recall_weighted": 0.471572,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.436566,
            "f1": 0.421534,
            "f1_weighted": 0.421495,
            "precision": 0.454345,
            "precision_weighted": 0.454316,
            "recall": 0.436552,
            "recall_weighted": 0.436566,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.459309,
            "f1": 0.439761,
            "f1_weighted": 0.43968,
            "precision": 0.454604,
            "precision_weighted": 0.454597,
            "recall": 0.459458,
            "recall_weighted": 0.459309,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.465329,
            "f1": 0.444361,
            "f1_weighted": 0.444259,
            "precision": 0.45657,
            "precision_weighted": 0.456477,
            "recall": 0.465398,
            "recall_weighted": 0.465329,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.455518,
            "f1": 0.43319,
            "f1_weighted": 0.433125,
            "precision": 0.448788,
            "precision_weighted": 0.448703,
            "recall": 0.455573,
            "recall_weighted": 0.455518,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.427871,
            "f1": 0.401581,
            "f1_weighted": 0.401482,
            "precision": 0.417962,
            "precision_weighted": 0.417758,
            "recall": 0.427881,
            "recall_weighted": 0.427871,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.447715,
            "f1": 0.424558,
            "f1_weighted": 0.424496,
            "precision": 0.434995,
            "precision_weighted": 0.434892,
            "recall": 0.447738,
            "recall_weighted": 0.447715,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.455273,
        "f1": 0.434707,
        "f1_weighted": 0.434641,
        "precision": 0.450288,
        "precision_weighted": 0.450212,
        "recall": 0.455326,
        "recall_weighted": 0.455273,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.434707,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 25.874855041503906,
  "kg_co2_emissions": null
}