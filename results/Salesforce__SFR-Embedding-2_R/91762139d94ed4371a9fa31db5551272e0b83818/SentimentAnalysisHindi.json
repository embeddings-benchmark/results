{
  "dataset_revision": "1beac1b941da76a9c51e3e5b39d230fde9a80983",
  "evaluation_time": 23.983693838119507,
  "kg_co2_emissions": 0.0016328844261662737,
  "mteb_version": "1.18.0",
  "scores": {
    "train": [
      {
        "accuracy": 0.6330078125,
        "f1": 0.6173571859257712,
        "f1_weighted": 0.6383663213358741,
        "hf_subset": "default",
        "languages": [
          "hin-Deva"
        ],
        "main_score": 0.6173571859257712,
        "scores_per_experiment": [
          {
            "accuracy": 0.64453125,
            "f1": 0.6239122794097353,
            "f1_weighted": 0.6549446880745624
          },
          {
            "accuracy": 0.6181640625,
            "f1": 0.6001502732312299,
            "f1_weighted": 0.6309456808767024
          },
          {
            "accuracy": 0.63720703125,
            "f1": 0.6209187670668589,
            "f1_weighted": 0.6421866226224042
          },
          {
            "accuracy": 0.5830078125,
            "f1": 0.5803481575536091,
            "f1_weighted": 0.5824144264667706
          },
          {
            "accuracy": 0.6416015625,
            "f1": 0.6330909827723022,
            "f1_weighted": 0.6431263630330919
          },
          {
            "accuracy": 0.63671875,
            "f1": 0.6209395825178748,
            "f1_weighted": 0.6414005350202647
          },
          {
            "accuracy": 0.646484375,
            "f1": 0.6335630229959254,
            "f1_weighted": 0.650037120299408
          },
          {
            "accuracy": 0.6337890625,
            "f1": 0.6023655622761879,
            "f1_weighted": 0.6392850027612114
          },
          {
            "accuracy": 0.6064453125,
            "f1": 0.5908495822930422,
            "f1_weighted": 0.6157220513164781
          },
          {
            "accuracy": 0.68212890625,
            "f1": 0.6674336491409442,
            "f1_weighted": 0.6836007228878482
          }
        ]
      }
    ]
  },
  "task_name": "SentimentAnalysisHindi"
}