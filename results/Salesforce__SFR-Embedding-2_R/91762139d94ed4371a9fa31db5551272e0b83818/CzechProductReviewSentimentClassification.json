{
  "dataset_revision": "2e6fedf42c9c104e83dfd95c3a453721e683e244",
  "evaluation_time": 29.445870637893677,
  "kg_co2_emissions": 0.0019434905436580539,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.556689453125,
        "f1": 0.5411363995161691,
        "f1_weighted": 0.5410667974256355,
        "hf_subset": "default",
        "languages": [
          "ces-Latn"
        ],
        "main_score": 0.556689453125,
        "scores_per_experiment": [
          {
            "accuracy": 0.5595703125,
            "f1": 0.5560579756785341,
            "f1_weighted": 0.5559834676007251
          },
          {
            "accuracy": 0.55908203125,
            "f1": 0.524394505157712,
            "f1_weighted": 0.52431162143295
          },
          {
            "accuracy": 0.5205078125,
            "f1": 0.5003737263006892,
            "f1_weighted": 0.5002827678312736
          },
          {
            "accuracy": 0.51806640625,
            "f1": 0.5088163156967767,
            "f1_weighted": 0.5087504078331335
          },
          {
            "accuracy": 0.51904296875,
            "f1": 0.49642346622181527,
            "f1_weighted": 0.4963535253131065
          },
          {
            "accuracy": 0.54150390625,
            "f1": 0.5268631324416364,
            "f1_weighted": 0.5267953176820057
          },
          {
            "accuracy": 0.6025390625,
            "f1": 0.5871043338209478,
            "f1_weighted": 0.5870462569013369
          },
          {
            "accuracy": 0.60009765625,
            "f1": 0.5752589387614292,
            "f1_weighted": 0.57519364939871
          },
          {
            "accuracy": 0.5546875,
            "f1": 0.5569942566958243,
            "f1_weighted": 0.5569316770256125
          },
          {
            "accuracy": 0.591796875,
            "f1": 0.5790773443863261,
            "f1_weighted": 0.5790192832375005
          }
        ]
      }
    ]
  },
  "task_name": "CzechProductReviewSentimentClassification"
}