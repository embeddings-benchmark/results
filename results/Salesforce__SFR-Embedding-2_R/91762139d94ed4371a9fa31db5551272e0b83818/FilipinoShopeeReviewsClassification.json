{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 47.45882558822632,
  "kg_co2_emissions": 0.0027934856263849136,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.37548828125,
        "f1": 0.33348370335881306,
        "f1_weighted": 0.33346472283966777,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.37548828125,
        "scores_per_experiment": [
          {
            "accuracy": 0.36376953125,
            "f1": 0.3382853549688968,
            "f1_weighted": 0.3382272226929647
          },
          {
            "accuracy": 0.388671875,
            "f1": 0.3436235312229233,
            "f1_weighted": 0.34364998091051224
          },
          {
            "accuracy": 0.369140625,
            "f1": 0.31051069716302193,
            "f1_weighted": 0.31049750467219994
          },
          {
            "accuracy": 0.39453125,
            "f1": 0.3623846746035392,
            "f1_weighted": 0.3623675677086008
          },
          {
            "accuracy": 0.37451171875,
            "f1": 0.3226163841315074,
            "f1_weighted": 0.3225682031584157
          },
          {
            "accuracy": 0.3818359375,
            "f1": 0.3275405141864007,
            "f1_weighted": 0.32750050982394674
          },
          {
            "accuracy": 0.38525390625,
            "f1": 0.3387943734717617,
            "f1_weighted": 0.3387636165557188
          },
          {
            "accuracy": 0.37646484375,
            "f1": 0.34516429113098634,
            "f1_weighted": 0.3451810843582547
          },
          {
            "accuracy": 0.375,
            "f1": 0.3445616295167441,
            "f1_weighted": 0.3445316446502761
          },
          {
            "accuracy": 0.345703125,
            "f1": 0.30135558319234923,
            "f1_weighted": 0.301359893865788
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.375390625,
        "f1": 0.3327852598133261,
        "f1_weighted": 0.33276598136727636,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.375390625,
        "scores_per_experiment": [
          {
            "accuracy": 0.37158203125,
            "f1": 0.3466720311844016,
            "f1_weighted": 0.3466185820004406
          },
          {
            "accuracy": 0.38037109375,
            "f1": 0.32977106520443644,
            "f1_weighted": 0.32979938232438044
          },
          {
            "accuracy": 0.3779296875,
            "f1": 0.3190308247477825,
            "f1_weighted": 0.31901775295111545
          },
          {
            "accuracy": 0.380859375,
            "f1": 0.3476293903512672,
            "f1_weighted": 0.3475956021013106
          },
          {
            "accuracy": 0.3701171875,
            "f1": 0.32107380039399985,
            "f1_weighted": 0.3210333894028017
          },
          {
            "accuracy": 0.3759765625,
            "f1": 0.32087166647476906,
            "f1_weighted": 0.32084963613830475
          },
          {
            "accuracy": 0.36328125,
            "f1": 0.3146769382398036,
            "f1_weighted": 0.3146512240049108
          },
          {
            "accuracy": 0.3828125,
            "f1": 0.3483513390140503,
            "f1_weighted": 0.34836120804389586
          },
          {
            "accuracy": 0.39208984375,
            "f1": 0.36420166176787316,
            "f1_weighted": 0.36416135886984075
          },
          {
            "accuracy": 0.35888671875,
            "f1": 0.31557388075487697,
            "f1_weighted": 0.3155716778357627
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}