{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 19459.822299957275,
  "kg_co2_emissions": 1.8170366632807047,
  "mteb_version": "1.18.0",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.06429909042268593,
        "map": 0.08138614254122427,
        "mrr": 0.06429909042268593,
        "nAUC_map_diff1": 0.11542127886386462,
        "nAUC_map_max": 0.022284571230218084,
        "nAUC_map_std": 0.21792702286093726,
        "nAUC_mrr_diff1": 0.10988916232491425,
        "nAUC_mrr_max": 0.012370868995758962,
        "nAUC_mrr_std": 0.17706078109632456
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09865760651968951,
        "map": 0.11847004503068315,
        "mrr": 0.09865760651968951,
        "nAUC_map_diff1": 0.14735942582034442,
        "nAUC_map_max": -0.013291353319149206,
        "nAUC_map_std": 0.051794948079853236,
        "nAUC_mrr_diff1": 0.1477194511636017,
        "nAUC_mrr_max": -0.015950568658997407,
        "nAUC_mrr_std": 0.042291549473185246
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.10897741572289557,
        "map": 0.12724966870356702,
        "mrr": 0.10897741572289557,
        "nAUC_map_diff1": 0.13725954956938863,
        "nAUC_map_max": -0.010102042423406211,
        "nAUC_map_std": 0.08187643563948123,
        "nAUC_mrr_diff1": 0.1434310155326506,
        "nAUC_mrr_max": -0.0013521683120664768,
        "nAUC_mrr_std": 0.07213185504669048
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.11217178390168199,
        "map": 0.13054410892305435,
        "mrr": 0.11217178390168199,
        "nAUC_map_diff1": 0.20592248116691983,
        "nAUC_map_max": 0.08537115974750506,
        "nAUC_map_std": 0.1348913840904542,
        "nAUC_mrr_diff1": 0.205523762055008,
        "nAUC_mrr_max": 0.08494869147986124,
        "nAUC_mrr_std": 0.11952038223252519
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08144740296457853,
        "map": 0.09696557516187357,
        "mrr": 0.08144740296457853,
        "nAUC_map_diff1": 0.2061722638735288,
        "nAUC_map_max": -0.02699237858583697,
        "nAUC_map_std": 0.0336904402941696,
        "nAUC_mrr_diff1": 0.2069687695580719,
        "nAUC_mrr_max": -0.028164651329031898,
        "nAUC_mrr_std": 0.024264080543885656
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.12482431096958384,
        "map": 0.1434299174265978,
        "mrr": 0.12482431096958384,
        "nAUC_map_diff1": 0.1415544286447483,
        "nAUC_map_max": -0.032530773141302934,
        "nAUC_map_std": 0.008022029846812087,
        "nAUC_mrr_diff1": 0.1438091314300694,
        "nAUC_mrr_max": -0.029066700497397388,
        "nAUC_mrr_std": 0.012167655118936343
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}