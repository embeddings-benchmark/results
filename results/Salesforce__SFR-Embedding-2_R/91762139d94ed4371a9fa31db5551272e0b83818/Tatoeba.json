{
  "dataset_revision": "69e8f12da6e31d59addadda9a9c8a2e601a0e282",
  "evaluation_time": 954.3071985244751,
  "kg_co2_emissions": 0.05012174726490575,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.329,
        "f1": 0.28220719362337016,
        "hf_subset": "tat-eng",
        "languages": [
          "tat-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.28220719362337016,
        "precision": 0.2682577131202131,
        "recall": 0.329
      },
      {
        "accuracy": 0.926,
        "f1": 0.9055333333333333,
        "hf_subset": "srp-eng",
        "languages": [
          "srp-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.9055333333333333,
        "precision": 0.89625,
        "recall": 0.926
      },
      {
        "accuracy": 0.907,
        "f1": 0.8819000000000001,
        "hf_subset": "ara-eng",
        "languages": [
          "ara-Arab",
          "eng-Latn"
        ],
        "main_score": 0.8819000000000001,
        "precision": 0.8706166666666667,
        "recall": 0.907
      },
      {
        "accuracy": 0.4233576642335766,
        "f1": 0.3740238674545244,
        "hf_subset": "cha-eng",
        "languages": [
          "cha-Latn",
          "eng-Latn"
        ],
        "main_score": 0.3740238674545244,
        "precision": 0.36253821199076675,
        "recall": 0.4233576642335766
      },
      {
        "accuracy": 0.947,
        "f1": 0.9318333333333333,
        "hf_subset": "ces-eng",
        "languages": [
          "ces-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9318333333333333,
        "precision": 0.9246666666666667,
        "recall": 0.947
      },
      {
        "accuracy": 0.635,
        "f1": 0.5844137832325912,
        "hf_subset": "oci-eng",
        "languages": [
          "oci-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5844137832325912,
        "precision": 0.5675521825396824,
        "recall": 0.635
      },
      {
        "accuracy": 0.7665369649805448,
        "f1": 0.732180983737404,
        "hf_subset": "nov-eng",
        "languages": [
          "nov-Latn",
          "eng-Latn"
        ],
        "main_score": 0.732180983737404,
        "precision": 0.7207244765610524,
        "recall": 0.7665369649805448
      },
      {
        "accuracy": 0.771,
        "f1": 0.7268214285714285,
        "hf_subset": "nds-eng",
        "languages": [
          "nds-Latn",
          "eng-Latn"
        ],
        "main_score": 0.7268214285714285,
        "precision": 0.7101888278388279,
        "recall": 0.771
      },
      {
        "accuracy": 0.569,
        "f1": 0.5124651489725631,
        "hf_subset": "est-eng",
        "languages": [
          "est-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5124651489725631,
        "precision": 0.4936474386724386,
        "recall": 0.569
      },
      {
        "accuracy": 0.781,
        "f1": 0.7359373015873015,
        "hf_subset": "ido-eng",
        "languages": [
          "ido-Latn",
          "eng-Latn"
        ],
        "main_score": 0.7359373015873015,
        "precision": 0.7191428571428571,
        "recall": 0.781
      },
      {
        "accuracy": 0.091,
        "f1": 0.07668100071239607,
        "hf_subset": "cor-eng",
        "languages": [
          "cor-Latn",
          "eng-Latn"
        ],
        "main_score": 0.07668100071239607,
        "precision": 0.0736191915535445,
        "recall": 0.091
      },
      {
        "accuracy": 0.6076923076923076,
        "f1": 0.5448717948717948,
        "hf_subset": "swh-eng",
        "languages": [
          "swh-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5448717948717948,
        "precision": 0.5211965811965812,
        "recall": 0.6076923076923076
      },
      {
        "accuracy": 0.924,
        "f1": 0.9078999999999999,
        "hf_subset": "cat-eng",
        "languages": [
          "cat-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9078999999999999,
        "precision": 0.9008690476190476,
        "recall": 0.924
      },
      {
        "accuracy": 0.951,
        "f1": 0.9383333333333332,
        "hf_subset": "ukr-eng",
        "languages": [
          "ukr-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.9383333333333332,
        "precision": 0.9321666666666667,
        "recall": 0.951
      },
      {
        "accuracy": 0.42,
        "f1": 0.37283080808080804,
        "hf_subset": "ceb-eng",
        "languages": [
          "ceb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.37283080808080804,
        "precision": 0.3572514013764014,
        "recall": 0.42
      },
      {
        "accuracy": 0.886,
        "f1": 0.8611047619047619,
        "hf_subset": "yue-eng",
        "languages": [
          "yue-Hant",
          "eng-Latn"
        ],
        "main_score": 0.8611047619047619,
        "precision": 0.85025,
        "recall": 0.886
      },
      {
        "accuracy": 0.36752136752136755,
        "f1": 0.29886779886779885,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.29886779886779885,
        "precision": 0.27744539411206076,
        "recall": 0.36752136752136755
      },
      {
        "accuracy": 0.32272727272727275,
        "f1": 0.2875173645427656,
        "hf_subset": "mon-eng",
        "languages": [
          "mon-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.2875173645427656,
        "precision": 0.27773668566282206,
        "recall": 0.32272727272727275
      },
      {
        "accuracy": 0.884,
        "f1": 0.8547761904761904,
        "hf_subset": "epo-eng",
        "languages": [
          "epo-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8547761904761904,
        "precision": 0.8421102564102564,
        "recall": 0.884
      },
      {
        "accuracy": 0.938,
        "f1": 0.9213333333333332,
        "hf_subset": "ind-eng",
        "languages": [
          "ind-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9213333333333332,
        "precision": 0.9136666666666667,
        "recall": 0.938
      },
      {
        "accuracy": 0.996,
        "f1": 0.9946666666666666,
        "hf_subset": "deu-eng",
        "languages": [
          "deu-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9946666666666666,
        "precision": 0.994,
        "recall": 0.996
      },
      {
        "accuracy": 0.865,
        "f1": 0.8330333333333333,
        "hf_subset": "mkd-eng",
        "languages": [
          "mkd-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.8330333333333333,
        "precision": 0.8190690476190476,
        "recall": 0.865
      },
      {
        "accuracy": 0.6007237635705669,
        "f1": 0.5418152826068103,
        "hf_subset": "gla-eng",
        "languages": [
          "gla-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5418152826068103,
        "precision": 0.5191579546977617,
        "recall": 0.6007237635705669
      },
      {
        "accuracy": 0.943,
        "f1": 0.9288,
        "hf_subset": "zsm-eng",
        "languages": [
          "zsm-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9288,
        "precision": 0.9221666666666667,
        "recall": 0.943
      },
      {
        "accuracy": 0.19642857142857142,
        "f1": 0.15828136810279667,
        "hf_subset": "amh-eng",
        "languages": [
          "amh-Ethi",
          "eng-Latn"
        ],
        "main_score": 0.15828136810279667,
        "precision": 0.14927957294028724,
        "recall": 0.19642857142857142
      },
      {
        "accuracy": 0.8110236220472441,
        "f1": 0.7758530183727034,
        "hf_subset": "ast-eng",
        "languages": [
          "ast-Latn",
          "eng-Latn"
        ],
        "main_score": 0.7758530183727034,
        "precision": 0.7631233595800524,
        "recall": 0.8110236220472441
      },
      {
        "accuracy": 0.6172506738544474,
        "f1": 0.5547225236578336,
        "hf_subset": "hye-eng",
        "languages": [
          "hye-Armn",
          "eng-Latn"
        ],
        "main_score": 0.5547225236578336,
        "precision": 0.5325108030633636,
        "recall": 0.6172506738544474
      },
      {
        "accuracy": 0.6428571428571429,
        "f1": 0.597172619047619,
        "hf_subset": "swg-eng",
        "languages": [
          "swg-Latn",
          "eng-Latn"
        ],
        "main_score": 0.597172619047619,
        "precision": 0.5819302721088435,
        "recall": 0.6428571428571429
      },
      {
        "accuracy": 0.9322033898305084,
        "f1": 0.9130885122410547,
        "hf_subset": "bos-eng",
        "languages": [
          "bos-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9130885122410547,
        "precision": 0.9051318267419961,
        "recall": 0.9322033898305084
      },
      {
        "accuracy": 0.765,
        "f1": 0.7161603174603174,
        "hf_subset": "aze-eng",
        "languages": [
          "aze-Latn",
          "eng-Latn"
        ],
        "main_score": 0.7161603174603174,
        "precision": 0.6969016233766234,
        "recall": 0.765
      },
      {
        "accuracy": 0.988,
        "f1": 0.9853333333333333,
        "hf_subset": "spa-eng",
        "languages": [
          "spa-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9853333333333333,
        "precision": 0.984,
        "recall": 0.988
      },
      {
        "accuracy": 0.954,
        "f1": 0.9437333333333333,
        "hf_subset": "por-eng",
        "languages": [
          "por-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9437333333333333,
        "precision": 0.93875,
        "recall": 0.954
      },
      {
        "accuracy": 0.912,
        "f1": 0.8927833333333333,
        "hf_subset": "hun-eng",
        "languages": [
          "hun-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8927833333333333,
        "precision": 0.8843928571428571,
        "recall": 0.912
      },
      {
        "accuracy": 0.83,
        "f1": 0.7915380952380953,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.7915380952380953,
        "precision": 0.7753500000000001,
        "recall": 0.83
      },
      {
        "accuracy": 0.5880239520958084,
        "f1": 0.5192301377930121,
        "hf_subset": "orv-eng",
        "languages": [
          "orv-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.5192301377930121,
        "precision": 0.4948731109210151,
        "recall": 0.5880239520958084
      },
      {
        "accuracy": 0.6242171189979123,
        "f1": 0.5717895187623788,
        "hf_subset": "dsb-eng",
        "languages": [
          "dsb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5717895187623788,
        "precision": 0.5527570666401563,
        "recall": 0.6242171189979123
      },
      {
        "accuracy": 0.657,
        "f1": 0.6089039072039072,
        "hf_subset": "lvs-eng",
        "languages": [
          "lvs-Latn",
          "eng-Latn"
        ],
        "main_score": 0.6089039072039072,
        "precision": 0.5929475396825397,
        "recall": 0.657
      },
      {
        "accuracy": 0.82,
        "f1": 0.7766666666666667,
        "hf_subset": "cbk-eng",
        "languages": [
          "cbk-Latn",
          "eng-Latn"
        ],
        "main_score": 0.7766666666666667,
        "precision": 0.7589651515151515,
        "recall": 0.82
      },
      {
        "accuracy": 0.944,
        "f1": 0.9305666666666667,
        "hf_subset": "dan-eng",
        "languages": [
          "dan-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9305666666666667,
        "precision": 0.92425,
        "recall": 0.944
      },
      {
        "accuracy": 0.874,
        "f1": 0.8417333333333333,
        "hf_subset": "isl-eng",
        "languages": [
          "isl-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8417333333333333,
        "precision": 0.8275075757575757,
        "recall": 0.874
      },
      {
        "accuracy": 0.858,
        "f1": 0.8230222222222222,
        "hf_subset": "bel-eng",
        "languages": [
          "bel-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.8230222222222222,
        "precision": 0.808375,
        "recall": 0.858
      },
      {
        "accuracy": 0.308,
        "f1": 0.25659838494838494,
        "hf_subset": "uig-eng",
        "languages": [
          "uig-Arab",
          "eng-Latn"
        ],
        "main_score": 0.25659838494838494,
        "precision": 0.2413512588718471,
        "recall": 0.308
      },
      {
        "accuracy": 0.65,
        "f1": 0.6048079365079364,
        "hf_subset": "lit-eng",
        "languages": [
          "lit-Latn",
          "eng-Latn"
        ],
        "main_score": 0.6048079365079364,
        "precision": 0.5882392246642246,
        "recall": 0.65
      },
      {
        "accuracy": 0.48180494905385735,
        "f1": 0.41253650118278945,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.41253650118278945,
        "precision": 0.3892882131900883,
        "recall": 0.48180494905385735
      },
      {
        "accuracy": 0.892,
        "f1": 0.8648666666666668,
        "hf_subset": "pes-eng",
        "languages": [
          "pes-Arab",
          "eng-Latn"
        ],
        "main_score": 0.8648666666666668,
        "precision": 0.85275,
        "recall": 0.892
      },
      {
        "accuracy": 0.877,
        "f1": 0.8503666666666667,
        "hf_subset": "nno-eng",
        "languages": [
          "nno-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8503666666666667,
        "precision": 0.8392166666666667,
        "recall": 0.877
      },
      {
        "accuracy": 0.8507462686567164,
        "f1": 0.8186567164179105,
        "hf_subset": "ang-eng",
        "languages": [
          "ang-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8186567164179105,
        "precision": 0.806592039800995,
        "recall": 0.8507462686567164
      },
      {
        "accuracy": 0.893,
        "f1": 0.8687166666666667,
        "hf_subset": "slk-eng",
        "languages": [
          "slk-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8687166666666667,
        "precision": 0.8578761904761905,
        "recall": 0.893
      },
      {
        "accuracy": 0.4489571899012075,
        "f1": 0.3907482382235401,
        "hf_subset": "arq-eng",
        "languages": [
          "arq-Arab",
          "eng-Latn"
        ],
        "main_score": 0.3907482382235401,
        "precision": 0.3722329596335128,
        "recall": 0.4489571899012075
      },
      {
        "accuracy": 0.4434782608695652,
        "f1": 0.3886000690131125,
        "hf_subset": "kaz-eng",
        "languages": [
          "kaz-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.3886000690131125,
        "precision": 0.37098515564456797,
        "recall": 0.4434782608695652
      },
      {
        "accuracy": 0.972,
        "f1": 0.9636666666666667,
        "hf_subset": "pol-eng",
        "languages": [
          "pol-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9636666666666667,
        "precision": 0.9596666666666667,
        "recall": 0.972
      },
      {
        "accuracy": 0.33098591549295775,
        "f1": 0.2976609657947686,
        "hf_subset": "xho-eng",
        "languages": [
          "xho-Latn",
          "eng-Latn"
        ],
        "main_score": 0.2976609657947686,
        "precision": 0.28622246298302634,
        "recall": 0.33098591549295775
      },
      {
        "accuracy": 0.3195121951219512,
        "f1": 0.285849795828275,
        "hf_subset": "kur-eng",
        "languages": [
          "kur-Latn",
          "eng-Latn"
        ],
        "main_score": 0.285849795828275,
        "precision": 0.2763038408587747,
        "recall": 0.3195121951219512
      },
      {
        "accuracy": 0.962,
        "f1": 0.9503333333333333,
        "hf_subset": "vie-eng",
        "languages": [
          "vie-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9503333333333333,
        "precision": 0.9445,
        "recall": 0.962
      },
      {
        "accuracy": 0.913,
        "f1": 0.8920333333333333,
        "hf_subset": "glg-eng",
        "languages": [
          "glg-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8920333333333333,
        "precision": 0.8830547619047618,
        "recall": 0.913
      },
      {
        "accuracy": 0.94,
        "f1": 0.9258166666666666,
        "hf_subset": "swe-eng",
        "languages": [
          "swe-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9258166666666666,
        "precision": 0.9193928571428571,
        "recall": 0.94
      },
      {
        "accuracy": 0.765,
        "f1": 0.7206878223226049,
        "hf_subset": "lfn-eng",
        "languages": [
          "lfn-Latn",
          "eng-Latn"
        ],
        "main_score": 0.7206878223226049,
        "precision": 0.7030565656565657,
        "recall": 0.765
      },
      {
        "accuracy": 0.366,
        "f1": 0.3226340746054519,
        "hf_subset": "war-eng",
        "languages": [
          "war-Latn",
          "eng-Latn"
        ],
        "main_score": 0.3226340746054519,
        "precision": 0.3082607605357605,
        "recall": 0.366
      },
      {
        "accuracy": 0.2936320754716981,
        "f1": 0.23986864684682166,
        "hf_subset": "yid-eng",
        "languages": [
          "yid-Hebr",
          "eng-Latn"
        ],
        "main_score": 0.23986864684682166,
        "precision": 0.22436584486415356,
        "recall": 0.2936320754716981
      },
      {
        "accuracy": 0.144,
        "f1": 0.12293851426351426,
        "hf_subset": "bre-eng",
        "languages": [
          "bre-Latn",
          "eng-Latn"
        ],
        "main_score": 0.12293851426351426,
        "precision": 0.11690119047619046,
        "recall": 0.144
      },
      {
        "accuracy": 0.972,
        "f1": 0.964,
        "hf_subset": "nld-eng",
        "languages": [
          "nld-Latn",
          "eng-Latn"
        ],
        "main_score": 0.964,
        "precision": 0.96,
        "recall": 0.972
      },
      {
        "accuracy": 0.9324817518248175,
        "f1": 0.9137469586374696,
        "hf_subset": "tha-eng",
        "languages": [
          "tha-Thai",
          "eng-Latn"
        ],
        "main_score": 0.9137469586374696,
        "precision": 0.9049574209245741,
        "recall": 0.9324817518248175
      },
      {
        "accuracy": 0.5897435897435898,
        "f1": 0.5256151256151257,
        "hf_subset": "gsw-eng",
        "languages": [
          "gsw-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5256151256151257,
        "precision": 0.5031339031339032,
        "recall": 0.5897435897435898
      },
      {
        "accuracy": 0.949,
        "f1": 0.933,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.933,
        "precision": 0.9251666666666667,
        "recall": 0.949
      },
      {
        "accuracy": 0.081,
        "f1": 0.072897530392313,
        "hf_subset": "ber-eng",
        "languages": [
          "ber-Tfng",
          "eng-Latn"
        ],
        "main_score": 0.072897530392313,
        "precision": 0.07134752937538653,
        "recall": 0.081
      },
      {
        "accuracy": 0.632,
        "f1": 0.5892112026862026,
        "hf_subset": "sqi-eng",
        "languages": [
          "sqi-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5892112026862026,
        "precision": 0.5729763157894737,
        "recall": 0.632
      },
      {
        "accuracy": 0.6936416184971098,
        "f1": 0.6430956968529223,
        "hf_subset": "fry-eng",
        "languages": [
          "fry-Latn",
          "eng-Latn"
        ],
        "main_score": 0.6430956968529223,
        "precision": 0.6258670520231214,
        "recall": 0.6936416184971098
      },
      {
        "accuracy": 0.43349753694581283,
        "f1": 0.3698025291621351,
        "hf_subset": "tuk-eng",
        "languages": [
          "tuk-Latn",
          "eng-Latn"
        ],
        "main_score": 0.3698025291621351,
        "precision": 0.3495095216770094,
        "recall": 0.43349753694581283
      },
      {
        "accuracy": 0.937,
        "f1": 0.9182333333333333,
        "hf_subset": "tur-eng",
        "languages": [
          "tur-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9182333333333333,
        "precision": 0.9096166666666665,
        "recall": 0.937
      },
      {
        "accuracy": 0.8736330498177399,
        "f1": 0.8455245038477116,
        "hf_subset": "slv-eng",
        "languages": [
          "slv-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8455245038477116,
        "precision": 0.834372890508978,
        "recall": 0.8736330498177399
      },
      {
        "accuracy": 0.95,
        "f1": 0.9346666666666666,
        "hf_subset": "rus-eng",
        "languages": [
          "rus-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.9346666666666666,
        "precision": 0.9271666666666666,
        "recall": 0.95
      },
      {
        "accuracy": 0.7204968944099379,
        "f1": 0.670584131764256,
        "hf_subset": "hsb-eng",
        "languages": [
          "hsb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.670584131764256,
        "precision": 0.651785303493378,
        "recall": 0.7204968944099379
      },
      {
        "accuracy": 0.5670241286863271,
        "f1": 0.5083456162544634,
        "hf_subset": "kat-eng",
        "languages": [
          "kat-Geor",
          "eng-Latn"
        ],
        "main_score": 0.5083456162544634,
        "precision": 0.48624262484260905,
        "recall": 0.5670241286863271
      },
      {
        "accuracy": 0.928,
        "f1": 0.9103166666666666,
        "hf_subset": "kor-eng",
        "languages": [
          "kor-Hang",
          "eng-Latn"
        ],
        "main_score": 0.9103166666666666,
        "precision": 0.9023928571428571,
        "recall": 0.928
      },
      {
        "accuracy": 0.36585365853658536,
        "f1": 0.30746299770690017,
        "hf_subset": "jav-eng",
        "languages": [
          "jav-Latn",
          "eng-Latn"
        ],
        "main_score": 0.30746299770690017,
        "precision": 0.2923015873015873,
        "recall": 0.36585365853658536
      },
      {
        "accuracy": 0.937,
        "f1": 0.9195,
        "hf_subset": "ita-eng",
        "languages": [
          "ita-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9195,
        "precision": 0.9113333333333332,
        "recall": 0.937
      },
      {
        "accuracy": 0.5288461538461539,
        "f1": 0.46475191475191474,
        "hf_subset": "tzl-eng",
        "languages": [
          "tzl-Latn",
          "eng-Latn"
        ],
        "main_score": 0.46475191475191474,
        "precision": 0.4427350427350427,
        "recall": 0.5288461538461539
      },
      {
        "accuracy": 0.888,
        "f1": 0.8612746031746031,
        "hf_subset": "afr-eng",
        "languages": [
          "afr-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8612746031746031,
        "precision": 0.8504916666666666,
        "recall": 0.888
      },
      {
        "accuracy": 0.6285714285714286,
        "f1": 0.5738837353123069,
        "hf_subset": "pms-eng",
        "languages": [
          "pms-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5738837353123069,
        "precision": 0.5544452690166977,
        "recall": 0.6285714285714286
      },
      {
        "accuracy": 0.685,
        "f1": 0.627057142857143,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.627057142857143,
        "precision": 0.6051846893302776,
        "recall": 0.685
      },
      {
        "accuracy": 0.816,
        "f1": 0.770952380952381,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.770952380952381,
        "precision": 0.75125,
        "recall": 0.816
      },
      {
        "accuracy": 0.127,
        "f1": 0.10617372742200328,
        "hf_subset": "kzj-eng",
        "languages": [
          "kzj-Latn",
          "eng-Latn"
        ],
        "main_score": 0.10617372742200328,
        "precision": 0.10036593031151855,
        "recall": 0.127
      },
      {
        "accuracy": 0.961,
        "f1": 0.9501666666666666,
        "hf_subset": "cmn-eng",
        "languages": [
          "cmn-Hans",
          "eng-Latn"
        ],
        "main_score": 0.9501666666666666,
        "precision": 0.9448333333333334,
        "recall": 0.961
      },
      {
        "accuracy": 0.7044025157232704,
        "f1": 0.6505790156733552,
        "hf_subset": "arz-eng",
        "languages": [
          "arz-Arab",
          "eng-Latn"
        ],
        "main_score": 0.6505790156733552,
        "precision": 0.6310980424187972,
        "recall": 0.7044025157232704
      },
      {
        "accuracy": 0.94,
        "f1": 0.9246666666666667,
        "hf_subset": "ron-eng",
        "languages": [
          "ron-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9246666666666667,
        "precision": 0.9178666666666667,
        "recall": 0.94
      },
      {
        "accuracy": 0.904,
        "f1": 0.8823714285714286,
        "hf_subset": "tgl-eng",
        "languages": [
          "tgl-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8823714285714286,
        "precision": 0.8738333333333332,
        "recall": 0.904
      },
      {
        "accuracy": 0.872,
        "f1": 0.8406444444444445,
        "hf_subset": "lat-eng",
        "languages": [
          "lat-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8406444444444445,
        "precision": 0.8275333333333333,
        "recall": 0.872
      },
      {
        "accuracy": 0.952,
        "f1": 0.937,
        "hf_subset": "bul-eng",
        "languages": [
          "bul-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.937,
        "precision": 0.9298333333333334,
        "recall": 0.952
      },
      {
        "accuracy": 0.647887323943662,
        "f1": 0.5890959392719957,
        "hf_subset": "max-eng",
        "languages": [
          "max-Deva",
          "eng-Latn"
        ],
        "main_score": 0.5890959392719957,
        "precision": 0.570612564274536,
        "recall": 0.647887323943662
      },
      {
        "accuracy": 0.947,
        "f1": 0.9325,
        "hf_subset": "jpn-eng",
        "languages": [
          "jpn-Jpan",
          "eng-Latn"
        ],
        "main_score": 0.9325,
        "precision": 0.9258333333333334,
        "recall": 0.947
      },
      {
        "accuracy": 0.829,
        "f1": 0.7895722222222222,
        "hf_subset": "heb-eng",
        "languages": [
          "heb-Hebr",
          "eng-Latn"
        ],
        "main_score": 0.7895722222222222,
        "precision": 0.7728789682539682,
        "recall": 0.829
      },
      {
        "accuracy": 0.7366412213740458,
        "f1": 0.6907215557978916,
        "hf_subset": "fao-eng",
        "languages": [
          "fao-Latn",
          "eng-Latn"
        ],
        "main_score": 0.6907215557978916,
        "precision": 0.6729371137768084,
        "recall": 0.7366412213740458
      },
      {
        "accuracy": 0.876,
        "f1": 0.8479428571428572,
        "hf_subset": "fin-eng",
        "languages": [
          "fin-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8479428571428572,
        "precision": 0.8367833333333333,
        "recall": 0.876
      },
      {
        "accuracy": 0.963,
        "f1": 0.9531666666666666,
        "hf_subset": "fra-eng",
        "languages": [
          "fra-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9531666666666666,
        "precision": 0.9485,
        "recall": 0.963
      },
      {
        "accuracy": 0.943,
        "f1": 0.9292333333333334,
        "hf_subset": "hrv-eng",
        "languages": [
          "hrv-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9292333333333334,
        "precision": 0.9230833333333334,
        "recall": 0.943
      },
      {
        "accuracy": 0.906,
        "f1": 0.8795666666666666,
        "hf_subset": "ell-eng",
        "languages": [
          "ell-Grek",
          "eng-Latn"
        ],
        "main_score": 0.8795666666666666,
        "precision": 0.86775,
        "recall": 0.906
      },
      {
        "accuracy": 0.157,
        "f1": 0.13615546897546899,
        "hf_subset": "mhr-eng",
        "languages": [
          "mhr-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.13615546897546899,
        "precision": 0.13011824899238691,
        "recall": 0.157
      },
      {
        "accuracy": 0.6883116883116883,
        "f1": 0.6300404219658878,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.6300404219658878,
        "precision": 0.6080611307884034,
        "recall": 0.6883116883116883
      },
      {
        "accuracy": 0.7003257328990228,
        "f1": 0.6395843027764851,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.6395843027764851,
        "precision": 0.613300760043431,
        "recall": 0.7003257328990228
      },
      {
        "accuracy": 0.908,
        "f1": 0.8838666666666667,
        "hf_subset": "wuu-eng",
        "languages": [
          "wuu-Hans",
          "eng-Latn"
        ],
        "main_score": 0.8838666666666667,
        "precision": 0.8731166666666667,
        "recall": 0.908
      },
      {
        "accuracy": 0.869,
        "f1": 0.8368833333333334,
        "hf_subset": "ile-eng",
        "languages": [
          "ile-Latn",
          "eng-Latn"
        ],
        "main_score": 0.8368833333333334,
        "precision": 0.8229095238095238,
        "recall": 0.869
      },
      {
        "accuracy": 0.115,
        "f1": 0.09673959504735367,
        "hf_subset": "dtp-eng",
        "languages": [
          "dtp-Latn",
          "eng-Latn"
        ],
        "main_score": 0.09673959504735367,
        "precision": 0.0923114511831158,
        "recall": 0.115
      },
      {
        "accuracy": 0.964,
        "f1": 0.9551666666666666,
        "hf_subset": "ina-eng",
        "languages": [
          "ina-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9551666666666666,
        "precision": 0.951,
        "recall": 0.964
      },
      {
        "accuracy": 0.7,
        "f1": 0.6478387667887668,
        "hf_subset": "gle-eng",
        "languages": [
          "gle-Latn",
          "eng-Latn"
        ],
        "main_score": 0.6478387667887668,
        "precision": 0.6284580086580086,
        "recall": 0.7
      },
      {
        "accuracy": 0.6991304347826087,
        "f1": 0.646570801179497,
        "hf_subset": "cym-eng",
        "languages": [
          "cym-Latn",
          "eng-Latn"
        ],
        "main_score": 0.646570801179497,
        "precision": 0.6260828157349897,
        "recall": 0.6991304347826087
      },
      {
        "accuracy": 0.6363636363636364,
        "f1": 0.5732354601919819,
        "hf_subset": "csb-eng",
        "languages": [
          "csb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.5732354601919819,
        "precision": 0.549425695077869,
        "recall": 0.6363636363636364
      },
      {
        "accuracy": 0.132,
        "f1": 0.1095434837341814,
        "hf_subset": "pam-eng",
        "languages": [
          "pam-Latn",
          "eng-Latn"
        ],
        "main_score": 0.1095434837341814,
        "precision": 0.10286761441173205,
        "recall": 0.132
      },
      {
        "accuracy": 0.4883177570093458,
        "f1": 0.43890650496958905,
        "hf_subset": "uzb-eng",
        "languages": [
          "uzb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.43890650496958905,
        "precision": 0.42211188992731047,
        "recall": 0.4883177570093458
      },
      {
        "accuracy": 0.968,
        "f1": 0.9578333333333332,
        "hf_subset": "nob-eng",
        "languages": [
          "nob-Latn",
          "eng-Latn"
        ],
        "main_score": 0.9578333333333332,
        "precision": 0.953,
        "recall": 0.968
      },
      {
        "accuracy": 0.041,
        "f1": 0.032439560439560436,
        "hf_subset": "kab-eng",
        "languages": [
          "kab-Latn",
          "eng-Latn"
        ],
        "main_score": 0.032439560439560436,
        "precision": 0.030338095238095244,
        "recall": 0.041
      },
      {
        "accuracy": 0.278393351800554,
        "f1": 0.2302444833258997,
        "hf_subset": "khm-eng",
        "languages": [
          "khm-Khmr",
          "eng-Latn"
        ],
        "main_score": 0.2302444833258997,
        "precision": 0.21503231302400277,
        "recall": 0.278393351800554
      },
      {
        "accuracy": 0.352,
        "f1": 0.30884461501855115,
        "hf_subset": "eus-eng",
        "languages": [
          "eus-Latn",
          "eng-Latn"
        ],
        "main_score": 0.30884461501855115,
        "precision": 0.29501640225884396,
        "recall": 0.352
      }
    ]
  },
  "task_name": "Tatoeba"
}