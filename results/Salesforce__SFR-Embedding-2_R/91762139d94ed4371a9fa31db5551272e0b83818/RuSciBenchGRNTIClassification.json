{
  "dataset_revision": "673a610d6d3dd91a547a0d57ae1b56f37ebbf6a1",
  "evaluation_time": 180.28583979606628,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.672607421875,
        "f1": 0.6548581749172834,
        "f1_weighted": 0.6549445185338045,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.672607421875,
        "scores_per_experiment": [
          {
            "accuracy": 0.68603515625,
            "f1": 0.6753213094989184,
            "f1_weighted": 0.6753967689200607
          },
          {
            "accuracy": 0.66748046875,
            "f1": 0.6512496124273753,
            "f1_weighted": 0.6512580098953011
          },
          {
            "accuracy": 0.66650390625,
            "f1": 0.6431846812276119,
            "f1_weighted": 0.643329868574312
          },
          {
            "accuracy": 0.68212890625,
            "f1": 0.6649372687261403,
            "f1_weighted": 0.6650084611483312
          },
          {
            "accuracy": 0.67529296875,
            "f1": 0.6585741060315871,
            "f1_weighted": 0.6586237071731367
          },
          {
            "accuracy": 0.66015625,
            "f1": 0.6426204935157264,
            "f1_weighted": 0.6427308456429128
          },
          {
            "accuracy": 0.66845703125,
            "f1": 0.6488069093408658,
            "f1_weighted": 0.6489148054785124
          },
          {
            "accuracy": 0.6640625,
            "f1": 0.6454136962081495,
            "f1_weighted": 0.6454945228323586
          },
          {
            "accuracy": 0.69482421875,
            "f1": 0.6798153295785568,
            "f1_weighted": 0.6799141963481644
          },
          {
            "accuracy": 0.6611328125,
            "f1": 0.6386583426179027,
            "f1_weighted": 0.6387739993249557
          }
        ]
      }
    ]
  },
  "task_name": "RuSciBenchGRNTIClassification"
}