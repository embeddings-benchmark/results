{
  "dataset_revision": "7fb2f514ea683c5282dfec0a9672ece8de90ac50",
  "evaluation_time": 44.43402433395386,
  "kg_co2_emissions": 0.0036824798152822183,
  "mteb_version": "1.18.0",
  "scores": {
    "train": [
      {
        "accuracy": 0.52333984375,
        "f1": 0.5045718610538292,
        "f1_weighted": 0.5188113178479907,
        "hf_subset": "default",
        "languages": [
          "sin-Sinh"
        ],
        "main_score": 0.52333984375,
        "scores_per_experiment": [
          {
            "accuracy": 0.505859375,
            "f1": 0.4978853081154188,
            "f1_weighted": 0.5045896761290458
          },
          {
            "accuracy": 0.50146484375,
            "f1": 0.48354759532155206,
            "f1_weighted": 0.5082322078938766
          },
          {
            "accuracy": 0.5146484375,
            "f1": 0.476884438514466,
            "f1_weighted": 0.4892132373670939
          },
          {
            "accuracy": 0.57275390625,
            "f1": 0.5405432408527157,
            "f1_weighted": 0.561572236837597
          },
          {
            "accuracy": 0.533203125,
            "f1": 0.5196367414914074,
            "f1_weighted": 0.5344832821539124
          },
          {
            "accuracy": 0.54638671875,
            "f1": 0.5181203222956178,
            "f1_weighted": 0.5361450588721368
          },
          {
            "accuracy": 0.478515625,
            "f1": 0.4789775293441729,
            "f1_weighted": 0.47969441364832577
          },
          {
            "accuracy": 0.52099609375,
            "f1": 0.5134694216901745,
            "f1_weighted": 0.5272703171811342
          },
          {
            "accuracy": 0.529296875,
            "f1": 0.49226284581510893,
            "f1_weighted": 0.5146639384718319
          },
          {
            "accuracy": 0.5302734375,
            "f1": 0.5243911670976578,
            "f1_weighted": 0.5322488099249518
          }
        ]
      }
    ]
  },
  "task_name": "SinhalaNewsClassification"
}