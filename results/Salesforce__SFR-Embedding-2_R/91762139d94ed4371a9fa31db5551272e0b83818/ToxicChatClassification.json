{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.855069,
        "f1": 0.732568,
        "f1_weighted": 0.867881,
        "ap": 0.359141,
        "ap_weighted": 0.359141,
        "scores_per_experiment": [
          {
            "accuracy": 0.862543,
            "f1": 0.734713,
            "f1_weighted": 0.872668,
            "ap": 0.354479,
            "ap_weighted": 0.354479
          },
          {
            "accuracy": 0.864261,
            "f1": 0.739228,
            "f1_weighted": 0.8745,
            "ap": 0.3626,
            "ap_weighted": 0.3626
          },
          {
            "accuracy": 0.861684,
            "f1": 0.745211,
            "f1_weighted": 0.874263,
            "ap": 0.378492,
            "ap_weighted": 0.378492
          },
          {
            "accuracy": 0.887457,
            "f1": 0.770471,
            "f1_weighted": 0.893229,
            "ap": 0.412295,
            "ap_weighted": 0.412295
          },
          {
            "accuracy": 0.828179,
            "f1": 0.7005,
            "f1_weighted": 0.846994,
            "ap": 0.314003,
            "ap_weighted": 0.314003
          },
          {
            "accuracy": 0.876289,
            "f1": 0.74185,
            "f1_weighted": 0.88141,
            "ap": 0.359792,
            "ap_weighted": 0.359792
          },
          {
            "accuracy": 0.891753,
            "f1": 0.761637,
            "f1_weighted": 0.893568,
            "ap": 0.392345,
            "ap_weighted": 0.392345
          },
          {
            "accuracy": 0.792955,
            "f1": 0.686164,
            "f1_weighted": 0.82331,
            "ap": 0.321342,
            "ap_weighted": 0.321342
          },
          {
            "accuracy": 0.831615,
            "f1": 0.709881,
            "f1_weighted": 0.850666,
            "ap": 0.330542,
            "ap_weighted": 0.330542
          },
          {
            "accuracy": 0.853952,
            "f1": 0.736023,
            "f1_weighted": 0.8682,
            "ap": 0.365523,
            "ap_weighted": 0.365523
          }
        ],
        "main_score": 0.855069,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 167.0993673801422,
  "kg_co2_emissions": 0.015011556332226115
}