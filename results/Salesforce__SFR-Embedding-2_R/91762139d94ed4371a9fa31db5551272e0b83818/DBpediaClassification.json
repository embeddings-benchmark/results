{
  "dataset_revision": "9abd46cf7fc8b4c64290f26993c540b92aa145ac",
  "evaluation_time": 33.66385531425476,
  "kg_co2_emissions": 0.002216216197271425,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.9087890625,
        "f1": 0.9046383378801075,
        "f1_weighted": 0.9046588354225715,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.9087890625,
        "scores_per_experiment": [
          {
            "accuracy": 0.9287109375,
            "f1": 0.9264398100034822,
            "f1_weighted": 0.9264401558455274
          },
          {
            "accuracy": 0.912109375,
            "f1": 0.9092346481148941,
            "f1_weighted": 0.9092522808060113
          },
          {
            "accuracy": 0.89892578125,
            "f1": 0.8973426573427735,
            "f1_weighted": 0.8973097463024379
          },
          {
            "accuracy": 0.91552734375,
            "f1": 0.9113972257739376,
            "f1_weighted": 0.9114310538148964
          },
          {
            "accuracy": 0.91845703125,
            "f1": 0.9158718291792167,
            "f1_weighted": 0.9159441398328939
          },
          {
            "accuracy": 0.92529296875,
            "f1": 0.9230008939063071,
            "f1_weighted": 0.9229964269348812
          },
          {
            "accuracy": 0.9296875,
            "f1": 0.9278947285883413,
            "f1_weighted": 0.9279046602907658
          },
          {
            "accuracy": 0.90673828125,
            "f1": 0.9021561308355291,
            "f1_weighted": 0.9021912750699838
          },
          {
            "accuracy": 0.89892578125,
            "f1": 0.8937370089551931,
            "f1_weighted": 0.8936698830974833
          },
          {
            "accuracy": 0.853515625,
            "f1": 0.8393084461013997,
            "f1_weighted": 0.8394487322308328
          }
        ]
      }
    ]
  },
  "task_name": "DBpediaClassification"
}