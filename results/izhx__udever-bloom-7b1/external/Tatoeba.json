{
    "dataset_revision": "9080400076fbadbb4c4dcb136ff4eddc40b42553",
    "task_name": "Tatoeba",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "sqi-eng",
                "languages": [
                    "sqi-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.19,
                "f1": 0.15331629955575188,
                "precision": 0.1438509724403208,
                "recall": 0.19,
                "main_score": 0.15331629955575188
            },
            {
                "hf_subset": "fry-eng",
                "languages": [
                    "fry-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.3988439306358382,
                "f1": 0.323699421965318,
                "precision": 0.30036929993577394,
                "recall": 0.3988439306358382,
                "main_score": 0.323699421965318
            },
            {
                "hf_subset": "kur-eng",
                "languages": [
                    "kur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.15365853658536585,
                "f1": 0.1249755078527547,
                "precision": 0.11840415442997938,
                "recall": 0.15365853658536585,
                "main_score": 0.1249755078527547
            },
            {
                "hf_subset": "tur-eng",
                "languages": [
                    "tur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.111,
                "f1": 0.08955359175928436,
                "precision": 0.08324461412770236,
                "recall": 0.111,
                "main_score": 0.08955359175928436
            },
            {
                "hf_subset": "deu-eng",
                "languages": [
                    "deu-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.877,
                "f1": 0.8506214285714286,
                "precision": 0.8398761904761904,
                "recall": 0.877,
                "main_score": 0.8506214285714286
            },
            {
                "hf_subset": "nld-eng",
                "languages": [
                    "nld-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.56,
                "f1": 0.498456850459482,
                "precision": 0.4780084415584415,
                "recall": 0.56,
                "main_score": 0.498456850459482
            },
            {
                "hf_subset": "ron-eng",
                "languages": [
                    "ron-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.381,
                "f1": 0.3385465329991646,
                "precision": 0.3237519841269841,
                "recall": 0.381,
                "main_score": 0.3385465329991646
            },
            {
                "hf_subset": "ang-eng",
                "languages": [
                    "ang-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.4253731343283582,
                "f1": 0.3467903986560703,
                "precision": 0.32171286425017764,
                "recall": 0.4253731343283582,
                "main_score": 0.3467903986560703
            },
            {
                "hf_subset": "ido-eng",
                "languages": [
                    "ido-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.539,
                "f1": 0.47839098124098123,
                "precision": 0.4567887667887668,
                "recall": 0.539,
                "main_score": 0.47839098124098123
            },
            {
                "hf_subset": "jav-eng",
                "languages": [
                    "jav-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.2634146341463415,
                "f1": 0.22264125162260023,
                "precision": 0.21384015912351637,
                "recall": 0.2634146341463415,
                "main_score": 0.22264125162260023
            },
            {
                "hf_subset": "isl-eng",
                "languages": [
                    "isl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.102,
                "f1": 0.08001233870597418,
                "precision": 0.0738383820456082,
                "recall": 0.102,
                "main_score": 0.08001233870597418
            },
            {
                "hf_subset": "slv-eng",
                "languages": [
                    "slv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.17253948967193197,
                "f1": 0.13055189087650387,
                "precision": 0.12105642744272276,
                "recall": 0.17253948967193197,
                "main_score": 0.13055189087650387
            },
            {
                "hf_subset": "cym-eng",
                "languages": [
                    "cym-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.10260869565217391,
                "f1": 0.0831837824011737,
                "precision": 0.07879315672736052,
                "recall": 0.10260869565217391,
                "main_score": 0.0831837824011737
            },
            {
                "hf_subset": "kaz-eng",
                "languages": [
                    "kaz-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.11826086956521738,
                "f1": 0.09663030581871163,
                "precision": 0.09152605557273077,
                "recall": 0.11826086956521738,
                "main_score": 0.09663030581871163
            },
            {
                "hf_subset": "est-eng",
                "languages": [
                    "est-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.068,
                "f1": 0.05608697757594542,
                "precision": 0.05333727335466467,
                "recall": 0.068,
                "main_score": 0.05608697757594542
            },
            {
                "hf_subset": "heb-eng",
                "languages": [
                    "heb-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.113,
                "f1": 0.07486638489921733,
                "precision": 0.06580321536442861,
                "recall": 0.113,
                "main_score": 0.07486638489921733
            },
            {
                "hf_subset": "gla-eng",
                "languages": [
                    "gla-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.041013268998793734,
                "f1": 0.030988364784130123,
                "precision": 0.02925923150618102,
                "recall": 0.041013268998793734,
                "main_score": 0.030988364784130123
            },
            {
                "hf_subset": "mar-eng",
                "languages": [
                    "mar-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.763,
                "f1": 0.7155912698412699,
                "precision": 0.6955511904761904,
                "recall": 0.763,
                "main_score": 0.7155912698412699
            },
            {
                "hf_subset": "lat-eng",
                "languages": [
                    "lat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.536,
                "f1": 0.46748110856852276,
                "precision": 0.44410496160186563,
                "recall": 0.536,
                "main_score": 0.46748110856852276
            },
            {
                "hf_subset": "bel-eng",
                "languages": [
                    "bel-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.234,
                "f1": 0.18485309948823103,
                "precision": 0.1712104734130107,
                "recall": 0.234,
                "main_score": 0.18485309948823103
            },
            {
                "hf_subset": "pms-eng",
                "languages": [
                    "pms-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.41523809523809524,
                "f1": 0.36577269291555004,
                "precision": 0.3500219198790627,
                "recall": 0.41523809523809524,
                "main_score": 0.36577269291555004
            },
            {
                "hf_subset": "gle-eng",
                "languages": [
                    "gle-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.049,
                "f1": 0.03909842412258181,
                "precision": 0.0370996941210328,
                "recall": 0.049,
                "main_score": 0.03909842412258181
            },
            {
                "hf_subset": "pes-eng",
                "languages": [
                    "pes-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.269,
                "f1": 0.21587309161426804,
                "precision": 0.19877234126984125,
                "recall": 0.269,
                "main_score": 0.21587309161426804
            },
            {
                "hf_subset": "nob-eng",
                "languages": [
                    "nob-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.373,
                "f1": 0.3194053167592641,
                "precision": 0.3041440545746428,
                "recall": 0.373,
                "main_score": 0.3194053167592641
            },
            {
                "hf_subset": "bul-eng",
                "languages": [
                    "bul-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.344,
                "f1": 0.28460500740394357,
                "precision": 0.26630818170746556,
                "recall": 0.344,
                "main_score": 0.28460500740394357
            },
            {
                "hf_subset": "cbk-eng",
                "languages": [
                    "cbk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.675,
                "f1": 0.6149236715898481,
                "precision": 0.5923266755904913,
                "recall": 0.675,
                "main_score": 0.6149236715898481
            },
            {
                "hf_subset": "hun-eng",
                "languages": [
                    "hun-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.079,
                "f1": 0.06652063929922994,
                "precision": 0.06392931096681097,
                "recall": 0.079,
                "main_score": 0.06652063929922994
            },
            {
                "hf_subset": "uig-eng",
                "languages": [
                    "uig-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.026000000000000002,
                "f1": 0.020216271963330784,
                "precision": 0.019467343791901313,
                "recall": 0.026000000000000002,
                "main_score": 0.020216271963330784
            },
            {
                "hf_subset": "rus-eng",
                "languages": [
                    "rus-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.7659999999999999,
                "f1": 0.7123357142857142,
                "precision": 0.6903261904761905,
                "recall": 0.7659999999999999,
                "main_score": 0.7123357142857142
            },
            {
                "hf_subset": "spa-eng",
                "languages": [
                    "spa-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.986,
                "f1": 0.9813333333333333,
                "precision": 0.9789999999999999,
                "recall": 0.986,
                "main_score": 0.9813333333333333
            },
            {
                "hf_subset": "hye-eng",
                "languages": [
                    "hye-Armn",
                    "eng-Latn"
                ],
                "accuracy": 0.018867924528301886,
                "f1": 0.00918401642133914,
                "precision": 0.008343646123610833,
                "recall": 0.018867924528301886,
                "main_score": 0.00918401642133914
            },
            {
                "hf_subset": "tel-eng",
                "languages": [
                    "tel-Telu",
                    "eng-Latn"
                ],
                "accuracy": 0.841880341880342,
                "f1": 0.8056369556369557,
                "precision": 0.7902421652421653,
                "recall": 0.841880341880342,
                "main_score": 0.8056369556369557
            },
            {
                "hf_subset": "afr-eng",
                "languages": [
                    "afr-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.272,
                "f1": 0.2255873107448107,
                "precision": 0.21136109508747228,
                "recall": 0.272,
                "main_score": 0.2255873107448107
            },
            {
                "hf_subset": "mon-eng",
                "languages": [
                    "mon-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.09090909090909091,
                "f1": 0.0737323521273764,
                "precision": 0.0701229523252768,
                "recall": 0.09090909090909091,
                "main_score": 0.0737323521273764
            },
            {
                "hf_subset": "arz-eng",
                "languages": [
                    "arz-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.7924528301886792,
                "f1": 0.7480483178596387,
                "precision": 0.7283368273934311,
                "recall": 0.7924528301886792,
                "main_score": 0.7480483178596387
            },
            {
                "hf_subset": "hrv-eng",
                "languages": [
                    "hrv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.213,
                "f1": 0.17754399705471685,
                "precision": 0.1681516621898026,
                "recall": 0.213,
                "main_score": 0.17754399705471685
            },
            {
                "hf_subset": "nov-eng",
                "languages": [
                    "nov-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.5992217898832685,
                "f1": 0.5492807451951421,
                "precision": 0.5307115063924402,
                "recall": 0.5992217898832685,
                "main_score": 0.5492807451951421
            },
            {
                "hf_subset": "gsw-eng",
                "languages": [
                    "gsw-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.3076923076923077,
                "f1": 0.23700990367657032,
                "precision": 0.21666666666666665,
                "recall": 0.3076923076923077,
                "main_score": 0.23700990367657032
            },
            {
                "hf_subset": "nds-eng",
                "languages": [
                    "nds-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.35600000000000004,
                "f1": 0.2987713276919159,
                "precision": 0.2807062211509473,
                "recall": 0.35600000000000004,
                "main_score": 0.2987713276919159
            },
            {
                "hf_subset": "ukr-eng",
                "languages": [
                    "ukr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.381,
                "f1": 0.31123585858585856,
                "precision": 0.28995893769823305,
                "recall": 0.381,
                "main_score": 0.31123585858585856
            },
            {
                "hf_subset": "uzb-eng",
                "languages": [
                    "uzb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.10747663551401869,
                "f1": 0.08280338473537247,
                "precision": 0.07806134675293554,
                "recall": 0.10747663551401869,
                "main_score": 0.08280338473537247
            },
            {
                "hf_subset": "lit-eng",
                "languages": [
                    "lit-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.076,
                "f1": 0.058720950404702224,
                "precision": 0.055577773615273614,
                "recall": 0.076,
                "main_score": 0.058720950404702224
            },
            {
                "hf_subset": "ina-eng",
                "languages": [
                    "ina-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.868,
                "f1": 0.8372833333333333,
                "precision": 0.8242595238095239,
                "recall": 0.868,
                "main_score": 0.8372833333333333
            },
            {
                "hf_subset": "lfn-eng",
                "languages": [
                    "lfn-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.52,
                "f1": 0.46480581322115344,
                "precision": 0.4452753032676945,
                "recall": 0.52,
                "main_score": 0.46480581322115344
            },
            {
                "hf_subset": "zsm-eng",
                "languages": [
                    "zsm-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.904,
                "f1": 0.8810999999999999,
                "precision": 0.8710333333333334,
                "recall": 0.904,
                "main_score": 0.8810999999999999
            },
            {
                "hf_subset": "ita-eng",
                "languages": [
                    "ita-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.795,
                "f1": 0.7495746031746031,
                "precision": 0.7303249999999999,
                "recall": 0.795,
                "main_score": 0.7495746031746031
            },
            {
                "hf_subset": "cmn-eng",
                "languages": [
                    "cmn-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.9670000000000001,
                "f1": 0.9570000000000001,
                "precision": 0.9521666666666667,
                "recall": 0.9670000000000001,
                "main_score": 0.9570000000000001
            },
            {
                "hf_subset": "lvs-eng",
                "languages": [
                    "lvs-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.107,
                "f1": 0.08576412755390277,
                "precision": 0.08046714349557488,
                "recall": 0.107,
                "main_score": 0.08576412755390277
            },
            {
                "hf_subset": "glg-eng",
                "languages": [
                    "glg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.86,
                "f1": 0.8254523809523809,
                "precision": 0.8106166666666665,
                "recall": 0.86,
                "main_score": 0.8254523809523809
            },
            {
                "hf_subset": "ceb-eng",
                "languages": [
                    "ceb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.11,
                "f1": 0.09080509354193564,
                "precision": 0.0857587968815845,
                "recall": 0.11,
                "main_score": 0.09080509354193564
            },
            {
                "hf_subset": "bre-eng",
                "languages": [
                    "bre-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.096,
                "f1": 0.07409451659451659,
                "precision": 0.06812106944189741,
                "recall": 0.096,
                "main_score": 0.07409451659451659
            },
            {
                "hf_subset": "ben-eng",
                "languages": [
                    "ben-Beng",
                    "eng-Latn"
                ],
                "accuracy": 0.871,
                "f1": 0.8388999999999999,
                "precision": 0.82395,
                "recall": 0.871,
                "main_score": 0.8388999999999999
            },
            {
                "hf_subset": "swg-eng",
                "languages": [
                    "swg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.3482142857142857,
                "f1": 0.2917517006802721,
                "precision": 0.2740499084249084,
                "recall": 0.3482142857142857,
                "main_score": 0.2917517006802721
            },
            {
                "hf_subset": "arq-eng",
                "languages": [
                    "arq-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.429198682766191,
                "f1": 0.3721120707205811,
                "precision": 0.3523526784229309,
                "recall": 0.429198682766191,
                "main_score": 0.3721120707205811
            },
            {
                "hf_subset": "kab-eng",
                "languages": [
                    "kab-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.023,
                "f1": 0.015401826425879608,
                "precision": 0.01424235527544351,
                "recall": 0.023,
                "main_score": 0.015401826425879608
            },
            {
                "hf_subset": "fra-eng",
                "languages": [
                    "fra-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.956,
                "f1": 0.9432333333333334,
                "precision": 0.9372500000000001,
                "recall": 0.956,
                "main_score": 0.9432333333333334
            },
            {
                "hf_subset": "por-eng",
                "languages": [
                    "por-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.955,
                "f1": 0.9443333333333334,
                "precision": 0.939,
                "recall": 0.955,
                "main_score": 0.9443333333333334
            },
            {
                "hf_subset": "tat-eng",
                "languages": [
                    "tat-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.067,
                "f1": 0.049622522983552395,
                "precision": 0.045289627610175145,
                "recall": 0.067,
                "main_score": 0.049622522983552395
            },
            {
                "hf_subset": "oci-eng",
                "languages": [
                    "oci-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.508,
                "f1": 0.4573643858755624,
                "precision": 0.44010822829131657,
                "recall": 0.508,
                "main_score": 0.4573643858755624
            },
            {
                "hf_subset": "pol-eng",
                "languages": [
                    "pol-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.239,
                "f1": 0.20267261904761905,
                "precision": 0.19161424083163212,
                "recall": 0.239,
                "main_score": 0.20267261904761905
            },
            {
                "hf_subset": "war-eng",
                "languages": [
                    "war-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.134,
                "f1": 0.11232209832252994,
                "precision": 0.10714445160103056,
                "recall": 0.134,
                "main_score": 0.11232209832252994
            },
            {
                "hf_subset": "aze-eng",
                "languages": [
                    "aze-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.103,
                "f1": 0.08161916387744503,
                "precision": 0.07678631905405786,
                "recall": 0.103,
                "main_score": 0.08161916387744503
            },
            {
                "hf_subset": "vie-eng",
                "languages": [
                    "vie-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9670000000000001,
                "f1": 0.9583333333333335,
                "precision": 0.9541666666666667,
                "recall": 0.9670000000000001,
                "main_score": 0.9583333333333335
            },
            {
                "hf_subset": "nno-eng",
                "languages": [
                    "nno-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.249,
                "f1": 0.20794749162495066,
                "precision": 0.19575997295469916,
                "recall": 0.249,
                "main_score": 0.20794749162495066
            },
            {
                "hf_subset": "cha-eng",
                "languages": [
                    "cha-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.32116788321167883,
                "f1": 0.26960375391032326,
                "precision": 0.25498078211502523,
                "recall": 0.32116788321167883,
                "main_score": 0.26960375391032326
            },
            {
                "hf_subset": "mhr-eng",
                "languages": [
                    "mhr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.049,
                "f1": 0.03251889552842259,
                "precision": 0.029281137342615296,
                "recall": 0.049,
                "main_score": 0.03251889552842259
            },
            {
                "hf_subset": "dan-eng",
                "languages": [
                    "dan-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.389,
                "f1": 0.3359595154442981,
                "precision": 0.31906759791342587,
                "recall": 0.389,
                "main_score": 0.3359595154442981
            },
            {
                "hf_subset": "ell-eng",
                "languages": [
                    "ell-Grek",
                    "eng-Latn"
                ],
                "accuracy": 0.169,
                "f1": 0.13082818919542666,
                "precision": 0.12125554724968518,
                "recall": 0.169,
                "main_score": 0.13082818919542666
            },
            {
                "hf_subset": "amh-eng",
                "languages": [
                    "amh-Ethi",
                    "eng-Latn"
                ],
                "accuracy": 0.005952380952380952,
                "f1": 0.0009920634920634922,
                "precision": 0.0005411255411255411,
                "recall": 0.005952380952380952,
                "main_score": 0.0009920634920634922
            },
            {
                "hf_subset": "pam-eng",
                "languages": [
                    "pam-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.081,
                "f1": 0.07033911671727207,
                "precision": 0.06759952905986985,
                "recall": 0.081,
                "main_score": 0.07033911671727207
            },
            {
                "hf_subset": "hsb-eng",
                "languages": [
                    "hsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.16149068322981366,
                "f1": 0.13314287609382625,
                "precision": 0.12588291889534126,
                "recall": 0.16149068322981366,
                "main_score": 0.13314287609382625
            },
            {
                "hf_subset": "srp-eng",
                "languages": [
                    "srp-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.223,
                "f1": 0.18754672526177102,
                "precision": 0.1777463320976479,
                "recall": 0.223,
                "main_score": 0.18754672526177102
            },
            {
                "hf_subset": "epo-eng",
                "languages": [
                    "epo-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.395,
                "f1": 0.33916594393738353,
                "precision": 0.32244738455988453,
                "recall": 0.395,
                "main_score": 0.33916594393738353
            },
            {
                "hf_subset": "kzj-eng",
                "languages": [
                    "kzj-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.075,
                "f1": 0.06300929449087343,
                "precision": 0.060555575817683505,
                "recall": 0.075,
                "main_score": 0.06300929449087343
            },
            {
                "hf_subset": "awa-eng",
                "languages": [
                    "awa-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.5714285714285714,
                "f1": 0.5301135372563944,
                "precision": 0.5178829107400535,
                "recall": 0.5714285714285714,
                "main_score": 0.5301135372563944
            },
            {
                "hf_subset": "fao-eng",
                "languages": [
                    "fao-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.16030534351145037,
                "f1": 0.14424487352192786,
                "precision": 0.1398739301411057,
                "recall": 0.16030534351145037,
                "main_score": 0.14424487352192786
            },
            {
                "hf_subset": "mal-eng",
                "languages": [
                    "mal-Mlym",
                    "eng-Latn"
                ],
                "accuracy": 0.9621542940320232,
                "f1": 0.950509461426492,
                "precision": 0.9446870451237264,
                "recall": 0.9621542940320232,
                "main_score": 0.950509461426492
            },
            {
                "hf_subset": "ile-eng",
                "languages": [
                    "ile-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.691,
                "f1": 0.6364957393483709,
                "precision": 0.6144357142857143,
                "recall": 0.691,
                "main_score": 0.6364957393483709
            },
            {
                "hf_subset": "bos-eng",
                "languages": [
                    "bos-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.23728813559322035,
                "f1": 0.19281200536513546,
                "precision": 0.1811042731593579,
                "recall": 0.23728813559322035,
                "main_score": 0.19281200536513546
            },
            {
                "hf_subset": "cor-eng",
                "languages": [
                    "cor-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.049,
                "f1": 0.03860277777777778,
                "precision": 0.03553962393468025,
                "recall": 0.049,
                "main_score": 0.03860277777777778
            },
            {
                "hf_subset": "cat-eng",
                "languages": [
                    "cat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.921,
                "f1": 0.9024190476190477,
                "precision": 0.8941666666666667,
                "recall": 0.921,
                "main_score": 0.9024190476190477
            },
            {
                "hf_subset": "eus-eng",
                "languages": [
                    "eus-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7879999999999999,
                "f1": 0.7453390756302521,
                "precision": 0.7279386904761904,
                "recall": 0.7879999999999999,
                "main_score": 0.7453390756302521
            },
            {
                "hf_subset": "yue-eng",
                "languages": [
                    "yue-Hant",
                    "eng-Latn"
                ],
                "accuracy": 0.916,
                "f1": 0.8939,
                "precision": 0.88375,
                "recall": 0.916,
                "main_score": 0.8939
            },
            {
                "hf_subset": "swe-eng",
                "languages": [
                    "swe-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.324,
                "f1": 0.2782439997910586,
                "precision": 0.26434715247715246,
                "recall": 0.324,
                "main_score": 0.2782439997910586
            },
            {
                "hf_subset": "dtp-eng",
                "languages": [
                    "dtp-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.068,
                "f1": 0.05258204523374802,
                "precision": 0.049405958256616145,
                "recall": 0.068,
                "main_score": 0.05258204523374802
            },
            {
                "hf_subset": "kat-eng",
                "languages": [
                    "kat-Geor",
                    "eng-Latn"
                ],
                "accuracy": 0.020107238605898123,
                "f1": 0.014770600435024532,
                "precision": 0.014215975441361408,
                "recall": 0.020107238605898123,
                "main_score": 0.014770600435024532
            },
            {
                "hf_subset": "jpn-eng",
                "languages": [
                    "jpn-Jpan",
                    "eng-Latn"
                ],
                "accuracy": 0.872,
                "f1": 0.8388333333333332,
                "precision": 0.8244166666666668,
                "recall": 0.872,
                "main_score": 0.8388333333333332
            },
            {
                "hf_subset": "csb-eng",
                "languages": [
                    "csb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.1422924901185771,
                "f1": 0.11043453048700425,
                "precision": 0.10285902503293806,
                "recall": 0.1422924901185771,
                "main_score": 0.11043453048700425
            },
            {
                "hf_subset": "xho-eng",
                "languages": [
                    "xho-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.09859154929577464,
                "f1": 0.0796015408691465,
                "precision": 0.07679678785726839,
                "recall": 0.09859154929577464,
                "main_score": 0.0796015408691465
            },
            {
                "hf_subset": "orv-eng",
                "languages": [
                    "orv-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.12574850299401197,
                "f1": 0.08435162337247867,
                "precision": 0.07540808434256832,
                "recall": 0.12574850299401197,
                "main_score": 0.08435162337247867
            },
            {
                "hf_subset": "ind-eng",
                "languages": [
                    "ind-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.935,
                "f1": 0.9190666666666665,
                "precision": 0.9114166666666668,
                "recall": 0.935,
                "main_score": 0.9190666666666665
            },
            {
                "hf_subset": "tuk-eng",
                "languages": [
                    "tuk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.08866995073891626,
                "f1": 0.06847922192749778,
                "precision": 0.06431102386508143,
                "recall": 0.08866995073891626,
                "main_score": 0.06847922192749778
            },
            {
                "hf_subset": "max-eng",
                "languages": [
                    "max-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.4612676056338028,
                "f1": 0.4144727338389311,
                "precision": 0.39803743513713863,
                "recall": 0.4612676056338028,
                "main_score": 0.4144727338389311
            },
            {
                "hf_subset": "swh-eng",
                "languages": [
                    "swh-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.35384615384615387,
                "f1": 0.2780912253371418,
                "precision": 0.25588007434161275,
                "recall": 0.35384615384615387,
                "main_score": 0.2780912253371418
            },
            {
                "hf_subset": "hin-eng",
                "languages": [
                    "hin-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.961,
                "f1": 0.9488333333333333,
                "precision": 0.943,
                "recall": 0.961,
                "main_score": 0.9488333333333333
            },
            {
                "hf_subset": "dsb-eng",
                "languages": [
                    "dsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.18162839248434237,
                "f1": 0.1500273275898725,
                "precision": 0.14135773519036146,
                "recall": 0.18162839248434237,
                "main_score": 0.1500273275898725
            },
            {
                "hf_subset": "ber-eng",
                "languages": [
                    "ber-Tfng",
                    "eng-Latn"
                ],
                "accuracy": 0.064,
                "f1": 0.05169780886652615,
                "precision": 0.049010948159167976,
                "recall": 0.064,
                "main_score": 0.05169780886652615
            },
            {
                "hf_subset": "tam-eng",
                "languages": [
                    "tam-Taml",
                    "eng-Latn"
                ],
                "accuracy": 0.8566775244299675,
                "f1": 0.8186753528773072,
                "precision": 0.8013029315960912,
                "recall": 0.8566775244299675,
                "main_score": 0.8186753528773072
            },
            {
                "hf_subset": "slk-eng",
                "languages": [
                    "slk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.147,
                "f1": 0.12296409553542204,
                "precision": 0.11643939628482972,
                "recall": 0.147,
                "main_score": 0.12296409553542204
            },
            {
                "hf_subset": "tgl-eng",
                "languages": [
                    "tgl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.14,
                "f1": 0.11188658083109301,
                "precision": 0.10439068547503426,
                "recall": 0.14,
                "main_score": 0.11188658083109301
            },
            {
                "hf_subset": "ast-eng",
                "languages": [
                    "ast-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7401574803149606,
                "f1": 0.6837270341207351,
                "precision": 0.6606299212598423,
                "recall": 0.7401574803149606,
                "main_score": 0.6837270341207351
            },
            {
                "hf_subset": "mkd-eng",
                "languages": [
                    "mkd-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.20200000000000004,
                "f1": 0.15584321026350167,
                "precision": 0.14220359087863854,
                "recall": 0.20200000000000004,
                "main_score": 0.15584321026350167
            },
            {
                "hf_subset": "khm-eng",
                "languages": [
                    "khm-Khmr",
                    "eng-Latn"
                ],
                "accuracy": 0.012465373961218837,
                "f1": 0.007009849184364421,
                "precision": 0.006369121979354991,
                "recall": 0.012465373961218837,
                "main_score": 0.007009849184364421
            },
            {
                "hf_subset": "ces-eng",
                "languages": [
                    "ces-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.155,
                "f1": 0.12992671904350203,
                "precision": 0.12323623108157993,
                "recall": 0.155,
                "main_score": 0.12992671904350203
            },
            {
                "hf_subset": "tzl-eng",
                "languages": [
                    "tzl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.375,
                "f1": 0.3270299145299145,
                "precision": 0.31066176470588236,
                "recall": 0.375,
                "main_score": 0.3270299145299145
            },
            {
                "hf_subset": "urd-eng",
                "languages": [
                    "urd-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.862,
                "f1": 0.8287166666666667,
                "precision": 0.8144261904761906,
                "recall": 0.862,
                "main_score": 0.8287166666666667
            },
            {
                "hf_subset": "ara-eng",
                "languages": [
                    "ara-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.925,
                "f1": 0.9061666666666668,
                "precision": 0.8971666666666668,
                "recall": 0.925,
                "main_score": 0.9061666666666668
            },
            {
                "hf_subset": "kor-eng",
                "languages": [
                    "kor-Hang",
                    "eng-Latn"
                ],
                "accuracy": 0.517,
                "f1": 0.4478806599832916,
                "precision": 0.4226749389499389,
                "recall": 0.517,
                "main_score": 0.4478806599832916
            },
            {
                "hf_subset": "yid-eng",
                "languages": [
                    "yid-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.009433962264150943,
                "f1": 0.0048704516529471924,
                "precision": 0.004117909409772616,
                "recall": 0.009433962264150943,
                "main_score": 0.0048704516529471924
            },
            {
                "hf_subset": "fin-eng",
                "languages": [
                    "fin-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.068,
                "f1": 0.05480668860234897,
                "precision": 0.051950673717918526,
                "recall": 0.068,
                "main_score": 0.05480668860234897
            },
            {
                "hf_subset": "tha-eng",
                "languages": [
                    "tha-Thai",
                    "eng-Latn"
                ],
                "accuracy": 0.10036496350364964,
                "f1": 0.06784271238735885,
                "precision": 0.061594623647444786,
                "recall": 0.10036496350364964,
                "main_score": 0.06784271238735885
            },
            {
                "hf_subset": "wuu-eng",
                "languages": [
                    "wuu-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.903,
                "f1": 0.8791499999999999,
                "precision": 0.8682595238095238,
                "recall": 0.903,
                "main_score": 0.8791499999999999
            }
        ]
    }
}