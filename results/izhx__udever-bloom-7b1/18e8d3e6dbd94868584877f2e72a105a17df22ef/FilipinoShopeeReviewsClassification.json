{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.275049,
        "f1": 0.270377,
        "f1_weighted": 0.270381,
        "scores_per_experiment": [
          {
            "accuracy": 0.29248,
            "f1": 0.287704,
            "f1_weighted": 0.287716
          },
          {
            "accuracy": 0.258789,
            "f1": 0.254511,
            "f1_weighted": 0.254527
          },
          {
            "accuracy": 0.26416,
            "f1": 0.25516,
            "f1_weighted": 0.255123
          },
          {
            "accuracy": 0.293457,
            "f1": 0.292284,
            "f1_weighted": 0.292266
          },
          {
            "accuracy": 0.271973,
            "f1": 0.264573,
            "f1_weighted": 0.264567
          },
          {
            "accuracy": 0.25293,
            "f1": 0.247671,
            "f1_weighted": 0.247659
          },
          {
            "accuracy": 0.249512,
            "f1": 0.244532,
            "f1_weighted": 0.244585
          },
          {
            "accuracy": 0.296875,
            "f1": 0.298524,
            "f1_weighted": 0.298548
          },
          {
            "accuracy": 0.31543,
            "f1": 0.306948,
            "f1_weighted": 0.306922
          },
          {
            "accuracy": 0.254883,
            "f1": 0.251866,
            "f1_weighted": 0.251899
          }
        ],
        "main_score": 0.275049,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.275977,
        "f1": 0.27131,
        "f1_weighted": 0.271315,
        "scores_per_experiment": [
          {
            "accuracy": 0.275391,
            "f1": 0.26984,
            "f1_weighted": 0.26984
          },
          {
            "accuracy": 0.255371,
            "f1": 0.248841,
            "f1_weighted": 0.248837
          },
          {
            "accuracy": 0.268066,
            "f1": 0.258059,
            "f1_weighted": 0.258031
          },
          {
            "accuracy": 0.294922,
            "f1": 0.295062,
            "f1_weighted": 0.295042
          },
          {
            "accuracy": 0.280762,
            "f1": 0.275896,
            "f1_weighted": 0.275892
          },
          {
            "accuracy": 0.253418,
            "f1": 0.25046,
            "f1_weighted": 0.25046
          },
          {
            "accuracy": 0.260742,
            "f1": 0.252771,
            "f1_weighted": 0.252837
          },
          {
            "accuracy": 0.310059,
            "f1": 0.309764,
            "f1_weighted": 0.309806
          },
          {
            "accuracy": 0.308105,
            "f1": 0.299898,
            "f1_weighted": 0.299873
          },
          {
            "accuracy": 0.25293,
            "f1": 0.252509,
            "f1_weighted": 0.252534
          }
        ],
        "main_score": 0.275977,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 120.33663010597229,
  "kg_co2_emissions": 0.009937957729472568
}