{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "evaluation_time": 29.525464296340942,
  "kg_co2_emissions": 0.006848618319968377,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.678466796875,
        "ap": 0.6223127837753338,
        "ap_weighted": 0.6223127837753338,
        "f1": 0.6762766473817112,
        "f1_weighted": 0.6762766473817112,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.678466796875,
        "scores_per_experiment": [
          {
            "accuracy": 0.71337890625,
            "ap": 0.6586663661858975,
            "ap_weighted": 0.6586663661858975,
            "f1": 0.7122724634559348,
            "f1_weighted": 0.7122724634559348
          },
          {
            "accuracy": 0.68701171875,
            "ap": 0.6288590125246791,
            "ap_weighted": 0.6288590125246791,
            "f1": 0.6870026892007335,
            "f1_weighted": 0.6870026892007335
          },
          {
            "accuracy": 0.6982421875,
            "ap": 0.6535037478885135,
            "ap_weighted": 0.6535037478885135,
            "f1": 0.6923256438081236,
            "f1_weighted": 0.6923256438081236
          },
          {
            "accuracy": 0.6064453125,
            "ap": 0.5639856794990723,
            "ap_weighted": 0.5639856794990723,
            "f1": 0.6061715116806174,
            "f1_weighted": 0.6061715116806174
          },
          {
            "accuracy": 0.720703125,
            "ap": 0.6561120269495413,
            "ap_weighted": 0.6561120269495413,
            "f1": 0.7204127592991607,
            "f1_weighted": 0.7204127592991607
          },
          {
            "accuracy": 0.6591796875,
            "ap": 0.6012117513020834,
            "ap_weighted": 0.6012117513020834,
            "f1": 0.6566439156367214,
            "f1_weighted": 0.6566439156367214
          },
          {
            "accuracy": 0.69091796875,
            "ap": 0.6288141790940571,
            "ap_weighted": 0.6288141790940571,
            "f1": 0.690251474274475,
            "f1_weighted": 0.690251474274475
          },
          {
            "accuracy": 0.68212890625,
            "ap": 0.6156960999818709,
            "ap_weighted": 0.6156960999818709,
            "f1": 0.6722820632508242,
            "f1_weighted": 0.6722820632508242
          },
          {
            "accuracy": 0.6611328125,
            "ap": 0.6041363655252661,
            "ap_weighted": 0.6041363655252661,
            "f1": 0.6602567044533174,
            "f1_weighted": 0.6602567044533174
          },
          {
            "accuracy": 0.66552734375,
            "ap": 0.612142608802356,
            "ap_weighted": 0.612142608802356,
            "f1": 0.6651472487572033,
            "f1_weighted": 0.6651472487572033
          }
        ]
      }
    ]
  },
  "task_name": "InappropriatenessClassification"
}