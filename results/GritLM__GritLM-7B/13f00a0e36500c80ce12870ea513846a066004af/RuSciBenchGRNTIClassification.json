{
  "dataset_revision": "673a610d6d3dd91a547a0d57ae1b56f37ebbf6a1",
  "evaluation_time": 42.400203704833984,
  "kg_co2_emissions": 0.01254825877298829,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.67978515625,
        "f1": 0.6688184674102005,
        "f1_weighted": 0.6689054490395934,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.67978515625,
        "scores_per_experiment": [
          {
            "accuracy": 0.68603515625,
            "f1": 0.6783063581296379,
            "f1_weighted": 0.6783796222204301
          },
          {
            "accuracy": 0.68505859375,
            "f1": 0.6750873839407513,
            "f1_weighted": 0.6751629317328599
          },
          {
            "accuracy": 0.6640625,
            "f1": 0.6501272466052858,
            "f1_weighted": 0.6502768334056338
          },
          {
            "accuracy": 0.69482421875,
            "f1": 0.6856261386454664,
            "f1_weighted": 0.6856808097339715
          },
          {
            "accuracy": 0.68701171875,
            "f1": 0.6772417429618586,
            "f1_weighted": 0.6773019779893052
          },
          {
            "accuracy": 0.6611328125,
            "f1": 0.6489748497334507,
            "f1_weighted": 0.6490842908806768
          },
          {
            "accuracy": 0.6767578125,
            "f1": 0.6632264145484237,
            "f1_weighted": 0.6633594422154904
          },
          {
            "accuracy": 0.68115234375,
            "f1": 0.6689096500394116,
            "f1_weighted": 0.6689690654907257
          },
          {
            "accuracy": 0.68994140625,
            "f1": 0.6808141401142505,
            "f1_weighted": 0.6808846692629896
          },
          {
            "accuracy": 0.671875,
            "f1": 0.6598707493834678,
            "f1_weighted": 0.6599548474638515
          }
        ]
      }
    ]
  },
  "task_name": "RuSciBenchGRNTIClassification"
}