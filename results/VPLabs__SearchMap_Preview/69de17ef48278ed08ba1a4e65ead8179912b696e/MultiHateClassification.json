{
  "dataset_revision": "8f95949846bb9e33c6aaf730ccfdb8fe6bcfb7a9",
  "task_name": "MultiHateClassification",
  "mteb_version": "1.36.10",
  "scores": {
    "test": [
      {
        "accuracy": 0.6099,
        "f1": 0.571876,
        "f1_weighted": 0.616215,
        "ap": 0.364251,
        "ap_weighted": 0.364251,
        "scores_per_experiment": [
          {
            "accuracy": 0.628,
            "f1": 0.599214,
            "f1_weighted": 0.639171,
            "ap": 0.380853,
            "ap_weighted": 0.380853
          },
          {
            "accuracy": 0.644,
            "f1": 0.604508,
            "f1_weighted": 0.650999,
            "ap": 0.381503,
            "ap_weighted": 0.381503
          },
          {
            "accuracy": 0.64,
            "f1": 0.596601,
            "f1_weighted": 0.645822,
            "ap": 0.374489,
            "ap_weighted": 0.374489
          },
          {
            "accuracy": 0.676,
            "f1": 0.620643,
            "f1_weighted": 0.674551,
            "ap": 0.393377,
            "ap_weighted": 0.393377
          },
          {
            "accuracy": 0.582,
            "f1": 0.568323,
            "f1_weighted": 0.596907,
            "ap": 0.367805,
            "ap_weighted": 0.367805
          },
          {
            "accuracy": 0.558,
            "f1": 0.536369,
            "f1_weighted": 0.573623,
            "ap": 0.341137,
            "ap_weighted": 0.341137
          },
          {
            "accuracy": 0.528,
            "f1": 0.521104,
            "f1_weighted": 0.542482,
            "ap": 0.344161,
            "ap_weighted": 0.344161
          },
          {
            "accuracy": 0.695,
            "f1": 0.586065,
            "f1_weighted": 0.665059,
            "ap": 0.375443,
            "ap_weighted": 0.375443
          },
          {
            "accuracy": 0.55,
            "f1": 0.527112,
            "f1_weighted": 0.565813,
            "ap": 0.33506,
            "ap_weighted": 0.33506
          },
          {
            "accuracy": 0.598,
            "f1": 0.558822,
            "f1_weighted": 0.607729,
            "ap": 0.348681,
            "ap_weighted": 0.348681
          }
        ],
        "main_score": 0.6099,
        "hf_subset": "eng",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 6.941431522369385,
  "kg_co2_emissions": null,
  "mteb_dataset_name": "MultiHateClassification"
}