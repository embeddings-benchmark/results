{
  "dataset_revision": "16f46f059d56eac7c65c3c9581a45e40199eb140",
  "task_name": "IN22ConvBitextMining",
  "mteb_version": "1.36.10",
  "scores": {
    "test": [
      {
        "precision": 0.001782,
        "recall": 0.011976,
        "f1": 0.002587,
        "accuracy": 0.011976,
        "main_score": 0.002587,
        "hf_subset": "asm_Beng-eng_Latn",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.003379,
        "recall": 0.013307,
        "f1": 0.004265,
        "accuracy": 0.013307,
        "main_score": 0.004265,
        "hf_subset": "ben_Beng-eng_Latn",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.017845,
        "recall": 0.035928,
        "f1": 0.020131,
        "accuracy": 0.035928,
        "main_score": 0.020131,
        "hf_subset": "brx_Deva-eng_Latn",
        "languages": [
          "brx-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.025547,
        "recall": 0.049235,
        "f1": 0.028978,
        "accuracy": 0.049235,
        "main_score": 0.028978,
        "hf_subset": "doi_Deva-eng_Latn",
        "languages": [
          "doi-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.0062,
        "recall": 0.015968,
        "f1": 0.007676,
        "accuracy": 0.015968,
        "main_score": 0.007676,
        "hf_subset": "eng_Latn-asm_Beng",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ]
      },
      {
        "precision": 0.018637,
        "recall": 0.038589,
        "f1": 0.022256,
        "accuracy": 0.038589,
        "main_score": 0.022256,
        "hf_subset": "eng_Latn-ben_Beng",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ]
      },
      {
        "precision": 0.028344,
        "recall": 0.057884,
        "f1": 0.0333,
        "accuracy": 0.057884,
        "main_score": 0.0333,
        "hf_subset": "eng_Latn-brx_Deva",
        "languages": [
          "eng-Latn",
          "brx-Deva"
        ]
      },
      {
        "precision": 0.044805,
        "recall": 0.080506,
        "f1": 0.051391,
        "accuracy": 0.080506,
        "main_score": 0.051391,
        "hf_subset": "eng_Latn-doi_Deva",
        "languages": [
          "eng-Latn",
          "doi-Deva"
        ]
      },
      {
        "precision": 0.044713,
        "recall": 0.075183,
        "f1": 0.050967,
        "accuracy": 0.075183,
        "main_score": 0.050967,
        "hf_subset": "eng_Latn-gom_Deva",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ]
      },
      {
        "precision": 0.004448,
        "recall": 0.017964,
        "f1": 0.006176,
        "accuracy": 0.017964,
        "main_score": 0.006176,
        "hf_subset": "eng_Latn-guj_Gujr",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ]
      },
      {
        "precision": 0.069643,
        "recall": 0.105123,
        "f1": 0.076805,
        "accuracy": 0.105123,
        "main_score": 0.076805,
        "hf_subset": "eng_Latn-hin_Deva",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ]
      },
      {
        "precision": 0.00032,
        "recall": 0.004657,
        "f1": 0.000521,
        "accuracy": 0.004657,
        "main_score": 0.000521,
        "hf_subset": "eng_Latn-kan_Knda",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ]
      },
      {
        "precision": 0.0215,
        "recall": 0.044578,
        "f1": 0.025392,
        "accuracy": 0.044578,
        "main_score": 0.025392,
        "hf_subset": "eng_Latn-kas_Arab",
        "languages": [
          "eng-Latn",
          "kas-Arab"
        ]
      },
      {
        "precision": 0.029534,
        "recall": 0.051231,
        "f1": 0.033504,
        "accuracy": 0.051231,
        "main_score": 0.033504,
        "hf_subset": "eng_Latn-mai_Deva",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ]
      },
      {
        "precision": 0.009944,
        "recall": 0.025948,
        "f1": 0.011919,
        "accuracy": 0.025948,
        "main_score": 0.011919,
        "hf_subset": "eng_Latn-mal_Mlym",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ]
      },
      {
        "precision": 0.019926,
        "recall": 0.037259,
        "f1": 0.02298,
        "accuracy": 0.037259,
        "main_score": 0.02298,
        "hf_subset": "eng_Latn-mar_Deva",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ]
      },
      {
        "precision": 6.7e-05,
        "recall": 0.003327,
        "f1": 0.00013,
        "accuracy": 0.003327,
        "main_score": 0.00013,
        "hf_subset": "eng_Latn-mni_Mtei",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ]
      },
      {
        "precision": 0.022748,
        "recall": 0.041916,
        "f1": 0.026133,
        "accuracy": 0.041916,
        "main_score": 0.026133,
        "hf_subset": "eng_Latn-npi_Deva",
        "languages": [
          "eng-Latn",
          "npi-Deva"
        ]
      },
      {
        "precision": 0.00043,
        "recall": 0.005323,
        "f1": 0.00074,
        "accuracy": 0.005323,
        "main_score": 0.00074,
        "hf_subset": "eng_Latn-ory_Orya",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ]
      },
      {
        "precision": 0.011678,
        "recall": 0.029275,
        "f1": 0.014034,
        "accuracy": 0.029275,
        "main_score": 0.014034,
        "hf_subset": "eng_Latn-pan_Guru",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ]
      },
      {
        "precision": 0.026471,
        "recall": 0.046574,
        "f1": 0.030338,
        "accuracy": 0.046574,
        "main_score": 0.030338,
        "hf_subset": "eng_Latn-san_Deva",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ]
      },
      {
        "precision": 3.3e-05,
        "recall": 0.001996,
        "f1": 6.4e-05,
        "accuracy": 0.001996,
        "main_score": 6.4e-05,
        "hf_subset": "eng_Latn-sat_Olck",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ]
      },
      {
        "precision": 0.047268,
        "recall": 0.077179,
        "f1": 0.053764,
        "accuracy": 0.077179,
        "main_score": 0.053764,
        "hf_subset": "eng_Latn-snd_Deva",
        "languages": [
          "eng-Latn",
          "snd-Deva"
        ]
      },
      {
        "precision": 0.016246,
        "recall": 0.036593,
        "f1": 0.019679,
        "accuracy": 0.036593,
        "main_score": 0.019679,
        "hf_subset": "eng_Latn-tam_Taml",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ]
      },
      {
        "precision": 0.010553,
        "recall": 0.029275,
        "f1": 0.013269,
        "accuracy": 0.029275,
        "main_score": 0.013269,
        "hf_subset": "eng_Latn-tel_Telu",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ]
      },
      {
        "precision": 0.037911,
        "recall": 0.067864,
        "f1": 0.04326,
        "accuracy": 0.067864,
        "main_score": 0.04326,
        "hf_subset": "eng_Latn-urd_Arab",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ]
      },
      {
        "precision": 0.021783,
        "recall": 0.039255,
        "f1": 0.024512,
        "accuracy": 0.039255,
        "main_score": 0.024512,
        "hf_subset": "gom_Deva-eng_Latn",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.009924,
        "recall": 0.014637,
        "f1": 0.010455,
        "accuracy": 0.014637,
        "main_score": 0.010455,
        "hf_subset": "guj_Gujr-eng_Latn",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.034514,
        "recall": 0.05855,
        "f1": 0.038022,
        "accuracy": 0.05855,
        "main_score": 0.038022,
        "hf_subset": "hin_Deva-eng_Latn",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.00071,
        "recall": 0.002661,
        "f1": 0.000752,
        "accuracy": 0.002661,
        "main_score": 0.000752,
        "hf_subset": "kan_Knda-eng_Latn",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.023372,
        "recall": 0.037259,
        "f1": 0.025453,
        "accuracy": 0.037259,
        "main_score": 0.025453,
        "hf_subset": "kas_Arab-eng_Latn",
        "languages": [
          "kas-Arab",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.008504,
        "recall": 0.029275,
        "f1": 0.010605,
        "accuracy": 0.029275,
        "main_score": 0.010605,
        "hf_subset": "mai_Deva-eng_Latn",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.011288,
        "recall": 0.016633,
        "f1": 0.012359,
        "accuracy": 0.016633,
        "main_score": 0.012359,
        "hf_subset": "mal_Mlym-eng_Latn",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.009982,
        "recall": 0.028609,
        "f1": 0.011617,
        "accuracy": 0.028609,
        "main_score": 0.011617,
        "hf_subset": "mar_Deva-eng_Latn",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.000683,
        "recall": 0.002661,
        "f1": 0.000701,
        "accuracy": 0.002661,
        "main_score": 0.000701,
        "hf_subset": "mni_Mtei-eng_Latn",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.005831,
        "recall": 0.021956,
        "f1": 0.0074,
        "accuracy": 0.021956,
        "main_score": 0.0074,
        "hf_subset": "npi_Deva-eng_Latn",
        "languages": [
          "npi-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.00074,
        "recall": 0.002661,
        "f1": 0.000807,
        "accuracy": 0.002661,
        "main_score": 0.000807,
        "hf_subset": "ory_Orya-eng_Latn",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.017403,
        "recall": 0.024617,
        "f1": 0.018883,
        "accuracy": 0.024617,
        "main_score": 0.018883,
        "hf_subset": "pan_Guru-eng_Latn",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.007964,
        "recall": 0.025948,
        "f1": 0.009753,
        "accuracy": 0.025948,
        "main_score": 0.009753,
        "hf_subset": "san_Deva-eng_Latn",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.00067,
        "recall": 0.002661,
        "f1": 0.000897,
        "accuracy": 0.002661,
        "main_score": 0.000897,
        "hf_subset": "sat_Olck-eng_Latn",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.017567,
        "recall": 0.036593,
        "f1": 0.019747,
        "accuracy": 0.036593,
        "main_score": 0.019747,
        "hf_subset": "snd_Deva-eng_Latn",
        "languages": [
          "snd-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.015372,
        "recall": 0.025948,
        "f1": 0.016969,
        "accuracy": 0.025948,
        "main_score": 0.016969,
        "hf_subset": "tam_Taml-eng_Latn",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.009497,
        "recall": 0.017964,
        "f1": 0.01123,
        "accuracy": 0.017964,
        "main_score": 0.01123,
        "hf_subset": "tel_Telu-eng_Latn",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.017328,
        "recall": 0.02994,
        "f1": 0.01917,
        "accuracy": 0.02994,
        "main_score": 0.01917,
        "hf_subset": "urd_Arab-eng_Latn",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 167.3026828765869,
  "kg_co2_emissions": null,
  "mteb_dataset_name": "IN22ConvBitextMining"
}