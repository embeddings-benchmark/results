{
  "dataset_revision": "ec381535fe3ddf699297a023bcecaa548096ed68",
  "task_name": "IN22GenBitextMining",
  "mteb_version": "1.36.10",
  "scores": {
    "test": [
      {
        "precision": 0.007614,
        "recall": 0.011719,
        "f1": 0.007967,
        "accuracy": 0.011719,
        "main_score": 0.007967,
        "hf_subset": "asm_Beng-eng_Latn",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.011585,
        "recall": 0.024414,
        "f1": 0.013053,
        "accuracy": 0.024414,
        "main_score": 0.013053,
        "hf_subset": "ben_Beng-eng_Latn",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.06374,
        "recall": 0.09375,
        "f1": 0.068428,
        "accuracy": 0.09375,
        "main_score": 0.068428,
        "hf_subset": "brx_Deva-eng_Latn",
        "languages": [
          "brx-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.062598,
        "recall": 0.099609,
        "f1": 0.068487,
        "accuracy": 0.099609,
        "main_score": 0.068487,
        "hf_subset": "doi_Deva-eng_Latn",
        "languages": [
          "doi-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.024428,
        "recall": 0.044922,
        "f1": 0.02824,
        "accuracy": 0.044922,
        "main_score": 0.02824,
        "hf_subset": "eng_Latn-asm_Beng",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ]
      },
      {
        "precision": 0.046267,
        "recall": 0.082031,
        "f1": 0.053194,
        "accuracy": 0.082031,
        "main_score": 0.053194,
        "hf_subset": "eng_Latn-ben_Beng",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ]
      },
      {
        "precision": 0.140915,
        "recall": 0.219727,
        "f1": 0.158102,
        "accuracy": 0.219727,
        "main_score": 0.158102,
        "hf_subset": "eng_Latn-brx_Deva",
        "languages": [
          "eng-Latn",
          "brx-Deva"
        ]
      },
      {
        "precision": 0.193547,
        "recall": 0.276367,
        "f1": 0.211711,
        "accuracy": 0.276367,
        "main_score": 0.211711,
        "hf_subset": "eng_Latn-doi_Deva",
        "languages": [
          "eng-Latn",
          "doi-Deva"
        ]
      },
      {
        "precision": 0.151951,
        "recall": 0.228516,
        "f1": 0.16912,
        "accuracy": 0.228516,
        "main_score": 0.16912,
        "hf_subset": "eng_Latn-gom_Deva",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ]
      },
      {
        "precision": 0.007372,
        "recall": 0.02832,
        "f1": 0.009611,
        "accuracy": 0.02832,
        "main_score": 0.009611,
        "hf_subset": "eng_Latn-guj_Gujr",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ]
      },
      {
        "precision": 0.184792,
        "recall": 0.277344,
        "f1": 0.205924,
        "accuracy": 0.277344,
        "main_score": 0.205924,
        "hf_subset": "eng_Latn-hin_Deva",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ]
      },
      {
        "precision": 0.002028,
        "recall": 0.008789,
        "f1": 0.002596,
        "accuracy": 0.008789,
        "main_score": 0.002596,
        "hf_subset": "eng_Latn-kan_Knda",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ]
      },
      {
        "precision": 0.15584,
        "recall": 0.237305,
        "f1": 0.173259,
        "accuracy": 0.237305,
        "main_score": 0.173259,
        "hf_subset": "eng_Latn-kas_Arab",
        "languages": [
          "eng-Latn",
          "kas-Arab"
        ]
      },
      {
        "precision": 0.100104,
        "recall": 0.154297,
        "f1": 0.112857,
        "accuracy": 0.154297,
        "main_score": 0.112857,
        "hf_subset": "eng_Latn-mai_Deva",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ]
      },
      {
        "precision": 0.034471,
        "recall": 0.070312,
        "f1": 0.040247,
        "accuracy": 0.070312,
        "main_score": 0.040247,
        "hf_subset": "eng_Latn-mal_Mlym",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ]
      },
      {
        "precision": 0.07246,
        "recall": 0.117188,
        "f1": 0.081134,
        "accuracy": 0.117188,
        "main_score": 0.081134,
        "hf_subset": "eng_Latn-mar_Deva",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ]
      },
      {
        "precision": 0.001008,
        "recall": 0.00293,
        "f1": 0.001039,
        "accuracy": 0.00293,
        "main_score": 0.001039,
        "hf_subset": "eng_Latn-mni_Mtei",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ]
      },
      {
        "precision": 0.080776,
        "recall": 0.136719,
        "f1": 0.093551,
        "accuracy": 0.136719,
        "main_score": 0.093551,
        "hf_subset": "eng_Latn-npi_Deva",
        "languages": [
          "eng-Latn",
          "npi-Deva"
        ]
      },
      {
        "precision": 0.002004,
        "recall": 0.008789,
        "f1": 0.002527,
        "accuracy": 0.008789,
        "main_score": 0.002527,
        "hf_subset": "eng_Latn-ory_Orya",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ]
      },
      {
        "precision": 0.077372,
        "recall": 0.12207,
        "f1": 0.084946,
        "accuracy": 0.12207,
        "main_score": 0.084946,
        "hf_subset": "eng_Latn-pan_Guru",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ]
      },
      {
        "precision": 0.108676,
        "recall": 0.166016,
        "f1": 0.121466,
        "accuracy": 0.166016,
        "main_score": 0.121466,
        "hf_subset": "eng_Latn-san_Deva",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ]
      },
      {
        "precision": 0.000526,
        "recall": 0.006836,
        "f1": 0.000863,
        "accuracy": 0.006836,
        "main_score": 0.000863,
        "hf_subset": "eng_Latn-sat_Olck",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ]
      },
      {
        "precision": 0.188336,
        "recall": 0.276367,
        "f1": 0.208221,
        "accuracy": 0.276367,
        "main_score": 0.208221,
        "hf_subset": "eng_Latn-snd_Deva",
        "languages": [
          "eng-Latn",
          "snd-Deva"
        ]
      },
      {
        "precision": 0.057231,
        "recall": 0.121094,
        "f1": 0.069808,
        "accuracy": 0.121094,
        "main_score": 0.069808,
        "hf_subset": "eng_Latn-tam_Taml",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ]
      },
      {
        "precision": 0.050804,
        "recall": 0.097656,
        "f1": 0.058504,
        "accuracy": 0.097656,
        "main_score": 0.058504,
        "hf_subset": "eng_Latn-tel_Telu",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ]
      },
      {
        "precision": 0.152168,
        "recall": 0.234375,
        "f1": 0.170566,
        "accuracy": 0.234375,
        "main_score": 0.170566,
        "hf_subset": "eng_Latn-urd_Arab",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ]
      },
      {
        "precision": 0.052138,
        "recall": 0.085938,
        "f1": 0.056515,
        "accuracy": 0.085938,
        "main_score": 0.056515,
        "hf_subset": "gom_Deva-eng_Latn",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.011384,
        "recall": 0.016602,
        "f1": 0.012002,
        "accuracy": 0.016602,
        "main_score": 0.012002,
        "hf_subset": "guj_Gujr-eng_Latn",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.075575,
        "recall": 0.113281,
        "f1": 0.081149,
        "accuracy": 0.113281,
        "main_score": 0.081149,
        "hf_subset": "hin_Deva-eng_Latn",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.002116,
        "recall": 0.006836,
        "f1": 0.002674,
        "accuracy": 0.006836,
        "main_score": 0.002674,
        "hf_subset": "kan_Knda-eng_Latn",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.09126,
        "recall": 0.116211,
        "f1": 0.095492,
        "accuracy": 0.116211,
        "main_score": 0.095492,
        "hf_subset": "kas_Arab-eng_Latn",
        "languages": [
          "kas-Arab",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.022205,
        "recall": 0.051758,
        "f1": 0.025557,
        "accuracy": 0.051758,
        "main_score": 0.025557,
        "hf_subset": "mai_Deva-eng_Latn",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.027919,
        "recall": 0.050781,
        "f1": 0.032035,
        "accuracy": 0.050781,
        "main_score": 0.032035,
        "hf_subset": "mal_Mlym-eng_Latn",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.021012,
        "recall": 0.041016,
        "f1": 0.02326,
        "accuracy": 0.041016,
        "main_score": 0.02326,
        "hf_subset": "mar_Deva-eng_Latn",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.00012,
        "recall": 0.003906,
        "f1": 0.000217,
        "accuracy": 0.003906,
        "main_score": 0.000217,
        "hf_subset": "mni_Mtei-eng_Latn",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.022888,
        "recall": 0.049805,
        "f1": 0.026436,
        "accuracy": 0.049805,
        "main_score": 0.026436,
        "hf_subset": "npi_Deva-eng_Latn",
        "languages": [
          "npi-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.000668,
        "recall": 0.004883,
        "f1": 0.001074,
        "accuracy": 0.004883,
        "main_score": 0.001074,
        "hf_subset": "ory_Orya-eng_Latn",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.062946,
        "recall": 0.094727,
        "f1": 0.069038,
        "accuracy": 0.094727,
        "main_score": 0.069038,
        "hf_subset": "pan_Guru-eng_Latn",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.025829,
        "recall": 0.042969,
        "f1": 0.027681,
        "accuracy": 0.042969,
        "main_score": 0.027681,
        "hf_subset": "san_Deva-eng_Latn",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.001151,
        "recall": 0.004883,
        "f1": 0.001278,
        "accuracy": 0.004883,
        "main_score": 0.001278,
        "hf_subset": "sat_Olck-eng_Latn",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.081546,
        "recall": 0.116211,
        "f1": 0.086682,
        "accuracy": 0.116211,
        "main_score": 0.086682,
        "hf_subset": "snd_Deva-eng_Latn",
        "languages": [
          "snd-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.029812,
        "recall": 0.048828,
        "f1": 0.033182,
        "accuracy": 0.048828,
        "main_score": 0.033182,
        "hf_subset": "tam_Taml-eng_Latn",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.045165,
        "recall": 0.076172,
        "f1": 0.051535,
        "accuracy": 0.076172,
        "main_score": 0.051535,
        "hf_subset": "tel_Telu-eng_Latn",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.081943,
        "recall": 0.108398,
        "f1": 0.086267,
        "accuracy": 0.108398,
        "main_score": 0.086267,
        "hf_subset": "urd_Arab-eng_Latn",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 158.3666694164276,
  "kg_co2_emissions": null,
  "mteb_dataset_name": "IN22GenBitextMining"
}