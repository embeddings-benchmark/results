{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "task_name": "AmazonCounterfactualClassification",
  "mteb_version": "1.36.10",
  "scores": {
    "test": [
      {
        "accuracy": 0.651049,
        "f1": 0.529329,
        "f1_weighted": 0.718316,
        "ap": 0.159739,
        "ap_weighted": 0.159739,
        "scores_per_experiment": [
          {
            "accuracy": 0.708396,
            "f1": 0.579284,
            "f1_weighted": 0.763779,
            "ap": 0.194363,
            "ap_weighted": 0.194363
          },
          {
            "accuracy": 0.67916,
            "f1": 0.559158,
            "f1_weighted": 0.741231,
            "ap": 0.184438,
            "ap_weighted": 0.184438
          },
          {
            "accuracy": 0.604198,
            "f1": 0.493338,
            "f1_weighted": 0.680947,
            "ap": 0.140317,
            "ap_weighted": 0.140317
          },
          {
            "accuracy": 0.64093,
            "f1": 0.519312,
            "f1_weighted": 0.71071,
            "ap": 0.151439,
            "ap_weighted": 0.151439
          },
          {
            "accuracy": 0.597451,
            "f1": 0.495791,
            "f1_weighted": 0.675012,
            "ap": 0.148292,
            "ap_weighted": 0.148292
          },
          {
            "accuracy": 0.65967,
            "f1": 0.532378,
            "f1_weighted": 0.725511,
            "ap": 0.157261,
            "ap_weighted": 0.157261
          },
          {
            "accuracy": 0.704648,
            "f1": 0.555924,
            "f1_weighted": 0.75936,
            "ap": 0.161236,
            "ap_weighted": 0.161236
          },
          {
            "accuracy": 0.624438,
            "f1": 0.511693,
            "f1_weighted": 0.697432,
            "ap": 0.151913,
            "ap_weighted": 0.151913
          },
          {
            "accuracy": 0.655922,
            "f1": 0.535609,
            "f1_weighted": 0.722723,
            "ap": 0.164637,
            "ap_weighted": 0.164637
          },
          {
            "accuracy": 0.635682,
            "f1": 0.510802,
            "f1_weighted": 0.70646,
            "ap": 0.143497,
            "ap_weighted": 0.143497
          }
        ],
        "main_score": 0.651049,
        "hf_subset": "en-ext",
        "languages": [
          "eng-Latn"
        ]
      },
      {
        "accuracy": 0.644179,
        "f1": 0.585123,
        "f1_weighted": 0.679953,
        "ap": 0.277005,
        "ap_weighted": 0.277005,
        "scores_per_experiment": [
          {
            "accuracy": 0.583582,
            "f1": 0.527404,
            "f1_weighted": 0.626627,
            "ap": 0.234739,
            "ap_weighted": 0.234739
          },
          {
            "accuracy": 0.646269,
            "f1": 0.592691,
            "f1_weighted": 0.682649,
            "ap": 0.286478,
            "ap_weighted": 0.286478
          },
          {
            "accuracy": 0.579104,
            "f1": 0.537021,
            "f1_weighted": 0.622022,
            "ap": 0.25263,
            "ap_weighted": 0.25263
          },
          {
            "accuracy": 0.640299,
            "f1": 0.582656,
            "f1_weighted": 0.677106,
            "ap": 0.274408,
            "ap_weighted": 0.274408
          },
          {
            "accuracy": 0.677612,
            "f1": 0.615106,
            "f1_weighted": 0.709559,
            "ap": 0.298397,
            "ap_weighted": 0.298397
          },
          {
            "accuracy": 0.668657,
            "f1": 0.608842,
            "f1_weighted": 0.701988,
            "ap": 0.295066,
            "ap_weighted": 0.295066
          },
          {
            "accuracy": 0.656716,
            "f1": 0.578911,
            "f1_weighted": 0.689135,
            "ap": 0.257753,
            "ap_weighted": 0.257753
          },
          {
            "accuracy": 0.69403,
            "f1": 0.629843,
            "f1_weighted": 0.723708,
            "ap": 0.310651,
            "ap_weighted": 0.310651
          },
          {
            "accuracy": 0.659701,
            "f1": 0.592549,
            "f1_weighted": 0.693278,
            "ap": 0.275221,
            "ap_weighted": 0.275221
          },
          {
            "accuracy": 0.635821,
            "f1": 0.586206,
            "f1_weighted": 0.67346,
            "ap": 0.284702,
            "ap_weighted": 0.284702
          }
        ],
        "main_score": 0.644179,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 19.290899515151978,
  "kg_co2_emissions": null,
  "mteb_dataset_name": "AmazonCounterfactualClassification"
}