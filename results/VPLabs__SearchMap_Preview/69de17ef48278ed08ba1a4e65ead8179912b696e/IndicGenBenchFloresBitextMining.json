{
  "dataset_revision": "f8650438298df086750ff4973661bb58a201a5ee",
  "task_name": "IndicGenBenchFloresBitextMining",
  "mteb_version": "1.36.10",
  "scores": {
    "test": [
      {
        "precision": 0.057838,
        "recall": 0.075099,
        "f1": 0.060934,
        "accuracy": 0.075099,
        "main_score": 0.060934,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.06565,
        "recall": 0.12747,
        "f1": 0.07617,
        "accuracy": 0.12747,
        "main_score": 0.07617,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ]
      },
      {
        "precision": 0.048556,
        "recall": 0.068182,
        "f1": 0.052206,
        "accuracy": 0.068182,
        "main_score": 0.052206,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.028644,
        "recall": 0.082016,
        "f1": 0.035842,
        "accuracy": 0.082016,
        "main_score": 0.035842,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ]
      },
      {
        "precision": 0.091054,
        "recall": 0.110672,
        "f1": 0.09457,
        "accuracy": 0.110672,
        "main_score": 0.09457,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.122483,
        "recall": 0.201581,
        "f1": 0.138172,
        "accuracy": 0.201581,
        "main_score": 0.138172,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ]
      },
      {
        "precision": 0.06456,
        "recall": 0.088933,
        "f1": 0.069359,
        "accuracy": 0.088933,
        "main_score": 0.069359,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.039811,
        "recall": 0.096838,
        "f1": 0.04767,
        "accuracy": 0.096838,
        "main_score": 0.04767,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ]
      },
      {
        "precision": 0.04751,
        "recall": 0.071146,
        "f1": 0.052157,
        "accuracy": 0.071146,
        "main_score": 0.052157,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.04449,
        "recall": 0.089921,
        "f1": 0.0506,
        "accuracy": 0.089921,
        "main_score": 0.0506,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ]
      },
      {
        "precision": 0.062526,
        "recall": 0.085968,
        "f1": 0.065977,
        "accuracy": 0.085968,
        "main_score": 0.065977,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.078194,
        "recall": 0.150198,
        "f1": 0.091144,
        "accuracy": 0.150198,
        "main_score": 0.091144,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ]
      },
      {
        "precision": 0.07531,
        "recall": 0.101779,
        "f1": 0.080876,
        "accuracy": 0.101779,
        "main_score": 0.080876,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.052182,
        "recall": 0.119565,
        "f1": 0.063612,
        "accuracy": 0.119565,
        "main_score": 0.063612,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ]
      },
      {
        "precision": 0.102604,
        "recall": 0.129447,
        "f1": 0.108176,
        "accuracy": 0.129447,
        "main_score": 0.108176,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.055041,
        "recall": 0.144269,
        "f1": 0.068609,
        "accuracy": 0.144269,
        "main_score": 0.068609,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ]
      },
      {
        "precision": 0.076051,
        "recall": 0.097826,
        "f1": 0.080423,
        "accuracy": 0.097826,
        "main_score": 0.080423,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.089848,
        "recall": 0.157115,
        "f1": 0.10184,
        "accuracy": 0.157115,
        "main_score": 0.10184,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ]
      },
      {
        "precision": 0.041376,
        "recall": 0.059289,
        "f1": 0.044236,
        "accuracy": 0.059289,
        "main_score": 0.044236,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.038519,
        "recall": 0.094862,
        "f1": 0.047048,
        "accuracy": 0.094862,
        "main_score": 0.047048,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ]
      },
      {
        "precision": 0.092512,
        "recall": 0.117589,
        "f1": 0.097981,
        "accuracy": 0.117589,
        "main_score": 0.097981,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.113453,
        "recall": 0.184783,
        "f1": 0.126679,
        "accuracy": 0.184783,
        "main_score": 0.126679,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ]
      },
      {
        "precision": 0.092107,
        "recall": 0.116601,
        "f1": 0.096941,
        "accuracy": 0.116601,
        "main_score": 0.096941,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.092684,
        "recall": 0.171937,
        "f1": 0.106364,
        "accuracy": 0.171937,
        "main_score": 0.106364,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ]
      },
      {
        "precision": 0.063594,
        "recall": 0.085968,
        "f1": 0.068308,
        "accuracy": 0.085968,
        "main_score": 0.068308,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.048426,
        "recall": 0.110672,
        "f1": 0.058126,
        "accuracy": 0.110672,
        "main_score": 0.058126,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ]
      },
      {
        "precision": 0.070991,
        "recall": 0.097826,
        "f1": 0.076783,
        "accuracy": 0.097826,
        "main_score": 0.076783,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.0529,
        "recall": 0.110672,
        "f1": 0.061165,
        "accuracy": 0.110672,
        "main_score": 0.061165,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ]
      },
      {
        "precision": 0.162107,
        "recall": 0.187747,
        "f1": 0.168478,
        "accuracy": 0.187747,
        "main_score": 0.168478,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.082212,
        "recall": 0.182806,
        "f1": 0.101313,
        "accuracy": 0.182806,
        "main_score": 0.101313,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ]
      },
      {
        "precision": 0.085141,
        "recall": 0.106719,
        "f1": 0.089445,
        "accuracy": 0.106719,
        "main_score": 0.089445,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.078352,
        "recall": 0.148221,
        "f1": 0.089703,
        "accuracy": 0.148221,
        "main_score": 0.089703,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ]
      },
      {
        "precision": 0.078086,
        "recall": 0.09585,
        "f1": 0.082058,
        "accuracy": 0.09585,
        "main_score": 0.082058,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.110483,
        "recall": 0.188735,
        "f1": 0.127034,
        "accuracy": 0.188735,
        "main_score": 0.127034,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ]
      },
      {
        "precision": 0.063868,
        "recall": 0.082016,
        "f1": 0.067192,
        "accuracy": 0.082016,
        "main_score": 0.067192,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.113608,
        "recall": 0.197628,
        "f1": 0.132045,
        "accuracy": 0.197628,
        "main_score": 0.132045,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ]
      },
      {
        "precision": 0.048384,
        "recall": 0.061265,
        "f1": 0.051214,
        "accuracy": 0.061265,
        "main_score": 0.051214,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.03035,
        "recall": 0.082016,
        "f1": 0.03653,
        "accuracy": 0.082016,
        "main_score": 0.03653,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ]
      },
      {
        "precision": 0.073509,
        "recall": 0.085968,
        "f1": 0.076035,
        "accuracy": 0.085968,
        "main_score": 0.076035,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.070862,
        "recall": 0.146245,
        "f1": 0.082552,
        "accuracy": 0.146245,
        "main_score": 0.082552,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ]
      },
      {
        "precision": 0.069909,
        "recall": 0.085968,
        "f1": 0.073009,
        "accuracy": 0.085968,
        "main_score": 0.073009,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.097955,
        "recall": 0.162055,
        "f1": 0.109904,
        "accuracy": 0.162055,
        "main_score": 0.109904,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ]
      },
      {
        "precision": 0.11719,
        "recall": 0.140316,
        "f1": 0.122165,
        "accuracy": 0.140316,
        "main_score": 0.122165,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.091856,
        "recall": 0.177866,
        "f1": 0.106999,
        "accuracy": 0.177866,
        "main_score": 0.106999,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ]
      },
      {
        "precision": 0.061348,
        "recall": 0.077075,
        "f1": 0.064025,
        "accuracy": 0.077075,
        "main_score": 0.064025,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.104783,
        "recall": 0.16502,
        "f1": 0.116799,
        "accuracy": 0.16502,
        "main_score": 0.116799,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ]
      },
      {
        "precision": 0.094306,
        "recall": 0.117589,
        "f1": 0.09924,
        "accuracy": 0.117589,
        "main_score": 0.09924,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.102773,
        "recall": 0.177866,
        "f1": 0.115793,
        "accuracy": 0.177866,
        "main_score": 0.115793,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ]
      },
      {
        "precision": 0.093108,
        "recall": 0.115613,
        "f1": 0.097951,
        "accuracy": 0.115613,
        "main_score": 0.097951,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.110303,
        "recall": 0.190711,
        "f1": 0.125969,
        "accuracy": 0.190711,
        "main_score": 0.125969,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ]
      },
      {
        "precision": 0.087416,
        "recall": 0.113636,
        "f1": 0.092025,
        "accuracy": 0.113636,
        "main_score": 0.092025,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.076661,
        "recall": 0.158103,
        "f1": 0.090788,
        "accuracy": 0.158103,
        "main_score": 0.090788,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ]
      },
      {
        "precision": 0.069061,
        "recall": 0.085968,
        "f1": 0.072028,
        "accuracy": 0.085968,
        "main_score": 0.072028,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.10492,
        "recall": 0.174901,
        "f1": 0.118796,
        "accuracy": 0.174901,
        "main_score": 0.118796,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ]
      },
      {
        "precision": 0.090093,
        "recall": 0.110672,
        "f1": 0.094401,
        "accuracy": 0.110672,
        "main_score": 0.094401,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.102273,
        "recall": 0.18083,
        "f1": 0.117265,
        "accuracy": 0.18083,
        "main_score": 0.117265,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ]
      },
      {
        "precision": 0.014949,
        "recall": 0.023715,
        "f1": 0.016165,
        "accuracy": 0.023715,
        "main_score": 0.016165,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ]
      },
      {
        "precision": 0.004416,
        "recall": 0.027668,
        "f1": 0.006156,
        "accuracy": 0.027668,
        "main_score": 0.006156,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ]
      }
    ]
  },
  "evaluation_time": 664.0961632728577,
  "kg_co2_emissions": null,
  "mteb_dataset_name": "IndicGenBenchFloresBitextMining"
}