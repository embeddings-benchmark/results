{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "task_name": "MTOPIntentClassification",
  "mteb_version": "1.36.10",
  "scores": {
    "test": [
      {
        "accuracy": 0.682421,
        "f1": 0.491626,
        "f1_weighted": 0.707124,
        "scores_per_experiment": [
          {
            "accuracy": 0.677155,
            "f1": 0.492217,
            "f1_weighted": 0.704075
          },
          {
            "accuracy": 0.678751,
            "f1": 0.48165,
            "f1_weighted": 0.698065
          },
          {
            "accuracy": 0.720474,
            "f1": 0.496076,
            "f1_weighted": 0.741888
          },
          {
            "accuracy": 0.698586,
            "f1": 0.500503,
            "f1_weighted": 0.724371
          },
          {
            "accuracy": 0.680119,
            "f1": 0.499256,
            "f1_weighted": 0.703702
          },
          {
            "accuracy": 0.668947,
            "f1": 0.481784,
            "f1_weighted": 0.695507
          },
          {
            "accuracy": 0.647515,
            "f1": 0.488672,
            "f1_weighted": 0.673219
          },
          {
            "accuracy": 0.678751,
            "f1": 0.497326,
            "f1_weighted": 0.707832
          },
          {
            "accuracy": 0.692202,
            "f1": 0.493568,
            "f1_weighted": 0.718727
          },
          {
            "accuracy": 0.681715,
            "f1": 0.485211,
            "f1_weighted": 0.703855
          }
        ],
        "main_score": 0.682421,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 65.66299843788147,
  "kg_co2_emissions": null,
  "mteb_dataset_name": "MTOPIntentClassification"
}