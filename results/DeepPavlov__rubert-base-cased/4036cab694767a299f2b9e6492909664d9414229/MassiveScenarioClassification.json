{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 12.414028406143188,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test": [
      {
        "accuracy": 0.5679219905850708,
        "f1": 0.5656899157195868,
        "f1_weighted": 0.5696352359672663,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.5679219905850708,
        "scores_per_experiment": [
          {
            "accuracy": 0.5759919300605245,
            "f1": 0.5792436194113899,
            "f1_weighted": 0.5767923831119703
          },
          {
            "accuracy": 0.5813718897108272,
            "f1": 0.5733350282520377,
            "f1_weighted": 0.5837354364259991
          },
          {
            "accuracy": 0.5504371217215871,
            "f1": 0.5481799349033717,
            "f1_weighted": 0.5514491669620872
          },
          {
            "accuracy": 0.5830531271015468,
            "f1": 0.5782591259514115,
            "f1_weighted": 0.5863555026741236
          },
          {
            "accuracy": 0.5642232683254875,
            "f1": 0.5468673162298436,
            "f1_weighted": 0.5635648975015958
          },
          {
            "accuracy": 0.5696032279757902,
            "f1": 0.5647396224782916,
            "f1_weighted": 0.5681480526705073
          },
          {
            "accuracy": 0.5911230665770006,
            "f1": 0.5847902636171523,
            "f1_weighted": 0.597890995308132
          },
          {
            "accuracy": 0.5302622730329523,
            "f1": 0.5353742446380417,
            "f1_weighted": 0.5310867952630811
          },
          {
            "accuracy": 0.5601882985877606,
            "f1": 0.5742249216175531,
            "f1_weighted": 0.5599041479783708
          },
          {
            "accuracy": 0.5729657027572294,
            "f1": 0.5718850800967755,
            "f1_weighted": 0.5774249817767966
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5727496310870634,
        "f1": 0.5724713719848271,
        "f1_weighted": 0.5735779215409387,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.5727496310870634,
        "scores_per_experiment": [
          {
            "accuracy": 0.5873093949827841,
            "f1": 0.5879928818632777,
            "f1_weighted": 0.5859133253140065
          },
          {
            "accuracy": 0.5976389572060994,
            "f1": 0.589684157012183,
            "f1_weighted": 0.6008015288780904
          },
          {
            "accuracy": 0.5656665027053616,
            "f1": 0.5641260614314305,
            "f1_weighted": 0.5637079922538555
          },
          {
            "accuracy": 0.5691096901131333,
            "f1": 0.5681189980373104,
            "f1_weighted": 0.5708186465405108
          },
          {
            "accuracy": 0.5720609936055091,
            "f1": 0.5621331813210145,
            "f1_weighted": 0.5691905946751106
          },
          {
            "accuracy": 0.5641908509591737,
            "f1": 0.563275338726315,
            "f1_weighted": 0.5632699862054925
          },
          {
            "accuracy": 0.5818986719134285,
            "f1": 0.5791839213388218,
            "f1_weighted": 0.5874768314982747
          },
          {
            "accuracy": 0.5376291195277915,
            "f1": 0.5464771515504472,
            "f1_weighted": 0.5391057887912024
          },
          {
            "accuracy": 0.559272011805214,
            "f1": 0.573085798349211,
            "f1_weighted": 0.5587867242280146
          },
          {
            "accuracy": 0.5927201180521396,
            "f1": 0.5906362302182602,
            "f1_weighted": 0.596707797024829
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}