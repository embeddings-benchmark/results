{
  "dataset_revision": "5b7d477a8c62cdd18e2fed7e015497c20b4371ad",
  "evaluation_time": 1.6991829872131348,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.5772893772893772,
        "cosine_accuracy_threshold": 0.6457070112228394,
        "cosine_ap": 0.58985731403516,
        "cosine_f1": 0.6699653636813458,
        "cosine_f1_threshold": 0.34617292881011963,
        "cosine_precision": 0.5056011949215833,
        "cosine_recall": 0.9926686217008798,
        "dot_accuracy": 0.5560439560439561,
        "dot_accuracy_threshold": 57.91943359375,
        "dot_ap": 0.5842929109653656,
        "dot_f1": 0.6656891495601173,
        "dot_f1_threshold": 19.668575286865234,
        "dot_precision": 0.499266862170088,
        "dot_recall": 0.998533724340176,
        "euclidean_accuracy": 0.5706959706959707,
        "euclidean_accuracy_threshold": 7.748727321624756,
        "euclidean_ap": 0.5788634811072617,
        "euclidean_f1": 0.6740665993945509,
        "euclidean_f1_threshold": 11.592394828796387,
        "euclidean_precision": 0.5138461538461538,
        "euclidean_recall": 0.9794721407624634,
        "hf_subset": "russian",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.58985731403516,
        "manhattan_accuracy": 0.5677655677655677,
        "manhattan_accuracy_threshold": 168.7405242919922,
        "manhattan_ap": 0.5782646657342907,
        "manhattan_f1": 0.6727456940222898,
        "manhattan_f1_threshold": 252.24420166015625,
        "manhattan_precision": 0.5139318885448917,
        "manhattan_recall": 0.9736070381231672,
        "max_ap": 0.58985731403516,
        "max_f1": 0.6740665993945509,
        "max_precision": 0.5139318885448917,
        "max_recall": 0.998533724340176,
        "similarity_accuracy": 0.5772893772893772,
        "similarity_accuracy_threshold": 0.6457070708274841,
        "similarity_ap": 0.58985731403516,
        "similarity_f1": 0.6699653636813458,
        "similarity_f1_threshold": 0.3461729884147644,
        "similarity_precision": 0.5056011949215833,
        "similarity_recall": 0.9926686217008798
      }
    ]
  },
  "task_name": "XNLIV2"
}