{
  "dataset_revision": "09698e0180d87dc247ca447d3a1248b931ac0cdb",
  "evaluation_time": 3.5636658668518066,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.580952380952381,
        "cosine_accuracy_threshold": 0.5947901606559753,
        "cosine_ap": 0.6000993714938191,
        "cosine_f1": 0.6686507936507937,
        "cosine_f1_threshold": 0.3520946204662323,
        "cosine_precision": 0.5052473763118441,
        "cosine_recall": 0.9882697947214076,
        "dot_accuracy": 0.5582417582417583,
        "dot_accuracy_threshold": 48.4761962890625,
        "dot_ap": 0.5990057316376747,
        "dot_f1": 0.6666666666666666,
        "dot_f1_threshold": 25.26584243774414,
        "dot_precision": 0.5026061057334326,
        "dot_recall": 0.9897360703812317,
        "euclidean_accuracy": 0.578021978021978,
        "euclidean_accuracy_threshold": 7.140948295593262,
        "euclidean_ap": 0.5896667884933067,
        "euclidean_f1": 0.6683291770573566,
        "euclidean_f1_threshold": 11.949054718017578,
        "euclidean_precision": 0.5064247921390779,
        "euclidean_recall": 0.9824046920821115,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6000993714938191,
        "manhattan_accuracy": 0.5772893772893772,
        "manhattan_accuracy_threshold": 157.28817749023438,
        "manhattan_ap": 0.589341116636584,
        "manhattan_f1": 0.6686656671664168,
        "manhattan_f1_threshold": 260.399658203125,
        "manhattan_precision": 0.5072024260803639,
        "manhattan_recall": 0.9809384164222874,
        "max_ap": 0.6000993714938191,
        "max_f1": 0.6686656671664168,
        "max_precision": 0.5072024260803639,
        "max_recall": 0.9897360703812317,
        "similarity_accuracy": 0.580952380952381,
        "similarity_accuracy_threshold": 0.5947902202606201,
        "similarity_ap": 0.6000993714938191,
        "similarity_f1": 0.6686507936507937,
        "similarity_f1_threshold": 0.3520946204662323,
        "similarity_precision": 0.5052473763118441,
        "similarity_recall": 0.9882697947214076
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.5692307692307692,
        "cosine_accuracy_threshold": 0.6991117000579834,
        "cosine_ap": 0.6003272183437564,
        "cosine_f1": 0.6694003075345976,
        "cosine_f1_threshold": 0.4017314910888672,
        "cosine_precision": 0.5145784081954294,
        "cosine_recall": 0.9574780058651027,
        "dot_accuracy": 0.5472527472527473,
        "dot_accuracy_threshold": 42.64329528808594,
        "dot_ap": 0.5715013209590903,
        "dot_f1": 0.6669926650366748,
        "dot_f1_threshold": 20.86639976501465,
        "dot_precision": 0.5003668378576669,
        "dot_recall": 1.0,
        "euclidean_accuracy": 0.5772893772893772,
        "euclidean_accuracy_threshold": 7.1292901039123535,
        "euclidean_ap": 0.598611035278311,
        "euclidean_f1": 0.6676456191874693,
        "euclidean_f1_threshold": 13.571797370910645,
        "euclidean_precision": 0.5011021307861866,
        "euclidean_recall": 1.0,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6003272183437564,
        "manhattan_accuracy": 0.5772893772893772,
        "manhattan_accuracy_threshold": 156.185791015625,
        "manhattan_ap": 0.5990831057432653,
        "manhattan_f1": 0.6679900744416873,
        "manhattan_f1_threshold": 272.4861145019531,
        "manhattan_precision": 0.5048762190547637,
        "manhattan_recall": 0.9868035190615836,
        "max_ap": 0.6003272183437564,
        "max_f1": 0.6694003075345976,
        "max_precision": 0.5145784081954294,
        "max_recall": 1.0,
        "similarity_accuracy": 0.5692307692307692,
        "similarity_accuracy_threshold": 0.6991117000579834,
        "similarity_ap": 0.6003272183437564,
        "similarity_f1": 0.6694003075345976,
        "similarity_f1_threshold": 0.40173137187957764,
        "similarity_precision": 0.5145784081954294,
        "similarity_recall": 0.9574780058651027
      }
    ]
  },
  "task_name": "XNLI"
}