{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 24.058120489120483,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test": [
      {
        "accuracy": 0.5301613987895091,
        "f1": 0.503648129018201,
        "f1_weighted": 0.5371528502624019,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.5301613987895091,
        "scores_per_experiment": [
          {
            "accuracy": 0.5292535305985205,
            "f1": 0.508096307052023,
            "f1_weighted": 0.5356562560814354
          },
          {
            "accuracy": 0.5342972427706792,
            "f1": 0.5033506457335305,
            "f1_weighted": 0.5397087504385949
          },
          {
            "accuracy": 0.5302622730329523,
            "f1": 0.5007912204889389,
            "f1_weighted": 0.5318684635313944
          },
          {
            "accuracy": 0.5339609952925353,
            "f1": 0.5077428849720896,
            "f1_weighted": 0.5464824674984935
          },
          {
            "accuracy": 0.539340954942838,
            "f1": 0.5089328040743643,
            "f1_weighted": 0.5429754460209356
          },
          {
            "accuracy": 0.531271015467384,
            "f1": 0.5015817891852753,
            "f1_weighted": 0.537918411379773
          },
          {
            "accuracy": 0.5218560860793544,
            "f1": 0.4995364796959422,
            "f1_weighted": 0.5282274250776611
          },
          {
            "accuracy": 0.5164761264290518,
            "f1": 0.4879984657788682,
            "f1_weighted": 0.5237493086976167
          },
          {
            "accuracy": 0.5322797579018157,
            "f1": 0.5084816343701242,
            "f1_weighted": 0.5430117106828383
          },
          {
            "accuracy": 0.5326160053799597,
            "f1": 0.5099690588308545,
            "f1_weighted": 0.5419302632152767
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5342843089030989,
        "f1": 0.4985182768365918,
        "f1_weighted": 0.5410080458842499,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.5342843089030989,
        "scores_per_experiment": [
          {
            "accuracy": 0.5405804230201673,
            "f1": 0.5089754516125126,
            "f1_weighted": 0.5459545259146003
          },
          {
            "accuracy": 0.5509099852434826,
            "f1": 0.4959659421739848,
            "f1_weighted": 0.560325385443184
          },
          {
            "accuracy": 0.5253320216428923,
            "f1": 0.4870174468270864,
            "f1_weighted": 0.5284917252928911
          },
          {
            "accuracy": 0.5494343334972946,
            "f1": 0.5114949825146854,
            "f1_weighted": 0.5625795209618383
          },
          {
            "accuracy": 0.5332021642892277,
            "f1": 0.5026490936465333,
            "f1_weighted": 0.5388631865798033
          },
          {
            "accuracy": 0.5341859321200196,
            "f1": 0.5028703494772332,
            "f1_weighted": 0.5409685907099934
          },
          {
            "accuracy": 0.5282833251352681,
            "f1": 0.494651724727818,
            "f1_weighted": 0.5339518965559218
          },
          {
            "accuracy": 0.5115592720118052,
            "f1": 0.4783685499449977,
            "f1_weighted": 0.5154891296394862
          },
          {
            "accuracy": 0.5376291195277915,
            "f1": 0.5109054025458954,
            "f1_weighted": 0.5434782462356671
          },
          {
            "accuracy": 0.5317265125430398,
            "f1": 0.49228382489517053,
            "f1_weighted": 0.5399782515091136
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}