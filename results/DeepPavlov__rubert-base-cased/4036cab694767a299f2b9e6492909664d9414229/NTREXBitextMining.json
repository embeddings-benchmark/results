{
  "dataset_revision": "ed9a4403ed4adbfaf4aab56d5b2709e9f6c3ba33",
  "evaluation_time": 128.2676842212677,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test": [
      {
        "accuracy": 0.00100150225338007,
        "f1": 0.0005014174621879592,
        "hf_subset": "arb_Arab-rus_Cyrl",
        "languages": [
          "arb-Arab",
          "rus-Cyrl"
        ],
        "main_score": 0.0005014174621879592,
        "precision": 0.0005010845162550751,
        "recall": 0.00100150225338007
      },
      {
        "accuracy": 0.09464196294441662,
        "f1": 0.07236790547915391,
        "hf_subset": "bel_Cyrl-rus_Cyrl",
        "languages": [
          "bel-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.07236790547915391,
        "precision": 0.06852229467034265,
        "recall": 0.09464196294441662
      },
      {
        "accuracy": 0.0030045067601402104,
        "f1": 0.001187416379138043,
        "hf_subset": "ben_Beng-rus_Cyrl",
        "languages": [
          "ben-Beng",
          "rus-Cyrl"
        ],
        "main_score": 0.001187416379138043,
        "precision": 0.0010111317930498896,
        "recall": 0.0030045067601402104
      },
      {
        "accuracy": 0.012518778167250876,
        "f1": 0.008583180197247232,
        "hf_subset": "bos_Latn-rus_Cyrl",
        "languages": [
          "bos-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.008583180197247232,
        "precision": 0.008035822590488127,
        "recall": 0.012518778167250876
      },
      {
        "accuracy": 0.11316975463194792,
        "f1": 0.08938577024175752,
        "hf_subset": "bul_Cyrl-rus_Cyrl",
        "languages": [
          "bul-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.08938577024175752,
        "precision": 0.08474537374531824,
        "recall": 0.11316975463194792
      },
      {
        "accuracy": 0.019529293940911366,
        "f1": 0.014394910840794797,
        "hf_subset": "ces_Latn-rus_Cyrl",
        "languages": [
          "ces-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.014394910840794797,
        "precision": 0.013949153746686451,
        "recall": 0.019529293940911366
      },
      {
        "accuracy": 0.02704056084126189,
        "f1": 0.020038832417367147,
        "hf_subset": "deu_Latn-rus_Cyrl",
        "languages": [
          "deu-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.020038832417367147,
        "precision": 0.01905715962144547,
        "recall": 0.02704056084126189
      },
      {
        "accuracy": 0.00400600901352028,
        "f1": 0.0013556632712097495,
        "hf_subset": "ell_Grek-rus_Cyrl",
        "languages": [
          "ell-Grek",
          "rus-Cyrl"
        ],
        "main_score": 0.0013556632712097495,
        "precision": 0.0012013385611714363,
        "recall": 0.00400600901352028
      },
      {
        "accuracy": 0.08612919379068602,
        "f1": 0.07191932065415994,
        "hf_subset": "eng_Latn-rus_Cyrl",
        "languages": [
          "eng-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.07191932065415994,
        "precision": 0.06914464459973725,
        "recall": 0.08612919379068602
      },
      {
        "accuracy": 0.000500751126690035,
        "f1": 0.000500751126690035,
        "hf_subset": "fas_Arab-rus_Cyrl",
        "languages": [
          "fas-Arab",
          "rus-Cyrl"
        ],
        "main_score": 0.000500751126690035,
        "precision": 0.000500751126690035,
        "recall": 0.000500751126690035
      },
      {
        "accuracy": 0.009514271407110666,
        "f1": 0.005782942990390722,
        "hf_subset": "fin_Latn-rus_Cyrl",
        "languages": [
          "fin-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.005782942990390722,
        "precision": 0.005343668209902801,
        "recall": 0.009514271407110666
      },
      {
        "accuracy": 0.010015022533800702,
        "f1": 0.008546663532714037,
        "hf_subset": "fra_Latn-rus_Cyrl",
        "languages": [
          "fra-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.008546663532714037,
        "precision": 0.008373927234223805,
        "recall": 0.010015022533800702
      },
      {
        "accuracy": 0.006509764646970456,
        "f1": 0.0032974515300541185,
        "hf_subset": "heb_Hebr-rus_Cyrl",
        "languages": [
          "heb-Hebr",
          "rus-Cyrl"
        ],
        "main_score": 0.0032974515300541185,
        "precision": 0.003070204813010283,
        "recall": 0.006509764646970456
      },
      {
        "accuracy": 0.006009013520280421,
        "f1": 0.004063419786292483,
        "hf_subset": "hin_Deva-rus_Cyrl",
        "languages": [
          "hin-Deva",
          "rus-Cyrl"
        ],
        "main_score": 0.004063419786292483,
        "precision": 0.003892985768071123,
        "recall": 0.006009013520280421
      },
      {
        "accuracy": 0.016524787180771158,
        "f1": 0.011642229159820279,
        "hf_subset": "hrv_Latn-rus_Cyrl",
        "languages": [
          "hrv-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.011642229159820279,
        "precision": 0.011091301211721832,
        "recall": 0.016524787180771158
      },
      {
        "accuracy": 0.00801201802704056,
        "f1": 0.004450982516090896,
        "hf_subset": "hun_Latn-rus_Cyrl",
        "languages": [
          "hun-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.004450982516090896,
        "precision": 0.004128643266969254,
        "recall": 0.00801201802704056
      },
      {
        "accuracy": 0.009514271407110666,
        "f1": 0.004940411776429528,
        "hf_subset": "ind_Latn-rus_Cyrl",
        "languages": [
          "ind-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.004940411776429528,
        "precision": 0.004290506514909392,
        "recall": 0.009514271407110666
      },
      {
        "accuracy": 0.010015022533800702,
        "f1": 0.004336413296261511,
        "hf_subset": "jpn_Jpan-rus_Cyrl",
        "languages": [
          "jpn-Jpan",
          "rus-Cyrl"
        ],
        "main_score": 0.004336413296261511,
        "precision": 0.0035878803301568757,
        "recall": 0.010015022533800702
      },
      {
        "accuracy": 0.005508262393590386,
        "f1": 0.003578560871090878,
        "hf_subset": "kor_Hang-rus_Cyrl",
        "languages": [
          "kor-Hang",
          "rus-Cyrl"
        ],
        "main_score": 0.003578560871090878,
        "precision": 0.0035428908326369845,
        "recall": 0.005508262393590386
      },
      {
        "accuracy": 0.009013520280420632,
        "f1": 0.005092356202617286,
        "hf_subset": "lit_Latn-rus_Cyrl",
        "languages": [
          "lit-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.005092356202617286,
        "precision": 0.004573664107545671,
        "recall": 0.009013520280420632
      },
      {
        "accuracy": 0.05408112168252378,
        "f1": 0.04307194899232441,
        "hf_subset": "mkd_Cyrl-rus_Cyrl",
        "languages": [
          "mkd-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.04307194899232441,
        "precision": 0.04094259710630829,
        "recall": 0.05408112168252378
      },
      {
        "accuracy": 0.015523284927391086,
        "f1": 0.00987377896951988,
        "hf_subset": "nld_Latn-rus_Cyrl",
        "languages": [
          "nld-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.00987377896951988,
        "precision": 0.009078906984379105,
        "recall": 0.015523284927391086
      },
      {
        "accuracy": 0.020530796194291438,
        "f1": 0.014297751210150079,
        "hf_subset": "pol_Latn-rus_Cyrl",
        "languages": [
          "pol-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.014297751210150079,
        "precision": 0.013564913424916959,
        "recall": 0.020530796194291438
      },
      {
        "accuracy": 0.017526289434151226,
        "f1": 0.013288541757454805,
        "hf_subset": "por_Latn-rus_Cyrl",
        "languages": [
          "por-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.013288541757454805,
        "precision": 0.012611288298376742,
        "recall": 0.017526289434151226
      },
      {
        "accuracy": 0.0025037556334501754,
        "f1": 0.0004583601189106063,
        "hf_subset": "rus_Cyrl-arb_Arab",
        "languages": [
          "rus-Cyrl",
          "arb-Arab"
        ],
        "main_score": 0.0004583601189106063,
        "precision": 0.00028115410230328334,
        "recall": 0.0025037556334501754
      },
      {
        "accuracy": 0.09764646970455683,
        "f1": 0.06197993023208669,
        "hf_subset": "rus_Cyrl-bel_Cyrl",
        "languages": [
          "rus-Cyrl",
          "bel-Cyrl"
        ],
        "main_score": 0.06197993023208669,
        "precision": 0.055891202976346255,
        "recall": 0.09764646970455683
      },
      {
        "accuracy": 0.006009013520280421,
        "f1": 0.001674711785471812,
        "hf_subset": "rus_Cyrl-ben_Beng",
        "languages": [
          "rus-Cyrl",
          "ben-Beng"
        ],
        "main_score": 0.001674711785471812,
        "precision": 0.0012522897894965788,
        "recall": 0.006009013520280421
      },
      {
        "accuracy": 0.03254882323485228,
        "f1": 0.018127355167448934,
        "hf_subset": "rus_Cyrl-bos_Latn",
        "languages": [
          "rus-Cyrl",
          "bos-Latn"
        ],
        "main_score": 0.018127355167448934,
        "precision": 0.015922105302970484,
        "recall": 0.03254882323485228
      },
      {
        "accuracy": 0.12468703054581873,
        "f1": 0.08552625947540722,
        "hf_subset": "rus_Cyrl-bul_Cyrl",
        "languages": [
          "rus-Cyrl",
          "bul-Cyrl"
        ],
        "main_score": 0.08552625947540722,
        "precision": 0.07801180977332109,
        "recall": 0.12468703054581873
      },
      {
        "accuracy": 0.030545818728092138,
        "f1": 0.015137091841386379,
        "hf_subset": "rus_Cyrl-ces_Latn",
        "languages": [
          "rus-Cyrl",
          "ces-Latn"
        ],
        "main_score": 0.015137091841386379,
        "precision": 0.012948847097653256,
        "recall": 0.030545818728092138
      },
      {
        "accuracy": 0.06559839759639459,
        "f1": 0.038377370584086655,
        "hf_subset": "rus_Cyrl-deu_Latn",
        "languages": [
          "rus-Cyrl",
          "deu-Latn"
        ],
        "main_score": 0.038377370584086655,
        "precision": 0.033958555237956274,
        "recall": 0.06559839759639459
      },
      {
        "accuracy": 0.0035052578868302454,
        "f1": 0.0006312927601513907,
        "hf_subset": "rus_Cyrl-ell_Grek",
        "languages": [
          "rus-Cyrl",
          "ell-Grek"
        ],
        "main_score": 0.0006312927601513907,
        "precision": 0.00041908837517240196,
        "recall": 0.0035052578868302454
      },
      {
        "accuracy": 0.20480721081622433,
        "f1": 0.15144682261906675,
        "hf_subset": "rus_Cyrl-eng_Latn",
        "languages": [
          "rus-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.15144682261906675,
        "precision": 0.1383832354178553,
        "recall": 0.20480721081622433
      },
      {
        "accuracy": 0.007511266900350526,
        "f1": 0.000790633840916639,
        "hf_subset": "rus_Cyrl-fas_Arab",
        "languages": [
          "rus-Cyrl",
          "fas-Arab"
        ],
        "main_score": 0.000790633840916639,
        "precision": 0.0005084336735836448,
        "recall": 0.007511266900350526
      },
      {
        "accuracy": 0.019529293940911366,
        "f1": 0.008970950644782148,
        "hf_subset": "rus_Cyrl-fin_Latn",
        "languages": [
          "rus-Cyrl",
          "fin-Latn"
        ],
        "main_score": 0.008970950644782148,
        "precision": 0.00763799624862786,
        "recall": 0.019529293940911366
      },
      {
        "accuracy": 0.06860290435653481,
        "f1": 0.04251012184046304,
        "hf_subset": "rus_Cyrl-fra_Latn",
        "languages": [
          "rus-Cyrl",
          "fra-Latn"
        ],
        "main_score": 0.04251012184046304,
        "precision": 0.03786937408601485,
        "recall": 0.06860290435653481
      },
      {
        "accuracy": 0.0030045067601402104,
        "f1": 0.001614124013107103,
        "hf_subset": "rus_Cyrl-heb_Hebr",
        "languages": [
          "rus-Cyrl",
          "heb-Hebr"
        ],
        "main_score": 0.001614124013107103,
        "precision": 0.00139822670654887,
        "recall": 0.0030045067601402104
      },
      {
        "accuracy": 0.007511266900350526,
        "f1": 0.0030456923326537366,
        "hf_subset": "rus_Cyrl-hin_Deva",
        "languages": [
          "rus-Cyrl",
          "hin-Deva"
        ],
        "main_score": 0.0030456923326537366,
        "precision": 0.0025781302931222197,
        "recall": 0.007511266900350526
      },
      {
        "accuracy": 0.03104656985478217,
        "f1": 0.017558000135584402,
        "hf_subset": "rus_Cyrl-hrv_Latn",
        "languages": [
          "rus-Cyrl",
          "hrv-Latn"
        ],
        "main_score": 0.017558000135584402,
        "precision": 0.015667921596849557,
        "recall": 0.03104656985478217
      },
      {
        "accuracy": 0.013520280420630946,
        "f1": 0.005677155118217126,
        "hf_subset": "rus_Cyrl-hun_Latn",
        "languages": [
          "rus-Cyrl",
          "hun-Latn"
        ],
        "main_score": 0.005677155118217126,
        "precision": 0.004617691594741639,
        "recall": 0.013520280420630946
      },
      {
        "accuracy": 0.028042063094641963,
        "f1": 0.012952678595739876,
        "hf_subset": "rus_Cyrl-ind_Latn",
        "languages": [
          "rus-Cyrl",
          "ind-Latn"
        ],
        "main_score": 0.012952678595739876,
        "precision": 0.010924469915959818,
        "recall": 0.028042063094641963
      },
      {
        "accuracy": 0.014021031547320982,
        "f1": 0.007689756343228782,
        "hf_subset": "rus_Cyrl-jpn_Jpan",
        "languages": [
          "rus-Cyrl",
          "jpn-Jpan"
        ],
        "main_score": 0.007689756343228782,
        "precision": 0.006759739151543706,
        "recall": 0.014021031547320982
      },
      {
        "accuracy": 0.012518778167250876,
        "f1": 0.004750408185152917,
        "hf_subset": "rus_Cyrl-kor_Hang",
        "languages": [
          "rus-Cyrl",
          "kor-Hang"
        ],
        "main_score": 0.004750408185152917,
        "precision": 0.004062209988165018,
        "recall": 0.012518778167250876
      },
      {
        "accuracy": 0.022033049574361543,
        "f1": 0.010672800419042765,
        "hf_subset": "rus_Cyrl-lit_Latn",
        "languages": [
          "rus-Cyrl",
          "lit-Latn"
        ],
        "main_score": 0.010672800419042765,
        "precision": 0.009126261007727494,
        "recall": 0.022033049574361543
      },
      {
        "accuracy": 0.10515773660490736,
        "f1": 0.06664504043255452,
        "hf_subset": "rus_Cyrl-mkd_Cyrl",
        "languages": [
          "rus-Cyrl",
          "mkd-Cyrl"
        ],
        "main_score": 0.06664504043255452,
        "precision": 0.0593908567906567,
        "recall": 0.10515773660490736
      },
      {
        "accuracy": 0.04356534802203305,
        "f1": 0.021065537955301136,
        "hf_subset": "rus_Cyrl-nld_Latn",
        "languages": [
          "rus-Cyrl",
          "nld-Latn"
        ],
        "main_score": 0.021065537955301136,
        "precision": 0.018140774535792048,
        "recall": 0.04356534802203305
      },
      {
        "accuracy": 0.04807210816224337,
        "f1": 0.02411192248723054,
        "hf_subset": "rus_Cyrl-pol_Latn",
        "languages": [
          "rus-Cyrl",
          "pol-Latn"
        ],
        "main_score": 0.02411192248723054,
        "precision": 0.020682617158532222,
        "recall": 0.04807210816224337
      },
      {
        "accuracy": 0.06860290435653481,
        "f1": 0.03902119334061301,
        "hf_subset": "rus_Cyrl-por_Latn",
        "languages": [
          "rus-Cyrl",
          "por-Latn"
        ],
        "main_score": 0.03902119334061301,
        "precision": 0.03359134921393432,
        "recall": 0.06860290435653481
      },
      {
        "accuracy": 0.03254882323485228,
        "f1": 0.016291063337283435,
        "hf_subset": "rus_Cyrl-slk_Latn",
        "languages": [
          "rus-Cyrl",
          "slk-Latn"
        ],
        "main_score": 0.016291063337283435,
        "precision": 0.0136751740250059,
        "recall": 0.03254882323485228
      },
      {
        "accuracy": 0.03755633450175263,
        "f1": 0.018515087478817638,
        "hf_subset": "rus_Cyrl-slv_Latn",
        "languages": [
          "rus-Cyrl",
          "slv-Latn"
        ],
        "main_score": 0.018515087478817638,
        "precision": 0.015854619896175377,
        "recall": 0.03755633450175263
      },
      {
        "accuracy": 0.07961942914371557,
        "f1": 0.0467563322898654,
        "hf_subset": "rus_Cyrl-spa_Latn",
        "languages": [
          "rus-Cyrl",
          "spa-Latn"
        ],
        "main_score": 0.0467563322898654,
        "precision": 0.04160237368536061,
        "recall": 0.07961942914371557
      },
      {
        "accuracy": 0.09263895843765649,
        "f1": 0.05881200201662939,
        "hf_subset": "rus_Cyrl-srp_Cyrl",
        "languages": [
          "rus-Cyrl",
          "srp-Cyrl"
        ],
        "main_score": 0.05881200201662939,
        "precision": 0.05226311725830747,
        "recall": 0.09263895843765649
      },
      {
        "accuracy": 0.033049574361542315,
        "f1": 0.017361517873991733,
        "hf_subset": "rus_Cyrl-srp_Latn",
        "languages": [
          "rus-Cyrl",
          "srp-Latn"
        ],
        "main_score": 0.017361517873991733,
        "precision": 0.014271054379673485,
        "recall": 0.033049574361542315
      },
      {
        "accuracy": 0.01702553830746119,
        "f1": 0.005760900630751055,
        "hf_subset": "rus_Cyrl-swa_Latn",
        "languages": [
          "rus-Cyrl",
          "swa-Latn"
        ],
        "main_score": 0.005760900630751055,
        "precision": 0.004292664589572274,
        "recall": 0.01702553830746119
      },
      {
        "accuracy": 0.04256384576865298,
        "f1": 0.019669956763798322,
        "hf_subset": "rus_Cyrl-swe_Latn",
        "languages": [
          "rus-Cyrl",
          "swe-Latn"
        ],
        "main_score": 0.019669956763798322,
        "precision": 0.016333915318074266,
        "recall": 0.04256384576865298
      },
      {
        "accuracy": 0.00801201802704056,
        "f1": 0.002715418275394204,
        "hf_subset": "rus_Cyrl-tam_Taml",
        "languages": [
          "rus-Cyrl",
          "tam-Taml"
        ],
        "main_score": 0.002715418275394204,
        "precision": 0.001984447189757131,
        "recall": 0.00801201802704056
      },
      {
        "accuracy": 0.015523284927391086,
        "f1": 0.006942554140392233,
        "hf_subset": "rus_Cyrl-tur_Latn",
        "languages": [
          "rus-Cyrl",
          "tur-Latn"
        ],
        "main_score": 0.006942554140392233,
        "precision": 0.005770637329583482,
        "recall": 0.015523284927391086
      },
      {
        "accuracy": 0.6670005007511267,
        "f1": 0.6100758872679912,
        "hf_subset": "rus_Cyrl-ukr_Cyrl",
        "languages": [
          "rus-Cyrl",
          "ukr-Cyrl"
        ],
        "main_score": 0.6100758872679912,
        "precision": 0.5892828006996479,
        "recall": 0.6670005007511267
      },
      {
        "accuracy": 0.01602403605408112,
        "f1": 0.006069343771537281,
        "hf_subset": "rus_Cyrl-vie_Latn",
        "languages": [
          "rus-Cyrl",
          "vie-Latn"
        ],
        "main_score": 0.006069343771537281,
        "precision": 0.004777963443361732,
        "recall": 0.01602403605408112
      },
      {
        "accuracy": 0.02904356534802203,
        "f1": 0.017366220247813365,
        "hf_subset": "rus_Cyrl-zho_Hant",
        "languages": [
          "rus-Cyrl",
          "zho-Hant"
        ],
        "main_score": 0.017366220247813365,
        "precision": 0.015962559494034773,
        "recall": 0.02904356534802203
      },
      {
        "accuracy": 0.024536805207811718,
        "f1": 0.009971414971948343,
        "hf_subset": "rus_Cyrl-zul_Latn",
        "languages": [
          "rus-Cyrl",
          "zul-Latn"
        ],
        "main_score": 0.009971414971948343,
        "precision": 0.008218983347308048,
        "recall": 0.024536805207811718
      },
      {
        "accuracy": 0.014521782674011016,
        "f1": 0.010130217047293827,
        "hf_subset": "slk_Latn-rus_Cyrl",
        "languages": [
          "slk-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.010130217047293827,
        "precision": 0.009538606089489086,
        "recall": 0.014521782674011016
      },
      {
        "accuracy": 0.010515773660490736,
        "f1": 0.0071950661500354,
        "hf_subset": "slv_Latn-rus_Cyrl",
        "languages": [
          "slv-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0071950661500354,
        "precision": 0.006783279086298201,
        "recall": 0.010515773660490736
      },
      {
        "accuracy": 0.03204807210816224,
        "f1": 0.02406273673725775,
        "hf_subset": "spa_Latn-rus_Cyrl",
        "languages": [
          "spa-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.02406273673725775,
        "precision": 0.022742765919138173,
        "recall": 0.03204807210816224
      },
      {
        "accuracy": 0.05057586379569354,
        "f1": 0.040213285065542025,
        "hf_subset": "srp_Cyrl-rus_Cyrl",
        "languages": [
          "srp-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.040213285065542025,
        "precision": 0.03802987026202382,
        "recall": 0.05057586379569354
      },
      {
        "accuracy": 0.009013520280420632,
        "f1": 0.0061028803449088296,
        "hf_subset": "srp_Latn-rus_Cyrl",
        "languages": [
          "srp-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0061028803449088296,
        "precision": 0.005715412332160305,
        "recall": 0.009013520280420632
      },
      {
        "accuracy": 0.006009013520280421,
        "f1": 0.0028740144977895243,
        "hf_subset": "swa_Latn-rus_Cyrl",
        "languages": [
          "swa-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0028740144977895243,
        "precision": 0.00261556721788822,
        "recall": 0.006009013520280421
      },
      {
        "accuracy": 0.017526289434151226,
        "f1": 0.012552223308764406,
        "hf_subset": "swe_Latn-rus_Cyrl",
        "languages": [
          "swe-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.012552223308764406,
        "precision": 0.012020548555109803,
        "recall": 0.017526289434151226
      },
      {
        "accuracy": 0.007010515773660491,
        "f1": 0.004055244524020478,
        "hf_subset": "tam_Taml-rus_Cyrl",
        "languages": [
          "tam-Taml",
          "rus-Cyrl"
        ],
        "main_score": 0.004055244524020478,
        "precision": 0.0036972003498888613,
        "recall": 0.007010515773660491
      },
      {
        "accuracy": 0.007511266900350526,
        "f1": 0.0035710652405427355,
        "hf_subset": "tur_Latn-rus_Cyrl",
        "languages": [
          "tur-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0035710652405427355,
        "precision": 0.003104888064372448,
        "recall": 0.007511266900350526
      },
      {
        "accuracy": 0.5102653980971458,
        "f1": 0.45279894925361963,
        "hf_subset": "ukr_Cyrl-rus_Cyrl",
        "languages": [
          "ukr-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.45279894925361963,
        "precision": 0.43538432645316655,
        "recall": 0.5102653980971458
      },
      {
        "accuracy": 0.007511266900350526,
        "f1": 0.004306368005167955,
        "hf_subset": "vie_Latn-rus_Cyrl",
        "languages": [
          "vie-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.004306368005167955,
        "precision": 0.004028989522914878,
        "recall": 0.007511266900350526
      },
      {
        "accuracy": 0.02553830746119179,
        "f1": 0.01568806981653969,
        "hf_subset": "zho_Hant-rus_Cyrl",
        "languages": [
          "zho-Hant",
          "rus-Cyrl"
        ],
        "main_score": 0.01568806981653969,
        "precision": 0.014466219176003332,
        "recall": 0.02553830746119179
      },
      {
        "accuracy": 0.010515773660490736,
        "f1": 0.006251684272682039,
        "hf_subset": "zul_Latn-rus_Cyrl",
        "languages": [
          "zul-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.006251684272682039,
        "precision": 0.0058440307103778686,
        "recall": 0.010515773660490736
      }
    ]
  },
  "task_name": "NTREXBitextMining"
}