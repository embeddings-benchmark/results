{
    "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
    "task_name": "TwitterSemEval2015",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "default",
                "languages": [
                    "eng-Latn"
                ],
                "cos_sim_accuracy": 0.8568874053764081,
                "cos_sim_ap": 0.7326334732095694,
                "cos_sim_f1": 0.6801558376272464,
                "cos_sim_precision": 0.6493880489560834,
                "cos_sim_recall": 0.7139841688654355,
                "dot_accuracy": 0.8471121177802945,
                "dot_ap": 0.7033606362522604,
                "dot_f1": 0.6508875739644969,
                "dot_precision": 0.6350401606425703,
                "dot_recall": 0.6675461741424802,
                "euclidean_accuracy": 0.8580795136198367,
                "euclidean_ap": 0.7343201285001163,
                "euclidean_f1": 0.6833166833166834,
                "euclidean_precision": 0.6486486486486487,
                "euclidean_recall": 0.7218997361477573,
                "manhattan_accuracy": 0.8562317458425226,
                "manhattan_ap": 0.7321212085536185,
                "manhattan_f1": 0.6801681314482232,
                "manhattan_precision": 0.6574735286875153,
                "manhattan_recall": 0.7044854881266491,
                "max_accuracy": 0.8580795136198367,
                "max_ap": 0.7343201285001163,
                "max_f1": 0.6833166833166834,
                "main_score": 0.7343201285001163
            }
        ]
    }
}