{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.264014,
        "f1": 0.257573,
        "f1_weighted": 0.257554,
        "scores_per_experiment": [
          {
            "accuracy": 0.257324,
            "f1": 0.248021,
            "f1_weighted": 0.247981
          },
          {
            "accuracy": 0.268066,
            "f1": 0.25039,
            "f1_weighted": 0.250334
          },
          {
            "accuracy": 0.26123,
            "f1": 0.257728,
            "f1_weighted": 0.257728
          },
          {
            "accuracy": 0.281738,
            "f1": 0.276166,
            "f1_weighted": 0.27615
          },
          {
            "accuracy": 0.246094,
            "f1": 0.246005,
            "f1_weighted": 0.245988
          },
          {
            "accuracy": 0.259766,
            "f1": 0.248966,
            "f1_weighted": 0.248981
          },
          {
            "accuracy": 0.250977,
            "f1": 0.244551,
            "f1_weighted": 0.244515
          },
          {
            "accuracy": 0.283203,
            "f1": 0.27973,
            "f1_weighted": 0.279724
          },
          {
            "accuracy": 0.262695,
            "f1": 0.263481,
            "f1_weighted": 0.263445
          },
          {
            "accuracy": 0.269043,
            "f1": 0.260688,
            "f1_weighted": 0.260691
          }
        ],
        "main_score": 0.264014,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.269434,
        "f1": 0.262977,
        "f1_weighted": 0.262959,
        "scores_per_experiment": [
          {
            "accuracy": 0.278809,
            "f1": 0.269154,
            "f1_weighted": 0.269106
          },
          {
            "accuracy": 0.271484,
            "f1": 0.254719,
            "f1_weighted": 0.254673
          },
          {
            "accuracy": 0.247559,
            "f1": 0.244803,
            "f1_weighted": 0.244802
          },
          {
            "accuracy": 0.259766,
            "f1": 0.257816,
            "f1_weighted": 0.257788
          },
          {
            "accuracy": 0.260742,
            "f1": 0.259507,
            "f1_weighted": 0.259462
          },
          {
            "accuracy": 0.262695,
            "f1": 0.246371,
            "f1_weighted": 0.246398
          },
          {
            "accuracy": 0.278809,
            "f1": 0.273735,
            "f1_weighted": 0.273701
          },
          {
            "accuracy": 0.288086,
            "f1": 0.284968,
            "f1_weighted": 0.28498
          },
          {
            "accuracy": 0.27832,
            "f1": 0.278478,
            "f1_weighted": 0.278454
          },
          {
            "accuracy": 0.268066,
            "f1": 0.260222,
            "f1_weighted": 0.260225
          }
        ],
        "main_score": 0.269434,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 13.047415494918823,
  "kg_co2_emissions": 0.0002949015678161362
}