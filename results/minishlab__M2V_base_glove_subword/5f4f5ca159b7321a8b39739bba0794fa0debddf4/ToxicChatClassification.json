{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.483247,
        "f1": 0.434159,
        "f1_weighted": 0.543079,
        "ap": 0.165803,
        "ap_weighted": 0.165803,
        "scores_per_experiment": [
          {
            "accuracy": 0.301546,
            "f1": 0.297977,
            "f1_weighted": 0.335478,
            "ap": 0.141845,
            "ap_weighted": 0.141845
          },
          {
            "accuracy": 0.56701,
            "f1": 0.493284,
            "f1_weighted": 0.63808,
            "ap": 0.171828,
            "ap_weighted": 0.171828
          },
          {
            "accuracy": 0.27921,
            "f1": 0.278131,
            "f1_weighted": 0.299037,
            "ap": 0.143126,
            "ap_weighted": 0.143126
          },
          {
            "accuracy": 0.377148,
            "f1": 0.362405,
            "f1_weighted": 0.435037,
            "ap": 0.15265,
            "ap_weighted": 0.15265
          },
          {
            "accuracy": 0.590206,
            "f1": 0.5075,
            "f1_weighted": 0.658694,
            "ap": 0.174805,
            "ap_weighted": 0.174805
          },
          {
            "accuracy": 0.463058,
            "f1": 0.427785,
            "f1_weighted": 0.534216,
            "ap": 0.164846,
            "ap_weighted": 0.164846
          },
          {
            "accuracy": 0.520619,
            "f1": 0.466737,
            "f1_weighted": 0.593722,
            "ap": 0.171231,
            "ap_weighted": 0.171231
          },
          {
            "accuracy": 0.59622,
            "f1": 0.518361,
            "f1_weighted": 0.663431,
            "ap": 0.187367,
            "ap_weighted": 0.187367
          },
          {
            "accuracy": 0.626289,
            "f1": 0.527142,
            "f1_weighted": 0.689348,
            "ap": 0.176868,
            "ap_weighted": 0.176868
          },
          {
            "accuracy": 0.511168,
            "f1": 0.462273,
            "f1_weighted": 0.583746,
            "ap": 0.17346,
            "ap_weighted": 0.17346
          }
        ],
        "main_score": 0.483247,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 7.260880470275879,
  "kg_co2_emissions": 0.00016355515709702033
}