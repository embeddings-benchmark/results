{
  "dataset_revision": "59d12749a3c91a186063c7d729ec392fda94681c",
  "task_name": "DKHateClassification",
  "mteb_version": "1.34.21",
  "scores": {
    "test": [
      {
        "accuracy": 0.473252,
        "f1": 0.409509,
        "f1_weighted": 0.545491,
        "ap": 0.884028,
        "ap_weighted": 0.884028,
        "scores_per_experiment": [
          {
            "accuracy": 0.559271,
            "f1": 0.477726,
            "f1_weighted": 0.63266,
            "ap": 0.898684,
            "ap_weighted": 0.898684
          },
          {
            "accuracy": 0.43465,
            "f1": 0.396223,
            "f1_weighted": 0.510579,
            "ap": 0.88947,
            "ap_weighted": 0.88947
          },
          {
            "accuracy": 0.471125,
            "f1": 0.418468,
            "f1_weighted": 0.549843,
            "ap": 0.889387,
            "ap_weighted": 0.889387
          },
          {
            "accuracy": 0.386018,
            "f1": 0.354225,
            "f1_weighted": 0.461799,
            "ap": 0.876009,
            "ap_weighted": 0.876009
          },
          {
            "accuracy": 0.425532,
            "f1": 0.391319,
            "f1_weighted": 0.499659,
            "ap": 0.890741,
            "ap_weighted": 0.890741
          },
          {
            "accuracy": 0.3769,
            "f1": 0.345556,
            "f1_weighted": 0.453081,
            "ap": 0.872616,
            "ap_weighted": 0.872616
          },
          {
            "accuracy": 0.568389,
            "f1": 0.441119,
            "f1_weighted": 0.641347,
            "ap": 0.871383,
            "ap_weighted": 0.871383
          },
          {
            "accuracy": 0.510638,
            "f1": 0.435545,
            "f1_weighted": 0.590112,
            "ap": 0.884895,
            "ap_weighted": 0.884895
          },
          {
            "accuracy": 0.337386,
            "f1": 0.321072,
            "f1_weighted": 0.400085,
            "ap": 0.876802,
            "ap_weighted": 0.876802
          },
          {
            "accuracy": 0.662614,
            "f1": 0.513838,
            "f1_weighted": 0.715748,
            "ap": 0.890297,
            "ap_weighted": 0.890297
          }
        ],
        "main_score": 0.473252,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 8.467123985290527,
  "kg_co2_emissions": null
}