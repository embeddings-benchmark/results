{
  "dataset_revision": "bd27d0058bea2ad52470d9072a3b5da6b97c1ac3",
  "task_name": "VaccinChatNLClassification",
  "mteb_version": "2.1.17",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.365812,
            "f1": 0.392213,
            "f1_weighted": 0.35189,
            "precision": 0.371487,
            "precision_weighted": 0.45287,
            "recall": 0.529223,
            "recall_weighted": 0.365812,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.37094,
            "f1": 0.394438,
            "f1_weighted": 0.364594,
            "precision": 0.381493,
            "precision_weighted": 0.464688,
            "recall": 0.528167,
            "recall_weighted": 0.37094,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.381197,
            "f1": 0.408177,
            "f1_weighted": 0.378343,
            "precision": 0.395188,
            "precision_weighted": 0.47612,
            "recall": 0.525855,
            "recall_weighted": 0.381197,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.383761,
            "f1": 0.405222,
            "f1_weighted": 0.382027,
            "precision": 0.401851,
            "precision_weighted": 0.503363,
            "recall": 0.519593,
            "recall_weighted": 0.383761,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.400855,
            "f1": 0.436574,
            "f1_weighted": 0.403517,
            "precision": 0.441968,
            "precision_weighted": 0.520297,
            "recall": 0.536776,
            "recall_weighted": 0.400855,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.4,
            "f1": 0.442053,
            "f1_weighted": 0.396969,
            "precision": 0.437092,
            "precision_weighted": 0.504185,
            "recall": 0.567856,
            "recall_weighted": 0.4,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.395726,
            "f1": 0.423376,
            "f1_weighted": 0.392011,
            "precision": 0.429219,
            "precision_weighted": 0.518625,
            "recall": 0.545629,
            "recall_weighted": 0.395726,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.382051,
            "f1": 0.384046,
            "f1_weighted": 0.3791,
            "precision": 0.382923,
            "precision_weighted": 0.499395,
            "recall": 0.51103,
            "recall_weighted": 0.382051,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.387179,
            "f1": 0.425735,
            "f1_weighted": 0.382945,
            "precision": 0.41879,
            "precision_weighted": 0.47414,
            "recall": 0.559882,
            "recall_weighted": 0.387179,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.379487,
            "f1": 0.401891,
            "f1_weighted": 0.375526,
            "precision": 0.391702,
            "precision_weighted": 0.477869,
            "recall": 0.508903,
            "recall_weighted": 0.379487,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.384701,
        "f1": 0.411372,
        "f1_weighted": 0.380692,
        "precision": 0.405171,
        "precision_weighted": 0.489155,
        "recall": 0.533291,
        "recall_weighted": 0.384701,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.411372,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 24.091561794281006,
  "kg_co2_emissions": null
}