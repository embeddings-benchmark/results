{
  "dataset_revision": "bf5e20ee2d3ce2e24e4de50f5dd8573e0e0e2fec",
  "task_name": "DutchGovernmentBiasClassification",
  "mteb_version": "2.1.0",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.617021,
            "f1": 0.583274,
            "f1_weighted": 0.63973,
            "precision": 0.595693,
            "precision_weighted": 0.704771,
            "recall": 0.622664,
            "recall_weighted": 0.617021,
            "ap": 0.328339,
            "ap_weighted": 0.328339
          },
          {
            "accuracy": 0.647606,
            "f1": 0.601821,
            "f1_weighted": 0.6661,
            "precision": 0.603732,
            "precision_weighted": 0.707068,
            "recall": 0.62865,
            "recall_weighted": 0.647606,
            "ap": 0.335395,
            "ap_weighted": 0.335395
          },
          {
            "accuracy": 0.659574,
            "f1": 0.612543,
            "f1_weighted": 0.676807,
            "precision": 0.612513,
            "precision_weighted": 0.714007,
            "recall": 0.638396,
            "recall_weighted": 0.659574,
            "ap": 0.343541,
            "ap_weighted": 0.343541
          },
          {
            "accuracy": 0.591755,
            "f1": 0.568381,
            "f1_weighted": 0.616198,
            "precision": 0.594298,
            "precision_weighted": 0.708094,
            "recall": 0.621919,
            "recall_weighted": 0.591755,
            "ap": 0.325901,
            "ap_weighted": 0.325901
          },
          {
            "accuracy": 0.609043,
            "f1": 0.580548,
            "f1_weighted": 0.632594,
            "precision": 0.598482,
            "precision_weighted": 0.70968,
            "recall": 0.627082,
            "recall_weighted": 0.609043,
            "ap": 0.330417,
            "ap_weighted": 0.330417
          },
          {
            "accuracy": 0.625,
            "f1": 0.606274,
            "f1_weighted": 0.647152,
            "precision": 0.63471,
            "precision_weighted": 0.750587,
            "recall": 0.67391,
            "recall_weighted": 0.625,
            "ap": 0.362417,
            "ap_weighted": 0.362417
          },
          {
            "accuracy": 0.667553,
            "f1": 0.623976,
            "f1_weighted": 0.684916,
            "precision": 0.624008,
            "precision_weighted": 0.725237,
            "recall": 0.653624,
            "recall_weighted": 0.667553,
            "ap": 0.35525,
            "ap_weighted": 0.35525
          },
          {
            "accuracy": 0.550532,
            "f1": 0.532421,
            "f1_weighted": 0.57623,
            "precision": 0.570637,
            "precision_weighted": 0.686728,
            "recall": 0.590717,
            "recall_weighted": 0.550532,
            "ap": 0.305725,
            "ap_weighted": 0.305725
          },
          {
            "accuracy": 0.648936,
            "f1": 0.594284,
            "f1_weighted": 0.665173,
            "precision": 0.59377,
            "precision_weighted": 0.695572,
            "recall": 0.61318,
            "recall_weighted": 0.648936,
            "ap": 0.325439,
            "ap_weighted": 0.325439
          },
          {
            "accuracy": 0.632979,
            "f1": 0.595103,
            "f1_weighted": 0.654058,
            "precision": 0.60258,
            "precision_weighted": 0.709274,
            "recall": 0.630201,
            "recall_weighted": 0.632979,
            "ap": 0.334703,
            "ap_weighted": 0.334703
          }
        ],
        "accuracy": 0.625,
        "f1": 0.589862,
        "f1_weighted": 0.645896,
        "precision": 0.603042,
        "precision_weighted": 0.711102,
        "recall": 0.630034,
        "recall_weighted": 0.625,
        "ap": 0.334713,
        "ap_weighted": 0.334713,
        "main_score": 0.589862,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 3.949551582336426,
  "kg_co2_emissions": null
}