{
  "dataset_revision": "12ca3b695563788fead87a982ad1a068284413f4",
  "evaluation_time": 1.0906572341918945,
  "kg_co2_emissions": 3.6933916693715985e-05,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.66259765625,
          "accuracy_threshold": 0.2515348494052887,
          "ap": 0.7635748677793768,
          "f1": 0.7552495697074011,
          "f1_threshold": 0.15313732624053955,
          "precision": 0.6514251781472684,
          "recall": 0.8984438984438985
        },
        "dot": {
          "accuracy": 0.646484375,
          "accuracy_threshold": 3.4299144744873047,
          "ap": 0.7103531319618308,
          "f1": 0.7479975354282193,
          "f1_threshold": -0.42587903141975403,
          "precision": 0.5995061728395061,
          "recall": 0.9942669942669943
        },
        "euclidean": {
          "accuracy": 0.6875,
          "accuracy_threshold": 5.562836647033691,
          "ap": 0.7973348630071919,
          "f1": 0.7610993657505286,
          "f1_threshold": 5.771557807922363,
          "precision": 0.6679035250463822,
          "recall": 0.8845208845208845
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6875,
        "manhattan": {
          "accuracy": 0.68603515625,
          "accuracy_threshold": 87.10758972167969,
          "ap": 0.7946043588366379,
          "f1": 0.7617668356263577,
          "f1_threshold": 88.31982421875,
          "precision": 0.6826735885788449,
          "recall": 0.8615888615888616
        },
        "max": {
          "accuracy": 0.6875,
          "ap": 0.7973348630071919,
          "f1": 0.7617668356263577
        },
        "similarity": {
          "accuracy": 0.66259765625,
          "accuracy_threshold": 0.2515348196029663,
          "ap": 0.7635748677793768,
          "f1": 0.7552495697074011,
          "f1_threshold": 0.15313728153705597,
          "precision": 0.6514251781472684,
          "recall": 0.8984438984438985
        }
      }
    ]
  },
  "task_name": "LegalBenchPC"
}