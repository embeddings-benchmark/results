{
  "dataset_revision": "7a78eb7cc137fdd1c5826be1a9e9813177706509",
  "task_name": "DadoEvalCoarseClassification",
  "mteb_version": "1.38.32",
  "scores": {
    "test": [
      {
        "accuracy": 0.382149,
        "f1": 0.371001,
        "f1_weighted": 0.38424,
        "scores_per_experiment": [
          {
            "accuracy": 0.342441,
            "f1": 0.323702,
            "f1_weighted": 0.342273
          },
          {
            "accuracy": 0.411658,
            "f1": 0.401789,
            "f1_weighted": 0.4118
          },
          {
            "accuracy": 0.409836,
            "f1": 0.394581,
            "f1_weighted": 0.406793
          },
          {
            "accuracy": 0.404372,
            "f1": 0.396959,
            "f1_weighted": 0.391496
          },
          {
            "accuracy": 0.406193,
            "f1": 0.389573,
            "f1_weighted": 0.411584
          },
          {
            "accuracy": 0.384335,
            "f1": 0.37279,
            "f1_weighted": 0.392462
          },
          {
            "accuracy": 0.369763,
            "f1": 0.354925,
            "f1_weighted": 0.38364
          },
          {
            "accuracy": 0.347905,
            "f1": 0.337629,
            "f1_weighted": 0.351371
          },
          {
            "accuracy": 0.309654,
            "f1": 0.30194,
            "f1_weighted": 0.316652
          },
          {
            "accuracy": 0.435337,
            "f1": 0.43612,
            "f1_weighted": 0.434334
          }
        ],
        "main_score": 0.382149,
        "hf_subset": "default",
        "languages": [
          "ita-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 117.43243288993835,
  "kg_co2_emissions": null
}