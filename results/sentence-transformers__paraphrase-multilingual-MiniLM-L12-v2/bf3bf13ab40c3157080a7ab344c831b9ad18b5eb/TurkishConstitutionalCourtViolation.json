{
  "dataset_revision": "333f49b7ddc72fa4a86ec5bd756a28c585311c74",
  "task_name": "TurkishConstitutionalCourtViolation",
  "mteb_version": "2.4.2",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.663212,
            "f1": 0.501173,
            "f1_weighted": 0.73834,
            "precision": 0.539386,
            "precision_weighted": 0.877285,
            "recall": 0.617408,
            "recall_weighted": 0.663212,
            "ap": 0.935484,
            "ap_weighted": 0.935484
          },
          {
            "accuracy": 0.637306,
            "f1": 0.500518,
            "f1_weighted": 0.718566,
            "precision": 0.551001,
            "precision_weighted": 0.889905,
            "recall": 0.660134,
            "recall_weighted": 0.637306,
            "ap": 0.942515,
            "ap_weighted": 0.942515
          },
          {
            "accuracy": 0.647668,
            "f1": 0.499542,
            "f1_weighted": 0.726669,
            "precision": 0.544651,
            "precision_weighted": 0.883137,
            "recall": 0.637359,
            "recall_weighted": 0.647668,
            "ap": 0.938744,
            "ap_weighted": 0.938744
          },
          {
            "accuracy": 0.590674,
            "f1": 0.470314,
            "f1_weighted": 0.680943,
            "precision": 0.541558,
            "precision_weighted": 0.885369,
            "recall": 0.63471,
            "recall_weighted": 0.590674,
            "ap": 0.9384,
            "ap_weighted": 0.9384
          },
          {
            "accuracy": 0.683938,
            "f1": 0.484748,
            "f1_weighted": 0.751994,
            "precision": 0.515867,
            "precision_weighted": 0.858391,
            "recall": 0.543432,
            "recall_weighted": 0.683938,
            "ap": 0.92377,
            "ap_weighted": 0.92377
          },
          {
            "accuracy": 0.787565,
            "f1": 0.503358,
            "f1_weighted": 0.816764,
            "precision": 0.508264,
            "precision_weighted": 0.851527,
            "recall": 0.514654,
            "recall_weighted": 0.787565,
            "ap": 0.919333,
            "ap_weighted": 0.919333
          },
          {
            "accuracy": 0.601036,
            "f1": 0.469875,
            "f1_weighted": 0.689844,
            "precision": 0.534943,
            "precision_weighted": 0.878203,
            "recall": 0.611935,
            "recall_weighted": 0.601036,
            "ap": 0.934663,
            "ap_weighted": 0.934663
          },
          {
            "accuracy": 0.642487,
            "f1": 0.4795,
            "f1_weighted": 0.722472,
            "precision": 0.525713,
            "precision_weighted": 0.867543,
            "recall": 0.577684,
            "recall_weighted": 0.642487,
            "ap": 0.929148,
            "ap_weighted": 0.929148
          },
          {
            "accuracy": 0.57513,
            "f1": 0.460379,
            "f1_weighted": 0.667961,
            "precision": 0.53869,
            "precision_weighted": 0.88379,
            "recall": 0.626236,
            "recall_weighted": 0.57513,
            "ap": 0.937032,
            "ap_weighted": 0.937032
          },
          {
            "accuracy": 0.419689,
            "f1": 0.340171,
            "f1_weighted": 0.531252,
            "precision": 0.477601,
            "precision_weighted": 0.82293,
            "recall": 0.42779,
            "recall_weighted": 0.419689,
            "ap": 0.906424,
            "ap_weighted": 0.906424
          }
        ],
        "accuracy": 0.62487,
        "f1": 0.470958,
        "f1_weighted": 0.70448,
        "precision": 0.527767,
        "precision_weighted": 0.869808,
        "recall": 0.585134,
        "recall_weighted": 0.62487,
        "ap": 0.930551,
        "ap_weighted": 0.930551,
        "main_score": 0.470958,
        "hf_subset": "default",
        "languages": [
          "tur-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 7.72290301322937,
  "kg_co2_emissions": null
}