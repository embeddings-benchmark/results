{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "task_name": "GreekLegalCodeClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.036621,
        "f1": 0.022367,
        "f1_weighted": 0.031185,
        "scores_per_experiment": [
          {
            "accuracy": 0.032715,
            "f1": 0.02265,
            "f1_weighted": 0.029768
          },
          {
            "accuracy": 0.038574,
            "f1": 0.02111,
            "f1_weighted": 0.029142
          },
          {
            "accuracy": 0.039062,
            "f1": 0.023578,
            "f1_weighted": 0.032385
          },
          {
            "accuracy": 0.036621,
            "f1": 0.022465,
            "f1_weighted": 0.031425
          },
          {
            "accuracy": 0.028809,
            "f1": 0.016731,
            "f1_weighted": 0.025065
          },
          {
            "accuracy": 0.039551,
            "f1": 0.026364,
            "f1_weighted": 0.035557
          },
          {
            "accuracy": 0.036133,
            "f1": 0.022472,
            "f1_weighted": 0.031623
          },
          {
            "accuracy": 0.036133,
            "f1": 0.020054,
            "f1_weighted": 0.030196
          },
          {
            "accuracy": 0.039062,
            "f1": 0.022157,
            "f1_weighted": 0.033862
          },
          {
            "accuracy": 0.039551,
            "f1": 0.026091,
            "f1_weighted": 0.032829
          }
        ],
        "main_score": 0.036621,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.037451,
        "f1": 0.024924,
        "f1_weighted": 0.031832,
        "scores_per_experiment": [
          {
            "accuracy": 0.039062,
            "f1": 0.0249,
            "f1_weighted": 0.033395
          },
          {
            "accuracy": 0.034668,
            "f1": 0.02403,
            "f1_weighted": 0.028321
          },
          {
            "accuracy": 0.032715,
            "f1": 0.021529,
            "f1_weighted": 0.026817
          },
          {
            "accuracy": 0.033691,
            "f1": 0.023177,
            "f1_weighted": 0.027848
          },
          {
            "accuracy": 0.036133,
            "f1": 0.02367,
            "f1_weighted": 0.031827
          },
          {
            "accuracy": 0.044434,
            "f1": 0.032232,
            "f1_weighted": 0.038461
          },
          {
            "accuracy": 0.037109,
            "f1": 0.02261,
            "f1_weighted": 0.033715
          },
          {
            "accuracy": 0.035156,
            "f1": 0.022166,
            "f1_weighted": 0.026868
          },
          {
            "accuracy": 0.038086,
            "f1": 0.025939,
            "f1_weighted": 0.034429
          },
          {
            "accuracy": 0.043457,
            "f1": 0.02899,
            "f1_weighted": 0.036641
          }
        ],
        "main_score": 0.037451,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ]
  },
  "evaluation_time": 1220.3842837810516,
  "kg_co2_emissions": 0.03527362128967659
}