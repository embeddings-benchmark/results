{
  "dataset_revision": "9e9b1f8ef51616073f47f306f7f47dd91663f86a",
  "evaluation_time": 1.1788182258605957,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test.full": [
      {
        "cosine_accuracy": 0.7009803921568627,
        "cosine_accuracy_threshold": 0.7335044741630554,
        "cosine_ap": 0.8434563432994876,
        "cosine_f1": 0.7906632085957762,
        "cosine_f1_threshold": 0.4878019094467163,
        "cosine_precision": 0.6541998773758431,
        "cosine_recall": 0.9990636704119851,
        "dot_accuracy": 0.7077205882352942,
        "dot_accuracy_threshold": 71.124267578125,
        "dot_ap": 0.8411885786282449,
        "dot_f1": 0.797678916827853,
        "dot_f1_threshold": 64.40082550048828,
        "dot_precision": 0.6796308503625577,
        "dot_recall": 0.9653558052434457,
        "euclidean_accuracy": 0.6905637254901961,
        "euclidean_accuracy_threshold": 6.97967529296875,
        "euclidean_ap": 0.8393571376213969,
        "euclidean_f1": 0.7906632085957762,
        "euclidean_f1_threshold": 10.772109985351562,
        "euclidean_precision": 0.6541998773758431,
        "euclidean_recall": 0.9990636704119851,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.8434563432994876,
        "manhattan_accuracy": 0.6924019607843137,
        "manhattan_accuracy_threshold": 150.1930694580078,
        "manhattan_ap": 0.8388704372131187,
        "manhattan_f1": 0.7906632085957762,
        "manhattan_f1_threshold": 235.34559631347656,
        "manhattan_precision": 0.6541998773758431,
        "manhattan_recall": 0.9990636704119851,
        "max_ap": 0.8434563432994876,
        "max_f1": 0.797678916827853,
        "max_precision": 0.6796308503625577,
        "max_recall": 0.9990636704119851,
        "similarity_accuracy": 0.7009803921568627,
        "similarity_accuracy_threshold": 0.7335044145584106,
        "similarity_ap": 0.8434563432994876,
        "similarity_f1": 0.7906632085957762,
        "similarity_f1_threshold": 0.4878019690513611,
        "similarity_precision": 0.6541998773758431,
        "similarity_recall": 0.9990636704119851
      }
    ],
    "validation.full": [
      {
        "cosine_accuracy": 0.72090112640801,
        "cosine_accuracy_threshold": 0.7429521083831787,
        "cosine_ap": 0.839435850215384,
        "cosine_f1": 0.7994604316546763,
        "cosine_f1_threshold": 0.7429521083831787,
        "cosine_precision": 0.7383720930232558,
        "cosine_recall": 0.8715686274509804,
        "dot_accuracy": 0.7315394242803505,
        "dot_accuracy_threshold": 71.17228698730469,
        "dot_ap": 0.8371638870127546,
        "dot_f1": 0.8113537117903931,
        "dot_f1_threshold": 69.93193054199219,
        "dot_precision": 0.731496062992126,
        "dot_recall": 0.9107843137254902,
        "euclidean_accuracy": 0.7090112640801002,
        "euclidean_accuracy_threshold": 6.762685775756836,
        "euclidean_ap": 0.8341752682978163,
        "euclidean_f1": 0.7926530612244898,
        "euclidean_f1_threshold": 7.802102088928223,
        "euclidean_precision": 0.679020979020979,
        "euclidean_recall": 0.9519607843137254,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.839435850215384,
        "manhattan_accuracy": 0.7071339173967459,
        "manhattan_accuracy_threshold": 148.07891845703125,
        "manhattan_ap": 0.8349793169077887,
        "manhattan_f1": 0.7913434054716211,
        "manhattan_f1_threshold": 170.1165771484375,
        "manhattan_precision": 0.6780965710286914,
        "manhattan_recall": 0.95,
        "max_ap": 0.839435850215384,
        "max_f1": 0.8113537117903931,
        "max_precision": 0.7383720930232558,
        "max_recall": 0.9519607843137254,
        "similarity_accuracy": 0.72090112640801,
        "similarity_accuracy_threshold": 0.7429521083831787,
        "similarity_ap": 0.839435850215384,
        "similarity_f1": 0.7994604316546763,
        "similarity_f1_threshold": 0.7429521083831787,
        "similarity_precision": 0.7383720930232558,
        "similarity_recall": 0.8715686274509804
      }
    ]
  },
  "task_name": "OpusparcusPC"
}