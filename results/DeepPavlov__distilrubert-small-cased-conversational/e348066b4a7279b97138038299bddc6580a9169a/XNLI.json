{
  "dataset_revision": "09698e0180d87dc247ca447d3a1248b931ac0cdb",
  "evaluation_time": 1.2722318172454834,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.6131868131868132,
        "cosine_accuracy_threshold": 0.7879770398139954,
        "cosine_ap": 0.6297999858389947,
        "cosine_f1": 0.678381256656017,
        "cosine_f1_threshold": 0.7091454267501831,
        "cosine_precision": 0.532608695652174,
        "cosine_recall": 0.9340175953079178,
        "dot_accuracy": 0.6197802197802198,
        "dot_accuracy_threshold": 72.55451965332031,
        "dot_ap": 0.6400409003090605,
        "dot_f1": 0.6766666666666666,
        "dot_f1_threshold": 67.39791107177734,
        "dot_precision": 0.5447227191413238,
        "dot_recall": 0.8929618768328446,
        "euclidean_accuracy": 0.6058608058608058,
        "euclidean_accuracy_threshold": 6.026691436767578,
        "euclidean_ap": 0.6225291562395908,
        "euclidean_f1": 0.6760259179265659,
        "euclidean_f1_threshold": 7.3418121337890625,
        "euclidean_precision": 0.535042735042735,
        "euclidean_recall": 0.9178885630498533,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6400409003090605,
        "manhattan_accuracy": 0.6051282051282051,
        "manhattan_accuracy_threshold": 140.2854766845703,
        "manhattan_ap": 0.6235301792763703,
        "manhattan_f1": 0.6760411032990806,
        "manhattan_f1_threshold": 158.29849243164062,
        "manhattan_precision": 0.5355612682090831,
        "manhattan_recall": 0.9164222873900293,
        "max_ap": 0.6400409003090605,
        "max_f1": 0.678381256656017,
        "max_precision": 0.5447227191413238,
        "max_recall": 0.9340175953079178,
        "similarity_accuracy": 0.6131868131868132,
        "similarity_accuracy_threshold": 0.7879770994186401,
        "similarity_ap": 0.6298010485006571,
        "similarity_f1": 0.678381256656017,
        "similarity_f1_threshold": 0.7091455459594727,
        "similarity_precision": 0.532608695652174,
        "similarity_recall": 0.9340175953079178
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.613919413919414,
        "cosine_accuracy_threshold": 0.7980071306228638,
        "cosine_ap": 0.6198909811652834,
        "cosine_f1": 0.6696696696696697,
        "cosine_f1_threshold": 0.6585626602172852,
        "cosine_precision": 0.5083586626139818,
        "cosine_recall": 0.9809384164222874,
        "dot_accuracy": 0.6278388278388278,
        "dot_accuracy_threshold": 73.26905822753906,
        "dot_ap": 0.639558666831316,
        "dot_f1": 0.6706766917293233,
        "dot_f1_threshold": 62.53742980957031,
        "dot_precision": 0.5095201827875095,
        "dot_recall": 0.9809384164222874,
        "euclidean_accuracy": 0.5992673992673992,
        "euclidean_accuracy_threshold": 6.059993743896484,
        "euclidean_ap": 0.6117126385225953,
        "euclidean_f1": 0.6697015680323724,
        "euclidean_f1_threshold": 7.927886962890625,
        "euclidean_precision": 0.5111969111969112,
        "euclidean_recall": 0.9706744868035191,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.639558666831316,
        "manhattan_accuracy": 0.6036630036630036,
        "manhattan_accuracy_threshold": 131.0306396484375,
        "manhattan_ap": 0.6141506733216832,
        "manhattan_f1": 0.6713924050632912,
        "manhattan_f1_threshold": 171.4368438720703,
        "manhattan_precision": 0.5127610208816705,
        "manhattan_recall": 0.9721407624633431,
        "max_ap": 0.639558666831316,
        "max_f1": 0.6713924050632912,
        "max_precision": 0.5127610208816705,
        "max_recall": 0.9809384164222874,
        "similarity_accuracy": 0.613919413919414,
        "similarity_accuracy_threshold": 0.7980071306228638,
        "similarity_ap": 0.6198909811652834,
        "similarity_f1": 0.6696696696696697,
        "similarity_f1_threshold": 0.6585626602172852,
        "similarity_precision": 0.5083586626139818,
        "similarity_recall": 0.9809384164222874
      }
    ]
  },
  "task_name": "XNLI"
}