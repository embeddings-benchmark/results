{
  "dataset_revision": "edfaf9da55d3dd50d43143d90c1ac476895ae6de",
  "task_name": "ToxicConversationsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.588867,
        "f1": 0.442173,
        "f1_weighted": 0.680125,
        "ap": 0.087344,
        "ap_weighted": 0.087344,
        "scores_per_experiment": [
          {
            "accuracy": 0.506348,
            "f1": 0.401556,
            "f1_weighted": 0.612362,
            "ap": 0.083747,
            "ap_weighted": 0.083747
          },
          {
            "accuracy": 0.560547,
            "f1": 0.439416,
            "f1_weighted": 0.658775,
            "ap": 0.094723,
            "ap_weighted": 0.094723
          },
          {
            "accuracy": 0.708008,
            "f1": 0.506852,
            "f1_weighted": 0.771984,
            "ap": 0.09827,
            "ap_weighted": 0.09827
          },
          {
            "accuracy": 0.702148,
            "f1": 0.490486,
            "f1_weighted": 0.76693,
            "ap": 0.088257,
            "ap_weighted": 0.088257
          },
          {
            "accuracy": 0.539551,
            "f1": 0.405321,
            "f1_weighted": 0.643154,
            "ap": 0.076326,
            "ap_weighted": 0.076326
          },
          {
            "accuracy": 0.504883,
            "f1": 0.404169,
            "f1_weighted": 0.610381,
            "ap": 0.086718,
            "ap_weighted": 0.086718
          },
          {
            "accuracy": 0.641602,
            "f1": 0.47231,
            "f1_weighted": 0.723912,
            "ap": 0.091915,
            "ap_weighted": 0.091915
          },
          {
            "accuracy": 0.612305,
            "f1": 0.450222,
            "f1_weighted": 0.701509,
            "ap": 0.084637,
            "ap_weighted": 0.084637
          },
          {
            "accuracy": 0.526855,
            "f1": 0.407563,
            "f1_weighted": 0.63135,
            "ap": 0.080893,
            "ap_weighted": 0.080893
          },
          {
            "accuracy": 0.586426,
            "f1": 0.443831,
            "f1_weighted": 0.680893,
            "ap": 0.087951,
            "ap_weighted": 0.087951
          }
        ],
        "main_score": 0.588867,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 8.056021928787231,
  "kg_co2_emissions": 0.0002440261025550929
}