{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 11.05540418624878,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test": [
      {
        "accuracy": 0.6808002689979824,
        "f1": 0.6772693731162688,
        "f1_weighted": 0.6819588062414492,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6808002689979824,
        "scores_per_experiment": [
          {
            "accuracy": 0.6866173503698723,
            "f1": 0.683345534012693,
            "f1_weighted": 0.6858444685201335
          },
          {
            "accuracy": 0.6879623402824478,
            "f1": 0.6856151245439093,
            "f1_weighted": 0.6878677974777963
          },
          {
            "accuracy": 0.6661062542030934,
            "f1": 0.672092244905034,
            "f1_weighted": 0.6674309852434503
          },
          {
            "accuracy": 0.7020847343644923,
            "f1": 0.6877164769196017,
            "f1_weighted": 0.700483607706224
          },
          {
            "accuracy": 0.6772024209818427,
            "f1": 0.6646968469282268,
            "f1_weighted": 0.6718825083822316
          },
          {
            "accuracy": 0.6755211835911231,
            "f1": 0.6673082556693475,
            "f1_weighted": 0.6732863946669412
          },
          {
            "accuracy": 0.6842636180228648,
            "f1": 0.6785371582416898,
            "f1_weighted": 0.6918318532507362
          },
          {
            "accuracy": 0.6714862138533961,
            "f1": 0.6733345828938707,
            "f1_weighted": 0.6755208896092845
          },
          {
            "accuracy": 0.6819098856758574,
            "f1": 0.6767996035075746,
            "f1_weighted": 0.6826645588836879
          },
          {
            "accuracy": 0.6748486886348353,
            "f1": 0.6832479035407406,
            "f1_weighted": 0.6827749986740069
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6867191342843089,
        "f1": 0.6814415535770937,
        "f1_weighted": 0.6877940969099662,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6867191342843089,
        "scores_per_experiment": [
          {
            "accuracy": 0.70486965076242,
            "f1": 0.6965574484005275,
            "f1_weighted": 0.7024851385041593
          },
          {
            "accuracy": 0.6994589276930644,
            "f1": 0.6944053118042278,
            "f1_weighted": 0.7011295388524521
          },
          {
            "accuracy": 0.6650270536153468,
            "f1": 0.6674942104641644,
            "f1_weighted": 0.6666579740506552
          },
          {
            "accuracy": 0.7033939990162321,
            "f1": 0.6936497938676524,
            "f1_weighted": 0.6995308950999635
          },
          {
            "accuracy": 0.6842105263157895,
            "f1": 0.6734247892221293,
            "f1_weighted": 0.6787431748022715
          },
          {
            "accuracy": 0.676340383669454,
            "f1": 0.6697874017513084,
            "f1_weighted": 0.6758832457413653
          },
          {
            "accuracy": 0.6827348745696016,
            "f1": 0.6775819480389761,
            "f1_weighted": 0.690123277823103
          },
          {
            "accuracy": 0.675848499754058,
            "f1": 0.6748558426354876,
            "f1_weighted": 0.680330832814238
          },
          {
            "accuracy": 0.6896212493851451,
            "f1": 0.6884169715195532,
            "f1_weighted": 0.6902914960809532
          },
          {
            "accuracy": 0.6856861780619774,
            "f1": 0.6782418180669103,
            "f1_weighted": 0.6927653953304997
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}