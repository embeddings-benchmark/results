{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 18.515499591827393,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test": [
      {
        "accuracy": 0.6311701412239409,
        "f1": 0.5977048393155548,
        "f1_weighted": 0.6345083115681863,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6311701412239409,
        "scores_per_experiment": [
          {
            "accuracy": 0.6163416274377942,
            "f1": 0.5930752594286465,
            "f1_weighted": 0.6161208647371887
          },
          {
            "accuracy": 0.640551445864156,
            "f1": 0.6032506885462801,
            "f1_weighted": 0.6435668087063147
          },
          {
            "accuracy": 0.6237390719569603,
            "f1": 0.5907977581111594,
            "f1_weighted": 0.6258581801330929
          },
          {
            "accuracy": 0.66408876933423,
            "f1": 0.6144066377308172,
            "f1_weighted": 0.6696522052774034
          },
          {
            "accuracy": 0.6422326832548756,
            "f1": 0.6071002231275815,
            "f1_weighted": 0.6408949459805116
          },
          {
            "accuracy": 0.6193678547410895,
            "f1": 0.5816873027323197,
            "f1_weighted": 0.6238983112843589
          },
          {
            "accuracy": 0.6338264963012777,
            "f1": 0.603807631916221,
            "f1_weighted": 0.643555894343549
          },
          {
            "accuracy": 0.6176866173503699,
            "f1": 0.5938087119237124,
            "f1_weighted": 0.6204707535770267
          },
          {
            "accuracy": 0.6139878950907868,
            "f1": 0.5777953977863776,
            "f1_weighted": 0.6184475399466195
          },
          {
            "accuracy": 0.6398789509078682,
            "f1": 0.6113187818524338,
            "f1_weighted": 0.6426176116957978
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.631824889326119,
        "f1": 0.593092177557789,
        "f1_weighted": 0.6363514781221083,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.631824889326119,
        "scores_per_experiment": [
          {
            "accuracy": 0.6251844564682735,
            "f1": 0.6063067409856276,
            "f1_weighted": 0.6279601516697257
          },
          {
            "accuracy": 0.6443679291687162,
            "f1": 0.6002648096324668,
            "f1_weighted": 0.6504576411323124
          },
          {
            "accuracy": 0.6369896704377767,
            "f1": 0.599932875281553,
            "f1_weighted": 0.6410375940363199
          },
          {
            "accuracy": 0.6566650270536154,
            "f1": 0.6133035956479584,
            "f1_weighted": 0.6595565414969498
          },
          {
            "accuracy": 0.6345302508607968,
            "f1": 0.5903070073273384,
            "f1_weighted": 0.6395556739667153
          },
          {
            "accuracy": 0.6128873585833743,
            "f1": 0.5680980218167733,
            "f1_weighted": 0.6196940820357253
          },
          {
            "accuracy": 0.6246925725528775,
            "f1": 0.596754067340803,
            "f1_weighted": 0.632538567626111
          },
          {
            "accuracy": 0.6192818494835219,
            "f1": 0.5747087791126575,
            "f1_weighted": 0.6225217594018624
          },
          {
            "accuracy": 0.6291195277914412,
            "f1": 0.5923586555322975,
            "f1_weighted": 0.632416300324442
          },
          {
            "accuracy": 0.6345302508607968,
            "f1": 0.588887222900413,
            "f1_weighted": 0.6377764695309204
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}