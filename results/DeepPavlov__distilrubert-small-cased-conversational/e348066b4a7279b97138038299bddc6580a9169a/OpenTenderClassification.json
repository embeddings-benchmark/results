{
  "dataset_revision": "9af5657575a669dc18c7f897a67287ff7d1a0c65",
  "task_name": "OpenTenderClassification",
  "mteb_version": "2.1.17",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.153623,
            "f1": 0.150801,
            "f1_weighted": 0.150826,
            "precision": 0.155437,
            "precision_weighted": 0.15546,
            "recall": 0.153591,
            "recall_weighted": 0.153623,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.144928,
            "f1": 0.144143,
            "f1_weighted": 0.144095,
            "precision": 0.150668,
            "precision_weighted": 0.150616,
            "recall": 0.144983,
            "recall_weighted": 0.144928,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.149164,
            "f1": 0.146147,
            "f1_weighted": 0.146178,
            "precision": 0.150784,
            "precision_weighted": 0.150819,
            "recall": 0.149147,
            "recall_weighted": 0.149164,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.142921,
            "f1": 0.141799,
            "f1_weighted": 0.141802,
            "precision": 0.145305,
            "precision_weighted": 0.145302,
            "recall": 0.142904,
            "recall_weighted": 0.142921,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.158305,
            "f1": 0.156,
            "f1_weighted": 0.155982,
            "precision": 0.159775,
            "precision_weighted": 0.159754,
            "recall": 0.158316,
            "recall_weighted": 0.158305,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.144036,
            "f1": 0.141789,
            "f1_weighted": 0.141751,
            "precision": 0.146056,
            "precision_weighted": 0.146061,
            "recall": 0.144114,
            "recall_weighted": 0.144036,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.138016,
            "f1": 0.135474,
            "f1_weighted": 0.135447,
            "precision": 0.143066,
            "precision_weighted": 0.142998,
            "recall": 0.138006,
            "recall_weighted": 0.138016,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.145373,
            "f1": 0.148197,
            "f1_weighted": 0.148212,
            "precision": 0.16053,
            "precision_weighted": 0.160565,
            "recall": 0.145372,
            "recall_weighted": 0.145373,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.162988,
            "f1": 0.158764,
            "f1_weighted": 0.158702,
            "precision": 0.159059,
            "precision_weighted": 0.159002,
            "recall": 0.163047,
            "recall_weighted": 0.162988,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.148049,
            "f1": 0.144186,
            "f1_weighted": 0.144188,
            "precision": 0.147944,
            "precision_weighted": 0.147921,
            "recall": 0.148025,
            "recall_weighted": 0.148049,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.14874,
        "f1": 0.14673,
        "f1_weighted": 0.146718,
        "precision": 0.151862,
        "precision_weighted": 0.15185,
        "recall": 0.148751,
        "recall_weighted": 0.14874,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.14673,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 41.12736225128174,
  "kg_co2_emissions": null
}