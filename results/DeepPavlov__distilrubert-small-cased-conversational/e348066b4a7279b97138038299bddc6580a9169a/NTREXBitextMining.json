{
  "dataset_revision": "ed9a4403ed4adbfaf4aab56d5b2709e9f6c3ba33",
  "evaluation_time": 31.00135374069214,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test": [
      {
        "accuracy": 0.0025037556334501754,
        "f1": 0.001135076456858012,
        "hf_subset": "arb_Arab-rus_Cyrl",
        "languages": [
          "arb-Arab",
          "rus-Cyrl"
        ],
        "main_score": 0.001135076456858012,
        "precision": 0.0010772606172699704,
        "recall": 0.0025037556334501754
      },
      {
        "accuracy": 0.031547320981472206,
        "f1": 0.02126292729591064,
        "hf_subset": "bel_Cyrl-rus_Cyrl",
        "languages": [
          "bel-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.02126292729591064,
        "precision": 0.020041463002255074,
        "recall": 0.031547320981472206
      },
      {
        "accuracy": 0.00400600901352028,
        "f1": 0.0010053226367352731,
        "hf_subset": "ben_Beng-rus_Cyrl",
        "languages": [
          "ben-Beng",
          "rus-Cyrl"
        ],
        "main_score": 0.0010053226367352731,
        "precision": 0.0007919304122980064,
        "recall": 0.00400600901352028
      },
      {
        "accuracy": 0.004506760140210316,
        "f1": 0.0027400733519716857,
        "hf_subset": "bos_Latn-rus_Cyrl",
        "languages": [
          "bos-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0027400733519716857,
        "precision": 0.0026473790272304168,
        "recall": 0.004506760140210316
      },
      {
        "accuracy": 0.09113670505758638,
        "f1": 0.07261392272075445,
        "hf_subset": "bul_Cyrl-rus_Cyrl",
        "languages": [
          "bul-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.07261392272075445,
        "precision": 0.06884853631457213,
        "recall": 0.09113670505758638
      },
      {
        "accuracy": 0.005007511266900351,
        "f1": 0.003145487385716265,
        "hf_subset": "ces_Latn-rus_Cyrl",
        "languages": [
          "ces-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.003145487385716265,
        "precision": 0.0030840288994800173,
        "recall": 0.005007511266900351
      },
      {
        "accuracy": 0.0025037556334501754,
        "f1": 0.0020035123679433956,
        "hf_subset": "deu_Latn-rus_Cyrl",
        "languages": [
          "deu-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0020035123679433956,
        "precision": 0.0020032585661851478,
        "recall": 0.0025037556334501754
      },
      {
        "accuracy": 0.000500751126690035,
        "f1": 5.216157569687865e-07,
        "hf_subset": "ell_Grek-rus_Cyrl",
        "languages": [
          "ell-Grek",
          "rus-Cyrl"
        ],
        "main_score": 5.216157569687865e-07,
        "precision": 2.6094378670663626e-07,
        "recall": 0.000500751126690035
      },
      {
        "accuracy": 0.05908863294942414,
        "f1": 0.046881338480589674,
        "hf_subset": "eng_Latn-rus_Cyrl",
        "languages": [
          "eng-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.046881338480589674,
        "precision": 0.0446052010824761,
        "recall": 0.05908863294942414
      },
      {
        "accuracy": 0.0035052578868302454,
        "f1": 0.0016918587195466845,
        "hf_subset": "fas_Arab-rus_Cyrl",
        "languages": [
          "fas-Arab",
          "rus-Cyrl"
        ],
        "main_score": 0.0016918587195466845,
        "precision": 0.001513753451400266,
        "recall": 0.0035052578868302454
      },
      {
        "accuracy": 0.0025037556334501754,
        "f1": 0.0011834673864200979,
        "hf_subset": "fin_Latn-rus_Cyrl",
        "languages": [
          "fin-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0011834673864200979,
        "precision": 0.0011092833907829008,
        "recall": 0.0025037556334501754
      },
      {
        "accuracy": 0.00200300450676014,
        "f1": 0.0015027602131942853,
        "hf_subset": "fra_Latn-rus_Cyrl",
        "languages": [
          "fra-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0015027602131942853,
        "precision": 0.0015025069249443787,
        "recall": 0.00200300450676014
      },
      {
        "accuracy": 0.004506760140210316,
        "f1": 0.0025840062470326238,
        "hf_subset": "heb_Hebr-rus_Cyrl",
        "languages": [
          "heb-Hebr",
          "rus-Cyrl"
        ],
        "main_score": 0.0025840062470326238,
        "precision": 0.0022639033346929446,
        "recall": 0.004506760140210316
      },
      {
        "accuracy": 0.007010515773660491,
        "f1": 0.00501041845826136,
        "hf_subset": "hin_Deva-rus_Cyrl",
        "languages": [
          "hin-Deva",
          "rus-Cyrl"
        ],
        "main_score": 0.00501041845826136,
        "precision": 0.004820806484954669,
        "recall": 0.007010515773660491
      },
      {
        "accuracy": 0.0025037556334501754,
        "f1": 0.00033494905056315015,
        "hf_subset": "hrv_Latn-rus_Cyrl",
        "languages": [
          "hrv-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.00033494905056315015,
        "precision": 0.00019963048082767093,
        "recall": 0.0025037556334501754
      },
      {
        "accuracy": 0.0035052578868302454,
        "f1": 0.0015212241355151113,
        "hf_subset": "hun_Latn-rus_Cyrl",
        "languages": [
          "hun-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0015212241355151113,
        "precision": 0.0013616001883732397,
        "recall": 0.0035052578868302454
      },
      {
        "accuracy": 0.0035052578868302454,
        "f1": 0.002373141233727896,
        "hf_subset": "ind_Latn-rus_Cyrl",
        "languages": [
          "ind-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.002373141233727896,
        "precision": 0.0022721939033906987,
        "recall": 0.0035052578868302454
      },
      {
        "accuracy": 0.0030045067601402104,
        "f1": 0.001519494756221416,
        "hf_subset": "jpn_Jpan-rus_Cyrl",
        "languages": [
          "jpn-Jpan",
          "rus-Cyrl"
        ],
        "main_score": 0.001519494756221416,
        "precision": 0.0013533146473908523,
        "recall": 0.0030045067601402104
      },
      {
        "accuracy": 0.005508262393590386,
        "f1": 0.0031398085316972062,
        "hf_subset": "kor_Hang-rus_Cyrl",
        "languages": [
          "kor-Hang",
          "rus-Cyrl"
        ],
        "main_score": 0.0031398085316972062,
        "precision": 0.0030767672625495045,
        "recall": 0.005508262393590386
      },
      {
        "accuracy": 0.0035052578868302454,
        "f1": 0.0015541925002759975,
        "hf_subset": "lit_Latn-rus_Cyrl",
        "languages": [
          "lit-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0015541925002759975,
        "precision": 0.001362326491800176,
        "recall": 0.0035052578868302454
      },
      {
        "accuracy": 0.04556835252879319,
        "f1": 0.033440935859298845,
        "hf_subset": "mkd_Cyrl-rus_Cyrl",
        "languages": [
          "mkd-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.033440935859298845,
        "precision": 0.031352630516906364,
        "recall": 0.04556835252879319
      },
      {
        "accuracy": 0.0030045067601402104,
        "f1": 0.002170435139889321,
        "hf_subset": "nld_Latn-rus_Cyrl",
        "languages": [
          "nld-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.002170435139889321,
        "precision": 0.002003261433967267,
        "recall": 0.0030045067601402104
      },
      {
        "accuracy": 0.006509764646970456,
        "f1": 0.003241016765575822,
        "hf_subset": "pol_Latn-rus_Cyrl",
        "languages": [
          "pol-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.003241016765575822,
        "precision": 0.0028977934129691798,
        "recall": 0.006509764646970456
      },
      {
        "accuracy": 0.00200300450676014,
        "f1": 0.0012523874870830419,
        "hf_subset": "por_Latn-rus_Cyrl",
        "languages": [
          "por-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0012523874870830419,
        "precision": 0.001168674260542205,
        "recall": 0.00200300450676014
      },
      {
        "accuracy": 0.0030045067601402104,
        "f1": 0.0005459559065877601,
        "hf_subset": "rus_Cyrl-arb_Arab",
        "languages": [
          "rus-Cyrl",
          "arb-Arab"
        ],
        "main_score": 0.0005459559065877601,
        "precision": 0.0003667536982197726,
        "recall": 0.0030045067601402104
      },
      {
        "accuracy": 0.035553329994992486,
        "f1": 0.019379433325664625,
        "hf_subset": "rus_Cyrl-bel_Cyrl",
        "languages": [
          "rus-Cyrl",
          "bel-Cyrl"
        ],
        "main_score": 0.019379433325664625,
        "precision": 0.017539711681755846,
        "recall": 0.035553329994992486
      },
      {
        "accuracy": 0.0035052578868302454,
        "f1": 0.0006588457576831284,
        "hf_subset": "rus_Cyrl-ben_Beng",
        "languages": [
          "rus-Cyrl",
          "ben-Beng"
        ],
        "main_score": 0.0006588457576831284,
        "precision": 0.000588944560462499,
        "recall": 0.0035052578868302454
      },
      {
        "accuracy": 0.00801201802704056,
        "f1": 0.0024328219451257033,
        "hf_subset": "rus_Cyrl-bos_Latn",
        "languages": [
          "rus-Cyrl",
          "bos-Latn"
        ],
        "main_score": 0.0024328219451257033,
        "precision": 0.002048384734959002,
        "recall": 0.00801201802704056
      },
      {
        "accuracy": 0.08763144717075613,
        "f1": 0.05520115804312531,
        "hf_subset": "rus_Cyrl-bul_Cyrl",
        "languages": [
          "rus-Cyrl",
          "bul-Cyrl"
        ],
        "main_score": 0.05520115804312531,
        "precision": 0.04946314264263528,
        "recall": 0.08763144717075613
      },
      {
        "accuracy": 0.010515773660490736,
        "f1": 0.004051502432350274,
        "hf_subset": "rus_Cyrl-ces_Latn",
        "languages": [
          "rus-Cyrl",
          "ces-Latn"
        ],
        "main_score": 0.004051502432350274,
        "precision": 0.0034696872867618824,
        "recall": 0.010515773660490736
      },
      {
        "accuracy": 0.008512769153730596,
        "f1": 0.0021486335521942145,
        "hf_subset": "rus_Cyrl-deu_Latn",
        "languages": [
          "rus-Cyrl",
          "deu-Latn"
        ],
        "main_score": 0.0021486335521942145,
        "precision": 0.0016890276655222955,
        "recall": 0.008512769153730596
      },
      {
        "accuracy": 0.0025037556334501754,
        "f1": 0.0010075754439615497,
        "hf_subset": "rus_Cyrl-ell_Grek",
        "languages": [
          "rus-Cyrl",
          "ell-Grek"
        ],
        "main_score": 0.0010075754439615497,
        "precision": 0.0007541762401501718,
        "recall": 0.0025037556334501754
      },
      {
        "accuracy": 0.20931397095643464,
        "f1": 0.15856721396785614,
        "hf_subset": "rus_Cyrl-eng_Latn",
        "languages": [
          "rus-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.15856721396785614,
        "precision": 0.14779871364155184,
        "recall": 0.20931397095643464
      },
      {
        "accuracy": 0.004506760140210316,
        "f1": 0.0003765326646364321,
        "hf_subset": "rus_Cyrl-fas_Arab",
        "languages": [
          "rus-Cyrl",
          "fas-Arab"
        ],
        "main_score": 0.0003765326646364321,
        "precision": 0.0002078661012376561,
        "recall": 0.004506760140210316
      },
      {
        "accuracy": 0.009013520280420632,
        "f1": 0.0032284245869313687,
        "hf_subset": "rus_Cyrl-fin_Latn",
        "languages": [
          "rus-Cyrl",
          "fin-Latn"
        ],
        "main_score": 0.0032284245869313687,
        "precision": 0.002809641355595203,
        "recall": 0.009013520280420632
      },
      {
        "accuracy": 0.015523284927391086,
        "f1": 0.004875147136976979,
        "hf_subset": "rus_Cyrl-fra_Latn",
        "languages": [
          "rus-Cyrl",
          "fra-Latn"
        ],
        "main_score": 0.004875147136976979,
        "precision": 0.0038763236060134687,
        "recall": 0.015523284927391086
      },
      {
        "accuracy": 0.006009013520280421,
        "f1": 0.001312412619371413,
        "hf_subset": "rus_Cyrl-heb_Hebr",
        "languages": [
          "rus-Cyrl",
          "heb-Hebr"
        ],
        "main_score": 0.001312412619371413,
        "precision": 0.0008792026390487497,
        "recall": 0.006009013520280421
      },
      {
        "accuracy": 0.0025037556334501754,
        "f1": 0.00018245601883648253,
        "hf_subset": "rus_Cyrl-hin_Deva",
        "languages": [
          "rus-Cyrl",
          "hin-Deva"
        ],
        "main_score": 0.00018245601883648253,
        "precision": 9.923783977722028e-05,
        "recall": 0.0025037556334501754
      },
      {
        "accuracy": 0.006509764646970456,
        "f1": 0.002015548837454969,
        "hf_subset": "rus_Cyrl-hrv_Latn",
        "languages": [
          "rus-Cyrl",
          "hrv-Latn"
        ],
        "main_score": 0.002015548837454969,
        "precision": 0.0016092446594363117,
        "recall": 0.006509764646970456
      },
      {
        "accuracy": 0.007511266900350526,
        "f1": 0.0025732609905619653,
        "hf_subset": "rus_Cyrl-hun_Latn",
        "languages": [
          "rus-Cyrl",
          "hun-Latn"
        ],
        "main_score": 0.0025732609905619653,
        "precision": 0.002326797162401994,
        "recall": 0.007511266900350526
      },
      {
        "accuracy": 0.015022533800701052,
        "f1": 0.004793814248079387,
        "hf_subset": "rus_Cyrl-ind_Latn",
        "languages": [
          "rus-Cyrl",
          "ind-Latn"
        ],
        "main_score": 0.004793814248079387,
        "precision": 0.0038729832660281955,
        "recall": 0.015022533800701052
      },
      {
        "accuracy": 0.0015022533800701052,
        "f1": 0.0004775282726392112,
        "hf_subset": "rus_Cyrl-jpn_Jpan",
        "languages": [
          "rus-Cyrl",
          "jpn-Jpan"
        ],
        "main_score": 0.0004775282726392112,
        "precision": 0.00033414549685224353,
        "recall": 0.0015022533800701052
      },
      {
        "accuracy": 0.007010515773660491,
        "f1": 0.0015385009159760257,
        "hf_subset": "rus_Cyrl-kor_Hang",
        "languages": [
          "rus-Cyrl",
          "kor-Hang"
        ],
        "main_score": 0.0015385009159760257,
        "precision": 0.0011657797182804059,
        "recall": 0.007010515773660491
      },
      {
        "accuracy": 0.013019529293940912,
        "f1": 0.00332463539582098,
        "hf_subset": "rus_Cyrl-lit_Latn",
        "languages": [
          "rus-Cyrl",
          "lit-Latn"
        ],
        "main_score": 0.00332463539582098,
        "precision": 0.002483587952518003,
        "recall": 0.013019529293940912
      },
      {
        "accuracy": 0.06960440660991488,
        "f1": 0.039258177084627165,
        "hf_subset": "rus_Cyrl-mkd_Cyrl",
        "languages": [
          "rus-Cyrl",
          "mkd-Cyrl"
        ],
        "main_score": 0.039258177084627165,
        "precision": 0.03454181874099343,
        "recall": 0.06960440660991488
      },
      {
        "accuracy": 0.01702553830746119,
        "f1": 0.006480652918275913,
        "hf_subset": "rus_Cyrl-nld_Latn",
        "languages": [
          "rus-Cyrl",
          "nld-Latn"
        ],
        "main_score": 0.006480652918275913,
        "precision": 0.0055236354591122905,
        "recall": 0.01702553830746119
      },
      {
        "accuracy": 0.006009013520280421,
        "f1": 0.0014993086397182079,
        "hf_subset": "rus_Cyrl-pol_Latn",
        "languages": [
          "rus-Cyrl",
          "pol-Latn"
        ],
        "main_score": 0.0014993086397182079,
        "precision": 0.0013364223858548848,
        "recall": 0.006009013520280421
      },
      {
        "accuracy": 0.015523284927391086,
        "f1": 0.006828584623517593,
        "hf_subset": "rus_Cyrl-por_Latn",
        "languages": [
          "rus-Cyrl",
          "por-Latn"
        ],
        "main_score": 0.006828584623517593,
        "precision": 0.0055347520132407076,
        "recall": 0.015523284927391086
      },
      {
        "accuracy": 0.009514271407110666,
        "f1": 0.0026387573083140378,
        "hf_subset": "rus_Cyrl-slk_Latn",
        "languages": [
          "rus-Cyrl",
          "slk-Latn"
        ],
        "main_score": 0.0026387573083140378,
        "precision": 0.002079489748723559,
        "recall": 0.009514271407110666
      },
      {
        "accuracy": 0.010015022533800702,
        "f1": 0.0033898081927962813,
        "hf_subset": "rus_Cyrl-slv_Latn",
        "languages": [
          "rus-Cyrl",
          "slv-Latn"
        ],
        "main_score": 0.0033898081927962813,
        "precision": 0.0027190843930184443,
        "recall": 0.010015022533800702
      },
      {
        "accuracy": 0.013019529293940912,
        "f1": 0.0039825326482308115,
        "hf_subset": "rus_Cyrl-spa_Latn",
        "languages": [
          "rus-Cyrl",
          "spa-Latn"
        ],
        "main_score": 0.0039825326482308115,
        "precision": 0.003373058743999257,
        "recall": 0.013019529293940912
      },
      {
        "accuracy": 0.06459689534301452,
        "f1": 0.037246596307047714,
        "hf_subset": "rus_Cyrl-srp_Cyrl",
        "languages": [
          "rus-Cyrl",
          "srp-Cyrl"
        ],
        "main_score": 0.037246596307047714,
        "precision": 0.0328564542250953,
        "recall": 0.06459689534301452
      },
      {
        "accuracy": 0.010015022533800702,
        "f1": 0.003789705615126813,
        "hf_subset": "rus_Cyrl-srp_Latn",
        "languages": [
          "rus-Cyrl",
          "srp-Latn"
        ],
        "main_score": 0.003789705615126813,
        "precision": 0.0031043527629140296,
        "recall": 0.010015022533800702
      },
      {
        "accuracy": 0.006009013520280421,
        "f1": 0.001687014966014005,
        "hf_subset": "rus_Cyrl-swa_Latn",
        "languages": [
          "rus-Cyrl",
          "swa-Latn"
        ],
        "main_score": 0.001687014966014005,
        "precision": 0.0014038899057437478,
        "recall": 0.006009013520280421
      },
      {
        "accuracy": 0.024536805207811718,
        "f1": 0.010078729608180738,
        "hf_subset": "rus_Cyrl-swe_Latn",
        "languages": [
          "rus-Cyrl",
          "swe-Latn"
        ],
        "main_score": 0.010078729608180738,
        "precision": 0.008170644757895137,
        "recall": 0.024536805207811718
      },
      {
        "accuracy": 0.005508262393590386,
        "f1": 0.002549934532864861,
        "hf_subset": "rus_Cyrl-tam_Taml",
        "languages": [
          "rus-Cyrl",
          "tam-Taml"
        ],
        "main_score": 0.002549934532864861,
        "precision": 0.002276925652391562,
        "recall": 0.005508262393590386
      },
      {
        "accuracy": 0.013520280420630946,
        "f1": 0.005011851766246567,
        "hf_subset": "rus_Cyrl-tur_Latn",
        "languages": [
          "rus-Cyrl",
          "tur-Latn"
        ],
        "main_score": 0.005011851766246567,
        "precision": 0.004291813195409796,
        "recall": 0.013520280420630946
      },
      {
        "accuracy": 0.03605408112168253,
        "f1": 0.022317650597605657,
        "hf_subset": "rus_Cyrl-ukr_Cyrl",
        "languages": [
          "rus-Cyrl",
          "ukr-Cyrl"
        ],
        "main_score": 0.022317650597605657,
        "precision": 0.020310828059235515,
        "recall": 0.03605408112168253
      },
      {
        "accuracy": 0.009514271407110666,
        "f1": 0.0013128926651661665,
        "hf_subset": "rus_Cyrl-vie_Latn",
        "languages": [
          "rus-Cyrl",
          "vie-Latn"
        ],
        "main_score": 0.0013128926651661665,
        "precision": 0.000861158505236521,
        "recall": 0.009514271407110666
      },
      {
        "accuracy": 0.0015022533800701052,
        "f1": 1.6226095637678312e-05,
        "hf_subset": "rus_Cyrl-zho_Hant",
        "languages": [
          "rus-Cyrl",
          "zho-Hant"
        ],
        "main_score": 1.6226095637678312e-05,
        "precision": 8.15729173900563e-06,
        "recall": 0.0015022533800701052
      },
      {
        "accuracy": 0.011016524787180772,
        "f1": 0.0023712556291964506,
        "hf_subset": "rus_Cyrl-zul_Latn",
        "languages": [
          "rus-Cyrl",
          "zul-Latn"
        ],
        "main_score": 0.0023712556291964506,
        "precision": 0.0017002428278452611,
        "recall": 0.011016524787180772
      },
      {
        "accuracy": 0.0035052578868302454,
        "f1": 0.002017409713564051,
        "hf_subset": "slk_Latn-rus_Cyrl",
        "languages": [
          "slk-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.002017409713564051,
        "precision": 0.0020102722047246535,
        "recall": 0.0035052578868302454
      },
      {
        "accuracy": 0.004506760140210316,
        "f1": 0.0031690240586822118,
        "hf_subset": "slv_Latn-rus_Cyrl",
        "languages": [
          "slv-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0031690240586822118,
        "precision": 0.0030989101880141024,
        "recall": 0.004506760140210316
      },
      {
        "accuracy": 0.0030045067601402104,
        "f1": 0.0021704273582090305,
        "hf_subset": "spa_Latn-rus_Cyrl",
        "languages": [
          "spa-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0021704273582090305,
        "precision": 0.002003257539163723,
        "recall": 0.0030045067601402104
      },
      {
        "accuracy": 0.03405107661492238,
        "f1": 0.023296788141968002,
        "hf_subset": "srp_Cyrl-rus_Cyrl",
        "languages": [
          "srp-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.023296788141968002,
        "precision": 0.021555304689451042,
        "recall": 0.03405107661492238
      },
      {
        "accuracy": 0.0025037556334501754,
        "f1": 0.001753165366689563,
        "hf_subset": "srp_Latn-rus_Cyrl",
        "languages": [
          "srp-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.001753165366689563,
        "precision": 0.0016694387776734767,
        "recall": 0.0025037556334501754
      },
      {
        "accuracy": 0.00200300450676014,
        "f1": 0.0010497868477008004,
        "hf_subset": "swa_Latn-rus_Cyrl",
        "languages": [
          "swa-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0010497868477008004,
        "precision": 0.0010268369913921326,
        "recall": 0.00200300450676014
      },
      {
        "accuracy": 0.004506760140210316,
        "f1": 0.002653120490541468,
        "hf_subset": "swe_Latn-rus_Cyrl",
        "languages": [
          "swe-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.002653120490541468,
        "precision": 0.002454934533086928,
        "recall": 0.004506760140210316
      },
      {
        "accuracy": 0.00801201802704056,
        "f1": 0.0047976306244518156,
        "hf_subset": "tam_Taml-rus_Cyrl",
        "languages": [
          "tam-Taml",
          "rus-Cyrl"
        ],
        "main_score": 0.0047976306244518156,
        "precision": 0.004519513849490941,
        "recall": 0.00801201802704056
      },
      {
        "accuracy": 0.0035052578868302454,
        "f1": 0.0012878464722898039,
        "hf_subset": "tur_Latn-rus_Cyrl",
        "languages": [
          "tur-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0012878464722898039,
        "precision": 0.001029665439065564,
        "recall": 0.0035052578868302454
      },
      {
        "accuracy": 0.1442163244867301,
        "f1": 0.11861033666071143,
        "hf_subset": "ukr_Cyrl-rus_Cyrl",
        "languages": [
          "ukr-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.11861033666071143,
        "precision": 0.11306959079418102,
        "recall": 0.1442163244867301
      },
      {
        "accuracy": 0.0035052578868302454,
        "f1": 0.001912099712773508,
        "hf_subset": "vie_Latn-rus_Cyrl",
        "languages": [
          "vie-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.001912099712773508,
        "precision": 0.0017930573212550074,
        "recall": 0.0035052578868302454
      },
      {
        "accuracy": 0.0025037556334501754,
        "f1": 0.0010253475451272144,
        "hf_subset": "zho_Hant-rus_Cyrl",
        "languages": [
          "zho-Hant",
          "rus-Cyrl"
        ],
        "main_score": 0.0010253475451272144,
        "precision": 0.0008216870760686483,
        "recall": 0.0025037556334501754
      },
      {
        "accuracy": 0.0030045067601402104,
        "f1": 0.0013462393339268909,
        "hf_subset": "zul_Latn-rus_Cyrl",
        "languages": [
          "zul-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0013462393339268909,
        "precision": 0.0012573646159486419,
        "recall": 0.0030045067601402104
      }
    ]
  },
  "task_name": "NTREXBitextMining"
}