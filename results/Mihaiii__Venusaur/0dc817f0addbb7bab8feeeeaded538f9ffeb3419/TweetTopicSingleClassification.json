{
  "dataset_revision": "87b7a0d1c402dbb481db649569c556d9aa27ac05",
  "task_name": "TweetTopicSingleClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test_2021": [
      {
        "accuracy": 0.582575,
        "f1": 0.446978,
        "f1_weighted": 0.625384,
        "scores_per_experiment": [
          {
            "accuracy": 0.513881,
            "f1": 0.408662,
            "f1_weighted": 0.56914
          },
          {
            "accuracy": 0.527466,
            "f1": 0.429181,
            "f1_weighted": 0.561799
          },
          {
            "accuracy": 0.588305,
            "f1": 0.4559,
            "f1_weighted": 0.63209
          },
          {
            "accuracy": 0.591258,
            "f1": 0.471538,
            "f1_weighted": 0.629962
          },
          {
            "accuracy": 0.619019,
            "f1": 0.468274,
            "f1_weighted": 0.655413
          },
          {
            "accuracy": 0.633786,
            "f1": 0.486628,
            "f1_weighted": 0.663502
          },
          {
            "accuracy": 0.579445,
            "f1": 0.424858,
            "f1_weighted": 0.632753
          },
          {
            "accuracy": 0.603071,
            "f1": 0.460023,
            "f1_weighted": 0.648391
          },
          {
            "accuracy": 0.583579,
            "f1": 0.418687,
            "f1_weighted": 0.627913
          },
          {
            "accuracy": 0.585942,
            "f1": 0.446033,
            "f1_weighted": 0.632873
          }
        ],
        "main_score": 0.582575,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 7.358004093170166,
  "kg_co2_emissions": 0.00022057306629208343
}