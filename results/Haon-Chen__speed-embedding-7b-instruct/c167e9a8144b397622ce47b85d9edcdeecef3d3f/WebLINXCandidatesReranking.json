{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "task_name": "WebLINXCandidatesReranking",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "map": 0.139597,
        "mrr": 0.123321,
        "nAUC_map_max": 0.114305,
        "nAUC_map_std": -0.08242,
        "nAUC_map_diff1": 0.147103,
        "nAUC_mrr_max": 0.109982,
        "nAUC_mrr_std": -0.080096,
        "nAUC_mrr_diff1": 0.159282,
        "main_score": 0.123321,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_iid": [
      {
        "map": 0.119102,
        "mrr": 0.105246,
        "nAUC_map_max": 0.020869,
        "nAUC_map_std": 0.028616,
        "nAUC_map_diff1": 0.222593,
        "nAUC_mrr_max": 0.00246,
        "nAUC_mrr_std": 0.033961,
        "nAUC_mrr_diff1": 0.225698,
        "main_score": 0.105246,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_cat": [
      {
        "map": 0.09198,
        "mrr": 0.074634,
        "nAUC_map_max": 0.004039,
        "nAUC_map_std": 0.164552,
        "nAUC_map_diff1": 0.096145,
        "nAUC_mrr_max": -0.000696,
        "nAUC_mrr_std": 0.154009,
        "nAUC_mrr_diff1": 0.092922,
        "main_score": 0.074634,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_geo": [
      {
        "map": 0.126994,
        "mrr": 0.109399,
        "nAUC_map_max": -0.028852,
        "nAUC_map_std": 0.146013,
        "nAUC_map_diff1": 0.04913,
        "nAUC_mrr_max": -0.03157,
        "nAUC_mrr_std": 0.134933,
        "nAUC_mrr_diff1": 0.038524,
        "main_score": 0.109399,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_vis": [
      {
        "map": 0.100135,
        "mrr": 0.0817,
        "nAUC_map_max": -0.115699,
        "nAUC_map_std": 0.11062,
        "nAUC_map_diff1": 0.185904,
        "nAUC_mrr_max": -0.111357,
        "nAUC_mrr_std": 0.111738,
        "nAUC_mrr_diff1": 0.189511,
        "main_score": 0.0817,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_web": [
      {
        "map": 0.095355,
        "mrr": 0.08312,
        "nAUC_map_max": 0.052202,
        "nAUC_map_std": 0.135642,
        "nAUC_map_diff1": 0.077206,
        "nAUC_mrr_max": 0.041314,
        "nAUC_mrr_std": 0.132061,
        "nAUC_mrr_diff1": 0.103358,
        "main_score": 0.08312,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 131044.83907103539,
  "kg_co2_emissions": 12.06184395456124
}