{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.327148,
        "f1": 0.323204,
        "f1_weighted": 0.323195,
        "scores_per_experiment": [
          {
            "accuracy": 0.350586,
            "f1": 0.345093,
            "f1_weighted": 0.345098
          },
          {
            "accuracy": 0.304688,
            "f1": 0.300566,
            "f1_weighted": 0.300571
          },
          {
            "accuracy": 0.314941,
            "f1": 0.308172,
            "f1_weighted": 0.308137
          },
          {
            "accuracy": 0.345703,
            "f1": 0.344819,
            "f1_weighted": 0.344807
          },
          {
            "accuracy": 0.337891,
            "f1": 0.327139,
            "f1_weighted": 0.327101
          },
          {
            "accuracy": 0.34668,
            "f1": 0.331867,
            "f1_weighted": 0.331863
          },
          {
            "accuracy": 0.282227,
            "f1": 0.285944,
            "f1_weighted": 0.285969
          },
          {
            "accuracy": 0.342773,
            "f1": 0.344984,
            "f1_weighted": 0.344969
          },
          {
            "accuracy": 0.356934,
            "f1": 0.355324,
            "f1_weighted": 0.355293
          },
          {
            "accuracy": 0.289062,
            "f1": 0.288131,
            "f1_weighted": 0.288147
          }
        ],
        "main_score": 0.327148,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.327393,
        "f1": 0.323265,
        "f1_weighted": 0.32326,
        "scores_per_experiment": [
          {
            "accuracy": 0.341797,
            "f1": 0.336086,
            "f1_weighted": 0.336079
          },
          {
            "accuracy": 0.319336,
            "f1": 0.311804,
            "f1_weighted": 0.311817
          },
          {
            "accuracy": 0.304688,
            "f1": 0.299267,
            "f1_weighted": 0.29925
          },
          {
            "accuracy": 0.343262,
            "f1": 0.342904,
            "f1_weighted": 0.342876
          },
          {
            "accuracy": 0.341309,
            "f1": 0.328207,
            "f1_weighted": 0.328156
          },
          {
            "accuracy": 0.339844,
            "f1": 0.328973,
            "f1_weighted": 0.328979
          },
          {
            "accuracy": 0.312988,
            "f1": 0.313267,
            "f1_weighted": 0.313305
          },
          {
            "accuracy": 0.331543,
            "f1": 0.335079,
            "f1_weighted": 0.335089
          },
          {
            "accuracy": 0.353027,
            "f1": 0.348287,
            "f1_weighted": 0.348249
          },
          {
            "accuracy": 0.286133,
            "f1": 0.288772,
            "f1_weighted": 0.288796
          }
        ],
        "main_score": 0.327393,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 157.869366645813,
  "kg_co2_emissions": 0.013629706993857727
}