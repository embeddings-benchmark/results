{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 0.7333557498318765,
                "f1": 0.7228949738478356,
                "main_score": 0.7333557498318765
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 0.6584398117014123,
                "f1": 0.6471026362091463,
                "main_score": 0.6584398117014123
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 0.6976462676529925,
                "f1": 0.698229667407667,
                "main_score": 0.6976462676529925
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 0.7202420981842635,
                "f1": 0.7176576384895899,
                "main_score": 0.7202420981842635
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 0.727572293207801,
                "f1": 0.7276840765295256,
                "main_score": 0.727572293207801
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 0.6802286482851378,
                "f1": 0.6617237947327872,
                "main_score": 0.6802286482851378
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 0.7760928043039678,
                "f1": 0.7727094731234773,
                "main_score": 0.7760928043039678
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 0.7768325487558844,
                "f1": 0.7797530399082261,
                "main_score": 0.7768325487558844
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 0.7613315400134497,
                "f1": 0.7597558584796424,
                "main_score": 0.7613315400134497
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 0.8047410894418292,
                "f1": 0.8052244841473792,
                "main_score": 0.8047410894418292
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 0.769670477471419,
                "f1": 0.7737318805793145,
                "main_score": 0.769670477471419
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 0.7809683927370544,
                "f1": 0.7769773737430847,
                "main_score": 0.7809683927370544
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 0.7520847343644923,
                "f1": 0.7517071738727349,
                "main_score": 0.7520847343644923
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 0.7707464694014796,
                "f1": 0.7716136207698571,
                "main_score": 0.7707464694014796
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 0.7353396099529255,
                "f1": 0.7358296404484121,
                "main_score": 0.7353396099529255
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 0.7575319435104236,
                "f1": 0.7524674707850834,
                "main_score": 0.7575319435104236
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 0.770948217888366,
                "f1": 0.7647559490205027,
                "main_score": 0.770948217888366
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 0.7107599193006052,
                "f1": 0.7076028043093511,
                "main_score": 0.7107599193006052
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 0.7710490921318089,
                "f1": 0.7701215275283272,
                "main_score": 0.7710490921318089
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 0.7125756556825824,
                "f1": 0.7020605314648762,
                "main_score": 0.7125756556825824
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 0.7708137188971083,
                "f1": 0.7738992690574391,
                "main_score": 0.7708137188971083
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 0.7935440484196369,
                "f1": 0.7958964690002772,
                "main_score": 0.7935440484196369
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 0.6842299932750504,
                "f1": 0.6807844356925413,
                "main_score": 0.6842299932750504
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 0.6615669132481508,
                "f1": 0.6589383352608513,
                "main_score": 0.6615669132481508
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 0.6011432414256894,
                "f1": 0.5769910594559806,
                "main_score": 0.6011432414256894
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 0.7124747814391392,
                "f1": 0.7042455553830917,
                "main_score": 0.7124747814391392
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 0.7646267652992603,
                "f1": 0.7688545593083159,
                "main_score": 0.7646267652992603
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 0.7324815063887021,
                "f1": 0.7277805034658074,
                "main_score": 0.7324815063887021
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 0.7411566913248151,
                "f1": 0.7386147988001356,
                "main_score": 0.7411566913248151
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 0.700168123739072,
                "f1": 0.6938515920054571,
                "main_score": 0.700168123739072
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 0.7441156691324814,
                "f1": 0.7343474953408237,
                "main_score": 0.7441156691324814
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 0.6839609952925353,
                "f1": 0.6729731681109291,
                "main_score": 0.6839609952925353
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 0.7720914593140552,
                "f1": 0.7707066497935366,
                "main_score": 0.7720914593140552
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 0.7852387357094821,
                "f1": 0.7852595694732911,
                "main_score": 0.7852387357094821
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 0.7669132481506389,
                "f1": 0.7691201656350455,
                "main_score": 0.7669132481506389
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 0.771217215870881,
                "f1": 0.7741179937912505,
                "main_score": 0.771217215870881
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 0.7525891055817082,
                "f1": 0.758089244542887,
                "main_score": 0.7525891055817082
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 0.7770679219905852,
                "f1": 0.7821459594517711,
                "main_score": 0.7770679219905852
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 0.7483523873570949,
                "f1": 0.7486847028401978,
                "main_score": 0.7483523873570949
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 0.7471755211835911,
                "f1": 0.740214326485662,
                "main_score": 0.7471755211835911
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 0.7906523201075991,
                "f1": 0.7910545620325138,
                "main_score": 0.7906523201075991
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 0.6791862811028918,
                "f1": 0.6650386121217983,
                "main_score": 0.6791862811028918
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 0.7093140551445865,
                "f1": 0.70755435928495,
                "main_score": 0.7093140551445865
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 0.7240753194351042,
                "f1": 0.7161816115782923,
                "main_score": 0.7240753194351042
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 0.7518157363819771,
                "f1": 0.7508016717887205,
                "main_score": 0.7518157363819771
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 0.7286482851378614,
                "f1": 0.7239521180006291,
                "main_score": 0.7286482851378614
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 0.7646940147948891,
                "f1": 0.7670044085362349,
                "main_score": 0.7646940147948891
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 0.7189307330195024,
                "f1": 0.7157218253322981,
                "main_score": 0.7189307330195024
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 0.747511768661735,
                "f1": 0.7517918654541514,
                "main_score": 0.747511768661735
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 0.7869535978480162,
                "f1": 0.7890019070153316,
                "main_score": 0.7869535978480162
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 0.7545729657027571,
                "f1": 0.7619578371794673,
                "main_score": 0.7545729657027571
            }
        ]
    }
}