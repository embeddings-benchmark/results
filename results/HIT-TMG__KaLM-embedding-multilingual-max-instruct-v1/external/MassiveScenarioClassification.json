{
    "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 0.9018829858776058,
                "f1": 0.8925088012497998,
                "f1_weighted": 0.8984015465115704,
                "main_score": 0.9018829858776058
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 0.874747814391392,
                "f1": 0.8643632175686828,
                "f1_weighted": 0.8717273979239227,
                "main_score": 0.874747814391392
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 0.8486886348352387,
                "f1": 0.8455883152206087,
                "f1_weighted": 0.8460969784137692,
                "main_score": 0.8486886348352387
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 0.8897444519166107,
                "f1": 0.87632588826147,
                "f1_weighted": 0.8861529562960045,
                "main_score": 0.8897444519166107
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 0.8926698049764625,
                "f1": 0.8827744182451868,
                "f1_weighted": 0.8889264206068702,
                "main_score": 0.8926698049764625
            }
        ]
    }
}