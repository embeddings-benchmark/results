{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.36.5",
  "scores": {
    "test": [
      {
        "accuracy": 0.870746,
        "f1": 0.843495,
        "f1_weighted": 0.862711,
        "scores_per_experiment": [
          {
            "accuracy": 0.868863,
            "f1": 0.839854,
            "f1_weighted": 0.859451
          },
          {
            "accuracy": 0.869536,
            "f1": 0.849327,
            "f1_weighted": 0.862286
          },
          {
            "accuracy": 0.863147,
            "f1": 0.836531,
            "f1_weighted": 0.850233
          },
          {
            "accuracy": 0.870545,
            "f1": 0.844792,
            "f1_weighted": 0.862346
          },
          {
            "accuracy": 0.85844,
            "f1": 0.826055,
            "f1_weighted": 0.847129
          },
          {
            "accuracy": 0.878615,
            "f1": 0.851093,
            "f1_weighted": 0.875158
          },
          {
            "accuracy": 0.864492,
            "f1": 0.840487,
            "f1_weighted": 0.853137
          },
          {
            "accuracy": 0.871217,
            "f1": 0.841511,
            "f1_weighted": 0.862727
          },
          {
            "accuracy": 0.879623,
            "f1": 0.850761,
            "f1_weighted": 0.87456
          },
          {
            "accuracy": 0.882986,
            "f1": 0.854543,
            "f1_weighted": 0.880078
          }
        ],
        "main_score": 0.870746,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      },
      {
        "accuracy": 0.849361,
        "f1": 0.813451,
        "f1_weighted": 0.840686,
        "scores_per_experiment": [
          {
            "accuracy": 0.860121,
            "f1": 0.818744,
            "f1_weighted": 0.854465
          },
          {
            "accuracy": 0.848352,
            "f1": 0.817926,
            "f1_weighted": 0.837766
          },
          {
            "accuracy": 0.8423,
            "f1": 0.80507,
            "f1_weighted": 0.829994
          },
          {
            "accuracy": 0.847007,
            "f1": 0.815023,
            "f1_weighted": 0.838647
          },
          {
            "accuracy": 0.83961,
            "f1": 0.806622,
            "f1_weighted": 0.828101
          },
          {
            "accuracy": 0.855414,
            "f1": 0.817438,
            "f1_weighted": 0.851863
          },
          {
            "accuracy": 0.84499,
            "f1": 0.811117,
            "f1_weighted": 0.831874
          },
          {
            "accuracy": 0.848352,
            "f1": 0.813196,
            "f1_weighted": 0.839975
          },
          {
            "accuracy": 0.854069,
            "f1": 0.814282,
            "f1_weighted": 0.848964
          },
          {
            "accuracy": 0.853396,
            "f1": 0.815088,
            "f1_weighted": 0.84521
          }
        ],
        "main_score": 0.849361,
        "hf_subset": "zh-CN",
        "languages": [
          "cmo-Hans"
        ]
      }
    ]
  },
  "evaluation_time": 35.69178891181946,
  "kg_co2_emissions": null
}