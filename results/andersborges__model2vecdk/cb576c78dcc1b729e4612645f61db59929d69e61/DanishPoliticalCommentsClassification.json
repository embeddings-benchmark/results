{
  "dataset_revision": "d743dcd5abb03d5ab357757a0e83522fc6696fcd",
  "task_name": "DanishPoliticalCommentsClassification",
  "mteb_version": "2.1.11",
  "scores": {
    "train": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.305717,
            "f1": 0.274977,
            "f1_weighted": 0.331941,
            "precision": 0.293399,
            "precision_weighted": 0.432732,
            "recall": 0.342594,
            "recall_weighted": 0.305717,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.316125,
            "f1": 0.271942,
            "f1_weighted": 0.335317,
            "precision": 0.283404,
            "precision_weighted": 0.419279,
            "recall": 0.351362,
            "recall_weighted": 0.316125,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.307938,
            "f1": 0.278157,
            "f1_weighted": 0.327996,
            "precision": 0.282314,
            "precision_weighted": 0.39696,
            "recall": 0.339958,
            "recall_weighted": 0.307938,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.343741,
            "f1": 0.296887,
            "f1_weighted": 0.366336,
            "precision": 0.30628,
            "precision_weighted": 0.43766,
            "recall": 0.353563,
            "recall_weighted": 0.343741,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.344713,
            "f1": 0.305618,
            "f1_weighted": 0.36206,
            "precision": 0.307126,
            "precision_weighted": 0.429704,
            "recall": 0.376018,
            "recall_weighted": 0.344713,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.259784,
            "f1": 0.247882,
            "f1_weighted": 0.268657,
            "precision": 0.259831,
            "precision_weighted": 0.360773,
            "recall": 0.317297,
            "recall_weighted": 0.259784,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.3529,
            "f1": 0.326707,
            "f1_weighted": 0.37461,
            "precision": 0.335802,
            "precision_weighted": 0.460806,
            "recall": 0.379276,
            "recall_weighted": 0.3529,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.344296,
            "f1": 0.301468,
            "f1_weighted": 0.365617,
            "precision": 0.314087,
            "precision_weighted": 0.451357,
            "recall": 0.381793,
            "recall_weighted": 0.344296,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.348876,
            "f1": 0.292321,
            "f1_weighted": 0.362931,
            "precision": 0.293796,
            "precision_weighted": 0.406086,
            "recall": 0.351815,
            "recall_weighted": 0.348876,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.341105,
            "f1": 0.308862,
            "f1_weighted": 0.359884,
            "precision": 0.31347,
            "precision_weighted": 0.435297,
            "recall": 0.371903,
            "recall_weighted": 0.341105,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.32652,
        "f1": 0.290482,
        "f1_weighted": 0.345535,
        "precision": 0.298951,
        "precision_weighted": 0.423065,
        "recall": 0.356558,
        "recall_weighted": 0.32652,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.32652,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 36.577759981155396,
  "kg_co2_emissions": null
}