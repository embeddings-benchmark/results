{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "task_name": "InappropriatenessClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.526514,
        "f1": 0.522341,
        "f1_weighted": 0.522341,
        "ap": 0.514919,
        "ap_weighted": 0.514919,
        "scores_per_experiment": [
          {
            "accuracy": 0.52832,
            "f1": 0.510192,
            "f1_weighted": 0.510192,
            "ap": 0.515464,
            "ap_weighted": 0.515464
          },
          {
            "accuracy": 0.557129,
            "f1": 0.557126,
            "f1_weighted": 0.557126,
            "ap": 0.531812,
            "ap_weighted": 0.531812
          },
          {
            "accuracy": 0.525391,
            "f1": 0.525244,
            "f1_weighted": 0.525244,
            "ap": 0.513318,
            "ap_weighted": 0.513318
          },
          {
            "accuracy": 0.460449,
            "f1": 0.455202,
            "f1_weighted": 0.455202,
            "ap": 0.481532,
            "ap_weighted": 0.481532
          },
          {
            "accuracy": 0.522461,
            "f1": 0.522313,
            "f1_weighted": 0.522313,
            "ap": 0.511753,
            "ap_weighted": 0.511753
          },
          {
            "accuracy": 0.53125,
            "f1": 0.530084,
            "f1_weighted": 0.530084,
            "ap": 0.51671,
            "ap_weighted": 0.51671
          },
          {
            "accuracy": 0.578613,
            "f1": 0.578596,
            "f1_weighted": 0.578596,
            "ap": 0.545409,
            "ap_weighted": 0.545409
          },
          {
            "accuracy": 0.49707,
            "f1": 0.494059,
            "f1_weighted": 0.494059,
            "ap": 0.498543,
            "ap_weighted": 0.498543
          },
          {
            "accuracy": 0.533691,
            "f1": 0.532415,
            "f1_weighted": 0.532415,
            "ap": 0.517873,
            "ap_weighted": 0.517873
          },
          {
            "accuracy": 0.530762,
            "f1": 0.518176,
            "f1_weighted": 0.518176,
            "ap": 0.516779,
            "ap_weighted": 0.516779
          }
        ],
        "main_score": 0.526514,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 8.002587795257568,
  "kg_co2_emissions": 0.0002450996579856121
}