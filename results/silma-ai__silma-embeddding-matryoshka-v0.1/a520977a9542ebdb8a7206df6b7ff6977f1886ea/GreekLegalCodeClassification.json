{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "task_name": "GreekLegalCodeClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.044141,
        "f1": 0.040026,
        "f1_weighted": 0.045174,
        "scores_per_experiment": [
          {
            "accuracy": 0.043457,
            "f1": 0.041399,
            "f1_weighted": 0.044593
          },
          {
            "accuracy": 0.041016,
            "f1": 0.035743,
            "f1_weighted": 0.041965
          },
          {
            "accuracy": 0.035156,
            "f1": 0.031983,
            "f1_weighted": 0.035776
          },
          {
            "accuracy": 0.049316,
            "f1": 0.045687,
            "f1_weighted": 0.049357
          },
          {
            "accuracy": 0.047363,
            "f1": 0.046771,
            "f1_weighted": 0.048028
          },
          {
            "accuracy": 0.049316,
            "f1": 0.040229,
            "f1_weighted": 0.052763
          },
          {
            "accuracy": 0.049805,
            "f1": 0.043055,
            "f1_weighted": 0.048068
          },
          {
            "accuracy": 0.040039,
            "f1": 0.034427,
            "f1_weighted": 0.042276
          },
          {
            "accuracy": 0.041992,
            "f1": 0.040588,
            "f1_weighted": 0.042534
          },
          {
            "accuracy": 0.043945,
            "f1": 0.040378,
            "f1_weighted": 0.04638
          }
        ],
        "main_score": 0.044141,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.040674,
        "f1": 0.036332,
        "f1_weighted": 0.039779,
        "scores_per_experiment": [
          {
            "accuracy": 0.038574,
            "f1": 0.037012,
            "f1_weighted": 0.038122
          },
          {
            "accuracy": 0.039062,
            "f1": 0.036254,
            "f1_weighted": 0.03892
          },
          {
            "accuracy": 0.037598,
            "f1": 0.034143,
            "f1_weighted": 0.037501
          },
          {
            "accuracy": 0.041016,
            "f1": 0.034169,
            "f1_weighted": 0.041699
          },
          {
            "accuracy": 0.038574,
            "f1": 0.034208,
            "f1_weighted": 0.038116
          },
          {
            "accuracy": 0.042969,
            "f1": 0.035882,
            "f1_weighted": 0.041603
          },
          {
            "accuracy": 0.038086,
            "f1": 0.034784,
            "f1_weighted": 0.037266
          },
          {
            "accuracy": 0.045898,
            "f1": 0.040029,
            "f1_weighted": 0.04509
          },
          {
            "accuracy": 0.041504,
            "f1": 0.035278,
            "f1_weighted": 0.040186
          },
          {
            "accuracy": 0.043457,
            "f1": 0.041559,
            "f1_weighted": 0.039286
          }
        ],
        "main_score": 0.040674,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ]
  },
  "evaluation_time": 1194.1610934734344,
  "kg_co2_emissions": 0.03883208621434106
}