{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "task_name": "AmazonCounterfactualClassification",
  "mteb_version": "1.36.5",
  "scores": {
    "test": [
      {
        "accuracy": 0.928955,
        "f1": 0.896386,
        "f1_weighted": 0.931758,
        "ap": 0.723022,
        "ap_weighted": 0.723022,
        "scores_per_experiment": [
          {
            "accuracy": 0.928358,
            "f1": 0.895423,
            "f1_weighted": 0.931161,
            "ap": 0.720459,
            "ap_weighted": 0.720459
          },
          {
            "accuracy": 0.926866,
            "f1": 0.893504,
            "f1_weighted": 0.929801,
            "ap": 0.716139,
            "ap_weighted": 0.716139
          },
          {
            "accuracy": 0.938806,
            "f1": 0.909103,
            "f1_weighted": 0.940745,
            "ap": 0.752252,
            "ap_weighted": 0.752252
          },
          {
            "accuracy": 0.931343,
            "f1": 0.899286,
            "f1_weighted": 0.933887,
            "ap": 0.72926,
            "ap_weighted": 0.72926
          },
          {
            "accuracy": 0.925373,
            "f1": 0.891594,
            "f1_weighted": 0.928444,
            "ap": 0.711871,
            "ap_weighted": 0.711871
          },
          {
            "accuracy": 0.931343,
            "f1": 0.899286,
            "f1_weighted": 0.933887,
            "ap": 0.72926,
            "ap_weighted": 0.72926
          },
          {
            "accuracy": 0.926866,
            "f1": 0.893504,
            "f1_weighted": 0.929801,
            "ap": 0.716139,
            "ap_weighted": 0.716139
          },
          {
            "accuracy": 0.937313,
            "f1": 0.907121,
            "f1_weighted": 0.939368,
            "ap": 0.747536,
            "ap_weighted": 0.747536
          },
          {
            "accuracy": 0.929851,
            "f1": 0.897851,
            "f1_weighted": 0.932667,
            "ap": 0.726143,
            "ap_weighted": 0.726143
          },
          {
            "accuracy": 0.913433,
            "f1": 0.877182,
            "f1_weighted": 0.917815,
            "ap": 0.681157,
            "ap_weighted": 0.681157
          }
        ],
        "main_score": 0.928955,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 18.539973974227905,
  "kg_co2_emissions": null
}