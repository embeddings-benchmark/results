{
    "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
    "task_name": "MassiveIntentClassification",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 0.7537323470073974,
                "f1": 0.711836877753734,
                "f1_weighted": 0.7572073213955457,
                "main_score": 0.7537323470073974
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 0.7483523873570949,
                "f1": 0.7072375821116885,
                "f1_weighted": 0.7520800490010755,
                "main_score": 0.7483523873570949
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 0.7531607262945528,
                "f1": 0.7206063554897661,
                "f1_weighted": 0.7572438161355253,
                "main_score": 0.7531607262945528
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 0.767955615332885,
                "f1": 0.7308099648499755,
                "f1_weighted": 0.7718482068239667,
                "main_score": 0.767955615332885
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 0.7760591795561534,
                "f1": 0.7446676705370394,
                "f1_weighted": 0.7769888062336614,
                "main_score": 0.7760591795561534
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 0.7632145258910558,
                "f1": 0.7289824154178328,
                "f1_weighted": 0.7665393279794721,
                "main_score": 0.7632145258910558
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 0.7321788836583724,
                "f1": 0.7045594512246377,
                "f1_weighted": 0.7367862536499393,
                "main_score": 0.7321788836583724
            }
        ]
    }
}