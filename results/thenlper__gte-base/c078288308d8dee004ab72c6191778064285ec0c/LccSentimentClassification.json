{
  "dataset_revision": "de7ba3406ee55ea2cc52a0a41408fa6aede6d3c6",
  "task_name": "LccSentimentClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.392667,
        "f1": 0.374193,
        "f1_weighted": 0.405289,
        "scores_per_experiment": [
          {
            "accuracy": 0.453333,
            "f1": 0.438804,
            "f1_weighted": 0.467783
          },
          {
            "accuracy": 0.28,
            "f1": 0.282813,
            "f1_weighted": 0.27604
          },
          {
            "accuracy": 0.393333,
            "f1": 0.381421,
            "f1_weighted": 0.409017
          },
          {
            "accuracy": 0.366667,
            "f1": 0.35291,
            "f1_weighted": 0.389111
          },
          {
            "accuracy": 0.36,
            "f1": 0.351876,
            "f1_weighted": 0.375316
          },
          {
            "accuracy": 0.446667,
            "f1": 0.402468,
            "f1_weighted": 0.466293
          },
          {
            "accuracy": 0.42,
            "f1": 0.41066,
            "f1_weighted": 0.430173
          },
          {
            "accuracy": 0.4,
            "f1": 0.382223,
            "f1_weighted": 0.409637
          },
          {
            "accuracy": 0.38,
            "f1": 0.363255,
            "f1_weighted": 0.392505
          },
          {
            "accuracy": 0.426667,
            "f1": 0.375497,
            "f1_weighted": 0.437018
          }
        ],
        "main_score": 0.392667,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 13.541733741760254,
  "kg_co2_emissions": 0.0003911291889090366
}