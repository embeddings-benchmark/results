{
    "dataset_revision": "ad9991cb51e31e31e430383c75ffb2885547b5f0",
    "task_name": "CQADupstackEnglishRetrieval",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "default",
                "languages": [
                    "eng-Latn"
                ],
                "map_at_1": 0.3271,
                "map_at_10": 0.43297,
                "map_at_100": 0.44606999999999997,
                "map_at_1000": 0.44728999999999997,
                "map_at_20": 0.44014,
                "map_at_3": 0.40213,
                "map_at_5": 0.42004,
                "mrr_at_1": 0.40892,
                "mrr_at_10": 0.49394,
                "mrr_at_100": 0.50005,
                "mrr_at_1000": 0.50043,
                "mrr_at_20": 0.49764,
                "mrr_at_3": 0.47134,
                "mrr_at_5": 0.48522,
                "ndcg_at_1": 0.40892,
                "ndcg_at_10": 0.49047,
                "ndcg_at_100": 0.53267,
                "ndcg_at_1000": 0.55097,
                "ndcg_at_20": 0.50707,
                "ndcg_at_3": 0.44896,
                "ndcg_at_5": 0.46983,
                "precision_at_1": 0.40892,
                "precision_at_10": 0.09293,
                "precision_at_100": 0.01473,
                "precision_at_1000": 0.00192,
                "precision_at_20": 0.054459999999999995,
                "precision_at_3": 0.21592,
                "precision_at_5": 0.15541,
                "recall_at_1": 0.3271,
                "recall_at_10": 0.58593,
                "recall_at_100": 0.7624200000000001,
                "recall_at_1000": 0.87717,
                "recall_at_20": 0.64646,
                "recall_at_3": 0.46253,
                "recall_at_5": 0.51947,
                "main_score": 0.49047
            }
        ]
    }
}