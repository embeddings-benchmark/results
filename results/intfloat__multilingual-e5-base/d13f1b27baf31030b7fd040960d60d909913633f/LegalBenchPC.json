{
  "dataset_revision": "12ca3b695563788fead87a982ad1a068284413f4",
  "evaluation_time": 1.646167516708374,
  "kg_co2_emissions": 8.190433625725906e-05,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.6904296875,
          "accuracy_threshold": 0.7654256224632263,
          "ap": 0.7633756251372825,
          "f1": 0.7687231936654569,
          "f1_threshold": 0.7377811670303345,
          "precision": 0.643646408839779,
          "recall": 0.9541359541359541
        },
        "dot": {
          "accuracy": 0.6904296875,
          "accuracy_threshold": 0.7654252052307129,
          "ap": 0.763375518692113,
          "f1": 0.7687231936654569,
          "f1_threshold": 0.7377814054489136,
          "precision": 0.643646408839779,
          "recall": 0.9541359541359541
        },
        "euclidean": {
          "accuracy": 0.6904296875,
          "accuracy_threshold": 0.6849443316459656,
          "ap": 0.7633758964020882,
          "f1": 0.7687231936654569,
          "f1_threshold": 0.7241806387901306,
          "precision": 0.643646408839779,
          "recall": 0.9541359541359541
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6904296875,
        "manhattan": {
          "accuracy": 0.68505859375,
          "accuracy_threshold": 15.041943550109863,
          "ap": 0.7619700564741068,
          "f1": 0.7644263408010863,
          "f1_threshold": 15.801883697509766,
          "precision": 0.6527536231884058,
          "recall": 0.9221949221949222
        },
        "max": {
          "accuracy": 0.6904296875,
          "ap": 0.7633758964020882,
          "f1": 0.7687231936654569
        },
        "similarity": {
          "accuracy": 0.6904296875,
          "accuracy_threshold": 0.7654256224632263,
          "ap": 0.7633756251372825,
          "f1": 0.7687231936654569,
          "f1_threshold": 0.7377811670303345,
          "precision": 0.643646408839779,
          "recall": 0.9541359541359541
        }
      }
    ]
  },
  "task_name": "LegalBenchPC"
}