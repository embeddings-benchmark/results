{
  "dataset_revision": "62dad66fc2837b0ac5e5175fe7c265d2d502a386",
  "task_name": "SynPerChatbotConvSASurprise",
  "mteb_version": "1.25.8",
  "scores": {
    "test": [
      {
        "accuracy": 0.569421,
        "f1": 0.556294,
        "f1_weighted": 0.575228,
        "ap": 0.661395,
        "ap_weighted": 0.661395,
        "scores_per_experiment": [
          {
            "accuracy": 0.578512,
            "f1": 0.561251,
            "f1_weighted": 0.583547,
            "ap": 0.660913,
            "ap_weighted": 0.660913
          },
          {
            "accuracy": 0.644628,
            "f1": 0.627426,
            "f1_weighted": 0.647936,
            "ap": 0.699459,
            "ap_weighted": 0.699459
          },
          {
            "accuracy": 0.595041,
            "f1": 0.583667,
            "f1_weighted": 0.601297,
            "ap": 0.676274,
            "ap_weighted": 0.676274
          },
          {
            "accuracy": 0.603306,
            "f1": 0.588435,
            "f1_weighted": 0.608478,
            "ap": 0.677225,
            "ap_weighted": 0.677225
          },
          {
            "accuracy": 0.586777,
            "f1": 0.576331,
            "f1_weighted": 0.593375,
            "ap": 0.672522,
            "ap_weighted": 0.672522
          },
          {
            "accuracy": 0.570248,
            "f1": 0.567857,
            "f1_weighted": 0.576092,
            "ap": 0.676399,
            "ap_weighted": 0.676399
          },
          {
            "accuracy": 0.570248,
            "f1": 0.547729,
            "f1_weighted": 0.573584,
            "ap": 0.652458,
            "ap_weighted": 0.652458
          },
          {
            "accuracy": 0.495868,
            "f1": 0.489027,
            "f1_weighted": 0.504174,
            "ap": 0.627622,
            "ap_weighted": 0.627622
          },
          {
            "accuracy": 0.53719,
            "f1": 0.52549,
            "f1_weighted": 0.544579,
            "ap": 0.643656,
            "ap_weighted": 0.643656
          },
          {
            "accuracy": 0.512397,
            "f1": 0.495726,
            "f1_weighted": 0.519216,
            "ap": 0.627418,
            "ap_weighted": 0.627418
          }
        ],
        "main_score": 0.569421,
        "hf_subset": "default",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 9.472164869308472,
  "kg_co2_emissions": null
}