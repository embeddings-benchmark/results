{
  "dataset_revision": "9af5657575a669dc18c7f897a67287ff7d1a0c65",
  "task_name": "OpenTenderClassification",
  "mteb_version": "2.3.0",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.479599,
            "f1": 0.45892,
            "f1_weighted": 0.458915,
            "precision": 0.469449,
            "precision_weighted": 0.469435,
            "recall": 0.479606,
            "recall_weighted": 0.479599,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.46466,
            "f1": 0.442486,
            "f1_weighted": 0.442436,
            "precision": 0.461555,
            "precision_weighted": 0.46157,
            "recall": 0.464767,
            "recall_weighted": 0.46466,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.487848,
            "f1": 0.462572,
            "f1_weighted": 0.462542,
            "precision": 0.475776,
            "precision_weighted": 0.475756,
            "recall": 0.487881,
            "recall_weighted": 0.487848,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.470234,
            "f1": 0.446175,
            "f1_weighted": 0.446106,
            "precision": 0.452296,
            "precision_weighted": 0.452196,
            "recall": 0.470286,
            "recall_weighted": 0.470234,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.455964,
            "f1": 0.434657,
            "f1_weighted": 0.434626,
            "precision": 0.451348,
            "precision_weighted": 0.45137,
            "recall": 0.456002,
            "recall_weighted": 0.455964,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.464214,
            "f1": 0.44433,
            "f1_weighted": 0.444249,
            "precision": 0.455411,
            "precision_weighted": 0.455378,
            "recall": 0.464339,
            "recall_weighted": 0.464214,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.459086,
            "f1": 0.436884,
            "f1_weighted": 0.436764,
            "precision": 0.459436,
            "precision_weighted": 0.459333,
            "recall": 0.45916,
            "recall_weighted": 0.459086,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.465552,
            "f1": 0.441739,
            "f1_weighted": 0.441709,
            "precision": 0.454516,
            "precision_weighted": 0.454456,
            "recall": 0.465574,
            "recall_weighted": 0.465552,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.435006,
            "f1": 0.412051,
            "f1_weighted": 0.411897,
            "precision": 0.426388,
            "precision_weighted": 0.426189,
            "recall": 0.4351,
            "recall_weighted": 0.435006,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.46466,
            "f1": 0.439984,
            "f1_weighted": 0.439899,
            "precision": 0.451711,
            "precision_weighted": 0.451661,
            "recall": 0.464773,
            "recall_weighted": 0.46466,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.464682,
        "f1": 0.44198,
        "f1_weighted": 0.441914,
        "precision": 0.455789,
        "precision_weighted": 0.455734,
        "recall": 0.464749,
        "recall_weighted": 0.464682,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.44198,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 44.04291248321533,
  "kg_co2_emissions": null
}