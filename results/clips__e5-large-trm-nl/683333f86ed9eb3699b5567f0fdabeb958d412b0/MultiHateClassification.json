{
  "dataset_revision": "8f95949846bb9e33c6aaf730ccfdb8fe6bcfb7a9",
  "task_name": "MultiHateClassification",
  "mteb_version": "2.3.0",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.596,
            "f1": 0.543193,
            "f1_weighted": 0.60594,
            "precision": 0.543567,
            "precision_weighted": 0.620317,
            "recall": 0.548098,
            "recall_weighted": 0.596,
            "ap": 0.321878,
            "ap_weighted": 0.321878
          },
          {
            "accuracy": 0.521,
            "f1": 0.517837,
            "f1_weighted": 0.533615,
            "precision": 0.574231,
            "precision_weighted": 0.663097,
            "recall": 0.583515,
            "recall_weighted": 0.521,
            "ap": 0.33954,
            "ap_weighted": 0.33954
          },
          {
            "accuracy": 0.694,
            "f1": 0.641074,
            "f1_weighted": 0.696757,
            "precision": 0.638872,
            "precision_weighted": 0.700058,
            "recall": 0.64397,
            "recall_weighted": 0.694,
            "ap": 0.396525,
            "ap_weighted": 0.396525
          },
          {
            "accuracy": 0.602,
            "f1": 0.573618,
            "f1_weighted": 0.618061,
            "precision": 0.580384,
            "precision_weighted": 0.658355,
            "recall": 0.594858,
            "recall_weighted": 0.602,
            "ap": 0.349593,
            "ap_weighted": 0.349593
          },
          {
            "accuracy": 0.674,
            "f1": 0.633616,
            "f1_weighted": 0.682758,
            "precision": 0.630287,
            "precision_weighted": 0.69821,
            "recall": 0.645175,
            "recall_weighted": 0.674,
            "ap": 0.3922,
            "ap_weighted": 0.3922
          },
          {
            "accuracy": 0.711,
            "f1": 0.640436,
            "f1_weighted": 0.704788,
            "precision": 0.648026,
            "precision_weighted": 0.700809,
            "recall": 0.635801,
            "recall_weighted": 0.711,
            "ap": 0.396645,
            "ap_weighted": 0.396645
          },
          {
            "accuracy": 0.633,
            "f1": 0.59084,
            "f1_weighted": 0.643901,
            "precision": 0.590023,
            "precision_weighted": 0.662975,
            "recall": 0.601489,
            "recall_weighted": 0.633,
            "ap": 0.356342,
            "ap_weighted": 0.356342
          },
          {
            "accuracy": 0.654,
            "f1": 0.603136,
            "f1_weighted": 0.660536,
            "precision": 0.600787,
            "precision_weighted": 0.669691,
            "recall": 0.608721,
            "recall_weighted": 0.654,
            "ap": 0.363673,
            "ap_weighted": 0.363673
          },
          {
            "accuracy": 0.702,
            "f1": 0.648543,
            "f1_weighted": 0.703919,
            "precision": 0.646802,
            "precision_weighted": 0.70611,
            "recall": 0.650634,
            "recall_weighted": 0.702,
            "ap": 0.403745,
            "ap_weighted": 0.403745
          },
          {
            "accuracy": 0.733,
            "f1": 0.679364,
            "f1_weighted": 0.732344,
            "precision": 0.680269,
            "precision_weighted": 0.731726,
            "recall": 0.678507,
            "recall_weighted": 0.733,
            "ap": 0.43657,
            "ap_weighted": 0.43657
          }
        ],
        "accuracy": 0.652,
        "f1": 0.607166,
        "f1_weighted": 0.658262,
        "precision": 0.613325,
        "precision_weighted": 0.681135,
        "recall": 0.619077,
        "recall_weighted": 0.652,
        "ap": 0.375671,
        "ap_weighted": 0.375671,
        "main_score": 0.652,
        "hf_subset": "nld",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 12.44016981124878,
  "kg_co2_emissions": null
}