{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "task_name": "AmazonCounterfactualClassification",
  "mteb_version": "1.36.10",
  "scores": {
    "test": [
      {
        "accuracy": 0.695522,
        "f1": 0.634324,
        "f1_weighted": 0.724555,
        "ap": 0.321044,
        "ap_weighted": 0.321044,
        "scores_per_experiment": [
          {
            "accuracy": 0.649254,
            "f1": 0.583037,
            "f1_weighted": 0.684222,
            "ap": 0.268174,
            "ap_weighted": 0.268174
          },
          {
            "accuracy": 0.753731,
            "f1": 0.688285,
            "f1_weighted": 0.775263,
            "ap": 0.370247,
            "ap_weighted": 0.370247
          },
          {
            "accuracy": 0.58806,
            "f1": 0.552854,
            "f1_weighted": 0.629258,
            "ap": 0.272517,
            "ap_weighted": 0.272517
          },
          {
            "accuracy": 0.695522,
            "f1": 0.633294,
            "f1_weighted": 0.725284,
            "ap": 0.315682,
            "ap_weighted": 0.315682
          },
          {
            "accuracy": 0.707463,
            "f1": 0.651741,
            "f1_weighted": 0.736571,
            "ap": 0.340628,
            "ap_weighted": 0.340628
          },
          {
            "accuracy": 0.726866,
            "f1": 0.670555,
            "f1_weighted": 0.753497,
            "ap": 0.360029,
            "ap_weighted": 0.360029
          },
          {
            "accuracy": 0.753731,
            "f1": 0.671642,
            "f1_weighted": 0.77162,
            "ap": 0.339013,
            "ap_weighted": 0.339013
          },
          {
            "accuracy": 0.732836,
            "f1": 0.66528,
            "f1_weighted": 0.756851,
            "ap": 0.343198,
            "ap_weighted": 0.343198
          },
          {
            "accuracy": 0.702985,
            "f1": 0.645907,
            "f1_weighted": 0.732479,
            "ap": 0.333184,
            "ap_weighted": 0.333184
          },
          {
            "accuracy": 0.644776,
            "f1": 0.580651,
            "f1_weighted": 0.68051,
            "ap": 0.267764,
            "ap_weighted": 0.267764
          }
        ],
        "main_score": 0.695522,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 22.0602068901062,
  "kg_co2_emissions": null
}