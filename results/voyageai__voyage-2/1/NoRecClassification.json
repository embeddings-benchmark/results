{
  "dataset_revision": "5b740b7c42c73d586420812a35745fc37118862f",
  "task_name": "NoRecClassification",
  "mteb_version": "2.1.11",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.410645,
            "f1": 0.404317,
            "f1_weighted": 0.434457,
            "precision": 0.459318,
            "precision_weighted": 0.538208,
            "recall": 0.448265,
            "recall_weighted": 0.410645,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.473145,
            "f1": 0.422444,
            "f1_weighted": 0.476186,
            "precision": 0.421781,
            "precision_weighted": 0.479718,
            "recall": 0.423857,
            "recall_weighted": 0.473145,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.438477,
            "f1": 0.425181,
            "f1_weighted": 0.463244,
            "precision": 0.462687,
            "precision_weighted": 0.543783,
            "recall": 0.454082,
            "recall_weighted": 0.438477,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.50293,
            "f1": 0.432488,
            "f1_weighted": 0.491901,
            "precision": 0.442774,
            "precision_weighted": 0.502094,
            "recall": 0.443174,
            "recall_weighted": 0.50293,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.419922,
            "f1": 0.398556,
            "f1_weighted": 0.437777,
            "precision": 0.431137,
            "precision_weighted": 0.502853,
            "recall": 0.432797,
            "recall_weighted": 0.419922,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.428711,
            "f1": 0.409687,
            "f1_weighted": 0.438587,
            "precision": 0.417276,
            "precision_weighted": 0.472718,
            "recall": 0.427703,
            "recall_weighted": 0.428711,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.424805,
            "f1": 0.416542,
            "f1_weighted": 0.449847,
            "precision": 0.502012,
            "precision_weighted": 0.587527,
            "recall": 0.474609,
            "recall_weighted": 0.424805,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.5,
            "f1": 0.434959,
            "f1_weighted": 0.497855,
            "precision": 0.437141,
            "precision_weighted": 0.499451,
            "recall": 0.43641,
            "recall_weighted": 0.5,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.458984,
            "f1": 0.423873,
            "f1_weighted": 0.464202,
            "precision": 0.426503,
            "precision_weighted": 0.480961,
            "recall": 0.439422,
            "recall_weighted": 0.458984,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.435059,
            "f1": 0.416124,
            "f1_weighted": 0.45054,
            "precision": 0.433201,
            "precision_weighted": 0.500568,
            "recall": 0.434822,
            "recall_weighted": 0.435059,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.449268,
        "f1": 0.418417,
        "f1_weighted": 0.46046,
        "precision": 0.443383,
        "precision_weighted": 0.510788,
        "recall": 0.441514,
        "recall_weighted": 0.449268,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.449268,
        "hf_subset": "default",
        "languages": [
          "nob-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 70.94852709770203,
  "kg_co2_emissions": null
}