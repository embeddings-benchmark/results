{
    "dataset_revision": "6299947a7777084cc2d4b64235bf7190381ce755",
    "task_name": "MTOPIntentClassification",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 0.582421340629275,
                "f1": 0.40116960466226426,
                "main_score": 0.582421340629275
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 0.45069033530571984,
                "f1": 0.30468468273374966,
                "main_score": 0.45069033530571984
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 0.4880920613742495,
                "f1": 0.32659853754004475,
                "main_score": 0.4880920613742495
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 0.4433761352959598,
                "f1": 0.2930204743560644,
                "main_score": 0.4433761352959598
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 0.34198637504481894,
                "f1": 0.2206370603224841,
                "main_score": 0.34198637504481894
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 0.4311030741410488,
                "f1": 0.2692408933648504,
                "main_score": 0.4311030741410488
            }
        ]
    }
}