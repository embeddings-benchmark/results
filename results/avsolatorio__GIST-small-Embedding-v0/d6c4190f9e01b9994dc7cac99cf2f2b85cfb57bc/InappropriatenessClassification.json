{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "task_name": "InappropriatenessClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.512939,
        "f1": 0.51022,
        "f1_weighted": 0.51022,
        "ap": 0.506788,
        "ap_weighted": 0.506788,
        "scores_per_experiment": [
          {
            "accuracy": 0.532715,
            "f1": 0.527006,
            "f1_weighted": 0.527006,
            "ap": 0.517235,
            "ap_weighted": 0.517235
          },
          {
            "accuracy": 0.513184,
            "f1": 0.511059,
            "f1_weighted": 0.511059,
            "ap": 0.506745,
            "ap_weighted": 0.506745
          },
          {
            "accuracy": 0.532227,
            "f1": 0.531063,
            "f1_weighted": 0.531063,
            "ap": 0.517058,
            "ap_weighted": 0.517058
          },
          {
            "accuracy": 0.487793,
            "f1": 0.479727,
            "f1_weighted": 0.479727,
            "ap": 0.494016,
            "ap_weighted": 0.494016
          },
          {
            "accuracy": 0.524902,
            "f1": 0.524158,
            "f1_weighted": 0.524158,
            "ap": 0.513125,
            "ap_weighted": 0.513125
          },
          {
            "accuracy": 0.511719,
            "f1": 0.509155,
            "f1_weighted": 0.509155,
            "ap": 0.50602,
            "ap_weighted": 0.50602
          },
          {
            "accuracy": 0.50293,
            "f1": 0.502861,
            "f1_weighted": 0.502861,
            "ap": 0.501474,
            "ap_weighted": 0.501474
          },
          {
            "accuracy": 0.503906,
            "f1": 0.501372,
            "f1_weighted": 0.501372,
            "ap": 0.501966,
            "ap_weighted": 0.501966
          },
          {
            "accuracy": 0.513184,
            "f1": 0.513157,
            "f1_weighted": 0.513157,
            "ap": 0.506763,
            "ap_weighted": 0.506763
          },
          {
            "accuracy": 0.506836,
            "f1": 0.502645,
            "f1_weighted": 0.502645,
            "ap": 0.503475,
            "ap_weighted": 0.503475
          }
        ],
        "main_score": 0.512939,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 7.721869945526123,
  "kg_co2_emissions": 0.00022970359493590492
}