{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.298047,
        "f1": 0.289582,
        "f1_weighted": 0.289572,
        "scores_per_experiment": [
          {
            "accuracy": 0.304688,
            "f1": 0.302793,
            "f1_weighted": 0.302798
          },
          {
            "accuracy": 0.270508,
            "f1": 0.263481,
            "f1_weighted": 0.263491
          },
          {
            "accuracy": 0.288574,
            "f1": 0.274985,
            "f1_weighted": 0.274987
          },
          {
            "accuracy": 0.332031,
            "f1": 0.326187,
            "f1_weighted": 0.326179
          },
          {
            "accuracy": 0.289551,
            "f1": 0.280307,
            "f1_weighted": 0.280244
          },
          {
            "accuracy": 0.295898,
            "f1": 0.279133,
            "f1_weighted": 0.279103
          },
          {
            "accuracy": 0.253418,
            "f1": 0.253095,
            "f1_weighted": 0.25304
          },
          {
            "accuracy": 0.326172,
            "f1": 0.311945,
            "f1_weighted": 0.311962
          },
          {
            "accuracy": 0.319824,
            "f1": 0.31494,
            "f1_weighted": 0.314953
          },
          {
            "accuracy": 0.299805,
            "f1": 0.28895,
            "f1_weighted": 0.288964
          }
        ],
        "main_score": 0.298047,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.299805,
        "f1": 0.291794,
        "f1_weighted": 0.291791,
        "scores_per_experiment": [
          {
            "accuracy": 0.287109,
            "f1": 0.285432,
            "f1_weighted": 0.28544
          },
          {
            "accuracy": 0.280273,
            "f1": 0.273951,
            "f1_weighted": 0.273964
          },
          {
            "accuracy": 0.313477,
            "f1": 0.300199,
            "f1_weighted": 0.30021
          },
          {
            "accuracy": 0.323242,
            "f1": 0.318305,
            "f1_weighted": 0.318298
          },
          {
            "accuracy": 0.297852,
            "f1": 0.288708,
            "f1_weighted": 0.288651
          },
          {
            "accuracy": 0.274414,
            "f1": 0.257701,
            "f1_weighted": 0.257663
          },
          {
            "accuracy": 0.279297,
            "f1": 0.27927,
            "f1_weighted": 0.279227
          },
          {
            "accuracy": 0.335938,
            "f1": 0.321716,
            "f1_weighted": 0.321749
          },
          {
            "accuracy": 0.317383,
            "f1": 0.311989,
            "f1_weighted": 0.312015
          },
          {
            "accuracy": 0.289062,
            "f1": 0.28067,
            "f1_weighted": 0.280697
          }
        ],
        "main_score": 0.299805,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 13.459462404251099,
  "kg_co2_emissions": 0.00042759778274669303
}