{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 106.03756618499756,
  "kg_co2_emissions": 0.00552217027372773,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.730731201171875,
        "ap": 0.6639088652676239,
        "ap_weighted": 0.6639088652676239,
        "f1": 0.7295452420914966,
        "f1_weighted": 0.7295396594839569,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.730731201171875,
        "scores_per_experiment": [
          {
            "accuracy": 0.699066162109375,
            "ap": 0.640057649755915,
            "ap_weighted": 0.640057649755915,
            "f1": 0.6990096494968498,
            "f1_weighted": 0.6990114115827637
          },
          {
            "accuracy": 0.7509765625,
            "ap": 0.6824227207895787,
            "ap_weighted": 0.6824227207895787,
            "f1": 0.750318695445483,
            "f1_weighted": 0.7503132197387272
          },
          {
            "accuracy": 0.741180419921875,
            "ap": 0.6764751347473643,
            "ap_weighted": 0.6764751347473643,
            "f1": 0.7410915768243274,
            "f1_weighted": 0.7410895277248781
          },
          {
            "accuracy": 0.725189208984375,
            "ap": 0.6562822676701416,
            "ap_weighted": 0.6562822676701416,
            "f1": 0.7235082227417864,
            "f1_weighted": 0.7234990118582653
          },
          {
            "accuracy": 0.73724365234375,
            "ap": 0.66430655337512,
            "ap_weighted": 0.66430655337512,
            "f1": 0.7337980659490364,
            "f1_weighted": 0.7337851265151507
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}