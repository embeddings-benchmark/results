{
  "dataset_revision": "2fe05ee6b5832cda29f2ef7aaad7b7fe6a3609eb",
  "task_name": "HeadlineClassification",
  "mteb_version": "2.3.2",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.844727,
            "f1": 0.844174,
            "f1_weighted": 0.844146,
            "precision": 0.854207,
            "precision_weighted": 0.854212,
            "recall": 0.844786,
            "recall_weighted": 0.844727,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.862793,
            "f1": 0.862309,
            "f1_weighted": 0.862299,
            "precision": 0.865654,
            "precision_weighted": 0.865636,
            "recall": 0.862797,
            "recall_weighted": 0.862793,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.851562,
            "f1": 0.851577,
            "f1_weighted": 0.851554,
            "precision": 0.859292,
            "precision_weighted": 0.859289,
            "recall": 0.851612,
            "recall_weighted": 0.851562,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.842773,
            "f1": 0.842312,
            "f1_weighted": 0.842282,
            "precision": 0.851637,
            "precision_weighted": 0.851641,
            "recall": 0.842838,
            "recall_weighted": 0.842773,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.84668,
            "f1": 0.846413,
            "f1_weighted": 0.84639,
            "precision": 0.857501,
            "precision_weighted": 0.857513,
            "recall": 0.846736,
            "recall_weighted": 0.84668,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.850098,
            "f1": 0.849956,
            "f1_weighted": 0.849927,
            "precision": 0.85724,
            "precision_weighted": 0.857233,
            "recall": 0.850151,
            "recall_weighted": 0.850098,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.850586,
            "f1": 0.850117,
            "f1_weighted": 0.850099,
            "precision": 0.85761,
            "precision_weighted": 0.857613,
            "recall": 0.85062,
            "recall_weighted": 0.850586,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.862793,
            "f1": 0.862375,
            "f1_weighted": 0.862356,
            "precision": 0.866842,
            "precision_weighted": 0.866807,
            "recall": 0.8628,
            "recall_weighted": 0.862793,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.861816,
            "f1": 0.861804,
            "f1_weighted": 0.861788,
            "precision": 0.865143,
            "precision_weighted": 0.865129,
            "recall": 0.861837,
            "recall_weighted": 0.861816,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.867676,
            "f1": 0.86727,
            "f1_weighted": 0.86725,
            "precision": 0.869187,
            "precision_weighted": 0.869169,
            "recall": 0.867699,
            "recall_weighted": 0.867676,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.85415,
        "f1": 0.853831,
        "f1_weighted": 0.853809,
        "precision": 0.860431,
        "precision_weighted": 0.860424,
        "recall": 0.854188,
        "recall_weighted": 0.85415,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.85415,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 121.41083359718323,
  "kg_co2_emissions": null
}
