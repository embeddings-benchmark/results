{
  "dataset_revision": "de7ba3406ee55ea2cc52a0a41408fa6aede6d3c6",
  "task_name": "LccSentimentClassification",
  "mteb_version": "2.1.10",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.766667,
            "f1": 0.745862,
            "f1_weighted": 0.763197,
            "precision": 0.758055,
            "precision_weighted": 0.770172,
            "recall": 0.747499,
            "recall_weighted": 0.766667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.78,
            "f1": 0.769188,
            "f1_weighted": 0.780949,
            "precision": 0.761013,
            "precision_weighted": 0.785333,
            "recall": 0.782717,
            "recall_weighted": 0.78,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.78,
            "f1": 0.774846,
            "f1_weighted": 0.782285,
            "precision": 0.763004,
            "precision_weighted": 0.801765,
            "recall": 0.810914,
            "recall_weighted": 0.78,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.733333,
            "f1": 0.73081,
            "f1_weighted": 0.735486,
            "precision": 0.718023,
            "precision_weighted": 0.770037,
            "recall": 0.782802,
            "recall_weighted": 0.733333,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.773333,
            "f1": 0.759529,
            "f1_weighted": 0.771929,
            "precision": 0.763262,
            "precision_weighted": 0.779027,
            "recall": 0.768466,
            "recall_weighted": 0.773333,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.733333,
            "f1": 0.716138,
            "f1_weighted": 0.734276,
            "precision": 0.712761,
            "precision_weighted": 0.738807,
            "recall": 0.725234,
            "recall_weighted": 0.733333,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.76,
            "f1": 0.743607,
            "f1_weighted": 0.761317,
            "precision": 0.741719,
            "precision_weighted": 0.767837,
            "recall": 0.753718,
            "recall_weighted": 0.76,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.766667,
            "f1": 0.750523,
            "f1_weighted": 0.767354,
            "precision": 0.746122,
            "precision_weighted": 0.768477,
            "recall": 0.75555,
            "recall_weighted": 0.766667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.806667,
            "f1": 0.800065,
            "f1_weighted": 0.81062,
            "precision": 0.791582,
            "precision_weighted": 0.836434,
            "recall": 0.839398,
            "recall_weighted": 0.806667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.766667,
            "f1": 0.752274,
            "f1_weighted": 0.768183,
            "precision": 0.755531,
            "precision_weighted": 0.78252,
            "recall": 0.768981,
            "recall_weighted": 0.766667,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.766667,
        "f1": 0.754284,
        "f1_weighted": 0.767559,
        "precision": 0.751107,
        "precision_weighted": 0.780041,
        "recall": 0.773528,
        "recall_weighted": 0.766667,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.766667,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 69.43595767021179,
  "kg_co2_emissions": null
}