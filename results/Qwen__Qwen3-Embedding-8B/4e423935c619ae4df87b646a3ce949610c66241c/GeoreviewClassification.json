{
  "dataset_revision": "fb29d6137c54550329411023aee45a8ff29f0c62",
  "task_name": "GeoreviewClassification",
  "mteb_version": "2.3.2",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.57666,
            "f1": 0.544328,
            "f1_weighted": 0.544264,
            "precision": 0.542932,
            "precision_weighted": 0.542919,
            "recall": 0.576763,
            "recall_weighted": 0.57666,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.591797,
            "f1": 0.569557,
            "f1_weighted": 0.569457,
            "precision": 0.561721,
            "precision_weighted": 0.56166,
            "recall": 0.59193,
            "recall_weighted": 0.591797,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.588867,
            "f1": 0.573615,
            "f1_weighted": 0.573565,
            "precision": 0.568608,
            "precision_weighted": 0.568569,
            "recall": 0.58892,
            "recall_weighted": 0.588867,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.60791,
            "f1": 0.599488,
            "f1_weighted": 0.599462,
            "precision": 0.606048,
            "precision_weighted": 0.606018,
            "recall": 0.607946,
            "recall_weighted": 0.60791,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.610352,
            "f1": 0.60243,
            "f1_weighted": 0.602387,
            "precision": 0.60174,
            "precision_weighted": 0.601704,
            "recall": 0.610404,
            "recall_weighted": 0.610352,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.566895,
            "f1": 0.562722,
            "f1_weighted": 0.562743,
            "precision": 0.581727,
            "precision_weighted": 0.58165,
            "recall": 0.566792,
            "recall_weighted": 0.566895,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.580078,
            "f1": 0.562409,
            "f1_weighted": 0.562386,
            "precision": 0.579387,
            "precision_weighted": 0.579364,
            "recall": 0.580089,
            "recall_weighted": 0.580078,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.597168,
            "f1": 0.574611,
            "f1_weighted": 0.574544,
            "precision": 0.578142,
            "precision_weighted": 0.578109,
            "recall": 0.597246,
            "recall_weighted": 0.597168,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.591797,
            "f1": 0.584369,
            "f1_weighted": 0.584335,
            "precision": 0.581376,
            "precision_weighted": 0.581321,
            "recall": 0.59181,
            "recall_weighted": 0.591797,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.597168,
            "f1": 0.580671,
            "f1_weighted": 0.58066,
            "precision": 0.590292,
            "precision_weighted": 0.590242,
            "recall": 0.597154,
            "recall_weighted": 0.597168,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.590869,
        "f1": 0.57542,
        "f1_weighted": 0.57538,
        "precision": 0.579197,
        "precision_weighted": 0.579156,
        "recall": 0.590905,
        "recall_weighted": 0.590869,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.590869,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 515.094625711441,
  "kg_co2_emissions": null
}
