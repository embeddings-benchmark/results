{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 5073.653623342514,
  "kg_co2_emissions": 0.3415909532418471,
  "mteb_version": "1.18.0",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.0736933743534867,
        "map": 0.09067193132734602,
        "mrr": 0.0736933743534867,
        "nAUC_map_diff1": 0.1377911241514007,
        "nAUC_map_max": 0.20334322830695614,
        "nAUC_map_std": 0.30213131246678504,
        "nAUC_mrr_diff1": 0.1335936863794922,
        "nAUC_mrr_max": 0.19844388190197754,
        "nAUC_mrr_std": 0.27164033244253444
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.10539693840649902,
        "map": 0.1232781683115859,
        "mrr": 0.10539693840649902,
        "nAUC_map_diff1": 0.04390699257272313,
        "nAUC_map_max": 0.052399577219820276,
        "nAUC_map_std": -0.0392255071971488,
        "nAUC_mrr_diff1": 0.039489009677322724,
        "nAUC_mrr_max": 0.05330632835977423,
        "nAUC_mrr_std": -0.04796067483328915
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.11315236108351544,
        "map": 0.12841287102984317,
        "mrr": 0.11315236108351544,
        "nAUC_map_diff1": 0.07113843713128583,
        "nAUC_map_max": 0.12744950111258224,
        "nAUC_map_std": 0.07553213975561969,
        "nAUC_mrr_diff1": 0.07401936725226788,
        "nAUC_mrr_max": 0.12429575537216109,
        "nAUC_mrr_std": 0.06356286606872184
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.10562416485406292,
        "map": 0.12456249282213791,
        "mrr": 0.10562416485406292,
        "nAUC_map_diff1": 0.04679719904009665,
        "nAUC_map_max": 0.08063734179605021,
        "nAUC_map_std": 0.1319950345494264,
        "nAUC_mrr_diff1": 0.04865766262603925,
        "nAUC_mrr_max": 0.08281474396974728,
        "nAUC_mrr_std": 0.11345211354635548
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09286900722969424,
        "map": 0.1078019848181688,
        "mrr": 0.09286900722969424,
        "nAUC_map_diff1": -0.018030479733046022,
        "nAUC_map_max": 0.05884883283275807,
        "nAUC_map_std": 0.09722688866988786,
        "nAUC_mrr_diff1": -0.00903138710640092,
        "nAUC_mrr_max": 0.027398509758785525,
        "nAUC_mrr_std": 0.08058663277411224
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.14074399424130402,
        "map": 0.15589344165580038,
        "mrr": 0.14074399424130402,
        "nAUC_map_diff1": 0.15120078382585264,
        "nAUC_map_max": 0.03636135018358582,
        "nAUC_map_std": -0.03389850008727549,
        "nAUC_mrr_diff1": 0.1548836166195283,
        "nAUC_mrr_max": 0.034799676655584665,
        "nAUC_mrr_std": -0.041320506413122
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}