{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 22.649623155593872,
  "kg_co2_emissions": 0.000787235700968118,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.292529296875,
        "f1": 0.28413936171643195,
        "f1_weighted": 0.28413894974443754,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.292529296875,
        "scores_per_experiment": [
          {
            "accuracy": 0.279296875,
            "f1": 0.28019770153611356,
            "f1_weighted": 0.28020874028659626
          },
          {
            "accuracy": 0.3017578125,
            "f1": 0.2955458436251413,
            "f1_weighted": 0.2955490379568263
          },
          {
            "accuracy": 0.29931640625,
            "f1": 0.2871343104230172,
            "f1_weighted": 0.28713854397703104
          },
          {
            "accuracy": 0.31689453125,
            "f1": 0.3130902531303231,
            "f1_weighted": 0.31308506221912635
          },
          {
            "accuracy": 0.30126953125,
            "f1": 0.295007857760506,
            "f1_weighted": 0.29498536135909853
          },
          {
            "accuracy": 0.25439453125,
            "f1": 0.2319445376513965,
            "f1_weighted": 0.2319224824965074
          },
          {
            "accuracy": 0.23583984375,
            "f1": 0.23241124694367765,
            "f1_weighted": 0.2323673465559192
          },
          {
            "accuracy": 0.322265625,
            "f1": 0.31800589874874924,
            "f1_weighted": 0.318037222915798
          },
          {
            "accuracy": 0.3193359375,
            "f1": 0.3119586413264511,
            "f1_weighted": 0.311978478606827
          },
          {
            "accuracy": 0.294921875,
            "f1": 0.27609732601894404,
            "f1_weighted": 0.2761172210706455
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.29306640625,
        "f1": 0.2845686814616686,
        "f1_weighted": 0.28455871522248344,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.29306640625,
        "scores_per_experiment": [
          {
            "accuracy": 0.30810546875,
            "f1": 0.3084370680803012,
            "f1_weighted": 0.30843768966736473
          },
          {
            "accuracy": 0.2763671875,
            "f1": 0.2708196317365528,
            "f1_weighted": 0.27081621998558525
          },
          {
            "accuracy": 0.2919921875,
            "f1": 0.2810615018531486,
            "f1_weighted": 0.28106735309749253
          },
          {
            "accuracy": 0.31396484375,
            "f1": 0.3079153026134288,
            "f1_weighted": 0.3079137555674331
          },
          {
            "accuracy": 0.2890625,
            "f1": 0.2783895159585609,
            "f1_weighted": 0.2783388666091178
          },
          {
            "accuracy": 0.28125,
            "f1": 0.2635703682895745,
            "f1_weighted": 0.2635672651560217
          },
          {
            "accuracy": 0.23974609375,
            "f1": 0.23292227828868697,
            "f1_weighted": 0.2328527584482229
          },
          {
            "accuracy": 0.32275390625,
            "f1": 0.31913835152129827,
            "f1_weighted": 0.31914648868862705
          },
          {
            "accuracy": 0.3193359375,
            "f1": 0.3144636901853859,
            "f1_weighted": 0.3144791783827883
          },
          {
            "accuracy": 0.2880859375,
            "f1": 0.268969106089748,
            "f1_weighted": 0.26896757662218035
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}