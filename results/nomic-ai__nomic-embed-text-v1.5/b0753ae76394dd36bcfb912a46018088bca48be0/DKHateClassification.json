{
  "dataset_revision": "59d12749a3c91a186063c7d729ec392fda94681c",
  "task_name": "DKHateClassification",
  "mteb_version": "1.34.21",
  "scores": {
    "test": [
      {
        "accuracy": 0.590578,
        "f1": 0.488444,
        "f1_weighted": 0.657494,
        "ap": 0.895575,
        "ap_weighted": 0.895575,
        "scores_per_experiment": [
          {
            "accuracy": 0.629179,
            "f1": 0.512415,
            "f1_weighted": 0.69155,
            "ap": 0.898068,
            "ap_weighted": 0.898068
          },
          {
            "accuracy": 0.507599,
            "f1": 0.454093,
            "f1_weighted": 0.582403,
            "ap": 0.904499,
            "ap_weighted": 0.904499
          },
          {
            "accuracy": 0.604863,
            "f1": 0.502906,
            "f1_weighted": 0.671923,
            "ap": 0.899794,
            "ap_weighted": 0.899794
          },
          {
            "accuracy": 0.544073,
            "f1": 0.458169,
            "f1_weighted": 0.620141,
            "ap": 0.889256,
            "ap_weighted": 0.889256
          },
          {
            "accuracy": 0.6231,
            "f1": 0.504422,
            "f1_weighted": 0.686494,
            "ap": 0.894799,
            "ap_weighted": 0.894799
          },
          {
            "accuracy": 0.550152,
            "f1": 0.434077,
            "f1_weighted": 0.626496,
            "ap": 0.871393,
            "ap_weighted": 0.871393
          },
          {
            "accuracy": 0.534954,
            "f1": 0.460812,
            "f1_weighted": 0.61092,
            "ap": 0.895418,
            "ap_weighted": 0.895418
          },
          {
            "accuracy": 0.583587,
            "f1": 0.49468,
            "f1_weighted": 0.65381,
            "ap": 0.901968,
            "ap_weighted": 0.901968
          },
          {
            "accuracy": 0.735562,
            "f1": 0.565033,
            "f1_weighted": 0.769503,
            "ap": 0.899882,
            "ap_weighted": 0.899882
          },
          {
            "accuracy": 0.592705,
            "f1": 0.497836,
            "f1_weighted": 0.661701,
            "ap": 0.90067,
            "ap_weighted": 0.90067
          }
        ],
        "main_score": 0.590578,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 32.87482213973999,
  "kg_co2_emissions": null
}