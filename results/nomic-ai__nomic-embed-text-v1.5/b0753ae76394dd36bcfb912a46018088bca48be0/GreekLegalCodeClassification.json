{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "evaluation_time": 1770.526032447815,
  "kg_co2_emissions": 0.1511842128589458,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.026953125,
        "f1": 0.01141144127179289,
        "f1_weighted": 0.014307868124503784,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ],
        "main_score": 0.026953125,
        "scores_per_experiment": [
          {
            "accuracy": 0.0263671875,
            "f1": 0.00973057200909655,
            "f1_weighted": 0.01041779148754959
          },
          {
            "accuracy": 0.02783203125,
            "f1": 0.009954995617015109,
            "f1_weighted": 0.015185283871634678
          },
          {
            "accuracy": 0.0263671875,
            "f1": 0.011919195508302822,
            "f1_weighted": 0.01457520328866558
          },
          {
            "accuracy": 0.0263671875,
            "f1": 0.013923433176663164,
            "f1_weighted": 0.013518060201826724
          },
          {
            "accuracy": 0.02734375,
            "f1": 0.01571306905153299,
            "f1_weighted": 0.017169609435276345
          },
          {
            "accuracy": 0.0283203125,
            "f1": 0.011413271210801216,
            "f1_weighted": 0.016645668001205507
          },
          {
            "accuracy": 0.02734375,
            "f1": 0.009345316740724256,
            "f1_weighted": 0.016608803161007518
          },
          {
            "accuracy": 0.0244140625,
            "f1": 0.01082503892455447,
            "f1_weighted": 0.011472679786501856
          },
          {
            "accuracy": 0.02734375,
            "f1": 0.010903558147491835,
            "f1_weighted": 0.01381967322210469
          },
          {
            "accuracy": 0.02783203125,
            "f1": 0.010385962331746488,
            "f1_weighted": 0.01366590878926532
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.02646484375,
        "f1": 0.013107634651510084,
        "f1_weighted": 0.016845603641930308,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ],
        "main_score": 0.02646484375,
        "scores_per_experiment": [
          {
            "accuracy": 0.0244140625,
            "f1": 0.01297693463392494,
            "f1_weighted": 0.010479596063637938
          },
          {
            "accuracy": 0.0244140625,
            "f1": 0.014070254878274256,
            "f1_weighted": 0.015274645167004556
          },
          {
            "accuracy": 0.02587890625,
            "f1": 0.013785312058606542,
            "f1_weighted": 0.01813901004919887
          },
          {
            "accuracy": 0.021484375,
            "f1": 0.007832274990012584,
            "f1_weighted": 0.012032997591054488
          },
          {
            "accuracy": 0.0322265625,
            "f1": 0.01732755386734431,
            "f1_weighted": 0.02225549216613753
          },
          {
            "accuracy": 0.029296875,
            "f1": 0.012508998927854252,
            "f1_weighted": 0.021732440402761722
          },
          {
            "accuracy": 0.0234375,
            "f1": 0.012736253361540889,
            "f1_weighted": 0.018156750958634137
          },
          {
            "accuracy": 0.02978515625,
            "f1": 0.01353218488127631,
            "f1_weighted": 0.01845330941175475
          },
          {
            "accuracy": 0.03076171875,
            "f1": 0.015491019526557251,
            "f1_weighted": 0.019876967299845155
          },
          {
            "accuracy": 0.02294921875,
            "f1": 0.010815559389709512,
            "f1_weighted": 0.012054827309273968
          }
        ]
      }
    ]
  },
  "task_name": "GreekLegalCodeClassification"
}