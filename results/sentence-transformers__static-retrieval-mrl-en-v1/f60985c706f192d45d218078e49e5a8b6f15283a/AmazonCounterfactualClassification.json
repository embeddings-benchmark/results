{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "task_name": "AmazonCounterfactualClassification",
  "mteb_version": "1.34.26",
  "scores": {
    "test": [
      {
        "accuracy": 0.66791,
        "f1": 0.610209,
        "f1_weighted": 0.700201,
        "ap": 0.301203,
        "ap_weighted": 0.301203,
        "scores_per_experiment": [
          {
            "accuracy": 0.653731,
            "f1": 0.608381,
            "f1_weighted": 0.689535,
            "ap": 0.309418,
            "ap_weighted": 0.309418
          },
          {
            "accuracy": 0.747761,
            "f1": 0.663682,
            "f1_weighted": 0.766083,
            "ap": 0.329425,
            "ap_weighted": 0.329425
          },
          {
            "accuracy": 0.707463,
            "f1": 0.635319,
            "f1_weighted": 0.734093,
            "ap": 0.309468,
            "ap_weighted": 0.309468
          },
          {
            "accuracy": 0.69403,
            "f1": 0.636266,
            "f1_weighted": 0.724534,
            "ap": 0.322823,
            "ap_weighted": 0.322823
          },
          {
            "accuracy": 0.653731,
            "f1": 0.606557,
            "f1_weighted": 0.689519,
            "ap": 0.305585,
            "ap_weighted": 0.305585
          },
          {
            "accuracy": 0.58806,
            "f1": 0.545052,
            "f1_weighted": 0.630232,
            "ap": 0.257778,
            "ap_weighted": 0.257778
          },
          {
            "accuracy": 0.665672,
            "f1": 0.60199,
            "f1_weighted": 0.698938,
            "ap": 0.285807,
            "ap_weighted": 0.285807
          },
          {
            "accuracy": 0.741791,
            "f1": 0.664863,
            "f1_weighted": 0.762641,
            "ap": 0.335347,
            "ap_weighted": 0.335347
          },
          {
            "accuracy": 0.632836,
            "f1": 0.591123,
            "f1_weighted": 0.67065,
            "ap": 0.297761,
            "ap_weighted": 0.297761
          },
          {
            "accuracy": 0.59403,
            "f1": 0.548862,
            "f1_weighted": 0.635789,
            "ap": 0.258621,
            "ap_weighted": 0.258621
          }
        ],
        "main_score": 0.66791,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 2.4724466800689697,
  "kg_co2_emissions": null
}