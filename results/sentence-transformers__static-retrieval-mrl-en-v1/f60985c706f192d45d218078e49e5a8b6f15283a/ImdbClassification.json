{
  "dataset_revision": "3d86128a09e091d6018b6d26cad27f2739fc2db7",
  "task_name": "ImdbClassification",
  "mteb_version": "1.34.26",
  "scores": {
    "test": [
      {
        "accuracy": 0.609924,
        "f1": 0.596089,
        "f1_weighted": 0.596089,
        "ap": 0.569237,
        "ap_weighted": 0.569237,
        "scores_per_experiment": [
          {
            "accuracy": 0.66392,
            "f1": 0.659551,
            "f1_weighted": 0.659551,
            "ap": 0.616701,
            "ap_weighted": 0.616701
          },
          {
            "accuracy": 0.63932,
            "f1": 0.638659,
            "f1_weighted": 0.638659,
            "ap": 0.587541,
            "ap_weighted": 0.587541
          },
          {
            "accuracy": 0.55764,
            "f1": 0.557448,
            "f1_weighted": 0.557448,
            "ap": 0.532009,
            "ap_weighted": 0.532009
          },
          {
            "accuracy": 0.67296,
            "f1": 0.667317,
            "f1_weighted": 0.667317,
            "ap": 0.610213,
            "ap_weighted": 0.610213
          },
          {
            "accuracy": 0.59124,
            "f1": 0.590375,
            "f1_weighted": 0.590375,
            "ap": 0.553244,
            "ap_weighted": 0.553244
          },
          {
            "accuracy": 0.61904,
            "f1": 0.597321,
            "f1_weighted": 0.597321,
            "ap": 0.569196,
            "ap_weighted": 0.569196
          },
          {
            "accuracy": 0.61776,
            "f1": 0.61509,
            "f1_weighted": 0.61509,
            "ap": 0.575519,
            "ap_weighted": 0.575519
          },
          {
            "accuracy": 0.57608,
            "f1": 0.533908,
            "f1_weighted": 0.533908,
            "ap": 0.541654,
            "ap_weighted": 0.541654
          },
          {
            "accuracy": 0.56796,
            "f1": 0.518603,
            "f1_weighted": 0.518603,
            "ap": 0.546824,
            "ap_weighted": 0.546824
          },
          {
            "accuracy": 0.59332,
            "f1": 0.582619,
            "f1_weighted": 0.582619,
            "ap": 0.559471,
            "ap_weighted": 0.559471
          }
        ],
        "main_score": 0.609924,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 26.9999737739563,
  "kg_co2_emissions": null
}