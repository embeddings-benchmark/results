{
  "dataset_revision": "5a79d6472db143690c7ce6e974995d3610eee7f0",
  "task_name": "SanskritShlokasClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "train": [
      {
        "accuracy": 0.71201,
        "f1": 0.704384,
        "f1_weighted": 0.702426,
        "scores_per_experiment": [
          {
            "accuracy": 0.704961,
            "f1": 0.705646,
            "f1_weighted": 0.705272
          },
          {
            "accuracy": 0.718016,
            "f1": 0.712533,
            "f1_weighted": 0.709745
          },
          {
            "accuracy": 0.678851,
            "f1": 0.668564,
            "f1_weighted": 0.665048
          },
          {
            "accuracy": 0.644909,
            "f1": 0.603758,
            "f1_weighted": 0.595665
          },
          {
            "accuracy": 0.744125,
            "f1": 0.737194,
            "f1_weighted": 0.734454
          },
          {
            "accuracy": 0.744125,
            "f1": 0.740644,
            "f1_weighted": 0.741513
          },
          {
            "accuracy": 0.744125,
            "f1": 0.741949,
            "f1_weighted": 0.740405
          },
          {
            "accuracy": 0.718016,
            "f1": 0.711349,
            "f1_weighted": 0.709827
          },
          {
            "accuracy": 0.691906,
            "f1": 0.693162,
            "f1_weighted": 0.693121
          },
          {
            "accuracy": 0.73107,
            "f1": 0.729041,
            "f1_weighted": 0.729209
          }
        ],
        "main_score": 0.71201,
        "hf_subset": "default",
        "languages": [
          "san-Deva"
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.676042,
        "f1": 0.680313,
        "f1_weighted": 0.66671,
        "scores_per_experiment": [
          {
            "accuracy": 0.697917,
            "f1": 0.705801,
            "f1_weighted": 0.695047
          },
          {
            "accuracy": 0.677083,
            "f1": 0.683158,
            "f1_weighted": 0.667594
          },
          {
            "accuracy": 0.677083,
            "f1": 0.676849,
            "f1_weighted": 0.663522
          },
          {
            "accuracy": 0.625,
            "f1": 0.599155,
            "f1_weighted": 0.570693
          },
          {
            "accuracy": 0.75,
            "f1": 0.754595,
            "f1_weighted": 0.745371
          },
          {
            "accuracy": 0.708333,
            "f1": 0.719833,
            "f1_weighted": 0.710293
          },
          {
            "accuracy": 0.645833,
            "f1": 0.658973,
            "f1_weighted": 0.641222
          },
          {
            "accuracy": 0.666667,
            "f1": 0.66811,
            "f1_weighted": 0.652387
          },
          {
            "accuracy": 0.635417,
            "f1": 0.652146,
            "f1_weighted": 0.64261
          },
          {
            "accuracy": 0.677083,
            "f1": 0.684513,
            "f1_weighted": 0.678359
          }
        ],
        "main_score": 0.676042,
        "hf_subset": "default",
        "languages": [
          "san-Deva"
        ]
      }
    ]
  },
  "evaluation_time": 16.56412363052368,
  "kg_co2_emissions": 0.0005572032139064605
}