{
  "dataset_revision": "f6d2c31f4dc6b88f468552750bfec05b4b41b05a",
  "evaluation_time": 61.013256311416626,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.697021484375,
        "f1": 0.6911599239475726,
        "f1_weighted": 0.6911660321330073,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.697021484375,
        "scores_per_experiment": [
          {
            "accuracy": 0.73291015625,
            "f1": 0.7290164264693809,
            "f1_weighted": 0.7290120805104762
          },
          {
            "accuracy": 0.65771484375,
            "f1": 0.6437655614289288,
            "f1_weighted": 0.6437634798310743
          },
          {
            "accuracy": 0.69384765625,
            "f1": 0.6927153850703961,
            "f1_weighted": 0.6927247308982949
          },
          {
            "accuracy": 0.7275390625,
            "f1": 0.7273808292316185,
            "f1_weighted": 0.7273945250229659
          },
          {
            "accuracy": 0.70751953125,
            "f1": 0.7128382628243878,
            "f1_weighted": 0.7128495584490022
          },
          {
            "accuracy": 0.6337890625,
            "f1": 0.6238938912686618,
            "f1_weighted": 0.6239156811117212
          },
          {
            "accuracy": 0.69189453125,
            "f1": 0.677768984083424,
            "f1_weighted": 0.6777672089675827
          },
          {
            "accuracy": 0.689453125,
            "f1": 0.6822322707709277,
            "f1_weighted": 0.6822347448512209
          },
          {
            "accuracy": 0.70166015625,
            "f1": 0.6892500369750835,
            "f1_weighted": 0.6892298177422647
          },
          {
            "accuracy": 0.73388671875,
            "f1": 0.7327375913529174,
            "f1_weighted": 0.7327684939454711
          }
        ]
      }
    ]
  },
  "task_name": "RuReviewsClassification"
}