{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 10.240617036819458,
  "kg_co2_emissions": null,
  "mteb_version": "1.18.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.39453125,
        "f1": 0.3721769497695471,
        "f1_weighted": 0.37215175146677154,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.39453125,
        "scores_per_experiment": [
          {
            "accuracy": 0.40966796875,
            "f1": 0.3869097471446986,
            "f1_weighted": 0.38687585886615694
          },
          {
            "accuracy": 0.37646484375,
            "f1": 0.35196788048953154,
            "f1_weighted": 0.35196877733023596
          },
          {
            "accuracy": 0.3857421875,
            "f1": 0.3736302482939824,
            "f1_weighted": 0.3736018723628032
          },
          {
            "accuracy": 0.39306640625,
            "f1": 0.37367631089636644,
            "f1_weighted": 0.3736354799773757
          },
          {
            "accuracy": 0.38720703125,
            "f1": 0.35763451930862145,
            "f1_weighted": 0.3575829546830991
          },
          {
            "accuracy": 0.4072265625,
            "f1": 0.3776779896940723,
            "f1_weighted": 0.3776351023012192
          },
          {
            "accuracy": 0.38525390625,
            "f1": 0.36639950668710963,
            "f1_weighted": 0.36640221302849685
          },
          {
            "accuracy": 0.40673828125,
            "f1": 0.3850698598994804,
            "f1_weighted": 0.3850656407142965
          },
          {
            "accuracy": 0.4287109375,
            "f1": 0.40685801691649653,
            "f1_weighted": 0.40679830549229645
          },
          {
            "accuracy": 0.365234375,
            "f1": 0.3419454183651117,
            "f1_weighted": 0.34195130991173567
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.375048828125,
        "f1": 0.35343444615015046,
        "f1_weighted": 0.35340801263537863,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.375048828125,
        "scores_per_experiment": [
          {
            "accuracy": 0.3974609375,
            "f1": 0.37768935014724797,
            "f1_weighted": 0.3776526995793823
          },
          {
            "accuracy": 0.3603515625,
            "f1": 0.33755367423244165,
            "f1_weighted": 0.33754957667053637
          },
          {
            "accuracy": 0.353515625,
            "f1": 0.34445167344085226,
            "f1_weighted": 0.34443011154447273
          },
          {
            "accuracy": 0.3955078125,
            "f1": 0.3787354108275659,
            "f1_weighted": 0.3786945546113438
          },
          {
            "accuracy": 0.359375,
            "f1": 0.331584606802685,
            "f1_weighted": 0.33154373398798215
          },
          {
            "accuracy": 0.38916015625,
            "f1": 0.36185903165137556,
            "f1_weighted": 0.3618178234607547
          },
          {
            "accuracy": 0.36328125,
            "f1": 0.34015343289128097,
            "f1_weighted": 0.34016514321371133
          },
          {
            "accuracy": 0.3916015625,
            "f1": 0.3698868269507015,
            "f1_weighted": 0.3698638531296065
          },
          {
            "accuracy": 0.396484375,
            "f1": 0.37282687450902724,
            "f1_weighted": 0.3727589754962555
          },
          {
            "accuracy": 0.34375,
            "f1": 0.3196035800483266,
            "f1_weighted": 0.31960365465974117
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}