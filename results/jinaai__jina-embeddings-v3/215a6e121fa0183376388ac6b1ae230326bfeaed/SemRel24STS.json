{
  "dataset_revision": "ef5c383d1b87eb8feccde3dfb7f95e42b1b050dd",
  "evaluation_time": 10.337989091873169,
  "kg_co2_emissions": null,
  "mteb_version": "1.18.2",
  "scores": {
    "test": [
      {
        "cosine_pearson": 0.8464358709522742,
        "cosine_spearman": 0.824955916566814,
        "euclidean_pearson": 0.8290588642677003,
        "euclidean_spearman": 0.8251235467641678,
        "hf_subset": "afr",
        "languages": [
          "afr-Latn"
        ],
        "main_score": 0.824955916566814,
        "manhattan_pearson": 0.8269334486047517,
        "manhattan_spearman": 0.8243142072175691,
        "pearson": 0.8464358709522742,
        "spearman": 0.824955916566814
      },
      {
        "cosine_pearson": 0.8061532489728991,
        "cosine_spearman": 0.7843796896145816,
        "euclidean_pearson": 0.8033037609737169,
        "euclidean_spearman": 0.7849090505660836,
        "hf_subset": "amh",
        "languages": [
          "amh-Ethi"
        ],
        "main_score": 0.7843796896145816,
        "manhattan_pearson": 0.8011139907551552,
        "manhattan_spearman": 0.7822442403340321,
        "pearson": 0.8061532489728991,
        "spearman": 0.7843796896145816
      },
      {
        "cosine_pearson": 0.6502671915476228,
        "cosine_spearman": 0.6650262687629951,
        "euclidean_pearson": 0.6392623809401383,
        "euclidean_spearman": 0.6639897772118093,
        "hf_subset": "arb",
        "languages": [
          "arb-Arab"
        ],
        "main_score": 0.6650262687629951,
        "manhattan_pearson": 0.6382206343085315,
        "manhattan_spearman": 0.6627439699872396,
        "pearson": 0.6502671915476228,
        "spearman": 0.6650262687629951
      },
      {
        "cosine_pearson": 0.5320215584093182,
        "cosine_spearman": 0.48047682930545943,
        "euclidean_pearson": 0.5391455892099697,
        "euclidean_spearman": 0.4802201558936406,
        "hf_subset": "arq",
        "languages": [
          "arq-Arab"
        ],
        "main_score": 0.48047682930545943,
        "manhattan_pearson": 0.5369917341646824,
        "manhattan_spearman": 0.4759147133548368,
        "pearson": 0.5320215584093182,
        "spearman": 0.48047682930545943
      },
      {
        "cosine_pearson": 0.28592203316687964,
        "cosine_spearman": 0.2310341635430769,
        "euclidean_pearson": 0.2721302026097564,
        "euclidean_spearman": 0.23104914956134187,
        "hf_subset": "ary",
        "languages": [
          "ary-Arab"
        ],
        "main_score": 0.2310341635430769,
        "manhattan_pearson": 0.27619060904592085,
        "manhattan_spearman": 0.23351244719573433,
        "pearson": 0.28592203316687964,
        "spearman": 0.2310341635430769
      },
      {
        "cosine_pearson": 0.8385045074016273,
        "cosine_spearman": 0.8282185252870888,
        "euclidean_pearson": 0.8349311743424724,
        "euclidean_spearman": 0.8282443757367575,
        "hf_subset": "eng",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8282185252870888,
        "manhattan_pearson": 0.8345373255167827,
        "manhattan_spearman": 0.8277644516896036,
        "pearson": 0.8385045074016273,
        "spearman": 0.8282185252870888
      },
      {
        "cosine_pearson": 0.47861681721802496,
        "cosine_spearman": 0.45383057984462094,
        "euclidean_pearson": 0.48698741239054466,
        "euclidean_spearman": 0.4542189555181357,
        "hf_subset": "hau",
        "languages": [
          "hau-Latn"
        ],
        "main_score": 0.45383057984462094,
        "manhattan_pearson": 0.48781218724035436,
        "manhattan_spearman": 0.45662777253502057,
        "pearson": 0.47861681721802496,
        "spearman": 0.45383057984462094
      },
      {
        "cosine_pearson": 0.8145162493227158,
        "cosine_spearman": 0.8148996545044047,
        "euclidean_pearson": 0.7920363215209291,
        "euclidean_spearman": 0.8148536076737157,
        "hf_subset": "hin",
        "languages": [
          "hin-Deva"
        ],
        "main_score": 0.8148996545044047,
        "manhattan_pearson": 0.7893668105394067,
        "manhattan_spearman": 0.8133906482585205,
        "pearson": 0.8145162493227158,
        "spearman": 0.8148996545044047
      },
      {
        "cosine_pearson": 0.44961132585270536,
        "cosine_spearman": 0.45706201378866596,
        "euclidean_pearson": 0.47189142871651063,
        "euclidean_spearman": 0.45705858550545736,
        "hf_subset": "ind",
        "languages": [
          "ind-Latn"
        ],
        "main_score": 0.45706201378866596,
        "manhattan_pearson": 0.47573754884598946,
        "manhattan_spearman": 0.4606663261542594,
        "pearson": 0.44961132585270536,
        "spearman": 0.45706201378866596
      },
      {
        "cosine_pearson": 0.5308421413388202,
        "cosine_spearman": 0.5488609855775077,
        "euclidean_pearson": 0.5394554141128372,
        "euclidean_spearman": 0.5492819148093717,
        "hf_subset": "kin",
        "languages": [
          "kin-Latn"
        ],
        "main_score": 0.5488609855775077,
        "manhattan_pearson": 0.5380672808508755,
        "manhattan_spearman": 0.5454705021427545,
        "pearson": 0.5308421413388202,
        "spearman": 0.5488609855775077
      },
      {
        "cosine_pearson": 0.8436116997339786,
        "cosine_spearman": 0.8263381288798977,
        "euclidean_pearson": 0.8338019938930012,
        "euclidean_spearman": 0.8256245823603314,
        "hf_subset": "mar",
        "languages": [
          "mar-Deva"
        ],
        "main_score": 0.8263381288798977,
        "manhattan_pearson": 0.8337772871228495,
        "manhattan_spearman": 0.8296298328336367,
        "pearson": 0.8436116997339786,
        "spearman": 0.8263381288798977
      },
      {
        "cosine_pearson": 0.8533077390296027,
        "cosine_spearman": 0.8346188607668114,
        "euclidean_pearson": 0.8392142221326493,
        "euclidean_spearman": 0.8346839196485902,
        "hf_subset": "tel",
        "languages": [
          "tel-Telu"
        ],
        "main_score": 0.8346188607668114,
        "manhattan_pearson": 0.8379703359168404,
        "manhattan_spearman": 0.835240127271685,
        "pearson": 0.8533077390296027,
        "spearman": 0.8346188607668114
      }
    ]
  },
  "task_name": "SemRel24STS"
}