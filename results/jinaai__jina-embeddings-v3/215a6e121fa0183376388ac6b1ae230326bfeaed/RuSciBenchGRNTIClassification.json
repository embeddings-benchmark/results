{
  "dataset_revision": "673a610d6d3dd91a547a0d57ae1b56f37ebbf6a1",
  "evaluation_time": 129.65155601501465,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.599267578125,
        "f1": 0.5856482549031995,
        "f1_weighted": 0.5857311704118908,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.599267578125,
        "scores_per_experiment": [
          {
            "accuracy": 0.6083984375,
            "f1": 0.5935987042627385,
            "f1_weighted": 0.5936956064360758
          },
          {
            "accuracy": 0.59423828125,
            "f1": 0.5776711983468248,
            "f1_weighted": 0.5777629139270739
          },
          {
            "accuracy": 0.57568359375,
            "f1": 0.557597317321844,
            "f1_weighted": 0.5577911307203668
          },
          {
            "accuracy": 0.6181640625,
            "f1": 0.6113084919380553,
            "f1_weighted": 0.6113479046322874
          },
          {
            "accuracy": 0.61474609375,
            "f1": 0.6030225706345939,
            "f1_weighted": 0.6030545132525452
          },
          {
            "accuracy": 0.5771484375,
            "f1": 0.5608156244527799,
            "f1_weighted": 0.5608971247481399
          },
          {
            "accuracy": 0.6162109375,
            "f1": 0.602488561293025,
            "f1_weighted": 0.6026622175478387
          },
          {
            "accuracy": 0.5966796875,
            "f1": 0.5853404461004884,
            "f1_weighted": 0.5854265393807828
          },
          {
            "accuracy": 0.6025390625,
            "f1": 0.5916534198672841,
            "f1_weighted": 0.59167126609686
          },
          {
            "accuracy": 0.5888671875,
            "f1": 0.5729862148143614,
            "f1_weighted": 0.573002487376938
          }
        ]
      }
    ]
  },
  "task_name": "RuSciBenchGRNTIClassification"
}