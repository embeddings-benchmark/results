{
    "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 0.8082044384667114,
                "f1": 0.8053217664465089,
                "f1_weighted": 0.8094535087010511,
                "main_score": 0.8082044384667114
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 0.821049092131809,
                "f1": 0.8155343463694733,
                "f1_weighted": 0.8233509098770782,
                "main_score": 0.821049092131809
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 0.8258238063214526,
                "f1": 0.8227974449333072,
                "f1_weighted": 0.8281337569618209,
                "main_score": 0.8258238063214526
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 0.8397108271687962,
                "f1": 0.8356285606936076,
                "f1_weighted": 0.8410198745390771,
                "main_score": 0.8397108271687962
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 0.8471082716879623,
                "f1": 0.8409447062371401,
                "f1_weighted": 0.8473765765551342,
                "main_score": 0.8471082716879623
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 0.83093476798924,
                "f1": 0.8272656900752944,
                "f1_weighted": 0.8326606516503364,
                "main_score": 0.83093476798924
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 0.8405850706119705,
                "f1": 0.8364234048881223,
                "f1_weighted": 0.8417315768381876,
                "main_score": 0.8405850706119705
            }
        ]
    }
}