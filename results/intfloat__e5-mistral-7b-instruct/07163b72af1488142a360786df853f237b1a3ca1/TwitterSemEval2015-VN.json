{
    "dataset_revision": "9215a3c954078fd15c2bbecca914477d53944de1",
    "mteb_dataset_name": "TwitterSemEval2015-VN",
    "mteb_version": "1.38.41",
    "task_name": "TwitterSemEval2015-VN",
    "scores": {
        "test": [
            {
                "evaluation_time": 20.61,
                "hf_subset": "default",
                "dot_accuracy": 0.8571580968636655,
                "dot_ap": 0.7332129750138052,
                "dot_f1": 0.6906126046716615,
                "dot_accuracy_threshold": 0.896640956401825,
                "dot_f1_threshold": 0.8795703649520874,
                "dot_precision": 0.670517757809157,
                "dot_recall": 0.7119491140390731,
                "euclidean_accuracy": 0.8571580968636655,
                "euclidean_ap": 0.7332134716964842,
                "euclidean_f1": 0.6906126046716615,
                "euclidean_accuracy_threshold": 0.454662561416626,
                "euclidean_f1_threshold": 0.49077433347702026,
                "euclidean_precision": 0.670517757809157,
                "euclidean_recall": 0.7119491140390731,
                "manhattan_accuracy": 0.8556646042244506,
                "manhattan_ap": 0.730866386928275,
                "manhattan_f1": 0.6880000000000001,
                "manhattan_accuracy_threshold": 23.4317626953125,
                "manhattan_f1_threshold": 24.332645416259766,
                "manhattan_precision": 0.6563531353135313,
                "manhattan_recall": 0.7228532485233985,
                "max_accuracy": 0.8571580968636655,
                "max_ap": 0.7332134716964842,
                "max_f1": 0.6906126046716615,
                "main_score": 0.7332134716964842,
                "languages": [
                    "vie-Latn"
                ]
            }
        ]
    },
    "evaluation_time": null
}