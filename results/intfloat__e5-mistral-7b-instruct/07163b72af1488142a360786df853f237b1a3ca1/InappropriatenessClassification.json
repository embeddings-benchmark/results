{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "evaluation_time": 64.56821966171265,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test": [
      {
        "accuracy": 0.671875,
        "ap": 0.6178110096759174,
        "ap_weighted": 0.6178110096759174,
        "f1": 0.6688416877863311,
        "f1_weighted": 0.6688416877863311,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.671875,
        "scores_per_experiment": [
          {
            "accuracy": 0.71142578125,
            "ap": 0.6477456561352158,
            "ap_weighted": 0.6477456561352158,
            "f1": 0.7111348019929935,
            "f1_weighted": 0.7111348019929935
          },
          {
            "accuracy": 0.693359375,
            "ap": 0.630924013305009,
            "ap_weighted": 0.630924013305009,
            "f1": 0.6927120216902865,
            "f1_weighted": 0.6927120216902865
          },
          {
            "accuracy": 0.73046875,
            "ap": 0.6723673844537815,
            "ap_weighted": 0.6723673844537815,
            "f1": 0.7301352073943931,
            "f1_weighted": 0.7301352073943931
          },
          {
            "accuracy": 0.591796875,
            "ap": 0.5540235846280603,
            "ap_weighted": 0.5540235846280603,
            "f1": 0.5916562918866836,
            "f1_weighted": 0.5916562918866836
          },
          {
            "accuracy": 0.6767578125,
            "ap": 0.6225596370860043,
            "ap_weighted": 0.6225596370860043,
            "f1": 0.6761599021631124,
            "f1_weighted": 0.6761599021631124
          },
          {
            "accuracy": 0.630859375,
            "ap": 0.5796397006685575,
            "ap_weighted": 0.5796397006685575,
            "f1": 0.6269368927406942,
            "f1_weighted": 0.6269368927406942
          },
          {
            "accuracy": 0.728515625,
            "ap": 0.6712649087153518,
            "ap_weighted": 0.6712649087153518,
            "f1": 0.7280360590679327,
            "f1_weighted": 0.7280360590679327
          },
          {
            "accuracy": 0.6513671875,
            "ap": 0.5915148448043185,
            "ap_weighted": 0.5915148448043185,
            "f1": 0.6330135975545483,
            "f1_weighted": 0.6330135975545483
          },
          {
            "accuracy": 0.65869140625,
            "ap": 0.6084839556850283,
            "ap_weighted": 0.6084839556850283,
            "f1": 0.6571118972220964,
            "f1_weighted": 0.6571118972220964
          },
          {
            "accuracy": 0.6455078125,
            "ap": 0.5995864112778466,
            "ap_weighted": 0.5995864112778466,
            "f1": 0.6415202061505703,
            "f1_weighted": 0.6415202061505703
          }
        ]
      }
    ]
  },
  "task_name": "InappropriatenessClassification"
}