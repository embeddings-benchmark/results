{
  "dataset_revision": "f453803eff1e91579eb235dc1d7c38d39b3f1340",
  "task_name": "TweetSentimentExtractionVNClassification",
  "mteb_version": "1.38.42",
  "scores": {
    "test": [
      {
        "accuracy": 0.573511,
        "f1": 0.567534,
        "f1_weighted": 0.560887,
        "scores_per_experiment": [
          {
            "accuracy": 0.556901,
            "f1": 0.55921,
            "f1_weighted": 0.554477
          },
          {
            "accuracy": 0.526392,
            "f1": 0.512861,
            "f1_weighted": 0.500721
          },
          {
            "accuracy": 0.598063,
            "f1": 0.585918,
            "f1_weighted": 0.571969
          },
          {
            "accuracy": 0.620823,
            "f1": 0.624901,
            "f1_weighted": 0.61691
          },
          {
            "accuracy": 0.607748,
            "f1": 0.607617,
            "f1_weighted": 0.605788
          },
          {
            "accuracy": 0.608232,
            "f1": 0.605749,
            "f1_weighted": 0.601148
          },
          {
            "accuracy": 0.525424,
            "f1": 0.50519,
            "f1_weighted": 0.506371
          },
          {
            "accuracy": 0.612107,
            "f1": 0.610443,
            "f1_weighted": 0.610525
          },
          {
            "accuracy": 0.537046,
            "f1": 0.517176,
            "f1_weighted": 0.505161
          },
          {
            "accuracy": 0.542373,
            "f1": 0.546272,
            "f1_weighted": 0.5358
          }
        ],
        "main_score": 0.573511,
        "hf_subset": "default",
        "languages": [
          "vie-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 15.325141906738281,
  "kg_co2_emissions": null
}