{
  "dataset_revision": "9e9b1f8ef51616073f47f306f7f47dd91663f86a",
  "evaluation_time": 64.8620867729187,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test.full": [
      {
        "cosine_accuracy": 0.7873774509803921,
        "cosine_accuracy_threshold": 0.7746087908744812,
        "cosine_ap": 0.9143892148706383,
        "cosine_f1": 0.850790936297563,
        "cosine_f1_threshold": 0.7577604651451111,
        "cosine_precision": 0.7828481510621558,
        "cosine_recall": 0.9316479400749064,
        "dot_accuracy": 0.7873774509803921,
        "dot_accuracy_threshold": 0.7746087908744812,
        "dot_ap": 0.9143889521813813,
        "dot_f1": 0.850790936297563,
        "dot_f1_threshold": 0.7577604055404663,
        "dot_precision": 0.7828481510621558,
        "dot_recall": 0.9316479400749064,
        "euclidean_accuracy": 0.7873774509803921,
        "euclidean_accuracy_threshold": 0.671403169631958,
        "euclidean_ap": 0.9143892148706383,
        "euclidean_f1": 0.850790936297563,
        "euclidean_f1_threshold": 0.6960453987121582,
        "euclidean_precision": 0.7828481510621558,
        "euclidean_recall": 0.9316479400749064,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.9143892148706383,
        "manhattan_accuracy": 0.7910539215686274,
        "manhattan_accuracy_threshold": 32.60675048828125,
        "manhattan_ap": 0.9141972316139584,
        "manhattan_f1": 0.8518358531317494,
        "manhattan_f1_threshold": 33.702144622802734,
        "manhattan_precision": 0.7906976744186046,
        "manhattan_recall": 0.9232209737827716,
        "max_ap": 0.9143892148706383,
        "max_f1": 0.8518358531317494,
        "max_precision": 0.7906976744186046,
        "max_recall": 0.9316479400749064,
        "similarity_accuracy": 0.7873774509803921,
        "similarity_accuracy_threshold": 0.7746087908744812,
        "similarity_ap": 0.9143892148706383,
        "similarity_f1": 0.850790936297563,
        "similarity_f1_threshold": 0.7577604651451111,
        "similarity_precision": 0.7828481510621558,
        "similarity_recall": 0.9316479400749064
      }
    ],
    "validation.full": [
      {
        "cosine_accuracy": 0.811639549436796,
        "cosine_accuracy_threshold": 0.8106324672698975,
        "cosine_ap": 0.9199916139612094,
        "cosine_f1": 0.8558476881233,
        "cosine_f1_threshold": 0.774928092956543,
        "cosine_precision": 0.7959527824620574,
        "cosine_recall": 0.9254901960784314,
        "dot_accuracy": 0.811639549436796,
        "dot_accuracy_threshold": 0.8106324672698975,
        "dot_ap": 0.9199916139612094,
        "dot_f1": 0.8558476881233,
        "dot_f1_threshold": 0.7749281525611877,
        "dot_precision": 0.7959527824620574,
        "dot_recall": 0.9254901960784314,
        "euclidean_accuracy": 0.811639549436796,
        "euclidean_accuracy_threshold": 0.6154146194458008,
        "euclidean_ap": 0.9199916139612094,
        "euclidean_f1": 0.8558476881233,
        "euclidean_f1_threshold": 0.6709275245666504,
        "euclidean_precision": 0.7959527824620574,
        "euclidean_recall": 0.9254901960784314,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.9199916139612094,
        "manhattan_accuracy": 0.8110137672090113,
        "manhattan_accuracy_threshold": 31.274715423583984,
        "manhattan_ap": 0.9189148547280686,
        "manhattan_f1": 0.8575471698113207,
        "manhattan_f1_threshold": 31.299657821655273,
        "manhattan_precision": 0.8263636363636364,
        "manhattan_recall": 0.8911764705882353,
        "max_ap": 0.9199916139612094,
        "max_f1": 0.8575471698113207,
        "max_precision": 0.8263636363636364,
        "max_recall": 0.9254901960784314,
        "similarity_accuracy": 0.811639549436796,
        "similarity_accuracy_threshold": 0.8106324672698975,
        "similarity_ap": 0.9199916139612094,
        "similarity_f1": 0.8558476881233,
        "similarity_f1_threshold": 0.774928092956543,
        "similarity_precision": 0.7959527824620574,
        "similarity_recall": 0.9254901960784314
      }
    ]
  },
  "task_name": "OpusparcusPC"
}