{
  "dataset_revision": "ed9a4403ed4adbfaf4aab56d5b2709e9f6c3ba33",
  "evaluation_time": 2595.239732027054,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.89",
  "scores": {
    "test": [
      {
        "accuracy": 0.970956434651978,
        "f1": 0.9630111834418295,
        "hf_subset": "arb_Arab-rus_Cyrl",
        "languages": [
          "arb-Arab",
          "rus-Cyrl"
        ],
        "main_score": 0.9630111834418295,
        "precision": 0.9592722416958771,
        "recall": 0.970956434651978
      },
      {
        "accuracy": 0.9734601902854282,
        "f1": 0.9664329828075446,
        "hf_subset": "bel_Cyrl-rus_Cyrl",
        "languages": [
          "bel-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.9664329828075446,
        "precision": 0.9631947921882824,
        "recall": 0.9734601902854282
      },
      {
        "accuracy": 0.9679519278918377,
        "f1": 0.9581872809213821,
        "hf_subset": "ben_Beng-rus_Cyrl",
        "languages": [
          "ben-Beng",
          "rus-Cyrl"
        ],
        "main_score": 0.9581872809213821,
        "precision": 0.9534468369220497,
        "recall": 0.9679519278918377
      },
      {
        "accuracy": 0.9814722083124687,
        "f1": 0.9762143214822233,
        "hf_subset": "bos_Latn-rus_Cyrl",
        "languages": [
          "bos-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9762143214822233,
        "precision": 0.9736271073276582,
        "recall": 0.9814722083124687
      },
      {
        "accuracy": 0.9894842263395093,
        "f1": 0.986145885494909,
        "hf_subset": "bul_Cyrl-rus_Cyrl",
        "languages": [
          "bul-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.986145885494909,
        "precision": 0.9845601735937238,
        "recall": 0.9894842263395093
      },
      {
        "accuracy": 0.9839759639459189,
        "f1": 0.9793022867634785,
        "hf_subset": "ces_Latn-rus_Cyrl",
        "languages": [
          "ces-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9793022867634785,
        "precision": 0.9771323652144883,
        "recall": 0.9839759639459189
      },
      {
        "accuracy": 0.9844767150726089,
        "f1": 0.9798864964112837,
        "hf_subset": "deu_Latn-rus_Cyrl",
        "languages": [
          "deu-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9798864964112837,
        "precision": 0.9777165748622935,
        "recall": 0.9844767150726089
      },
      {
        "accuracy": 0.9829744616925388,
        "f1": 0.9781338674678685,
        "hf_subset": "ell_Grek-rus_Cyrl",
        "languages": [
          "ell-Grek",
          "rus-Cyrl"
        ],
        "main_score": 0.9781338674678685,
        "precision": 0.9757970288766482,
        "recall": 0.9829744616925388
      },
      {
        "accuracy": 0.9904857285928893,
        "f1": 0.9875646803538641,
        "hf_subset": "eng_Latn-rus_Cyrl",
        "languages": [
          "eng-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9875646803538641,
        "precision": 0.986229344016024,
        "recall": 0.9904857285928893
      },
      {
        "accuracy": 0.9694541812719079,
        "f1": 0.9620263728926723,
        "hf_subset": "fas_Arab-rus_Cyrl",
        "languages": [
          "fas-Arab",
          "rus-Cyrl"
        ],
        "main_score": 0.9620263728926723,
        "precision": 0.9584376564847271,
        "recall": 0.9694541812719079
      },
      {
        "accuracy": 0.9734601902854282,
        "f1": 0.9669838090469036,
        "hf_subset": "fin_Latn-rus_Cyrl",
        "languages": [
          "fin-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9669838090469036,
        "precision": 0.964004339843098,
        "recall": 0.9734601902854282
      },
      {
        "accuracy": 0.9834752128192289,
        "f1": 0.9788015356367884,
        "hf_subset": "fra_Latn-rus_Cyrl",
        "languages": [
          "fra-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9788015356367884,
        "precision": 0.9765481555666834,
        "recall": 0.9834752128192289
      },
      {
        "accuracy": 0.9734601902854282,
        "f1": 0.9656150893006176,
        "hf_subset": "heb_Hebr-rus_Cyrl",
        "languages": [
          "heb-Hebr",
          "rus-Cyrl"
        ],
        "main_score": 0.9656150893006176,
        "precision": 0.9618594558504424,
        "recall": 0.9734601902854282
      },
      {
        "accuracy": 0.9744616925388082,
        "f1": 0.9666165915539976,
        "hf_subset": "hin_Deva-rus_Cyrl",
        "languages": [
          "hin-Deva",
          "rus-Cyrl"
        ],
        "main_score": 0.9666165915539976,
        "precision": 0.9628609581038223,
        "recall": 0.9744616925388082
      },
      {
        "accuracy": 0.9819729594391587,
        "f1": 0.9761308629611083,
        "hf_subset": "hrv_Latn-rus_Cyrl",
        "languages": [
          "hrv-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9761308629611083,
        "precision": 0.9732932732431981,
        "recall": 0.9819729594391587
      },
      {
        "accuracy": 0.9749624436654982,
        "f1": 0.9679352361876148,
        "hf_subset": "hun_Latn-rus_Cyrl",
        "languages": [
          "hun-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9679352361876148,
        "precision": 0.9646970455683526,
        "recall": 0.9749624436654982
      },
      {
        "accuracy": 0.9889834752128193,
        "f1": 0.9853947588048739,
        "hf_subset": "ind_Latn-rus_Cyrl",
        "languages": [
          "ind-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9853947588048739,
        "precision": 0.9836421298614588,
        "recall": 0.9889834752128193
      },
      {
        "accuracy": 0.9804707060590886,
        "f1": 0.9748289100317141,
        "hf_subset": "jpn_Jpan-rus_Cyrl",
        "languages": [
          "jpn-Jpan",
          "rus-Cyrl"
        ],
        "main_score": 0.9748289100317141,
        "precision": 0.9720831246870305,
        "recall": 0.9804707060590886
      },
      {
        "accuracy": 0.9789684526790186,
        "f1": 0.972542146553163,
        "hf_subset": "kor_Hang-rus_Cyrl",
        "languages": [
          "kor-Hang",
          "rus-Cyrl"
        ],
        "main_score": 0.972542146553163,
        "precision": 0.9694541812719079,
        "recall": 0.9789684526790186
      },
      {
        "accuracy": 0.9424136204306459,
        "f1": 0.9285356606338078,
        "hf_subset": "lit_Latn-rus_Cyrl",
        "languages": [
          "lit-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9285356606338078,
        "precision": 0.9225087631447171,
        "recall": 0.9424136204306459
      },
      {
        "accuracy": 0.9799699549323986,
        "f1": 0.9745785344683691,
        "hf_subset": "mkd_Cyrl-rus_Cyrl",
        "languages": [
          "mkd-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.9745785344683691,
        "precision": 0.9720998163912535,
        "recall": 0.9799699549323986
      },
      {
        "accuracy": 0.9864797195793691,
        "f1": 0.9822567184109497,
        "hf_subset": "nld_Latn-rus_Cyrl",
        "languages": [
          "nld-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9822567184109497,
        "precision": 0.9802620597563011,
        "recall": 0.9864797195793691
      },
      {
        "accuracy": 0.9799699549323986,
        "f1": 0.9748789851443832,
        "hf_subset": "pol_Latn-rus_Cyrl",
        "languages": [
          "pol-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9748789851443832,
        "precision": 0.972458688032048,
        "recall": 0.9799699549323986
      },
      {
        "accuracy": 0.9859789684526791,
        "f1": 0.9816391253546988,
        "hf_subset": "por_Latn-rus_Cyrl",
        "languages": [
          "por-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9816391253546988,
        "precision": 0.9795526623268235,
        "recall": 0.9859789684526791
      },
      {
        "accuracy": 0.9689534301452178,
        "f1": 0.9602737439492571,
        "hf_subset": "rus_Cyrl-arb_Arab",
        "languages": [
          "rus-Cyrl",
          "arb-Arab"
        ],
        "main_score": 0.9602737439492571,
        "precision": 0.9561008178935069,
        "recall": 0.9689534301452178
      },
      {
        "accuracy": 0.9624436654982473,
        "f1": 0.9518444333166416,
        "hf_subset": "rus_Cyrl-bel_Cyrl",
        "languages": [
          "rus-Cyrl",
          "bel-Cyrl"
        ],
        "main_score": 0.9518444333166416,
        "precision": 0.9466700050075113,
        "recall": 0.9624436654982473
      },
      {
        "accuracy": 0.9579369053580371,
        "f1": 0.9453346686696712,
        "hf_subset": "rus_Cyrl-ben_Beng",
        "languages": [
          "rus-Cyrl",
          "ben-Beng"
        ],
        "main_score": 0.9453346686696712,
        "precision": 0.9392421966282757,
        "recall": 0.9579369053580371
      },
      {
        "accuracy": 0.985478217325989,
        "f1": 0.9814722083124687,
        "hf_subset": "rus_Cyrl-bos_Latn",
        "languages": [
          "rus-Cyrl",
          "bos-Latn"
        ],
        "main_score": 0.9814722083124687,
        "precision": 0.9794692038057086,
        "recall": 0.985478217325989
      },
      {
        "accuracy": 0.9919879819729595,
        "f1": 0.9893173092972791,
        "hf_subset": "rus_Cyrl-bul_Cyrl",
        "languages": [
          "rus-Cyrl",
          "bul-Cyrl"
        ],
        "main_score": 0.9893173092972791,
        "precision": 0.9879819729594391,
        "recall": 0.9919879819729595
      },
      {
        "accuracy": 0.9849774661992989,
        "f1": 0.9805541645802036,
        "hf_subset": "rus_Cyrl-ces_Latn",
        "languages": [
          "rus-Cyrl",
          "ces-Latn"
        ],
        "main_score": 0.9805541645802036,
        "precision": 0.9783842430312134,
        "recall": 0.9849774661992989
      },
      {
        "accuracy": 0.9889834752128193,
        "f1": 0.9856451343682191,
        "hf_subset": "rus_Cyrl-deu_Latn",
        "languages": [
          "rus-Cyrl",
          "deu-Latn"
        ],
        "main_score": 0.9856451343682191,
        "precision": 0.9839759639459189,
        "recall": 0.9889834752128193
      },
      {
        "accuracy": 0.9804707060590886,
        "f1": 0.9742113169754631,
        "hf_subset": "rus_Cyrl-ell_Grek",
        "languages": [
          "rus-Cyrl",
          "ell-Grek"
        ],
        "main_score": 0.9742113169754631,
        "precision": 0.971123351694208,
        "recall": 0.9804707060590886
      },
      {
        "accuracy": 0.9924887330996495,
        "f1": 0.9900684359873142,
        "hf_subset": "rus_Cyrl-eng_Latn",
        "languages": [
          "rus-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.9900684359873142,
        "precision": 0.9889000166917044,
        "recall": 0.9924887330996495
      },
      {
        "accuracy": 0.971957936905358,
        "f1": 0.9638457686529794,
        "hf_subset": "rus_Cyrl-fas_Arab",
        "languages": [
          "rus-Cyrl",
          "fas-Arab"
        ],
        "main_score": 0.9638457686529794,
        "precision": 0.9600233683859122,
        "recall": 0.971957936905358
      },
      {
        "accuracy": 0.9679519278918377,
        "f1": 0.9594725421465531,
        "hf_subset": "rus_Cyrl-fin_Latn",
        "languages": [
          "rus-Cyrl",
          "fin-Latn"
        ],
        "main_score": 0.9594725421465531,
        "precision": 0.9555583375062594,
        "recall": 0.9679519278918377
      },
      {
        "accuracy": 0.9879819729594391,
        "f1": 0.9849774661992989,
        "hf_subset": "rus_Cyrl-fra_Latn",
        "languages": [
          "rus-Cyrl",
          "fra-Latn"
        ],
        "main_score": 0.9849774661992989,
        "precision": 0.9835586713403439,
        "recall": 0.9879819729594391
      },
      {
        "accuracy": 0.9694541812719079,
        "f1": 0.9601402103154733,
        "hf_subset": "rus_Cyrl-heb_Hebr",
        "languages": [
          "rus-Cyrl",
          "heb-Hebr"
        ],
        "main_score": 0.9601402103154733,
        "precision": 0.9556417960273744,
        "recall": 0.9694541812719079
      },
      {
        "accuracy": 0.9699549323985979,
        "f1": 0.9610248706392923,
        "hf_subset": "rus_Cyrl-hin_Deva",
        "languages": [
          "rus-Cyrl",
          "hin-Deva"
        ],
        "main_score": 0.9610248706392923,
        "precision": 0.956685027541312,
        "recall": 0.9699549323985979
      },
      {
        "accuracy": 0.9834752128192289,
        "f1": 0.9788015356367884,
        "hf_subset": "rus_Cyrl-hrv_Latn",
        "languages": [
          "rus-Cyrl",
          "hrv-Latn"
        ],
        "main_score": 0.9788015356367884,
        "precision": 0.9765481555666833,
        "recall": 0.9834752128192289
      },
      {
        "accuracy": 0.9829744616925388,
        "f1": 0.9781338674678685,
        "hf_subset": "rus_Cyrl-hun_Latn",
        "languages": [
          "rus-Cyrl",
          "hun-Latn"
        ],
        "main_score": 0.9781338674678685,
        "precision": 0.9757135703555333,
        "recall": 0.9829744616925388
      },
      {
        "accuracy": 0.9894842263395093,
        "f1": 0.986062426973794,
        "hf_subset": "rus_Cyrl-ind_Latn",
        "languages": [
          "rus-Cyrl",
          "ind-Latn"
        ],
        "main_score": 0.986062426973794,
        "precision": 0.9843932565514939,
        "recall": 0.9894842263395093
      },
      {
        "accuracy": 0.9809714571857787,
        "f1": 0.9754965782006343,
        "hf_subset": "rus_Cyrl-jpn_Jpan",
        "languages": [
          "rus-Cyrl",
          "jpn-Jpan"
        ],
        "main_score": 0.9754965782006343,
        "precision": 0.9729177098981805,
        "recall": 0.9809714571857787
      },
      {
        "accuracy": 0.9774661992989484,
        "f1": 0.9705391420464029,
        "hf_subset": "rus_Cyrl-kor_Hang",
        "languages": [
          "rus-Cyrl",
          "kor-Hang"
        ],
        "main_score": 0.9705391420464029,
        "precision": 0.9672008012018027,
        "recall": 0.9774661992989484
      },
      {
        "accuracy": 0.9323985978968453,
        "f1": 0.914037723251544,
        "hf_subset": "rus_Cyrl-lit_Latn",
        "languages": [
          "rus-Cyrl",
          "lit-Latn"
        ],
        "main_score": 0.914037723251544,
        "precision": 0.9058170589217158,
        "recall": 0.9323985978968453
      },
      {
        "accuracy": 0.9819729594391587,
        "f1": 0.9766316140877984,
        "hf_subset": "rus_Cyrl-mkd_Cyrl",
        "languages": [
          "rus-Cyrl",
          "mkd-Cyrl"
        ],
        "main_score": 0.9766316140877984,
        "precision": 0.9740443999332331,
        "recall": 0.9819729594391587
      },
      {
        "accuracy": 0.9859789684526791,
        "f1": 0.9816725087631447,
        "hf_subset": "rus_Cyrl-nld_Latn",
        "languages": [
          "rus-Cyrl",
          "nld-Latn"
        ],
        "main_score": 0.9816725087631447,
        "precision": 0.979594391587381,
        "recall": 0.9859789684526791
      },
      {
        "accuracy": 0.9814722083124687,
        "f1": 0.9770489066933733,
        "hf_subset": "rus_Cyrl-pol_Latn",
        "languages": [
          "rus-Cyrl",
          "pol-Latn"
        ],
        "main_score": 0.9770489066933733,
        "precision": 0.9748789851443832,
        "recall": 0.9814722083124687
      },
      {
        "accuracy": 0.9834752128192289,
        "f1": 0.9786346185945585,
        "hf_subset": "rus_Cyrl-por_Latn",
        "languages": [
          "rus-Cyrl",
          "por-Latn"
        ],
        "main_score": 0.9786346185945585,
        "precision": 0.9762977800033383,
        "recall": 0.9834752128192289
      },
      {
        "accuracy": 0.9784677015523285,
        "f1": 0.9720413954264729,
        "hf_subset": "rus_Cyrl-slk_Latn",
        "languages": [
          "rus-Cyrl",
          "slk-Latn"
        ],
        "main_score": 0.9720413954264729,
        "precision": 0.9688699716241029,
        "recall": 0.9784677015523285
      },
      {
        "accuracy": 0.9749624436654982,
        "f1": 0.9678684693707227,
        "hf_subset": "rus_Cyrl-slv_Latn",
        "languages": [
          "rus-Cyrl",
          "slv-Latn"
        ],
        "main_score": 0.9678684693707227,
        "precision": 0.9644466700050075,
        "recall": 0.9749624436654982
      },
      {
        "accuracy": 0.9874812218327491,
        "f1": 0.9840594224670338,
        "hf_subset": "rus_Cyrl-spa_Latn",
        "languages": [
          "rus-Cyrl",
          "spa-Latn"
        ],
        "main_score": 0.9840594224670338,
        "precision": 0.9823902520447338,
        "recall": 0.9874812218327491
      },
      {
        "accuracy": 0.9864797195793691,
        "f1": 0.9828075446503088,
        "hf_subset": "rus_Cyrl-srp_Cyrl",
        "languages": [
          "rus-Cyrl",
          "srp-Cyrl"
        ],
        "main_score": 0.9828075446503088,
        "precision": 0.9810549157068937,
        "recall": 0.9864797195793691
      },
      {
        "accuracy": 0.9824737105658488,
        "f1": 0.9770489066933733,
        "hf_subset": "rus_Cyrl-srp_Latn",
        "languages": [
          "rus-Cyrl",
          "srp-Latn"
        ],
        "main_score": 0.9770489066933733,
        "precision": 0.9743782340176933,
        "recall": 0.9824737105658488
      },
      {
        "accuracy": 0.8978467701552328,
        "f1": 0.870939742947755,
        "hf_subset": "rus_Cyrl-swa_Latn",
        "languages": [
          "rus-Cyrl",
          "swa-Latn"
        ],
        "main_score": 0.870939742947755,
        "precision": 0.8586546486396263,
        "recall": 0.8978467701552328
      },
      {
        "accuracy": 0.9819729594391587,
        "f1": 0.9767985311300283,
        "hf_subset": "rus_Cyrl-swe_Latn",
        "languages": [
          "rus-Cyrl",
          "swe-Latn"
        ],
        "main_score": 0.9767985311300283,
        "precision": 0.9742947754965782,
        "recall": 0.9819729594391587
      },
      {
        "accuracy": 0.8487731597396094,
        "f1": 0.8110332164914037,
        "hf_subset": "rus_Cyrl-tam_Taml",
        "languages": [
          "rus-Cyrl",
          "tam-Taml"
        ],
        "main_score": 0.8110332164914037,
        "precision": 0.7944416624937406,
        "recall": 0.8487731597396094
      },
      {
        "accuracy": 0.9744616925388082,
        "f1": 0.9682356868636287,
        "hf_subset": "rus_Cyrl-tur_Latn",
        "languages": [
          "rus-Cyrl",
          "tur-Latn"
        ],
        "main_score": 0.9682356868636287,
        "precision": 0.965322984476715,
        "recall": 0.9744616925388082
      },
      {
        "accuracy": 0.985478217325989,
        "f1": 0.9816391253546988,
        "hf_subset": "rus_Cyrl-ukr_Cyrl",
        "languages": [
          "rus-Cyrl",
          "ukr-Cyrl"
        ],
        "main_score": 0.9816391253546988,
        "precision": 0.9797195793690536,
        "recall": 0.985478217325989
      },
      {
        "accuracy": 0.9749624436654982,
        "f1": 0.9686195960607578,
        "hf_subset": "rus_Cyrl-vie_Latn",
        "languages": [
          "rus-Cyrl",
          "vie-Latn"
        ],
        "main_score": 0.9686195960607578,
        "precision": 0.9655316307795027,
        "recall": 0.9749624436654982
      },
      {
        "accuracy": 0.9824737105658488,
        "f1": 0.9780837923551995,
        "hf_subset": "rus_Cyrl-zho_Hant",
        "languages": [
          "rus-Cyrl",
          "zho-Hant"
        ],
        "main_score": 0.9780837923551995,
        "precision": 0.9760891337005508,
        "recall": 0.9824737105658488
      },
      {
        "accuracy": 0.7055583375062594,
        "f1": 0.6535832460174856,
        "hf_subset": "rus_Cyrl-zul_Latn",
        "languages": [
          "rus-Cyrl",
          "zul-Latn"
        ],
        "main_score": 0.6535832460174856,
        "precision": 0.6330820997007777,
        "recall": 0.7055583375062594
      },
      {
        "accuracy": 0.9814722083124687,
        "f1": 0.9760807878484393,
        "hf_subset": "slk_Latn-rus_Cyrl",
        "languages": [
          "slk-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9760807878484393,
        "precision": 0.9735019195459856,
        "recall": 0.9814722083124687
      },
      {
        "accuracy": 0.9754631947921882,
        "f1": 0.9691370388916708,
        "hf_subset": "slv_Latn-rus_Cyrl",
        "languages": [
          "slv-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9691370388916708,
        "precision": 0.9662410282089801,
        "recall": 0.9754631947921882
      },
      {
        "accuracy": 0.9839759639459189,
        "f1": 0.9795025872141545,
        "hf_subset": "spa_Latn-rus_Cyrl",
        "languages": [
          "spa-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9795025872141545,
        "precision": 0.977424470038391,
        "recall": 0.9839759639459189
      },
      {
        "accuracy": 0.9839759639459189,
        "f1": 0.9793356701719246,
        "hf_subset": "srp_Cyrl-rus_Cyrl",
        "languages": [
          "srp-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.9793356701719246,
        "precision": 0.9771740944750458,
        "recall": 0.9839759639459189
      },
      {
        "accuracy": 0.9819729594391587,
        "f1": 0.9770489066933733,
        "hf_subset": "srp_Latn-rus_Cyrl",
        "languages": [
          "srp-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9770489066933733,
        "precision": 0.9747120681021533,
        "recall": 0.9819729594391587
      },
      {
        "accuracy": 0.9053580370555834,
        "f1": 0.8839592722416959,
        "hf_subset": "swa_Latn-rus_Cyrl",
        "languages": [
          "swa-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.8839592722416959,
        "precision": 0.8743448506092472,
        "recall": 0.9053580370555834
      },
      {
        "accuracy": 0.9829744616925388,
        "f1": 0.9781672508763145,
        "hf_subset": "swe_Latn-rus_Cyrl",
        "languages": [
          "swe-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9781672508763145,
        "precision": 0.9759222166583209,
        "recall": 0.9829744616925388
      },
      {
        "accuracy": 0.8257386079118678,
        "f1": 0.7881095452702863,
        "hf_subset": "tam_Taml-rus_Cyrl",
        "languages": [
          "tam-Taml",
          "rus-Cyrl"
        ],
        "main_score": 0.7881095452702863,
        "precision": 0.772622028280516,
        "recall": 0.8257386079118678
      },
      {
        "accuracy": 0.9764646970455684,
        "f1": 0.9700383909197128,
        "hf_subset": "tur_Latn-rus_Cyrl",
        "languages": [
          "tur-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9700383909197128,
        "precision": 0.9669504256384577,
        "recall": 0.9764646970455684
      },
      {
        "accuracy": 0.9839759639459189,
        "f1": 0.9792188282423635,
        "hf_subset": "ukr_Cyrl-rus_Cyrl",
        "languages": [
          "ukr-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.9792188282423635,
        "precision": 0.9768819896511434,
        "recall": 0.9839759639459189
      },
      {
        "accuracy": 0.9769654481722584,
        "f1": 0.9702053079619429,
        "hf_subset": "vie_Latn-rus_Cyrl",
        "languages": [
          "vie-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9702053079619429,
        "precision": 0.9669504256384577,
        "recall": 0.9769654481722584
      },
      {
        "accuracy": 0.9809714571857787,
        "f1": 0.9752462026372892,
        "hf_subset": "zho_Hant-rus_Cyrl",
        "languages": [
          "zho-Hant",
          "rus-Cyrl"
        ],
        "main_score": 0.9752462026372892,
        "precision": 0.9725004172926055,
        "recall": 0.9809714571857787
      },
      {
        "accuracy": 0.6985478217325989,
        "f1": 0.6565555640490625,
        "hf_subset": "zul_Latn-rus_Cyrl",
        "languages": [
          "zul-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.6565555640490625,
        "precision": 0.6410506703098533,
        "recall": 0.6985478217325989
      }
    ]
  },
  "task_name": "NTREXBitextMining"
}