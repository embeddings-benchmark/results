{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 181.57835292816162,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.87",
  "scores": {
    "test": [
      {
        "accuracy": 0.796133154001345,
        "f1": 0.7952898965496682,
        "f1_weighted": 0.7956380229429693,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.796133154001345,
        "scores_per_experiment": [
          {
            "accuracy": 0.8106926698049765,
            "f1": 0.8119607579805928,
            "f1_weighted": 0.808952471888905
          },
          {
            "accuracy": 0.7841291190316073,
            "f1": 0.7879847537689461,
            "f1_weighted": 0.7832988740313662
          },
          {
            "accuracy": 0.7757229320780095,
            "f1": 0.7739127057991633,
            "f1_weighted": 0.7753724764723224
          },
          {
            "accuracy": 0.7874915938130465,
            "f1": 0.7804895800356708,
            "f1_weighted": 0.7854861851353839
          },
          {
            "accuracy": 0.7975790181573639,
            "f1": 0.7950445014417743,
            "f1_weighted": 0.7961293949258942
          },
          {
            "accuracy": 0.8029589778076665,
            "f1": 0.7979208867489657,
            "f1_weighted": 0.7993525068524117
          },
          {
            "accuracy": 0.7874915938130465,
            "f1": 0.792440809532875,
            "f1_weighted": 0.7918227084418084
          },
          {
            "accuracy": 0.7804303967720242,
            "f1": 0.7828735340392949,
            "f1_weighted": 0.7833181723301287
          },
          {
            "accuracy": 0.816408876933423,
            "f1": 0.8129185848739806,
            "f1_weighted": 0.8153398223697473
          },
          {
            "accuracy": 0.8184263618022865,
            "f1": 0.8173528512754189,
            "f1_weighted": 0.817307616981725
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.7921790457452041,
        "f1": 0.7892043328778999,
        "f1_weighted": 0.7911381570365098,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.7921790457452041,
        "scores_per_experiment": [
          {
            "accuracy": 0.8101328086571569,
            "f1": 0.8104913634270079,
            "f1_weighted": 0.8065139836318226
          },
          {
            "accuracy": 0.7732415150024594,
            "f1": 0.7804148157352587,
            "f1_weighted": 0.7747280739339627
          },
          {
            "accuracy": 0.7860304968027545,
            "f1": 0.7817952584352729,
            "f1_weighted": 0.7873576576361604
          },
          {
            "accuracy": 0.7688145597638957,
            "f1": 0.7596101158864218,
            "f1_weighted": 0.7647806190595331
          },
          {
            "accuracy": 0.7953762911952779,
            "f1": 0.7903346890452706,
            "f1_weighted": 0.7929540787331218
          },
          {
            "accuracy": 0.7879980324643384,
            "f1": 0.7800897970761944,
            "f1_weighted": 0.7861379372269708
          },
          {
            "accuracy": 0.7899655681259223,
            "f1": 0.7932086836967396,
            "f1_weighted": 0.7922932531839408
          },
          {
            "accuracy": 0.7791441219872111,
            "f1": 0.7797167575456095,
            "f1_weighted": 0.7801532567055122
          },
          {
            "accuracy": 0.822429906542056,
            "f1": 0.8134856169930228,
            "f1_weighted": 0.8195146290060238
          },
          {
            "accuracy": 0.8086571569109691,
            "f1": 0.802896230938202,
            "f1_weighted": 0.806948081248049
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}