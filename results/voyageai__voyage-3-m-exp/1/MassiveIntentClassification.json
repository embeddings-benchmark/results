{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.26.3",
  "scores": {
    "test": [
      {
        "accuracy": 0.919435,
        "f1": 0.917639,
        "f1_weighted": 0.921532,
        "scores_per_experiment": [
          {
            "accuracy": 0.932078,
            "f1": 0.932515,
            "f1_weighted": 0.936953
          },
          {
            "accuracy": 0.917283,
            "f1": 0.926372,
            "f1_weighted": 0.920389
          },
          {
            "accuracy": 0.904506,
            "f1": 0.90823,
            "f1_weighted": 0.903372
          },
          {
            "accuracy": 0.911903,
            "f1": 0.916146,
            "f1_weighted": 0.915408
          },
          {
            "accuracy": 0.8961,
            "f1": 0.899534,
            "f1_weighted": 0.893739
          },
          {
            "accuracy": 0.927707,
            "f1": 0.9207,
            "f1_weighted": 0.933415
          },
          {
            "accuracy": 0.902488,
            "f1": 0.913894,
            "f1_weighted": 0.904408
          },
          {
            "accuracy": 0.898453,
            "f1": 0.886642,
            "f1_weighted": 0.896314
          },
          {
            "accuracy": 0.949899,
            "f1": 0.934045,
            "f1_weighted": 0.953786
          },
          {
            "accuracy": 0.953934,
            "f1": 0.93831,
            "f1_weighted": 0.957539
          }
        ],
        "main_score": 0.919435,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 59.477566957473755,
  "kg_co2_emissions": null
}