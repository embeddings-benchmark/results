{
  "dataset_revision": "ffb8a34c9637fb20256e8c7be02504d16af4bd6b",
  "task_name": "OdiaNewsClassification",
  "mteb_version": "2.1.14",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.914062,
            "f1": 0.912256,
            "f1_weighted": 0.914193,
            "precision": 0.90537,
            "precision_weighted": 0.919514,
            "recall": 0.924001,
            "recall_weighted": 0.914062,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.928223,
            "f1": 0.925936,
            "f1_weighted": 0.92831,
            "precision": 0.923678,
            "precision_weighted": 0.92935,
            "recall": 0.929083,
            "recall_weighted": 0.928223,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.906738,
            "f1": 0.904715,
            "f1_weighted": 0.906827,
            "precision": 0.898009,
            "precision_weighted": 0.914843,
            "recall": 0.918699,
            "recall_weighted": 0.906738,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.921875,
            "f1": 0.92002,
            "f1_weighted": 0.922047,
            "precision": 0.91505,
            "precision_weighted": 0.923503,
            "recall": 0.926282,
            "recall_weighted": 0.921875,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.921387,
            "f1": 0.919018,
            "f1_weighted": 0.921498,
            "precision": 0.915854,
            "precision_weighted": 0.922097,
            "recall": 0.922649,
            "recall_weighted": 0.921387,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.924805,
            "f1": 0.921531,
            "f1_weighted": 0.92494,
            "precision": 0.918141,
            "precision_weighted": 0.925763,
            "recall": 0.925562,
            "recall_weighted": 0.924805,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.935059,
            "f1": 0.932339,
            "f1_weighted": 0.935105,
            "precision": 0.929952,
            "precision_weighted": 0.93573,
            "recall": 0.935261,
            "recall_weighted": 0.935059,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.919922,
            "f1": 0.917347,
            "f1_weighted": 0.920078,
            "precision": 0.912252,
            "precision_weighted": 0.922468,
            "recall": 0.924499,
            "recall_weighted": 0.919922,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.922852,
            "f1": 0.920605,
            "f1_weighted": 0.923135,
            "precision": 0.915352,
            "precision_weighted": 0.924942,
            "recall": 0.927496,
            "recall_weighted": 0.922852,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.907715,
            "f1": 0.905696,
            "f1_weighted": 0.90819,
            "precision": 0.898276,
            "precision_weighted": 0.914875,
            "recall": 0.919358,
            "recall_weighted": 0.907715,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.920264,
        "f1": 0.917946,
        "f1_weighted": 0.920432,
        "precision": 0.913193,
        "precision_weighted": 0.923309,
        "recall": 0.925289,
        "recall_weighted": 0.920264,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.917946,
        "hf_subset": "default",
        "languages": [
          "ory-Orya"
        ]
      }
    ]
  },
  "evaluation_time": 61.254486083984375,
  "kg_co2_emissions": null
}