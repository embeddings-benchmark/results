{
  "dataset_revision": "1beac1b941da76a9c51e3e5b39d230fde9a80983",
  "task_name": "SentimentAnalysisHindi",
  "mteb_version": "2.1.14",
  "scores": {
    "train": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.632812,
            "f1": 0.634286,
            "f1_weighted": 0.632606,
            "precision": 0.617542,
            "precision_weighted": 0.650352,
            "recall": 0.684932,
            "recall_weighted": 0.632812,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.670898,
            "f1": 0.658031,
            "f1_weighted": 0.676203,
            "precision": 0.648063,
            "precision_weighted": 0.700632,
            "recall": 0.700427,
            "recall_weighted": 0.670898,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.645996,
            "f1": 0.641456,
            "f1_weighted": 0.64794,
            "precision": 0.636524,
            "precision_weighted": 0.662133,
            "recall": 0.658624,
            "recall_weighted": 0.645996,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.541504,
            "f1": 0.54029,
            "f1_weighted": 0.541727,
            "precision": 0.529326,
            "precision_weighted": 0.565259,
            "recall": 0.592919,
            "recall_weighted": 0.541504,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.645508,
            "f1": 0.663065,
            "f1_weighted": 0.645433,
            "precision": 0.654539,
            "precision_weighted": 0.648827,
            "recall": 0.675541,
            "recall_weighted": 0.645508,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.585938,
            "f1": 0.572765,
            "f1_weighted": 0.592037,
            "precision": 0.56951,
            "precision_weighted": 0.625678,
            "recall": 0.627429,
            "recall_weighted": 0.585938,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.587402,
            "f1": 0.581702,
            "f1_weighted": 0.572853,
            "precision": 0.595137,
            "precision_weighted": 0.637232,
            "recall": 0.649644,
            "recall_weighted": 0.587402,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.687988,
            "f1": 0.685987,
            "f1_weighted": 0.688832,
            "precision": 0.670042,
            "precision_weighted": 0.696635,
            "recall": 0.715352,
            "recall_weighted": 0.687988,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.590332,
            "f1": 0.580896,
            "f1_weighted": 0.594136,
            "precision": 0.579959,
            "precision_weighted": 0.62943,
            "recall": 0.637253,
            "recall_weighted": 0.590332,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.665039,
            "f1": 0.671836,
            "f1_weighted": 0.661385,
            "precision": 0.666101,
            "precision_weighted": 0.662821,
            "recall": 0.682698,
            "recall_weighted": 0.665039,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.625342,
        "f1": 0.623031,
        "f1_weighted": 0.625315,
        "precision": 0.616674,
        "precision_weighted": 0.6479,
        "recall": 0.662482,
        "recall_weighted": 0.625342,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.623031,
        "hf_subset": "default",
        "languages": [
          "hin-Deva"
        ]
      }
    ]
  },
  "evaluation_time": 102.69162678718567,
  "kg_co2_emissions": null
}