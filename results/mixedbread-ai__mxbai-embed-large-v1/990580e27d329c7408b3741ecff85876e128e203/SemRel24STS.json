{
  "dataset_revision": "ef5c383d1b87eb8feccde3dfb7f95e42b1b050dd",
  "evaluation_time": 37.527618169784546,
  "kg_co2_emissions": 0.0019267921595006678,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "cosine_pearson": 0.7267856590915556,
        "cosine_spearman": 0.7248893435666028,
        "euclidean_pearson": 0.7099272551100164,
        "euclidean_spearman": 0.7178150531523287,
        "hf_subset": "afr",
        "languages": [
          "afr-Latn"
        ],
        "main_score": 0.7248893435666028,
        "manhattan_pearson": 0.7111439274570689,
        "manhattan_spearman": 0.7197647791094574,
        "pearson": 0.7267856590915556,
        "spearman": 0.7248893435666028
      },
      {
        "cosine_pearson": 0.19929620073258805,
        "cosine_spearman": 0.18703506598016958,
        "euclidean_pearson": 0.1792101763152348,
        "euclidean_spearman": 0.17527178395086723,
        "hf_subset": "amh",
        "languages": [
          "amh-Ethi"
        ],
        "main_score": 0.18703506598016958,
        "manhattan_pearson": 0.179555018235162,
        "manhattan_spearman": 0.17511223129209574,
        "pearson": 0.19929620073258805,
        "spearman": 0.18703506598016958
      },
      {
        "cosine_pearson": 0.12535622777332475,
        "cosine_spearman": 0.11733422866120538,
        "euclidean_pearson": 0.09312370949538462,
        "euclidean_spearman": 0.07806391851686711,
        "hf_subset": "arb",
        "languages": [
          "arb-Arab"
        ],
        "main_score": 0.11733422866120538,
        "manhattan_pearson": 0.09214473074445902,
        "manhattan_spearman": 0.07709014188497319,
        "pearson": 0.12535622777332475,
        "spearman": 0.11733422866120538
      },
      {
        "cosine_pearson": 0.34378377015594563,
        "cosine_spearman": 0.3207520124641019,
        "euclidean_pearson": 0.305542415634728,
        "euclidean_spearman": 0.27372677510699817,
        "hf_subset": "arq",
        "languages": [
          "arq-Arab"
        ],
        "main_score": 0.3207520124641019,
        "manhattan_pearson": 0.30673730186896664,
        "manhattan_spearman": 0.2750552608945962,
        "pearson": 0.34378377015594563,
        "spearman": 0.3207520124641019
      },
      {
        "cosine_pearson": 0.008182825782608504,
        "cosine_spearman": -0.035036611873620525,
        "euclidean_pearson": -0.039477021706486086,
        "euclidean_spearman": -0.0731494728229995,
        "hf_subset": "ary",
        "languages": [
          "ary-Arab"
        ],
        "main_score": -0.035036611873620525,
        "manhattan_pearson": -0.040507629231267744,
        "manhattan_spearman": -0.07165265689505271,
        "pearson": 0.008182825782608504,
        "spearman": -0.035036611873620525
      },
      {
        "cosine_pearson": 0.8173815892333032,
        "cosine_spearman": 0.8053534458911855,
        "euclidean_pearson": 0.8011885848364372,
        "euclidean_spearman": 0.7803872488657608,
        "hf_subset": "eng",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8053534458911855,
        "manhattan_pearson": 0.8003545395668636,
        "manhattan_spearman": 0.779184454866018,
        "pearson": 0.8173815892333032,
        "spearman": 0.8053534458911855
      },
      {
        "cosine_pearson": 0.30878049699994087,
        "cosine_spearman": 0.28654961691128855,
        "euclidean_pearson": 0.32364700930259815,
        "euclidean_spearman": 0.2887274308287782,
        "hf_subset": "hau",
        "languages": [
          "hau-Latn"
        ],
        "main_score": 0.28654961691128855,
        "manhattan_pearson": 0.3225115382731009,
        "manhattan_spearman": 0.285791499266227,
        "pearson": 0.30878049699994087,
        "spearman": 0.28654961691128855
      },
      {
        "cosine_pearson": 0.423911127394369,
        "cosine_spearman": 0.4177177412219256,
        "euclidean_pearson": 0.4461575370519954,
        "euclidean_spearman": 0.4185152988541954,
        "hf_subset": "hin",
        "languages": [
          "hin-Deva"
        ],
        "main_score": 0.4177177412219256,
        "manhattan_pearson": 0.44540589626882116,
        "manhattan_spearman": 0.417451397355564,
        "pearson": 0.423911127394369,
        "spearman": 0.4177177412219256
      },
      {
        "cosine_pearson": 0.37262154385913515,
        "cosine_spearman": 0.3930353965859797,
        "euclidean_pearson": 0.4047134021581165,
        "euclidean_spearman": 0.3785035633502126,
        "hf_subset": "ind",
        "languages": [
          "ind-Latn"
        ],
        "main_score": 0.3930353965859797,
        "manhattan_pearson": 0.404985240066917,
        "manhattan_spearman": 0.3786268496886736,
        "pearson": 0.37262154385913515,
        "spearman": 0.3930353965859797
      },
      {
        "cosine_pearson": 0.42341177391393275,
        "cosine_spearman": 0.3917149870141553,
        "euclidean_pearson": 0.43319337916481826,
        "euclidean_spearman": 0.4047994651707408,
        "hf_subset": "kin",
        "languages": [
          "kin-Latn"
        ],
        "main_score": 0.3917149870141553,
        "manhattan_pearson": 0.4340278251155622,
        "manhattan_spearman": 0.40677020424717303,
        "pearson": 0.42341177391393275,
        "spearman": 0.3917149870141553
      },
      {
        "cosine_pearson": 0.42274736047793615,
        "cosine_spearman": 0.42397704878526515,
        "euclidean_pearson": 0.45566954695371514,
        "euclidean_spearman": 0.4308489599263378,
        "hf_subset": "mar",
        "languages": [
          "mar-Deva"
        ],
        "main_score": 0.42397704878526515,
        "manhattan_pearson": 0.4520218321895298,
        "manhattan_spearman": 0.42722497827038497,
        "pearson": 0.42274736047793615,
        "spearman": 0.42397704878526515
      },
      {
        "cosine_pearson": 0.21473774871651236,
        "cosine_spearman": 0.2776198385981615,
        "euclidean_pearson": 0.26197738909527435,
        "euclidean_spearman": 0.2736428424447327,
        "hf_subset": "tel",
        "languages": [
          "tel-Telu"
        ],
        "main_score": 0.2776198385981615,
        "manhattan_pearson": 0.2622090338706578,
        "manhattan_spearman": 0.2729235777351637,
        "pearson": 0.21473774871651236,
        "spearman": 0.2776198385981615
      }
    ]
  },
  "task_name": "SemRel24STS"
}