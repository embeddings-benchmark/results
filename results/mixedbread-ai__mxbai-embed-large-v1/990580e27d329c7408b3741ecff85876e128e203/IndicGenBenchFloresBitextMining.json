{
  "dataset_revision": "f8650438298df086750ff4973661bb58a201a5ee",
  "evaluation_time": 643.9265882968903,
  "kg_co2_emissions": 0.036085220752698,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.040513833992094864,
        "f1": 0.03482964470234872,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.03482964470234872,
        "precision": 0.03385609054731646,
        "recall": 0.040513833992094864
      },
      {
        "accuracy": 0.07114624505928854,
        "f1": 0.03505670638640795,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ],
        "main_score": 0.03505670638640795,
        "precision": 0.030255486628001392,
        "recall": 0.07114624505928854
      },
      {
        "accuracy": 0.058300395256917,
        "f1": 0.04723136588300038,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ],
        "main_score": 0.04723136588300038,
        "precision": 0.04464393527187046,
        "recall": 0.058300395256917
      },
      {
        "accuracy": 0.1007905138339921,
        "f1": 0.05403774514440675,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ],
        "main_score": 0.05403774514440675,
        "precision": 0.04600672347234411,
        "recall": 0.1007905138339921
      },
      {
        "accuracy": 0.022727272727272728,
        "f1": 0.02009476682551456,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.02009476682551456,
        "precision": 0.019270046443959486,
        "recall": 0.022727272727272728
      },
      {
        "accuracy": 0.11067193675889328,
        "f1": 0.06987068973660643,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ],
        "main_score": 0.06987068973660643,
        "precision": 0.062322566138252294,
        "recall": 0.11067193675889328
      },
      {
        "accuracy": 0.06521739130434782,
        "f1": 0.05091111763628796,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ],
        "main_score": 0.05091111763628796,
        "precision": 0.04746036730598929,
        "recall": 0.06521739130434782
      },
      {
        "accuracy": 0.11462450592885376,
        "f1": 0.06434154206730322,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ],
        "main_score": 0.06434154206730322,
        "precision": 0.054849539565598016,
        "recall": 0.11462450592885376
      },
      {
        "accuracy": 0.06422924901185771,
        "f1": 0.050383810654086765,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.050383810654086765,
        "precision": 0.04628497149848082,
        "recall": 0.06422924901185771
      },
      {
        "accuracy": 0.10474308300395258,
        "f1": 0.06523260021872888,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ],
        "main_score": 0.06523260021872888,
        "precision": 0.05794060419750329,
        "recall": 0.10474308300395258
      },
      {
        "accuracy": 0.023715415019762844,
        "f1": 0.020377565612189154,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.020377565612189154,
        "precision": 0.019632602158324033,
        "recall": 0.023715415019762844
      },
      {
        "accuracy": 0.08596837944664032,
        "f1": 0.047884896321304354,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ],
        "main_score": 0.047884896321304354,
        "precision": 0.04249805661491087,
        "recall": 0.08596837944664032
      },
      {
        "accuracy": 0.04940711462450593,
        "f1": 0.04165864825711373,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.04165864825711373,
        "precision": 0.03992636389880616,
        "recall": 0.04940711462450593
      },
      {
        "accuracy": 0.10276679841897234,
        "f1": 0.06231084922996871,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ],
        "main_score": 0.06231084922996871,
        "precision": 0.05550428816058563,
        "recall": 0.10276679841897234
      },
      {
        "accuracy": 0.10869565217391304,
        "f1": 0.09261662547478655,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.09261662547478655,
        "precision": 0.08763247621943275,
        "recall": 0.10869565217391304
      },
      {
        "accuracy": 0.1482213438735178,
        "f1": 0.07908770884793322,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ],
        "main_score": 0.07908770884793322,
        "precision": 0.06660654976485718,
        "recall": 0.1482213438735178
      },
      {
        "accuracy": 0.024703557312252964,
        "f1": 0.02049222289606218,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.02049222289606218,
        "precision": 0.019798973480208495,
        "recall": 0.024703557312252964
      },
      {
        "accuracy": 0.08498023715415019,
        "f1": 0.05037516828054506,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ],
        "main_score": 0.05037516828054506,
        "precision": 0.04307125770881417,
        "recall": 0.08498023715415019
      },
      {
        "accuracy": 0.02766798418972332,
        "f1": 0.0219989605737041,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ],
        "main_score": 0.0219989605737041,
        "precision": 0.021076331217052637,
        "recall": 0.02766798418972332
      },
      {
        "accuracy": 0.06521739130434782,
        "f1": 0.03167024286826037,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ],
        "main_score": 0.03167024286826037,
        "precision": 0.026850221503529465,
        "recall": 0.06521739130434782
      },
      {
        "accuracy": 0.030632411067193676,
        "f1": 0.027406875074859262,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ],
        "main_score": 0.027406875074859262,
        "precision": 0.026434005525921945,
        "recall": 0.030632411067193676
      },
      {
        "accuracy": 0.11561264822134387,
        "f1": 0.07384112570087241,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ],
        "main_score": 0.07384112570087241,
        "precision": 0.06661243705732418,
        "recall": 0.11561264822134387
      },
      {
        "accuracy": 0.046442687747035576,
        "f1": 0.03966535385159097,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ],
        "main_score": 0.03966535385159097,
        "precision": 0.0379991021138692,
        "recall": 0.046442687747035576
      },
      {
        "accuracy": 0.10573122529644269,
        "f1": 0.060840565203263,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ],
        "main_score": 0.060840565203263,
        "precision": 0.0540439212564663,
        "recall": 0.10573122529644269
      },
      {
        "accuracy": 0.0691699604743083,
        "f1": 0.05896342954064093,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ],
        "main_score": 0.05896342954064093,
        "precision": 0.05616218735497911,
        "recall": 0.0691699604743083
      },
      {
        "accuracy": 0.11758893280632411,
        "f1": 0.06719429112686405,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ],
        "main_score": 0.06719429112686405,
        "precision": 0.057800679796675476,
        "recall": 0.11758893280632411
      },
      {
        "accuracy": 0.07114624505928854,
        "f1": 0.059429232060120414,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ],
        "main_score": 0.059429232060120414,
        "precision": 0.05576582411620774,
        "recall": 0.07114624505928854
      },
      {
        "accuracy": 0.11166007905138339,
        "f1": 0.06588219208608151,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ],
        "main_score": 0.06588219208608151,
        "precision": 0.0577055750165926,
        "recall": 0.11166007905138339
      },
      {
        "accuracy": 0.11561264822134387,
        "f1": 0.10579664263883935,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ],
        "main_score": 0.10579664263883935,
        "precision": 0.10254354134668811,
        "recall": 0.11561264822134387
      },
      {
        "accuracy": 0.1541501976284585,
        "f1": 0.08785615328939125,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ],
        "main_score": 0.08785615328939125,
        "precision": 0.07431781697634986,
        "recall": 0.1541501976284585
      },
      {
        "accuracy": 0.04940711462450593,
        "f1": 0.04249970370114665,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ],
        "main_score": 0.04249970370114665,
        "precision": 0.04078214293208059,
        "recall": 0.04940711462450593
      },
      {
        "accuracy": 0.09486166007905138,
        "f1": 0.05300179629226538,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ],
        "main_score": 0.05300179629226538,
        "precision": 0.04733205806287494,
        "recall": 0.09486166007905138
      },
      {
        "accuracy": 0.020750988142292492,
        "f1": 0.01753489077272312,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01753489077272312,
        "precision": 0.017051393802965307,
        "recall": 0.020750988142292492
      },
      {
        "accuracy": 0.09189723320158102,
        "f1": 0.05760367423507953,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ],
        "main_score": 0.05760367423507953,
        "precision": 0.050486091846704724,
        "recall": 0.09189723320158102
      },
      {
        "accuracy": 0.014822134387351778,
        "f1": 0.011703259737487304,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ],
        "main_score": 0.011703259737487304,
        "precision": 0.011204082407566046,
        "recall": 0.014822134387351778
      },
      {
        "accuracy": 0.09486166007905138,
        "f1": 0.059950775837721446,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ],
        "main_score": 0.059950775837721446,
        "precision": 0.053972797515594625,
        "recall": 0.09486166007905138
      },
      {
        "accuracy": 0.024703557312252964,
        "f1": 0.022235784959570126,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ],
        "main_score": 0.022235784959570126,
        "precision": 0.021575733432664914,
        "recall": 0.024703557312252964
      },
      {
        "accuracy": 0.07509881422924901,
        "f1": 0.03556134415003081,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ],
        "main_score": 0.03556134415003081,
        "precision": 0.030213236260322743,
        "recall": 0.07509881422924901
      },
      {
        "accuracy": 0.0266798418972332,
        "f1": 0.02219934598599917,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ],
        "main_score": 0.02219934598599917,
        "precision": 0.02144931564644893,
        "recall": 0.0266798418972332
      },
      {
        "accuracy": 0.10276679841897234,
        "f1": 0.059251873005172366,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ],
        "main_score": 0.059251873005172366,
        "precision": 0.05238443879825674,
        "recall": 0.10276679841897234
      },
      {
        "accuracy": 0.023715415019762844,
        "f1": 0.01983310851599614,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01983310851599614,
        "precision": 0.01930476767505246,
        "recall": 0.023715415019762844
      },
      {
        "accuracy": 0.09288537549407115,
        "f1": 0.05502673455135156,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ],
        "main_score": 0.05502673455135156,
        "precision": 0.04855232499598044,
        "recall": 0.09288537549407115
      },
      {
        "accuracy": 0.04841897233201581,
        "f1": 0.04291674509065813,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ],
        "main_score": 0.04291674509065813,
        "precision": 0.04115802675585284,
        "recall": 0.04841897233201581
      },
      {
        "accuracy": 0.12549407114624506,
        "f1": 0.07296703227381346,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ],
        "main_score": 0.07296703227381346,
        "precision": 0.06403770032849378,
        "recall": 0.12549407114624506
      },
      {
        "accuracy": 0.012845849802371542,
        "f1": 0.009565166794003207,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ],
        "main_score": 0.009565166794003207,
        "precision": 0.009064561716756004,
        "recall": 0.012845849802371542
      },
      {
        "accuracy": 0.07806324110671936,
        "f1": 0.04967611495679039,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ],
        "main_score": 0.04967611495679039,
        "precision": 0.04397740092493335,
        "recall": 0.07806324110671936
      },
      {
        "accuracy": 0.03162055335968379,
        "f1": 0.025498012444197797,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ],
        "main_score": 0.025498012444197797,
        "precision": 0.023952092555688084,
        "recall": 0.03162055335968379
      },
      {
        "accuracy": 0.10968379446640317,
        "f1": 0.06749847277582775,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ],
        "main_score": 0.06749847277582775,
        "precision": 0.060468868454955915,
        "recall": 0.10968379446640317
      },
      {
        "accuracy": 0.030632411067193676,
        "f1": 0.025706328954761062,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ],
        "main_score": 0.025706328954761062,
        "precision": 0.024381528847477383,
        "recall": 0.030632411067193676
      },
      {
        "accuracy": 0.10573122529644269,
        "f1": 0.062279136598878265,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ],
        "main_score": 0.062279136598878265,
        "precision": 0.05433811367188192,
        "recall": 0.10573122529644269
      },
      {
        "accuracy": 0.05632411067193676,
        "f1": 0.04516803761790779,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ],
        "main_score": 0.04516803761790779,
        "precision": 0.043242486128652585,
        "recall": 0.05632411067193676
      },
      {
        "accuracy": 0.08596837944664032,
        "f1": 0.041536986132307455,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ],
        "main_score": 0.041536986132307455,
        "precision": 0.03372260011236236,
        "recall": 0.08596837944664032
      },
      {
        "accuracy": 0.015810276679841896,
        "f1": 0.01252632855110441,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01252632855110441,
        "precision": 0.011862650721035877,
        "recall": 0.015810276679841896
      },
      {
        "accuracy": 0.09387351778656126,
        "f1": 0.05920812376333726,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ],
        "main_score": 0.05920812376333726,
        "precision": 0.05244979841433283,
        "recall": 0.09387351778656126
      },
      {
        "accuracy": 0.023715415019762844,
        "f1": 0.019941890439098865,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ],
        "main_score": 0.019941890439098865,
        "precision": 0.019111297100157684,
        "recall": 0.023715415019762844
      },
      {
        "accuracy": 0.1067193675889328,
        "f1": 0.0651220648195036,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ],
        "main_score": 0.0651220648195036,
        "precision": 0.05712180306712979,
        "recall": 0.1067193675889328
      },
      {
        "accuracy": 0.012845849802371542,
        "f1": 0.008908890416451851,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ],
        "main_score": 0.008908890416451851,
        "precision": 0.007912976385743405,
        "recall": 0.012845849802371542
      },
      {
        "accuracy": 0.024703557312252964,
        "f1": 0.007480193777039386,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ],
        "main_score": 0.007480193777039386,
        "precision": 0.006384784865279616,
        "recall": 0.024703557312252964
      }
    ],
    "validation": [
      {
        "accuracy": 0.04613841524573721,
        "f1": 0.03988146883637479,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.03988146883637479,
        "precision": 0.03821232597032116,
        "recall": 0.04613841524573721
      },
      {
        "accuracy": 0.07723169508525576,
        "f1": 0.03362937271507405,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ],
        "main_score": 0.03362937271507405,
        "precision": 0.02747306849283933,
        "recall": 0.07723169508525576
      },
      {
        "accuracy": 0.06920762286860582,
        "f1": 0.054959221726916106,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ],
        "main_score": 0.054959221726916106,
        "precision": 0.05108738915158172,
        "recall": 0.06920762286860582
      },
      {
        "accuracy": 0.10832497492477432,
        "f1": 0.06379111660832078,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ],
        "main_score": 0.06379111660832078,
        "precision": 0.056020082348424836,
        "recall": 0.10832497492477432
      },
      {
        "accuracy": 0.013039117352056168,
        "f1": 0.01170385075780017,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01170385075780017,
        "precision": 0.011535644277230032,
        "recall": 0.013039117352056168
      },
      {
        "accuracy": 0.10431293881644935,
        "f1": 0.059800839698844144,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ],
        "main_score": 0.059800839698844144,
        "precision": 0.05244559605108191,
        "recall": 0.10431293881644935
      },
      {
        "accuracy": 0.0802407221664995,
        "f1": 0.06453607777125957,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ],
        "main_score": 0.06453607777125957,
        "precision": 0.06076673344047756,
        "recall": 0.0802407221664995
      },
      {
        "accuracy": 0.11735205616850551,
        "f1": 0.0706400079332455,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ],
        "main_score": 0.0706400079332455,
        "precision": 0.062321982893014195,
        "recall": 0.11735205616850551
      },
      {
        "accuracy": 0.09127382146439318,
        "f1": 0.07320838908529294,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.07320838908529294,
        "precision": 0.06890093868293937,
        "recall": 0.09127382146439318
      },
      {
        "accuracy": 0.13941825476429287,
        "f1": 0.08890978082590002,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ],
        "main_score": 0.08890978082590002,
        "precision": 0.07918589574839473,
        "recall": 0.13941825476429287
      },
      {
        "accuracy": 0.01805416248746239,
        "f1": 0.016384553108360896,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.016384553108360896,
        "precision": 0.01604918167117692,
        "recall": 0.01805416248746239
      },
      {
        "accuracy": 0.08625877632898696,
        "f1": 0.045568294760371254,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ],
        "main_score": 0.045568294760371254,
        "precision": 0.039372288564850595,
        "recall": 0.08625877632898696
      },
      {
        "accuracy": 0.05315947843530592,
        "f1": 0.0478798799499273,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.0478798799499273,
        "precision": 0.04642406112077134,
        "recall": 0.05315947843530592
      },
      {
        "accuracy": 0.10832497492477432,
        "f1": 0.06248041849786273,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ],
        "main_score": 0.06248041849786273,
        "precision": 0.05440891983566045,
        "recall": 0.10832497492477432
      },
      {
        "accuracy": 0.11334002006018054,
        "f1": 0.09655766276522341,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.09655766276522341,
        "precision": 0.09242089661677533,
        "recall": 0.11334002006018054
      },
      {
        "accuracy": 0.1354062186559679,
        "f1": 0.07096403631291794,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ],
        "main_score": 0.07096403631291794,
        "precision": 0.0587785537208807,
        "recall": 0.1354062186559679
      },
      {
        "accuracy": 0.026078234704112337,
        "f1": 0.02332209167353282,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.02332209167353282,
        "precision": 0.022712057144076606,
        "recall": 0.026078234704112337
      },
      {
        "accuracy": 0.10531594784353059,
        "f1": 0.0630145804245529,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ],
        "main_score": 0.0630145804245529,
        "precision": 0.05501229381077592,
        "recall": 0.10531594784353059
      },
      {
        "accuracy": 0.05015045135406219,
        "f1": 0.041891930431351816,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ],
        "main_score": 0.041891930431351816,
        "precision": 0.04027808056574307,
        "recall": 0.05015045135406219
      },
      {
        "accuracy": 0.06820461384152457,
        "f1": 0.032259398180096786,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ],
        "main_score": 0.032259398180096786,
        "precision": 0.02643776577482203,
        "recall": 0.06820461384152457
      },
      {
        "accuracy": 0.023069207622868605,
        "f1": 0.022400534938147774,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ],
        "main_score": 0.022400534938147774,
        "precision": 0.022066198595787363,
        "recall": 0.023069207622868605
      },
      {
        "accuracy": 0.11033099297893681,
        "f1": 0.06301847672030175,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ],
        "main_score": 0.06301847672030175,
        "precision": 0.054768283812450066,
        "recall": 0.11033099297893681
      },
      {
        "accuracy": 0.03610832497492478,
        "f1": 0.033502644686195335,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ],
        "main_score": 0.033502644686195335,
        "precision": 0.03284961837383809,
        "recall": 0.03610832497492478
      },
      {
        "accuracy": 0.10130391173520562,
        "f1": 0.04774618110591527,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ],
        "main_score": 0.04774618110591527,
        "precision": 0.03995541697207408,
        "recall": 0.10130391173520562
      },
      {
        "accuracy": 0.08525576730190572,
        "f1": 0.07100576727377139,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ],
        "main_score": 0.07100576727377139,
        "precision": 0.06721608628716008,
        "recall": 0.08525576730190572
      },
      {
        "accuracy": 0.11534603811434303,
        "f1": 0.06503349257936548,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ],
        "main_score": 0.06503349257936548,
        "precision": 0.056926905118461824,
        "recall": 0.11534603811434303
      },
      {
        "accuracy": 0.08224674022066199,
        "f1": 0.06932048734194302,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ],
        "main_score": 0.06932048734194302,
        "precision": 0.0649766760599258,
        "recall": 0.08224674022066199
      },
      {
        "accuracy": 0.11534603811434303,
        "f1": 0.06876899678374879,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ],
        "main_score": 0.06876899678374879,
        "precision": 0.06067155199837176,
        "recall": 0.11534603811434303
      },
      {
        "accuracy": 0.14744232698094284,
        "f1": 0.13664820650512238,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ],
        "main_score": 0.13664820650512238,
        "precision": 0.13350415001618612,
        "recall": 0.14744232698094284
      },
      {
        "accuracy": 0.17251755265797392,
        "f1": 0.09345358649510234,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ],
        "main_score": 0.09345358649510234,
        "precision": 0.07627246940465232,
        "recall": 0.17251755265797392
      },
      {
        "accuracy": 0.04312938816449348,
        "f1": 0.039286698314269626,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ],
        "main_score": 0.039286698314269626,
        "precision": 0.03794826508511041,
        "recall": 0.04312938816449348
      },
      {
        "accuracy": 0.09729187562688064,
        "f1": 0.048531317909370666,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ],
        "main_score": 0.048531317909370666,
        "precision": 0.041005808871741455,
        "recall": 0.09729187562688064
      },
      {
        "accuracy": 0.010030090270812437,
        "f1": 0.008093016108366506,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.008093016108366506,
        "precision": 0.007774359347472469,
        "recall": 0.010030090270812437
      },
      {
        "accuracy": 0.10030090270812438,
        "f1": 0.06116761624165657,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ],
        "main_score": 0.06116761624165657,
        "precision": 0.05470567194077909,
        "recall": 0.10030090270812438
      },
      {
        "accuracy": 0.012036108324974924,
        "f1": 0.01053366284420271,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01053366284420271,
        "precision": 0.010365461710208024,
        "recall": 0.012036108324974924
      },
      {
        "accuracy": 0.09127382146439318,
        "f1": 0.056240423074138735,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ],
        "main_score": 0.056240423074138735,
        "precision": 0.05079988777654866,
        "recall": 0.09127382146439318
      },
      {
        "accuracy": 0.028084252758274825,
        "f1": 0.025416479323026552,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ],
        "main_score": 0.025416479323026552,
        "precision": 0.024777793588377593,
        "recall": 0.028084252758274825
      },
      {
        "accuracy": 0.07221664994984955,
        "f1": 0.03215353652715552,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ],
        "main_score": 0.03215353652715552,
        "precision": 0.026761488222674122,
        "recall": 0.07221664994984955
      },
      {
        "accuracy": 0.015045135406218655,
        "f1": 0.012706881552160845,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ],
        "main_score": 0.012706881552160845,
        "precision": 0.01223776150253696,
        "recall": 0.015045135406218655
      },
      {
        "accuracy": 0.10030090270812438,
        "f1": 0.05766651096087211,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ],
        "main_score": 0.05766651096087211,
        "precision": 0.050617349734348435,
        "recall": 0.10030090270812438
      },
      {
        "accuracy": 0.01805416248746239,
        "f1": 0.016718891593774775,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ],
        "main_score": 0.016718891593774775,
        "precision": 0.016550687258462756,
        "recall": 0.01805416248746239
      },
      {
        "accuracy": 0.08826479438314945,
        "f1": 0.051421052218168,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ],
        "main_score": 0.051421052218168,
        "precision": 0.04528854439916569,
        "recall": 0.08826479438314945
      },
      {
        "accuracy": 0.02106318956870612,
        "f1": 0.01939360182146998,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01939360182146998,
        "precision": 0.019058219590851973,
        "recall": 0.02106318956870612
      },
      {
        "accuracy": 0.1213640922768305,
        "f1": 0.06508515973354805,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ],
        "main_score": 0.06508515973354805,
        "precision": 0.05472909864187937,
        "recall": 0.1213640922768305
      },
      {
        "accuracy": 0.006018054162487462,
        "f1": 0.004515615097619643,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ],
        "main_score": 0.004515615097619643,
        "precision": 0.004347410762307627,
        "recall": 0.006018054162487462
      },
      {
        "accuracy": 0.09327983951855567,
        "f1": 0.060555619382375456,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ],
        "main_score": 0.060555619382375456,
        "precision": 0.054850294094796734,
        "recall": 0.09327983951855567
      },
      {
        "accuracy": 0.0160481444332999,
        "f1": 0.013877045635693065,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ],
        "main_score": 0.013877045635693065,
        "precision": 0.01337449849548646,
        "recall": 0.0160481444332999
      },
      {
        "accuracy": 0.10230692076228685,
        "f1": 0.05724108211819022,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ],
        "main_score": 0.05724108211819022,
        "precision": 0.05055347154759357,
        "recall": 0.10230692076228685
      },
      {
        "accuracy": 0.01805416248746239,
        "f1": 0.016549648946840523,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ],
        "main_score": 0.016549648946840523,
        "precision": 0.015880976262119692,
        "recall": 0.01805416248746239
      },
      {
        "accuracy": 0.10732196589769308,
        "f1": 0.06271675315671527,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ],
        "main_score": 0.06271675315671527,
        "precision": 0.05554131977109284,
        "recall": 0.10732196589769308
      },
      {
        "accuracy": 0.07221664994984955,
        "f1": 0.059978456096771805,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ],
        "main_score": 0.059978456096771805,
        "precision": 0.05688263399749595,
        "recall": 0.07221664994984955
      },
      {
        "accuracy": 0.10631895687061184,
        "f1": 0.05265975388154807,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ],
        "main_score": 0.05265975388154807,
        "precision": 0.043688626536350916,
        "recall": 0.10631895687061184
      },
      {
        "accuracy": 0.009027081243731194,
        "f1": 0.007357440242055429,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ],
        "main_score": 0.007357440242055429,
        "precision": 0.00702208458368997,
        "recall": 0.009027081243731194
      },
      {
        "accuracy": 0.09327983951855567,
        "f1": 0.056151436630068545,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ],
        "main_score": 0.056151436630068545,
        "precision": 0.049961216322881905,
        "recall": 0.09327983951855567
      },
      {
        "accuracy": 0.012036108324974924,
        "f1": 0.010199313788359615,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ],
        "main_score": 0.010199313788359615,
        "precision": 0.009863950826839493,
        "recall": 0.012036108324974924
      },
      {
        "accuracy": 0.09729187562688064,
        "f1": 0.05868471521765719,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ],
        "main_score": 0.05868471521765719,
        "precision": 0.05265627372201448,
        "recall": 0.09729187562688064
      },
      {
        "accuracy": 0.010030090270812437,
        "f1": 0.008268394928374868,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ],
        "main_score": 0.008268394928374868,
        "precision": 0.007897024406552992,
        "recall": 0.010030090270812437
      },
      {
        "accuracy": 0.020060180541624874,
        "f1": 0.003401612008049635,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ],
        "main_score": 0.003401612008049635,
        "precision": 0.002058866072625686,
        "recall": 0.020060180541624874
      }
    ]
  },
  "task_name": "IndicGenBenchFloresBitextMining"
}