{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 10597.73523902893,
  "kg_co2_emissions": 0.7599005884975694,
  "mteb_version": "1.18.0",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.05835150258605314,
        "map": 0.0772521500075875,
        "mrr": 0.05835150258605314,
        "nAUC_map_diff1": -0.12688505306219908,
        "nAUC_map_max": -0.13653144385231258,
        "nAUC_map_std": 0.24127373860810133,
        "nAUC_mrr_diff1": -0.10604851336380337,
        "nAUC_mrr_max": -0.14145834398932303,
        "nAUC_mrr_std": 0.18239908871583377
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08696562568613016,
        "map": 0.1078361522160272,
        "mrr": 0.08696562568613016,
        "nAUC_map_diff1": 0.07274565241665368,
        "nAUC_map_max": -0.016562240599570777,
        "nAUC_map_std": -0.028977031014944355,
        "nAUC_mrr_diff1": 0.059419600553112066,
        "nAUC_mrr_max": -0.021448683878740604,
        "nAUC_mrr_std": -0.034377571755950284
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08547861889308343,
        "map": 0.10490820862277618,
        "mrr": 0.08547861889308343,
        "nAUC_map_diff1": 0.08389973622220358,
        "nAUC_map_max": 0.08208839268228268,
        "nAUC_map_std": 0.19387086122696792,
        "nAUC_mrr_diff1": 0.07764338283748515,
        "nAUC_mrr_max": 0.09254144407832077,
        "nAUC_mrr_std": 0.19389197626190466
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08969369992869426,
        "map": 0.10856701803713496,
        "mrr": 0.08969369992869426,
        "nAUC_map_diff1": -0.009521642203294082,
        "nAUC_map_max": -0.059328499708926626,
        "nAUC_map_std": 0.1532183760675008,
        "nAUC_mrr_diff1": -0.0034435447003541494,
        "nAUC_mrr_max": -0.05487314515189706,
        "nAUC_mrr_std": 0.13444262432864026
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.06814049941435436,
        "map": 0.08520992785064652,
        "mrr": 0.06814049941435436,
        "nAUC_map_diff1": 0.11837948557721527,
        "nAUC_map_max": 0.0775270311000246,
        "nAUC_map_std": 0.20159170371699253,
        "nAUC_mrr_diff1": 0.11770364464305197,
        "nAUC_mrr_max": 0.07979688759319671,
        "nAUC_mrr_std": 0.18776238925687205
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.12124647706892135,
        "map": 0.1409662051755548,
        "mrr": 0.12124647706892135,
        "nAUC_map_diff1": 0.09242741265903519,
        "nAUC_map_max": -0.0968735003395129,
        "nAUC_map_std": 0.060910010220756976,
        "nAUC_mrr_diff1": 0.09330331057752135,
        "nAUC_mrr_max": -0.09286842345676005,
        "nAUC_mrr_std": 0.05612882272201166
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}