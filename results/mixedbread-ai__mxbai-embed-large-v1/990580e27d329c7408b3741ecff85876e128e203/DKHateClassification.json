{
  "dataset_revision": "59d12749a3c91a186063c7d729ec392fda94681c",
  "task_name": "DKHateClassification",
  "mteb_version": "1.34.21",
  "scores": {
    "test": [
      {
        "accuracy": 0.574772,
        "f1": 0.47504,
        "f1_weighted": 0.643563,
        "ap": 0.89053,
        "ap_weighted": 0.89053,
        "scores_per_experiment": [
          {
            "accuracy": 0.462006,
            "f1": 0.404796,
            "f1_weighted": 0.543335,
            "ap": 0.880994,
            "ap_weighted": 0.880994
          },
          {
            "accuracy": 0.601824,
            "f1": 0.497335,
            "f1_weighted": 0.669393,
            "ap": 0.896897,
            "ap_weighted": 0.896897
          },
          {
            "accuracy": 0.699088,
            "f1": 0.570536,
            "f1_weighted": 0.746938,
            "ap": 0.912552,
            "ap_weighted": 0.912552
          },
          {
            "accuracy": 0.531915,
            "f1": 0.440476,
            "f1_weighted": 0.610291,
            "ap": 0.880555,
            "ap_weighted": 0.880555
          },
          {
            "accuracy": 0.620061,
            "f1": 0.498457,
            "f1_weighted": 0.683865,
            "ap": 0.891963,
            "ap_weighted": 0.891963
          },
          {
            "accuracy": 0.544073,
            "f1": 0.441489,
            "f1_weighted": 0.621193,
            "ap": 0.877458,
            "ap_weighted": 0.877458
          },
          {
            "accuracy": 0.455927,
            "f1": 0.39807,
            "f1_weighted": 0.538174,
            "ap": 0.877891,
            "ap_weighted": 0.877891
          },
          {
            "accuracy": 0.559271,
            "f1": 0.474713,
            "f1_weighted": 0.632938,
            "ap": 0.89618,
            "ap_weighted": 0.89618
          },
          {
            "accuracy": 0.711246,
            "f1": 0.557326,
            "f1_weighted": 0.753297,
            "ap": 0.901579,
            "ap_weighted": 0.901579
          },
          {
            "accuracy": 0.56231,
            "f1": 0.467206,
            "f1_weighted": 0.636203,
            "ap": 0.889229,
            "ap_weighted": 0.889229
          }
        ],
        "main_score": 0.574772,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 70.60862612724304,
  "kg_co2_emissions": null
}