{
  "dataset_revision": "9abd46cf7fc8b4c64290f26993c540b92aa145ac",
  "evaluation_time": 17.13159155845642,
  "kg_co2_emissions": 0.0007697004089590811,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.857275390625,
        "f1": 0.85313603101404,
        "f1_weighted": 0.8531674176026078,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.857275390625,
        "scores_per_experiment": [
          {
            "accuracy": 0.861328125,
            "f1": 0.858809654666851,
            "f1_weighted": 0.8588730062418496
          },
          {
            "accuracy": 0.86181640625,
            "f1": 0.8566230930434523,
            "f1_weighted": 0.8566608691471246
          },
          {
            "accuracy": 0.85595703125,
            "f1": 0.8525120882051969,
            "f1_weighted": 0.8525324745199233
          },
          {
            "accuracy": 0.8486328125,
            "f1": 0.8443944247723879,
            "f1_weighted": 0.8443952553458556
          },
          {
            "accuracy": 0.84130859375,
            "f1": 0.8380070755302695,
            "f1_weighted": 0.8380575174861065
          },
          {
            "accuracy": 0.87109375,
            "f1": 0.8644406021601052,
            "f1_weighted": 0.8644448797590353
          },
          {
            "accuracy": 0.8603515625,
            "f1": 0.8559974604672832,
            "f1_weighted": 0.8560399847308342
          },
          {
            "accuracy": 0.83984375,
            "f1": 0.8352696169192938,
            "f1_weighted": 0.8352956689710359
          },
          {
            "accuracy": 0.87451171875,
            "f1": 0.8709322005416374,
            "f1_weighted": 0.8709595622602301
          },
          {
            "accuracy": 0.85791015625,
            "f1": 0.8543740938339228,
            "f1_weighted": 0.8544149575640831
          }
        ]
      }
    ]
  },
  "task_name": "DBpediaClassification"
}