{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "evaluation_time": 20.396709442138672,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.515234375,
        "ap": 0.5081697568026293,
        "ap_weighted": 0.5081697568026293,
        "f1": 0.5123938457720904,
        "f1_weighted": 0.5123938457720904,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.515234375,
        "scores_per_experiment": [
          {
            "accuracy": 0.51708984375,
            "ap": 0.5089022364097968,
            "ap_weighted": 0.5089022364097968,
            "f1": 0.5130298473193431,
            "f1_weighted": 0.5130298473193431
          },
          {
            "accuracy": 0.53076171875,
            "ap": 0.5162169198932269,
            "ap_weighted": 0.5162169198932269,
            "f1": 0.5287138964564607,
            "f1_weighted": 0.5287138964564607
          },
          {
            "accuracy": 0.509765625,
            "ap": 0.5049671443221071,
            "ap_weighted": 0.5049671443221071,
            "f1": 0.5076578867469856,
            "f1_weighted": 0.5076578867469856
          },
          {
            "accuracy": 0.48291015625,
            "ap": 0.49182567572800495,
            "ap_weighted": 0.49182567572800495,
            "f1": 0.477038928533968,
            "f1_weighted": 0.477038928533968
          },
          {
            "accuracy": 0.5322265625,
            "ap": 0.5170889478211009,
            "ap_weighted": 0.5170889478211009,
            "f1": 0.5317402507143287,
            "f1_weighted": 0.5317402507143287
          },
          {
            "accuracy": 0.53173828125,
            "ap": 0.5169016672922924,
            "ap_weighted": 0.5169016672922924,
            "f1": 0.5316684944174315,
            "f1_weighted": 0.5316684944174315
          },
          {
            "accuracy": 0.53955078125,
            "ap": 0.5214996173641012,
            "ap_weighted": 0.5214996173641012,
            "f1": 0.5385578834768243,
            "f1_weighted": 0.5385578834768243
          },
          {
            "accuracy": 0.4951171875,
            "ap": 0.49758365952258726,
            "ap_weighted": 0.49758365952258726,
            "f1": 0.4948160744156931,
            "f1_weighted": 0.4948160744156931
          },
          {
            "accuracy": 0.51171875,
            "ap": 0.5059763675124792,
            "ap_weighted": 0.5059763675124792,
            "f1": 0.508002171709164,
            "f1_weighted": 0.508002171709164
          },
          {
            "accuracy": 0.50146484375,
            "ap": 0.5007353321605961,
            "ap_weighted": 0.5007353321605961,
            "f1": 0.4927130239307045,
            "f1_weighted": 0.4927130239307045
          }
        ]
      }
    ]
  },
  "task_name": "InappropriatenessClassification"
}