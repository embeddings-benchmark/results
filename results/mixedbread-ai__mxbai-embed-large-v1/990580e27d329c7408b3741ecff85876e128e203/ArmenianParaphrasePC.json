{
  "dataset_revision": "f43b4f32987048043a8b31e5e26be4d360c2438f",
  "evaluation_time": 8.78101897239685,
  "kg_co2_emissions": 0.0004918508133964471,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.7612244897959184,
        "cosine_accuracy_threshold": 0.8643739223480225,
        "cosine_ap": 0.8716392150085285,
        "cosine_f1": 0.8306010928961749,
        "cosine_f1_threshold": 0.8028419017791748,
        "cosine_precision": 0.7761702127659574,
        "cosine_recall": 0.8932419196865817,
        "dot_accuracy": 0.7428571428571429,
        "dot_accuracy_threshold": 294.0711669921875,
        "dot_ap": 0.8471867411914324,
        "dot_f1": 0.8289755679382769,
        "dot_f1_threshold": 276.66632080078125,
        "dot_precision": 0.7370426829268293,
        "dot_recall": 0.9471106758080313,
        "euclidean_accuracy": 0.753061224489796,
        "euclidean_accuracy_threshold": 10.308734893798828,
        "euclidean_ap": 0.8707185737683181,
        "euclidean_f1": 0.8276497695852534,
        "euclidean_f1_threshold": 11.787585258483887,
        "euclidean_precision": 0.7815491731940818,
        "euclidean_recall": 0.8795298726738492,
        "hf_subset": "default",
        "languages": [
          "hye-Armn"
        ],
        "main_score": 0.8716392150085285,
        "manhattan_accuracy": 0.7523809523809524,
        "manhattan_accuracy_threshold": 255.8012237548828,
        "manhattan_ap": 0.8708885975531304,
        "manhattan_f1": 0.827996340347667,
        "manhattan_f1_threshold": 304.865234375,
        "manhattan_precision": 0.776824034334764,
        "manhattan_recall": 0.8863858961802155,
        "max_accuracy": 0.7612244897959184,
        "max_ap": 0.8716392150085285,
        "max_f1": 0.8306010928961749,
        "max_precision": 0.7815491731940818,
        "max_recall": 0.9471106758080313,
        "similarity_accuracy": 0.7612244897959184,
        "similarity_accuracy_threshold": 0.8643739223480225,
        "similarity_ap": 0.8716392150085285,
        "similarity_f1": 0.8306010928961749,
        "similarity_f1_threshold": 0.8028419017791748,
        "similarity_precision": 0.7761702127659574,
        "similarity_recall": 0.8932419196865817
      }
    ]
  },
  "task_name": "ArmenianParaphrasePC"
}