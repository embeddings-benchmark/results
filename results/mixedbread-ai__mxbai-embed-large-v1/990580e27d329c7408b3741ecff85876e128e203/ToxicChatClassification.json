{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.787285,
        "f1": 0.666465,
        "f1_weighted": 0.815423,
        "ap": 0.288373,
        "ap_weighted": 0.288373,
        "scores_per_experiment": [
          {
            "accuracy": 0.667526,
            "f1": 0.556192,
            "f1_weighted": 0.722715,
            "ap": 0.190246,
            "ap_weighted": 0.190246
          },
          {
            "accuracy": 0.835911,
            "f1": 0.706451,
            "f1_weighted": 0.852491,
            "ap": 0.319083,
            "ap_weighted": 0.319083
          },
          {
            "accuracy": 0.830756,
            "f1": 0.715433,
            "f1_weighted": 0.851144,
            "ap": 0.344087,
            "ap_weighted": 0.344087
          },
          {
            "accuracy": 0.831615,
            "f1": 0.708762,
            "f1_weighted": 0.850466,
            "ap": 0.328101,
            "ap_weighted": 0.328101
          },
          {
            "accuracy": 0.828179,
            "f1": 0.665302,
            "f1_weighted": 0.840214,
            "ap": 0.252184,
            "ap_weighted": 0.252184
          },
          {
            "accuracy": 0.802405,
            "f1": 0.683209,
            "f1_weighted": 0.828782,
            "ap": 0.303549,
            "ap_weighted": 0.303549
          },
          {
            "accuracy": 0.823883,
            "f1": 0.688732,
            "f1_weighted": 0.842385,
            "ap": 0.294443,
            "ap_weighted": 0.294443
          },
          {
            "accuracy": 0.64433,
            "f1": 0.560647,
            "f1_weighted": 0.704291,
            "ap": 0.218107,
            "ap_weighted": 0.218107
          },
          {
            "accuracy": 0.806701,
            "f1": 0.686368,
            "f1_weighted": 0.831903,
            "ap": 0.305783,
            "ap_weighted": 0.305783
          },
          {
            "accuracy": 0.801546,
            "f1": 0.69355,
            "f1_weighted": 0.829835,
            "ap": 0.328147,
            "ap_weighted": 0.328147
          }
        ],
        "main_score": 0.787285,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 9.655133485794067,
  "kg_co2_emissions": 0.00040168489481152075
}