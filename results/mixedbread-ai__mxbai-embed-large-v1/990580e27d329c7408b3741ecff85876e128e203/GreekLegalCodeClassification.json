{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "evaluation_time": 2057.2259018421173,
  "kg_co2_emissions": 0.08013091914190588,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.079345703125,
        "f1": 0.0689507749534526,
        "f1_weighted": 0.08398177303079093,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ],
        "main_score": 0.079345703125,
        "scores_per_experiment": [
          {
            "accuracy": 0.07177734375,
            "f1": 0.06476020210896583,
            "f1_weighted": 0.07312083710515656
          },
          {
            "accuracy": 0.0830078125,
            "f1": 0.06908605917121317,
            "f1_weighted": 0.08973201033405044
          },
          {
            "accuracy": 0.07568359375,
            "f1": 0.06712578630042661,
            "f1_weighted": 0.07830846284389445
          },
          {
            "accuracy": 0.0830078125,
            "f1": 0.07413059085041811,
            "f1_weighted": 0.08976973615684354
          },
          {
            "accuracy": 0.08154296875,
            "f1": 0.0706364641519888,
            "f1_weighted": 0.0890271785805632
          },
          {
            "accuracy": 0.07666015625,
            "f1": 0.06543662846423005,
            "f1_weighted": 0.07932176651296732
          },
          {
            "accuracy": 0.08154296875,
            "f1": 0.0668480302790178,
            "f1_weighted": 0.0850532099951028
          },
          {
            "accuracy": 0.08251953125,
            "f1": 0.07552527851141713,
            "f1_weighted": 0.08653004132503792
          },
          {
            "accuracy": 0.08642578125,
            "f1": 0.06941900978104108,
            "f1_weighted": 0.09324718274596656
          },
          {
            "accuracy": 0.0712890625,
            "f1": 0.06653969991580748,
            "f1_weighted": 0.07570730470832654
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.083837890625,
        "f1": 0.0722390499013548,
        "f1_weighted": 0.0886805188873651,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ],
        "main_score": 0.083837890625,
        "scores_per_experiment": [
          {
            "accuracy": 0.080078125,
            "f1": 0.07111885213891901,
            "f1_weighted": 0.0827424980297205
          },
          {
            "accuracy": 0.07177734375,
            "f1": 0.06446076080144435,
            "f1_weighted": 0.07500314104769837
          },
          {
            "accuracy": 0.0810546875,
            "f1": 0.07155841978684264,
            "f1_weighted": 0.08953956051435563
          },
          {
            "accuracy": 0.08056640625,
            "f1": 0.06947444676341324,
            "f1_weighted": 0.08485594692153081
          },
          {
            "accuracy": 0.09423828125,
            "f1": 0.07845586095583494,
            "f1_weighted": 0.09727738537814246
          },
          {
            "accuracy": 0.08642578125,
            "f1": 0.06887319358006286,
            "f1_weighted": 0.09264146962135011
          },
          {
            "accuracy": 0.0869140625,
            "f1": 0.07422559808076197,
            "f1_weighted": 0.09241386812181838
          },
          {
            "accuracy": 0.09130859375,
            "f1": 0.08359689797798465,
            "f1_weighted": 0.09689958624271927
          },
          {
            "accuracy": 0.0830078125,
            "f1": 0.07031744524473069,
            "f1_weighted": 0.08785551194924686
          },
          {
            "accuracy": 0.0830078125,
            "f1": 0.07030902368355375,
            "f1_weighted": 0.08757622104706858
          }
        ]
      }
    ]
  },
  "task_name": "GreekLegalCodeClassification"
}