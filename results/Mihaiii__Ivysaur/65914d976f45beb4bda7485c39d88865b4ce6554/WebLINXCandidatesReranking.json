{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "task_name": "WebLINXCandidatesReranking",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "map": 0.136099,
        "mrr": 0.118352,
        "nAUC_map_max": 0.018232,
        "nAUC_map_std": 0.020082,
        "nAUC_map_diff1": 0.28443,
        "nAUC_mrr_max": 0.022519,
        "nAUC_mrr_std": 0.023258,
        "nAUC_mrr_diff1": 0.288044,
        "main_score": 0.118352,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_iid": [
      {
        "map": 0.109064,
        "mrr": 0.091731,
        "nAUC_map_max": 0.043901,
        "nAUC_map_std": 0.09703,
        "nAUC_map_diff1": 0.068512,
        "nAUC_mrr_max": 0.058937,
        "nAUC_mrr_std": 0.085801,
        "nAUC_mrr_diff1": 0.06475,
        "main_score": 0.091731,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_cat": [
      {
        "map": 0.078043,
        "mrr": 0.060288,
        "nAUC_map_max": 0.046733,
        "nAUC_map_std": -0.053942,
        "nAUC_map_diff1": 0.147448,
        "nAUC_mrr_max": 0.046333,
        "nAUC_mrr_std": -0.058427,
        "nAUC_mrr_diff1": 0.159712,
        "main_score": 0.060288,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_geo": [
      {
        "map": 0.11196,
        "mrr": 0.093613,
        "nAUC_map_max": -0.008882,
        "nAUC_map_std": 0.044767,
        "nAUC_map_diff1": 0.074283,
        "nAUC_mrr_max": -0.011561,
        "nAUC_mrr_std": 0.031442,
        "nAUC_mrr_diff1": 0.072172,
        "main_score": 0.093613,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_vis": [
      {
        "map": 0.102676,
        "mrr": 0.085066,
        "nAUC_map_max": 0.020234,
        "nAUC_map_std": 0.089882,
        "nAUC_map_diff1": 0.051504,
        "nAUC_mrr_max": 0.023883,
        "nAUC_mrr_std": 0.082807,
        "nAUC_mrr_diff1": 0.056324,
        "main_score": 0.085066,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_web": [
      {
        "map": 0.08527,
        "mrr": 0.067867,
        "nAUC_map_max": 0.192689,
        "nAUC_map_std": 0.203205,
        "nAUC_map_diff1": 0.218632,
        "nAUC_mrr_max": 0.206078,
        "nAUC_mrr_std": 0.197552,
        "nAUC_mrr_diff1": 0.226811,
        "main_score": 0.067867,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 1136.862944841385,
  "kg_co2_emissions": 0.05585232381165446
}