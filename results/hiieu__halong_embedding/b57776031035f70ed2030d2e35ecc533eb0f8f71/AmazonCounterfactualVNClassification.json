{
  "dataset_revision": "b48bc27d383cfca5b6a47135a52390fa5f66b253",
  "task_name": "AmazonCounterfactualVNClassification",
  "mteb_version": "1.38.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.5603,
        "f1": 0.507862,
        "f1_weighted": 0.606781,
        "ap": 0.223249,
        "ap_weighted": 0.223249,
        "scores_per_experiment": [
          {
            "accuracy": 0.515021,
            "f1": 0.481794,
            "f1_weighted": 0.565144,
            "ap": 0.217953,
            "ap_weighted": 0.217953
          },
          {
            "accuracy": 0.611588,
            "f1": 0.558941,
            "f1_weighted": 0.655733,
            "ap": 0.254918,
            "ap_weighted": 0.254918
          },
          {
            "accuracy": 0.472103,
            "f1": 0.454374,
            "f1_weighted": 0.516847,
            "ap": 0.219646,
            "ap_weighted": 0.219646
          },
          {
            "accuracy": 0.491416,
            "f1": 0.46198,
            "f1_weighted": 0.541917,
            "ap": 0.209357,
            "ap_weighted": 0.209357
          },
          {
            "accuracy": 0.538627,
            "f1": 0.466833,
            "f1_weighted": 0.591107,
            "ap": 0.186202,
            "ap_weighted": 0.186202
          },
          {
            "accuracy": 0.590129,
            "f1": 0.543454,
            "f1_weighted": 0.636177,
            "ap": 0.248478,
            "ap_weighted": 0.248478
          },
          {
            "accuracy": 0.688841,
            "f1": 0.606664,
            "f1_weighted": 0.720863,
            "ap": 0.270725,
            "ap_weighted": 0.270725
          },
          {
            "accuracy": 0.579399,
            "f1": 0.516567,
            "f1_weighted": 0.627271,
            "ap": 0.216623,
            "ap_weighted": 0.216623
          },
          {
            "accuracy": 0.504292,
            "f1": 0.458878,
            "f1_weighted": 0.558452,
            "ap": 0.194667,
            "ap_weighted": 0.194667
          },
          {
            "accuracy": 0.611588,
            "f1": 0.529138,
            "f1_weighted": 0.654293,
            "ap": 0.21392,
            "ap_weighted": 0.21392
          }
        ],
        "main_score": 0.5603,
        "hf_subset": "default",
        "languages": [
          "vie-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 1.2444534301757812,
  "kg_co2_emissions": null
}