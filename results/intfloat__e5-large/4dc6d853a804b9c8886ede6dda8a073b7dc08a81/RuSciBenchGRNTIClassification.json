{
  "dataset_revision": "673a610d6d3dd91a547a0d57ae1b56f37ebbf6a1",
  "task_name": "RuSciBenchGRNTIClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.14834,
        "f1": 0.132294,
        "f1_weighted": 0.132412,
        "scores_per_experiment": [
          {
            "accuracy": 0.148926,
            "f1": 0.127571,
            "f1_weighted": 0.127704
          },
          {
            "accuracy": 0.148438,
            "f1": 0.129439,
            "f1_weighted": 0.129615
          },
          {
            "accuracy": 0.142578,
            "f1": 0.132297,
            "f1_weighted": 0.132329
          },
          {
            "accuracy": 0.159668,
            "f1": 0.143329,
            "f1_weighted": 0.143458
          },
          {
            "accuracy": 0.149414,
            "f1": 0.141841,
            "f1_weighted": 0.141879
          },
          {
            "accuracy": 0.138184,
            "f1": 0.116231,
            "f1_weighted": 0.116269
          },
          {
            "accuracy": 0.149902,
            "f1": 0.142856,
            "f1_weighted": 0.14309
          },
          {
            "accuracy": 0.144531,
            "f1": 0.119094,
            "f1_weighted": 0.11923
          },
          {
            "accuracy": 0.160645,
            "f1": 0.147363,
            "f1_weighted": 0.147477
          },
          {
            "accuracy": 0.141113,
            "f1": 0.122921,
            "f1_weighted": 0.123071
          }
        ],
        "main_score": 0.14834,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 41.797473430633545,
  "kg_co2_emissions": 0.0032211142803081267
}