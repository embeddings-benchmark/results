{
  "dataset_revision": "2269ed7d95d8abaab829f1592b4b2047372e9f81",
  "task_name": "DutchColaClassification",
  "mteb_version": "2.1.8",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.529167,
            "f1": 0.527906,
            "f1_weighted": 0.527906,
            "precision": 0.529481,
            "precision_weighted": 0.529481,
            "recall": 0.529167,
            "recall_weighted": 0.529167,
            "ap": 0.515354,
            "ap_weighted": 0.515354
          },
          {
            "accuracy": 0.527917,
            "f1": 0.52772,
            "f1_weighted": 0.52772,
            "precision": 0.527963,
            "precision_weighted": 0.527963,
            "recall": 0.527917,
            "recall_weighted": 0.527917,
            "ap": 0.514707,
            "ap_weighted": 0.514707
          },
          {
            "accuracy": 0.52,
            "f1": 0.519518,
            "f1_weighted": 0.519518,
            "precision": 0.520081,
            "precision_weighted": 0.520081,
            "recall": 0.52,
            "recall_weighted": 0.52,
            "ap": 0.510376,
            "ap_weighted": 0.510376
          },
          {
            "accuracy": 0.555417,
            "f1": 0.555352,
            "f1_weighted": 0.555352,
            "precision": 0.555449,
            "precision_weighted": 0.555449,
            "recall": 0.555417,
            "recall_weighted": 0.555417,
            "ap": 0.530707,
            "ap_weighted": 0.530707
          },
          {
            "accuracy": 0.545833,
            "f1": 0.545276,
            "f1_weighted": 0.545276,
            "precision": 0.546059,
            "precision_weighted": 0.546059,
            "recall": 0.545833,
            "recall_weighted": 0.545833,
            "ap": 0.525175,
            "ap_weighted": 0.525175
          },
          {
            "accuracy": 0.549167,
            "f1": 0.548183,
            "f1_weighted": 0.548183,
            "precision": 0.549599,
            "precision_weighted": 0.549599,
            "recall": 0.549167,
            "recall_weighted": 0.549167,
            "ap": 0.526794,
            "ap_weighted": 0.526794
          },
          {
            "accuracy": 0.53125,
            "f1": 0.53124,
            "f1_weighted": 0.53124,
            "precision": 0.531253,
            "precision_weighted": 0.531253,
            "recall": 0.53125,
            "recall_weighted": 0.53125,
            "ap": 0.516593,
            "ap_weighted": 0.516593
          },
          {
            "accuracy": 0.5275,
            "f1": 0.527311,
            "f1_weighted": 0.527311,
            "precision": 0.527544,
            "precision_weighted": 0.527544,
            "recall": 0.5275,
            "recall_weighted": 0.5275,
            "ap": 0.514538,
            "ap_weighted": 0.514538
          },
          {
            "accuracy": 0.545,
            "f1": 0.543069,
            "f1_weighted": 0.543069,
            "precision": 0.545774,
            "precision_weighted": 0.545774,
            "recall": 0.545,
            "recall_weighted": 0.545,
            "ap": 0.524292,
            "ap_weighted": 0.524292
          },
          {
            "accuracy": 0.55,
            "f1": 0.549995,
            "f1_weighted": 0.549995,
            "precision": 0.550002,
            "precision_weighted": 0.550002,
            "recall": 0.55,
            "recall_weighted": 0.55,
            "ap": 0.527483,
            "ap_weighted": 0.527483
          }
        ],
        "accuracy": 0.538125,
        "f1": 0.537557,
        "f1_weighted": 0.537557,
        "precision": 0.53832,
        "precision_weighted": 0.53832,
        "recall": 0.538125,
        "recall_weighted": 0.538125,
        "ap": 0.520602,
        "ap_weighted": 0.520602,
        "main_score": 0.537557,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 46.385027170181274,
  "kg_co2_emissions": null
}