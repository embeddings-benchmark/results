{
  "dataset_revision": "5a79d6472db143690c7ce6e974995d3610eee7f0",
  "task_name": "SanskritShlokasClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "train": [
      {
        "accuracy": 0.647258,
        "f1": 0.640526,
        "f1_weighted": 0.641204,
        "scores_per_experiment": [
          {
            "accuracy": 0.657963,
            "f1": 0.659183,
            "f1_weighted": 0.660845
          },
          {
            "accuracy": 0.699739,
            "f1": 0.684407,
            "f1_weighted": 0.689184
          },
          {
            "accuracy": 0.689295,
            "f1": 0.682704,
            "f1_weighted": 0.682261
          },
          {
            "accuracy": 0.569191,
            "f1": 0.549153,
            "f1_weighted": 0.54385
          },
          {
            "accuracy": 0.689295,
            "f1": 0.684863,
            "f1_weighted": 0.685719
          },
          {
            "accuracy": 0.650131,
            "f1": 0.64589,
            "f1_weighted": 0.648635
          },
          {
            "accuracy": 0.652742,
            "f1": 0.648882,
            "f1_weighted": 0.650755
          },
          {
            "accuracy": 0.613577,
            "f1": 0.607297,
            "f1_weighted": 0.603446
          },
          {
            "accuracy": 0.634465,
            "f1": 0.624612,
            "f1_weighted": 0.628315
          },
          {
            "accuracy": 0.616188,
            "f1": 0.618266,
            "f1_weighted": 0.619034
          }
        ],
        "main_score": 0.647258,
        "hf_subset": "default",
        "languages": [
          "san-Deva"
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.626042,
        "f1": 0.625586,
        "f1_weighted": 0.613511,
        "scores_per_experiment": [
          {
            "accuracy": 0.625,
            "f1": 0.620866,
            "f1_weighted": 0.613173
          },
          {
            "accuracy": 0.645833,
            "f1": 0.643219,
            "f1_weighted": 0.638197
          },
          {
            "accuracy": 0.666667,
            "f1": 0.670024,
            "f1_weighted": 0.660667
          },
          {
            "accuracy": 0.552083,
            "f1": 0.541196,
            "f1_weighted": 0.516472
          },
          {
            "accuracy": 0.677083,
            "f1": 0.678692,
            "f1_weighted": 0.665284
          },
          {
            "accuracy": 0.583333,
            "f1": 0.580488,
            "f1_weighted": 0.576875
          },
          {
            "accuracy": 0.614583,
            "f1": 0.619812,
            "f1_weighted": 0.603386
          },
          {
            "accuracy": 0.614583,
            "f1": 0.614846,
            "f1_weighted": 0.591778
          },
          {
            "accuracy": 0.708333,
            "f1": 0.708772,
            "f1_weighted": 0.706287
          },
          {
            "accuracy": 0.572917,
            "f1": 0.577945,
            "f1_weighted": 0.562996
          }
        ],
        "main_score": 0.626042,
        "hf_subset": "default",
        "languages": [
          "san-Deva"
        ]
      }
    ]
  },
  "evaluation_time": 29.33673644065857,
  "kg_co2_emissions": 0.0008907810734653355
}