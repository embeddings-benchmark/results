{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "task_name": "MTOPDomainClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.6227,
        "f1": 0.605649,
        "f1_weighted": 0.626543,
        "scores_per_experiment": [
          {
            "accuracy": 0.641873,
            "f1": 0.626004,
            "f1_weighted": 0.645434
          },
          {
            "accuracy": 0.579614,
            "f1": 0.569561,
            "f1_weighted": 0.584245
          },
          {
            "accuracy": 0.606612,
            "f1": 0.58858,
            "f1_weighted": 0.609841
          },
          {
            "accuracy": 0.642424,
            "f1": 0.625253,
            "f1_weighted": 0.648009
          },
          {
            "accuracy": 0.64573,
            "f1": 0.625146,
            "f1_weighted": 0.649207
          },
          {
            "accuracy": 0.581267,
            "f1": 0.558008,
            "f1_weighted": 0.579154
          },
          {
            "accuracy": 0.637466,
            "f1": 0.617992,
            "f1_weighted": 0.644332
          },
          {
            "accuracy": 0.635813,
            "f1": 0.625264,
            "f1_weighted": 0.643729
          },
          {
            "accuracy": 0.647383,
            "f1": 0.629957,
            "f1_weighted": 0.650499
          },
          {
            "accuracy": 0.608815,
            "f1": 0.590722,
            "f1_weighted": 0.610979
          }
        ],
        "main_score": 0.6227,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.636686,
        "f1": 0.614991,
        "f1_weighted": 0.640436,
        "scores_per_experiment": [
          {
            "accuracy": 0.662722,
            "f1": 0.644372,
            "f1_weighted": 0.666506
          },
          {
            "accuracy": 0.581572,
            "f1": 0.565469,
            "f1_weighted": 0.587372
          },
          {
            "accuracy": 0.622147,
            "f1": 0.601545,
            "f1_weighted": 0.626178
          },
          {
            "accuracy": 0.660186,
            "f1": 0.633896,
            "f1_weighted": 0.665488
          },
          {
            "accuracy": 0.649479,
            "f1": 0.624547,
            "f1_weighted": 0.651981
          },
          {
            "accuracy": 0.610876,
            "f1": 0.586161,
            "f1_weighted": 0.609586
          },
          {
            "accuracy": 0.651733,
            "f1": 0.626786,
            "f1_weighted": 0.659163
          },
          {
            "accuracy": 0.649197,
            "f1": 0.632201,
            "f1_weighted": 0.656144
          },
          {
            "accuracy": 0.664131,
            "f1": 0.642645,
            "f1_weighted": 0.666674
          },
          {
            "accuracy": 0.614821,
            "f1": 0.592286,
            "f1_weighted": 0.615266
          }
        ],
        "main_score": 0.636686,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 36.41907978057861,
  "kg_co2_emissions": null
}