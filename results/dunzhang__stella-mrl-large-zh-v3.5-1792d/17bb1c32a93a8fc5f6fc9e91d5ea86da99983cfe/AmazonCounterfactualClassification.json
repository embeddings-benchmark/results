{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "task_name": "AmazonCounterfactualClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.626609,
        "f1": 0.604729,
        "f1_weighted": 0.640847,
        "ap": 0.763605,
        "ap_weighted": 0.763605,
        "scores_per_experiment": [
          {
            "accuracy": 0.585837,
            "f1": 0.559962,
            "f1_weighted": 0.602094,
            "ap": 0.734491,
            "ap_weighted": 0.734491
          },
          {
            "accuracy": 0.639485,
            "f1": 0.60592,
            "f1_weighted": 0.651332,
            "ap": 0.754825,
            "ap_weighted": 0.754825
          },
          {
            "accuracy": 0.650215,
            "f1": 0.623095,
            "f1_weighted": 0.663015,
            "ap": 0.767937,
            "ap_weighted": 0.767937
          },
          {
            "accuracy": 0.630901,
            "f1": 0.621875,
            "f1_weighted": 0.644943,
            "ap": 0.788491,
            "ap_weighted": 0.788491
          },
          {
            "accuracy": 0.61588,
            "f1": 0.602008,
            "f1_weighted": 0.631346,
            "ap": 0.768054,
            "ap_weighted": 0.768054
          },
          {
            "accuracy": 0.656652,
            "f1": 0.640765,
            "f1_weighted": 0.670595,
            "ap": 0.789266,
            "ap_weighted": 0.789266
          },
          {
            "accuracy": 0.654506,
            "f1": 0.625833,
            "f1_weighted": 0.666731,
            "ap": 0.768386,
            "ap_weighted": 0.768386
          },
          {
            "accuracy": 0.553648,
            "f1": 0.533866,
            "f1_weighted": 0.571782,
            "ap": 0.724335,
            "ap_weighted": 0.724335
          },
          {
            "accuracy": 0.622318,
            "f1": 0.592528,
            "f1_weighted": 0.63603,
            "ap": 0.74965,
            "ap_weighted": 0.74965
          },
          {
            "accuracy": 0.656652,
            "f1": 0.641435,
            "f1_weighted": 0.670602,
            "ap": 0.790612,
            "ap_weighted": 0.790612
          }
        ],
        "main_score": 0.626609,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.609315,
        "f1": 0.587027,
        "f1_weighted": 0.623876,
        "ap": 0.751928,
        "ap_weighted": 0.751928,
        "scores_per_experiment": [
          {
            "accuracy": 0.58137,
            "f1": 0.553589,
            "f1_weighted": 0.597228,
            "ap": 0.728495,
            "ap_weighted": 0.728495
          },
          {
            "accuracy": 0.601713,
            "f1": 0.560504,
            "f1_weighted": 0.61324,
            "ap": 0.727195,
            "ap_weighted": 0.727195
          },
          {
            "accuracy": 0.646681,
            "f1": 0.623448,
            "f1_weighted": 0.6601,
            "ap": 0.769336,
            "ap_weighted": 0.769336
          },
          {
            "accuracy": 0.634904,
            "f1": 0.624288,
            "f1_weighted": 0.649036,
            "ap": 0.785013,
            "ap_weighted": 0.785013
          },
          {
            "accuracy": 0.588865,
            "f1": 0.571887,
            "f1_weighted": 0.605296,
            "ap": 0.745214,
            "ap_weighted": 0.745214
          },
          {
            "accuracy": 0.592077,
            "f1": 0.581716,
            "f1_weighted": 0.607513,
            "ap": 0.758465,
            "ap_weighted": 0.758465
          },
          {
            "accuracy": 0.623126,
            "f1": 0.593028,
            "f1_weighted": 0.636398,
            "ap": 0.748034,
            "ap_weighted": 0.748034
          },
          {
            "accuracy": 0.589936,
            "f1": 0.568753,
            "f1_weighted": 0.606206,
            "ap": 0.740224,
            "ap_weighted": 0.740224
          },
          {
            "accuracy": 0.598501,
            "f1": 0.569863,
            "f1_weighted": 0.613355,
            "ap": 0.73653,
            "ap_weighted": 0.73653
          },
          {
            "accuracy": 0.635974,
            "f1": 0.623196,
            "f1_weighted": 0.650387,
            "ap": 0.780769,
            "ap_weighted": 0.780769
          }
        ],
        "main_score": 0.609315,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 46.47205638885498,
  "kg_co2_emissions": null
}