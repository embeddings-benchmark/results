{
  "dataset_revision": "9fa4d0824015fe537ae2c8166781f5c79873da2c",
  "task_name": "IndonesianIdClickbaitClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "train": [
      {
        "accuracy": 0.508838,
        "f1": 0.502665,
        "f1_weighted": 0.506727,
        "ap": 0.426563,
        "ap_weighted": 0.426563,
        "scores_per_experiment": [
          {
            "accuracy": 0.513672,
            "f1": 0.509539,
            "f1_weighted": 0.516794,
            "ap": 0.425536,
            "ap_weighted": 0.425536
          },
          {
            "accuracy": 0.509277,
            "f1": 0.508897,
            "f1_weighted": 0.506694,
            "ap": 0.433506,
            "ap_weighted": 0.433506
          },
          {
            "accuracy": 0.516602,
            "f1": 0.508051,
            "f1_weighted": 0.518502,
            "ap": 0.423696,
            "ap_weighted": 0.423696
          },
          {
            "accuracy": 0.495117,
            "f1": 0.489223,
            "f1_weighted": 0.498064,
            "ap": 0.414935,
            "ap_weighted": 0.414935
          },
          {
            "accuracy": 0.52002,
            "f1": 0.504803,
            "f1_weighted": 0.51879,
            "ap": 0.421845,
            "ap_weighted": 0.421845
          },
          {
            "accuracy": 0.508301,
            "f1": 0.505319,
            "f1_weighted": 0.511507,
            "ap": 0.423911,
            "ap_weighted": 0.423911
          },
          {
            "accuracy": 0.508789,
            "f1": 0.50862,
            "f1_weighted": 0.507151,
            "ap": 0.432424,
            "ap_weighted": 0.432424
          },
          {
            "accuracy": 0.51709,
            "f1": 0.502986,
            "f1_weighted": 0.516477,
            "ap": 0.42091,
            "ap_weighted": 0.42091
          },
          {
            "accuracy": 0.51416,
            "f1": 0.511644,
            "f1_weighted": 0.505996,
            "ap": 0.440156,
            "ap_weighted": 0.440156
          },
          {
            "accuracy": 0.485352,
            "f1": 0.477567,
            "f1_weighted": 0.467291,
            "ap": 0.42871,
            "ap_weighted": 0.42871
          }
        ],
        "main_score": 0.502665,
        "hf_subset": "default",
        "languages": [
          "ind-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 11.0634183883667,
  "kg_co2_emissions": 0.00032623443811245013
}