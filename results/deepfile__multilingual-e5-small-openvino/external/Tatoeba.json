{
    "dataset_revision": "9080400076fbadbb4c4dcb136ff4eddc40b42553",
    "task_name": "Tatoeba",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "sqi-eng",
                "languages": [
                    "sqi-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.91,
                "f1": 0.8855666666666666,
                "precision": 0.8746166666666667,
                "recall": 0.91,
                "main_score": 0.8855666666666666
            },
            {
                "hf_subset": "fry-eng",
                "languages": [
                    "fry-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.5722543352601156,
                "f1": 0.5103220478943021,
                "precision": 0.48815028901734103,
                "recall": 0.5722543352601156,
                "main_score": 0.5103220478943021
            },
            {
                "hf_subset": "kur-eng",
                "languages": [
                    "kur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.4658536585365854,
                "f1": 0.3966870798578116,
                "precision": 0.37416085946573746,
                "recall": 0.4658536585365854,
                "main_score": 0.3966870798578116
            },
            {
                "hf_subset": "tur-eng",
                "languages": [
                    "tur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.897,
                "f1": 0.8677999999999999,
                "precision": 0.8545333333333331,
                "recall": 0.897,
                "main_score": 0.8677999999999999
            },
            {
                "hf_subset": "deu-eng",
                "languages": [
                    "deu-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9739999999999999,
                "f1": 0.9658333333333331,
                "precision": 0.9620000000000001,
                "recall": 0.9739999999999999,
                "main_score": 0.9658333333333331
            },
            {
                "hf_subset": "nld-eng",
                "languages": [
                    "nld-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.924,
                "f1": 0.903,
                "precision": 0.8931666666666668,
                "recall": 0.924,
                "main_score": 0.903
            },
            {
                "hf_subset": "ron-eng",
                "languages": [
                    "ron-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8690000000000001,
                "f1": 0.8367190476190476,
                "precision": 0.8223333333333332,
                "recall": 0.8690000000000001,
                "main_score": 0.8367190476190476
            },
            {
                "hf_subset": "ang-eng",
                "languages": [
                    "ang-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.5,
                "f1": 0.4223229092632078,
                "precision": 0.39851634683724235,
                "recall": 0.5,
                "main_score": 0.4223229092632078
            },
            {
                "hf_subset": "ido-eng",
                "languages": [
                    "ido-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.763,
                "f1": 0.7086190476190477,
                "precision": 0.6868777777777777,
                "recall": 0.763,
                "main_score": 0.7086190476190477
            },
            {
                "hf_subset": "jav-eng",
                "languages": [
                    "jav-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.5707317073170731,
                "f1": 0.5065895892725161,
                "precision": 0.48264808362369327,
                "recall": 0.5707317073170731,
                "main_score": 0.5065895892725161
            },
            {
                "hf_subset": "isl-eng",
                "languages": [
                    "isl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.682,
                "f1": 0.6215650793650793,
                "precision": 0.5984964285714286,
                "recall": 0.682,
                "main_score": 0.6215650793650793
            },
            {
                "hf_subset": "slv-eng",
                "languages": [
                    "slv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7752126366950182,
                "f1": 0.7284962101487009,
                "precision": 0.7092171498003819,
                "recall": 0.7752126366950182,
                "main_score": 0.7284962101487009
            },
            {
                "hf_subset": "cym-eng",
                "languages": [
                    "cym-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7078260869565217,
                "f1": 0.6532422360248447,
                "precision": 0.6306306736741519,
                "recall": 0.7078260869565217,
                "main_score": 0.6532422360248447
            },
            {
                "hf_subset": "kaz-eng",
                "languages": [
                    "kaz-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.7843478260869565,
                "f1": 0.7302608695652172,
                "precision": 0.7063768115942028,
                "recall": 0.7843478260869565,
                "main_score": 0.7302608695652172
            },
            {
                "hf_subset": "est-eng",
                "languages": [
                    "est-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.609,
                "f1": 0.5530975369458128,
                "precision": 0.531304761904762,
                "recall": 0.609,
                "main_score": 0.5530975369458128
            },
            {
                "hf_subset": "heb-eng",
                "languages": [
                    "heb-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.7289999999999999,
                "f1": 0.679202380952381,
                "precision": 0.6582595238095237,
                "recall": 0.7289999999999999,
                "main_score": 0.679202380952381
            },
            {
                "hf_subset": "gla-eng",
                "languages": [
                    "gla-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.4680337756332931,
                "f1": 0.39421749005584955,
                "precision": 0.3697101116280851,
                "recall": 0.4680337756332931,
                "main_score": 0.39421749005584955
            },
            {
                "hf_subset": "mar-eng",
                "languages": [
                    "mar-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.898,
                "f1": 0.8679000000000001,
                "precision": 0.85375,
                "recall": 0.898,
                "main_score": 0.8679000000000001
            },
            {
                "hf_subset": "lat-eng",
                "languages": [
                    "lat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.472,
                "f1": 0.3995484348984349,
                "precision": 0.37561071428571424,
                "recall": 0.472,
                "main_score": 0.3995484348984349
            },
            {
                "hf_subset": "bel-eng",
                "languages": [
                    "bel-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.878,
                "f1": 0.8468190476190475,
                "precision": 0.8327500000000001,
                "recall": 0.878,
                "main_score": 0.8468190476190475
            },
            {
                "hf_subset": "pms-eng",
                "languages": [
                    "pms-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.4876190476190476,
                "f1": 0.4214965986394558,
                "precision": 0.3996743626743626,
                "recall": 0.4876190476190476,
                "main_score": 0.4214965986394558
            },
            {
                "hf_subset": "gle-eng",
                "languages": [
                    "gle-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.661,
                "f1": 0.5958580086580086,
                "precision": 0.5715023809523809,
                "recall": 0.661,
                "main_score": 0.5958580086580086
            },
            {
                "hf_subset": "pes-eng",
                "languages": [
                    "pes-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.873,
                "f1": 0.84,
                "precision": 0.8248666666666666,
                "recall": 0.873,
                "main_score": 0.84
            },
            {
                "hf_subset": "nob-eng",
                "languages": [
                    "nob-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.904,
                "f1": 0.8779523809523809,
                "precision": 0.866,
                "recall": 0.904,
                "main_score": 0.8779523809523809
            },
            {
                "hf_subset": "bul-eng",
                "languages": [
                    "bul-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.87,
                "f1": 0.8381000000000001,
                "precision": 0.8236666666666665,
                "recall": 0.87,
                "main_score": 0.8381000000000001
            },
            {
                "hf_subset": "cbk-eng",
                "languages": [
                    "cbk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.639,
                "f1": 0.577653318903319,
                "precision": 0.5550595238095238,
                "recall": 0.639,
                "main_score": 0.577653318903319
            },
            {
                "hf_subset": "hun-eng",
                "languages": [
                    "hun-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7609999999999999,
                "f1": 0.7183690476190477,
                "precision": 0.7004928571428573,
                "recall": 0.7609999999999999,
                "main_score": 0.7183690476190477
            },
            {
                "hf_subset": "uig-eng",
                "languages": [
                    "uig-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.6629999999999999,
                "f1": 0.5932626984126984,
                "precision": 0.5662535714285714,
                "recall": 0.6629999999999999,
                "main_score": 0.5932626984126984
            },
            {
                "hf_subset": "rus-eng",
                "languages": [
                    "rus-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.921,
                "f1": 0.8976666666666666,
                "main_score": 0.8976666666666666,
                "precision": 0.8865,
                "recall": 0.921
            },
            {
                "hf_subset": "spa-eng",
                "languages": [
                    "spa-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.931,
                "f1": 0.911,
                "precision": 0.9016666666666666,
                "recall": 0.931,
                "main_score": 0.911
            },
            {
                "hf_subset": "hye-eng",
                "languages": [
                    "hye-Armn",
                    "eng-Latn"
                ],
                "accuracy": 0.8571428571428571,
                "f1": 0.8229142600436403,
                "precision": 0.808076626877166,
                "recall": 0.8571428571428571,
                "main_score": 0.8229142600436403
            },
            {
                "hf_subset": "tel-eng",
                "languages": [
                    "tel-Telu",
                    "eng-Latn"
                ],
                "accuracy": 0.8888888888888888,
                "f1": 0.8578347578347579,
                "precision": 0.8443732193732193,
                "recall": 0.8888888888888888,
                "main_score": 0.8578347578347579
            },
            {
                "hf_subset": "afr-eng",
                "languages": [
                    "afr-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.885,
                "f1": 0.8567190476190476,
                "precision": 0.8443333333333333,
                "recall": 0.885,
                "main_score": 0.8567190476190476
            },
            {
                "hf_subset": "mon-eng",
                "languages": [
                    "mon-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.8272727272727274,
                "f1": 0.7821969696969695,
                "precision": 0.7618181818181818,
                "recall": 0.8272727272727274,
                "main_score": 0.7821969696969695
            },
            {
                "hf_subset": "arz-eng",
                "languages": [
                    "arz-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.610062893081761,
                "f1": 0.5513976240391334,
                "precision": 0.5292112499659669,
                "recall": 0.610062893081761,
                "main_score": 0.5513976240391334
            },
            {
                "hf_subset": "hrv-eng",
                "languages": [
                    "hrv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.895,
                "f1": 0.8686666666666666,
                "precision": 0.8569166666666668,
                "recall": 0.895,
                "main_score": 0.8686666666666666
            },
            {
                "hf_subset": "nov-eng",
                "languages": [
                    "nov-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7354085603112841,
                "f1": 0.6856031128404669,
                "precision": 0.6653047989623866,
                "recall": 0.7354085603112841,
                "main_score": 0.6856031128404669
            },
            {
                "hf_subset": "gsw-eng",
                "languages": [
                    "gsw-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.4358974358974359,
                "f1": 0.36452991452991446,
                "precision": 0.3381155881155882,
                "recall": 0.4358974358974359,
                "main_score": 0.36452991452991446
            },
            {
                "hf_subset": "nds-eng",
                "languages": [
                    "nds-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.596,
                "f1": 0.5326468975468975,
                "precision": 0.5086916666666667,
                "recall": 0.596,
                "main_score": 0.5326468975468975
            },
            {
                "hf_subset": "ukr-eng",
                "languages": [
                    "ukr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.852,
                "f1": 0.8161666666666665,
                "precision": 0.8002833333333335,
                "recall": 0.852,
                "main_score": 0.8161666666666665
            },
            {
                "hf_subset": "uzb-eng",
                "languages": [
                    "uzb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.6378504672897196,
                "f1": 0.5800029669188548,
                "precision": 0.5581580996884735,
                "recall": 0.6378504672897196,
                "main_score": 0.5800029669188548
            },
            {
                "hf_subset": "lit-eng",
                "languages": [
                    "lit-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.665,
                "f1": 0.6151833333333334,
                "precision": 0.5962236369910283,
                "recall": 0.665,
                "main_score": 0.6151833333333334
            },
            {
                "hf_subset": "ina-eng",
                "languages": [
                    "ina-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8859999999999999,
                "f1": 0.8560222222222221,
                "precision": 0.8427916666666665,
                "recall": 0.8859999999999999,
                "main_score": 0.8560222222222221
            },
            {
                "hf_subset": "lfn-eng",
                "languages": [
                    "lfn-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.587,
                "f1": 0.5273237595737597,
                "precision": 0.5063214035964035,
                "recall": 0.587,
                "main_score": 0.5273237595737597
            },
            {
                "hf_subset": "zsm-eng",
                "languages": [
                    "zsm-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.921,
                "f1": 0.8999666666666667,
                "precision": 0.8903333333333333,
                "recall": 0.921,
                "main_score": 0.8999666666666667
            },
            {
                "hf_subset": "ita-eng",
                "languages": [
                    "ita-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9010000000000001,
                "f1": 0.8755666666666667,
                "precision": 0.8636166666666668,
                "recall": 0.9010000000000001,
                "main_score": 0.8755666666666667
            },
            {
                "hf_subset": "cmn-eng",
                "languages": [
                    "cmn-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.914,
                "f1": 0.8889000000000001,
                "precision": 0.8771166666666665,
                "recall": 0.914,
                "main_score": 0.8889000000000001
            },
            {
                "hf_subset": "lvs-eng",
                "languages": [
                    "lvs-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.657,
                "f1": 0.6067427750410509,
                "precision": 0.5871785714285714,
                "recall": 0.657,
                "main_score": 0.6067427750410509
            },
            {
                "hf_subset": "glg-eng",
                "languages": [
                    "glg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8539999999999999,
                "f1": 0.8193190476190475,
                "precision": 0.8037833333333333,
                "recall": 0.8539999999999999,
                "main_score": 0.8193190476190475
            },
            {
                "hf_subset": "ceb-eng",
                "languages": [
                    "ceb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.47833333333333333,
                "f1": 0.42006625781625784,
                "precision": 0.4007738095238096,
                "recall": 0.47833333333333333,
                "main_score": 0.42006625781625784
            },
            {
                "hf_subset": "bre-eng",
                "languages": [
                    "bre-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.10400000000000001,
                "f1": 0.08244650072150071,
                "precision": 0.0766459706959707,
                "recall": 0.10400000000000001,
                "main_score": 0.08244650072150071
            },
            {
                "hf_subset": "ben-eng",
                "languages": [
                    "ben-Beng",
                    "eng-Latn"
                ],
                "accuracy": 0.826,
                "f1": 0.7776333333333334,
                "precision": 0.7557833333333331,
                "recall": 0.826,
                "main_score": 0.7776333333333334
            },
            {
                "hf_subset": "swg-eng",
                "languages": [
                    "swg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.5267857142857143,
                "f1": 0.44302721088435376,
                "precision": 0.41498015873015875,
                "recall": 0.5267857142857143,
                "main_score": 0.44302721088435376
            },
            {
                "hf_subset": "arq-eng",
                "languages": [
                    "arq-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.283205268935236,
                "f1": 0.22426666605171156,
                "precision": 0.20685900116470915,
                "recall": 0.283205268935236,
                "main_score": 0.22426666605171156
            },
            {
                "hf_subset": "kab-eng",
                "languages": [
                    "kab-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.22699999999999998,
                "f1": 0.17833970473970473,
                "precision": 0.16407335164835163,
                "recall": 0.22699999999999998,
                "main_score": 0.17833970473970473
            },
            {
                "hf_subset": "fra-eng",
                "languages": [
                    "fra-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.922,
                "f1": 0.8992999999999999,
                "precision": 0.8887,
                "recall": 0.922,
                "main_score": 0.8992999999999999
            },
            {
                "hf_subset": "por-eng",
                "languages": [
                    "por-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.914,
                "f1": 0.8925,
                "precision": 0.8821666666666667,
                "recall": 0.914,
                "main_score": 0.8925
            },
            {
                "hf_subset": "tat-eng",
                "languages": [
                    "tat-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.6919999999999998,
                "f1": 0.6338269841269841,
                "precision": 0.6114773809523809,
                "recall": 0.6919999999999998,
                "main_score": 0.6338269841269841
            },
            {
                "hf_subset": "oci-eng",
                "languages": [
                    "oci-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.488,
                "f1": 0.42839915639915643,
                "precision": 0.4077028711484594,
                "recall": 0.488,
                "main_score": 0.42839915639915643
            },
            {
                "hf_subset": "pol-eng",
                "languages": [
                    "pol-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.888,
                "f1": 0.8590666666666668,
                "precision": 0.8454166666666666,
                "recall": 0.888,
                "main_score": 0.8590666666666668
            },
            {
                "hf_subset": "war-eng",
                "languages": [
                    "war-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.466,
                "f1": 0.40858929208046857,
                "precision": 0.388382231146047,
                "recall": 0.466,
                "main_score": 0.40858929208046857
            },
            {
                "hf_subset": "aze-eng",
                "languages": [
                    "aze-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.84,
                "f1": 0.8014190476190476,
                "precision": 0.7845333333333333,
                "recall": 0.84,
                "main_score": 0.8014190476190476
            },
            {
                "hf_subset": "vie-eng",
                "languages": [
                    "vie-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.905,
                "f1": 0.8778333333333334,
                "precision": 0.865,
                "recall": 0.905,
                "main_score": 0.8778333333333334
            },
            {
                "hf_subset": "nno-eng",
                "languages": [
                    "nno-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.745,
                "f1": 0.6948397546897547,
                "precision": 0.6751869047619049,
                "recall": 0.745,
                "main_score": 0.6948397546897547
            },
            {
                "hf_subset": "cha-eng",
                "languages": [
                    "cha-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.3284671532846716,
                "f1": 0.27828177499710344,
                "precision": 0.2663451511991658,
                "recall": 0.3284671532846716,
                "main_score": 0.27828177499710344
            },
            {
                "hf_subset": "mhr-eng",
                "languages": [
                    "mhr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.08,
                "f1": 0.0607664116764988,
                "precision": 0.05544177607179943,
                "recall": 0.08,
                "main_score": 0.0607664116764988
            },
            {
                "hf_subset": "dan-eng",
                "languages": [
                    "dan-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8759999999999999,
                "f1": 0.8438555555555554,
                "precision": 0.8291583333333334,
                "recall": 0.8759999999999999,
                "main_score": 0.8438555555555554
            },
            {
                "hf_subset": "ell-eng",
                "languages": [
                    "ell-Grek",
                    "eng-Latn"
                ],
                "accuracy": 0.875,
                "f1": 0.8408333333333331,
                "precision": 0.8247333333333333,
                "recall": 0.875,
                "main_score": 0.8408333333333331
            },
            {
                "hf_subset": "amh-eng",
                "languages": [
                    "amh-Ethi",
                    "eng-Latn"
                ],
                "accuracy": 0.8095238095238095,
                "f1": 0.7613095238095238,
                "precision": 0.7405753968253967,
                "recall": 0.8095238095238095,
                "main_score": 0.7613095238095238
            },
            {
                "hf_subset": "pam-eng",
                "languages": [
                    "pam-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.088,
                "f1": 0.06971422975172975,
                "precision": 0.06557814916172301,
                "recall": 0.088,
                "main_score": 0.06971422975172975
            },
            {
                "hf_subset": "hsb-eng",
                "languages": [
                    "hsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.4409937888198758,
                "f1": 0.37016497420224126,
                "precision": 0.34694206184889415,
                "recall": 0.4409937888198758,
                "main_score": 0.37016497420224126
            },
            {
                "hf_subset": "srp-eng",
                "languages": [
                    "srp-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.843,
                "f1": 0.8032666666666667,
                "precision": 0.7860666666666666,
                "recall": 0.843,
                "main_score": 0.8032666666666667
            },
            {
                "hf_subset": "epo-eng",
                "languages": [
                    "epo-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.925,
                "f1": 0.9049666666666666,
                "precision": 0.8956666666666667,
                "recall": 0.925,
                "main_score": 0.9049666666666666
            },
            {
                "hf_subset": "kzj-eng",
                "languages": [
                    "kzj-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.1,
                "f1": 0.08268423529875141,
                "precision": 0.07878118605532398,
                "recall": 0.1,
                "main_score": 0.08268423529875141
            },
            {
                "hf_subset": "awa-eng",
                "languages": [
                    "awa-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.7922077922077922,
                "f1": 0.7427128427128425,
                "precision": 0.7228715728715729,
                "recall": 0.7922077922077922,
                "main_score": 0.7427128427128425
            },
            {
                "hf_subset": "fao-eng",
                "languages": [
                    "fao-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.6564885496183206,
                "f1": 0.5887495456197747,
                "precision": 0.5599236641221373,
                "recall": 0.6564885496183206,
                "main_score": 0.5887495456197747
            },
            {
                "hf_subset": "mal-eng",
                "languages": [
                    "mal-Mlym",
                    "eng-Latn"
                ],
                "accuracy": 0.9606986899563319,
                "f1": 0.9478408539543909,
                "precision": 0.9415332362930616,
                "recall": 0.9606986899563319,
                "main_score": 0.9478408539543909
            },
            {
                "hf_subset": "ile-eng",
                "languages": [
                    "ile-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.772,
                "f1": 0.7172571428571427,
                "precision": 0.6941000000000002,
                "recall": 0.772,
                "main_score": 0.7172571428571427
            },
            {
                "hf_subset": "bos-eng",
                "languages": [
                    "bos-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.864406779661017,
                "f1": 0.832391713747646,
                "precision": 0.8174199623352166,
                "recall": 0.864406779661017,
                "main_score": 0.832391713747646
            },
            {
                "hf_subset": "cor-eng",
                "languages": [
                    "cor-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.084,
                "f1": 0.060178287433980036,
                "precision": 0.054829865484756796,
                "recall": 0.084,
                "main_score": 0.060178287433980036
            },
            {
                "hf_subset": "cat-eng",
                "languages": [
                    "cat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.835,
                "f1": 0.7974833333333333,
                "precision": 0.7804837662337664,
                "recall": 0.835,
                "main_score": 0.7974833333333333
            },
            {
                "hf_subset": "eus-eng",
                "languages": [
                    "eus-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.604,
                "f1": 0.5446730158730159,
                "precision": 0.5223242424242424,
                "recall": 0.604,
                "main_score": 0.5446730158730159
            },
            {
                "hf_subset": "yue-eng",
                "languages": [
                    "yue-Hant",
                    "eng-Latn"
                ],
                "accuracy": 0.7490000000000001,
                "f1": 0.6968699134199134,
                "precision": 0.6759873015873016,
                "recall": 0.7490000000000001,
                "main_score": 0.6968699134199134
            },
            {
                "hf_subset": "swe-eng",
                "languages": [
                    "swe-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.88,
                "f1": 0.849652380952381,
                "precision": 0.8366166666666666,
                "recall": 0.88,
                "main_score": 0.849652380952381
            },
            {
                "hf_subset": "dtp-eng",
                "languages": [
                    "dtp-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.091,
                "f1": 0.07681244588744587,
                "precision": 0.0737004329004329,
                "recall": 0.091,
                "main_score": 0.07681244588744587
            },
            {
                "hf_subset": "kat-eng",
                "languages": [
                    "kat-Geor",
                    "eng-Latn"
                ],
                "accuracy": 0.8096514745308311,
                "f1": 0.7684220605132133,
                "precision": 0.7519606398962967,
                "recall": 0.8096514745308311,
                "main_score": 0.7684220605132133
            },
            {
                "hf_subset": "jpn-eng",
                "languages": [
                    "jpn-Jpan",
                    "eng-Latn"
                ],
                "accuracy": 0.8690000000000001,
                "f1": 0.83705,
                "precision": 0.823120634920635,
                "recall": 0.8690000000000001,
                "main_score": 0.83705
            },
            {
                "hf_subset": "csb-eng",
                "languages": [
                    "csb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.2964426877470356,
                "f1": 0.2398763072676116,
                "precision": 0.22506399397703747,
                "recall": 0.2964426877470356,
                "main_score": 0.2398763072676116
            },
            {
                "hf_subset": "xho-eng",
                "languages": [
                    "xho-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.704225352112676,
                "f1": 0.6284037558685445,
                "precision": 0.5956572769953052,
                "recall": 0.704225352112676,
                "main_score": 0.6284037558685445
            },
            {
                "hf_subset": "orv-eng",
                "languages": [
                    "orv-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.19640718562874251,
                "f1": 0.15125271011207755,
                "precision": 0.13865019261197495,
                "recall": 0.19640718562874251,
                "main_score": 0.15125271011207755
            },
            {
                "hf_subset": "ind-eng",
                "languages": [
                    "ind-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.902,
                "f1": 0.8780666666666666,
                "precision": 0.8670833333333331,
                "recall": 0.902,
                "main_score": 0.8780666666666666
            },
            {
                "hf_subset": "tuk-eng",
                "languages": [
                    "tuk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.2315270935960591,
                "f1": 0.18407224958949098,
                "precision": 0.16982385430661293,
                "recall": 0.2315270935960591,
                "main_score": 0.18407224958949098
            },
            {
                "hf_subset": "max-eng",
                "languages": [
                    "max-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.5598591549295775,
                "f1": 0.49947183098591535,
                "precision": 0.47778641546247175,
                "recall": 0.5598591549295775,
                "main_score": 0.49947183098591535
            },
            {
                "hf_subset": "swh-eng",
                "languages": [
                    "swh-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7307692307692306,
                "f1": 0.6674358974358974,
                "precision": 0.6406837606837607,
                "recall": 0.7307692307692306,
                "main_score": 0.6674358974358974
            },
            {
                "hf_subset": "hin-eng",
                "languages": [
                    "hin-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.949,
                "f1": 0.9325,
                "precision": 0.9243333333333332,
                "recall": 0.949,
                "main_score": 0.9325
            },
            {
                "hf_subset": "dsb-eng",
                "languages": [
                    "dsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.3778705636743215,
                "f1": 0.3163899658680452,
                "precision": 0.2972264397629742,
                "recall": 0.3778705636743215,
                "main_score": 0.3163899658680452
            },
            {
                "hf_subset": "ber-eng",
                "languages": [
                    "ber-Tfng",
                    "eng-Latn"
                ],
                "accuracy": 0.21600000000000003,
                "f1": 0.16916973026973028,
                "precision": 0.1571225147075147,
                "recall": 0.21600000000000003,
                "main_score": 0.16916973026973028
            },
            {
                "hf_subset": "tam-eng",
                "languages": [
                    "tam-Taml",
                    "eng-Latn"
                ],
                "accuracy": 0.8501628664495116,
                "f1": 0.8138514037536838,
                "precision": 0.7983170466883823,
                "recall": 0.8501628664495116,
                "main_score": 0.8138514037536838
            },
            {
                "hf_subset": "slk-eng",
                "languages": [
                    "slk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.834,
                "f1": 0.7996380952380951,
                "precision": 0.7848333333333334,
                "recall": 0.834,
                "main_score": 0.7996380952380951
            },
            {
                "hf_subset": "tgl-eng",
                "languages": [
                    "tgl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8320000000000001,
                "f1": 0.7926190476190476,
                "precision": 0.7758833333333334,
                "recall": 0.8320000000000001,
                "main_score": 0.7926190476190476
            },
            {
                "hf_subset": "ast-eng",
                "languages": [
                    "ast-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7559055118110236,
                "f1": 0.7166854143232096,
                "precision": 0.703018372703412,
                "recall": 0.7559055118110236,
                "main_score": 0.7166854143232096
            },
            {
                "hf_subset": "mkd-eng",
                "languages": [
                    "mkd-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.655,
                "f1": 0.5926095238095238,
                "precision": 0.5681909090909092,
                "recall": 0.655,
                "main_score": 0.5926095238095238
            },
            {
                "hf_subset": "khm-eng",
                "languages": [
                    "khm-Khmr",
                    "eng-Latn"
                ],
                "accuracy": 0.5526315789473685,
                "f1": 0.47986523325858504,
                "precision": 0.45339500065954363,
                "recall": 0.5526315789473685,
                "main_score": 0.47986523325858504
            },
            {
                "hf_subset": "ces-eng",
                "languages": [
                    "ces-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.829,
                "f1": 0.7883499999999999,
                "precision": 0.7704761904761905,
                "recall": 0.829,
                "main_score": 0.7883499999999999
            },
            {
                "hf_subset": "tzl-eng",
                "languages": [
                    "tzl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.4326923076923077,
                "f1": 0.36204212454212453,
                "precision": 0.3357371794871795,
                "recall": 0.4326923076923077,
                "main_score": 0.36204212454212453
            },
            {
                "hf_subset": "urd-eng",
                "languages": [
                    "urd-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.88,
                "f1": 0.8470666666666666,
                "precision": 0.8323166666666666,
                "recall": 0.88,
                "main_score": 0.8470666666666666
            },
            {
                "hf_subset": "ara-eng",
                "languages": [
                    "ara-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.774,
                "f1": 0.7254666666666667,
                "precision": 0.7054318181818181,
                "recall": 0.774,
                "main_score": 0.7254666666666667
            },
            {
                "hf_subset": "kor-eng",
                "languages": [
                    "kor-Hang",
                    "eng-Latn"
                ],
                "accuracy": 0.786,
                "f1": 0.741588888888889,
                "precision": 0.7230250000000001,
                "recall": 0.786,
                "main_score": 0.741588888888889
            },
            {
                "hf_subset": "yid-eng",
                "languages": [
                    "yid-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.7240566037735848,
                "f1": 0.6682587328813745,
                "precision": 0.6475039308176099,
                "recall": 0.7240566037735848,
                "main_score": 0.6682587328813745
            },
            {
                "hf_subset": "fin-eng",
                "languages": [
                    "fin-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.738,
                "f1": 0.6856357142857143,
                "precision": 0.663178822055138,
                "recall": 0.738,
                "main_score": 0.6856357142857143
            },
            {
                "hf_subset": "tha-eng",
                "languages": [
                    "tha-Thai",
                    "eng-Latn"
                ],
                "accuracy": 0.9178832116788321,
                "f1": 0.893552311435523,
                "precision": 0.8820559610705597,
                "recall": 0.9178832116788321,
                "main_score": 0.893552311435523
            },
            {
                "hf_subset": "wuu-eng",
                "languages": [
                    "wuu-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.743,
                "f1": 0.690508558108558,
                "precision": 0.66955,
                "recall": 0.743,
                "main_score": 0.690508558108558
            }
        ]
    }
}