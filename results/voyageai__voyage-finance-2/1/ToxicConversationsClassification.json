{
  "dataset_revision": "edfaf9da55d3dd50d43143d90c1ac476895ae6de",
  "task_name": "ToxicConversationsClassification",
  "mteb_version": "1.36.10",
  "scores": {
    "test": [
      {
        "accuracy": 0.618457,
        "f1": 0.472126,
        "f1_weighted": 0.698551,
        "ap": 0.105029,
        "ap_weighted": 0.105029,
        "scores_per_experiment": [
          {
            "accuracy": 0.713867,
            "f1": 0.522753,
            "f1_weighted": 0.776982,
            "ap": 0.11065,
            "ap_weighted": 0.11065
          },
          {
            "accuracy": 0.666504,
            "f1": 0.500353,
            "f1_weighted": 0.742897,
            "ap": 0.108289,
            "ap_weighted": 0.108289
          },
          {
            "accuracy": 0.700684,
            "f1": 0.519778,
            "f1_weighted": 0.767894,
            "ap": 0.113379,
            "ap_weighted": 0.113379
          },
          {
            "accuracy": 0.725586,
            "f1": 0.538481,
            "f1_weighted": 0.78585,
            "ap": 0.122498,
            "ap_weighted": 0.122498
          },
          {
            "accuracy": 0.344238,
            "f1": 0.306625,
            "f1_weighted": 0.44257,
            "ap": 0.080373,
            "ap_weighted": 0.080373
          },
          {
            "accuracy": 0.558105,
            "f1": 0.443,
            "f1_weighted": 0.656149,
            "ap": 0.09997,
            "ap_weighted": 0.09997
          },
          {
            "accuracy": 0.724121,
            "f1": 0.511579,
            "f1_weighted": 0.782802,
            "ap": 0.097135,
            "ap_weighted": 0.097135
          },
          {
            "accuracy": 0.537598,
            "f1": 0.438883,
            "f1_weighted": 0.637001,
            "ap": 0.10802,
            "ap_weighted": 0.10802
          },
          {
            "accuracy": 0.630371,
            "f1": 0.49052,
            "f1_weighted": 0.71522,
            "ap": 0.116087,
            "ap_weighted": 0.116087
          },
          {
            "accuracy": 0.583496,
            "f1": 0.449293,
            "f1_weighted": 0.678142,
            "ap": 0.09389,
            "ap_weighted": 0.09389
          }
        ],
        "main_score": 0.618457,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 59.766680002212524,
  "kg_co2_emissions": null
}