{
  "dataset_revision": "b07c6ce548f6a7ac8d546e1bbe197a0086409190",
  "task_name": "SweRecClassification",
  "mteb_version": "2.3.2",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.820801,
            "f1": 0.734887,
            "f1_weighted": 0.824835,
            "precision": 0.731392,
            "precision_weighted": 0.829619,
            "recall": 0.739974,
            "recall_weighted": 0.820801,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.73584,
            "f1": 0.682993,
            "f1_weighted": 0.765225,
            "precision": 0.705902,
            "precision_weighted": 0.828381,
            "recall": 0.719674,
            "recall_weighted": 0.73584,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.786621,
            "f1": 0.708665,
            "f1_weighted": 0.798964,
            "precision": 0.712242,
            "precision_weighted": 0.830572,
            "recall": 0.727351,
            "recall_weighted": 0.786621,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.802246,
            "f1": 0.726871,
            "f1_weighted": 0.81338,
            "precision": 0.725651,
            "precision_weighted": 0.831119,
            "recall": 0.740223,
            "recall_weighted": 0.802246,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.788086,
            "f1": 0.671632,
            "f1_weighted": 0.785794,
            "precision": 0.672905,
            "precision_weighted": 0.783691,
            "recall": 0.670671,
            "recall_weighted": 0.788086,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.819336,
            "f1": 0.708067,
            "f1_weighted": 0.812149,
            "precision": 0.717358,
            "precision_weighted": 0.807008,
            "recall": 0.703431,
            "recall_weighted": 0.819336,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.797363,
            "f1": 0.719461,
            "f1_weighted": 0.808154,
            "precision": 0.715774,
            "precision_weighted": 0.823864,
            "recall": 0.732898,
            "recall_weighted": 0.797363,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.748535,
            "f1": 0.70117,
            "f1_weighted": 0.780664,
            "precision": 0.723774,
            "precision_weighted": 0.851291,
            "recall": 0.748289,
            "recall_weighted": 0.748535,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.699219,
            "f1": 0.633939,
            "f1_weighted": 0.730325,
            "precision": 0.668272,
            "precision_weighted": 0.809425,
            "recall": 0.659245,
            "recall_weighted": 0.699219,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.787598,
            "f1": 0.696369,
            "f1_weighted": 0.794155,
            "precision": 0.693737,
            "precision_weighted": 0.802298,
            "recall": 0.701987,
            "recall_weighted": 0.787598,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.778564,
        "f1": 0.698405,
        "f1_weighted": 0.791364,
        "precision": 0.706701,
        "precision_weighted": 0.819727,
        "recall": 0.714374,
        "recall_weighted": 0.778564,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.778564,
        "hf_subset": "default",
        "languages": [
          "swe-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 94.28191447257996,
  "kg_co2_emissions": null
}