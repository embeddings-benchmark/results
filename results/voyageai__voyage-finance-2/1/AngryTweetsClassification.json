{
  "dataset_revision": "20b0e6081892e78179356fada741b7afa381443d",
  "task_name": "AngryTweetsClassification",
  "mteb_version": "2.3.2",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.567335,
            "f1": 0.563159,
            "f1_weighted": 0.567036,
            "precision": 0.564273,
            "precision_weighted": 0.57429,
            "recall": 0.570567,
            "recall_weighted": 0.567335,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.552053,
            "f1": 0.546146,
            "f1_weighted": 0.550832,
            "precision": 0.551861,
            "precision_weighted": 0.563976,
            "recall": 0.556495,
            "recall_weighted": 0.552053,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.545368,
            "f1": 0.526788,
            "f1_weighted": 0.531286,
            "precision": 0.531941,
            "precision_weighted": 0.537317,
            "recall": 0.540961,
            "recall_weighted": 0.545368,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.533906,
            "f1": 0.532214,
            "f1_weighted": 0.532074,
            "precision": 0.530079,
            "precision_weighted": 0.531739,
            "recall": 0.535986,
            "recall_weighted": 0.533906,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.533906,
            "f1": 0.523047,
            "f1_weighted": 0.532396,
            "precision": 0.523292,
            "precision_weighted": 0.533181,
            "recall": 0.525054,
            "recall_weighted": 0.533906,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.60745,
            "f1": 0.590521,
            "f1_weighted": 0.60309,
            "precision": 0.590541,
            "precision_weighted": 0.600947,
            "recall": 0.592537,
            "recall_weighted": 0.60745,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.52531,
            "f1": 0.52225,
            "f1_weighted": 0.530157,
            "precision": 0.528559,
            "precision_weighted": 0.542972,
            "recall": 0.524392,
            "recall_weighted": 0.52531,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.56447,
            "f1": 0.550993,
            "f1_weighted": 0.55953,
            "precision": 0.562989,
            "precision_weighted": 0.563576,
            "recall": 0.549143,
            "recall_weighted": 0.56447,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.581662,
            "f1": 0.575807,
            "f1_weighted": 0.583663,
            "precision": 0.576282,
            "precision_weighted": 0.587307,
            "recall": 0.57712,
            "recall_weighted": 0.581662,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.620821,
            "f1": 0.609788,
            "f1_weighted": 0.616363,
            "precision": 0.612693,
            "precision_weighted": 0.620169,
            "recall": 0.614899,
            "recall_weighted": 0.620821,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.563228,
        "f1": 0.554071,
        "f1_weighted": 0.560643,
        "precision": 0.557251,
        "precision_weighted": 0.565547,
        "recall": 0.558715,
        "recall_weighted": 0.563228,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.563228,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 37.37784457206726,
  "kg_co2_emissions": null
}