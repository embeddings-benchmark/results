{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.34.7",
  "scores": {
    "validation": [
      {
        "accuracy": 0.798672,
        "f1": 0.752349,
        "f1_weighted": 0.782994,
        "scores_per_experiment": [
          {
            "accuracy": 0.817019,
            "f1": 0.775271,
            "f1_weighted": 0.804303
          },
          {
            "accuracy": 0.825381,
            "f1": 0.775777,
            "f1_weighted": 0.814728
          },
          {
            "accuracy": 0.809641,
            "f1": 0.760264,
            "f1_weighted": 0.792767
          },
          {
            "accuracy": 0.812592,
            "f1": 0.756821,
            "f1_weighted": 0.796905
          },
          {
            "accuracy": 0.796852,
            "f1": 0.736452,
            "f1_weighted": 0.778508
          },
          {
            "accuracy": 0.77029,
            "f1": 0.730216,
            "f1_weighted": 0.757194
          },
          {
            "accuracy": 0.80423,
            "f1": 0.749062,
            "f1_weighted": 0.783394
          },
          {
            "accuracy": 0.78603,
            "f1": 0.742042,
            "f1_weighted": 0.770306
          },
          {
            "accuracy": 0.77275,
            "f1": 0.739807,
            "f1_weighted": 0.751396
          },
          {
            "accuracy": 0.791933,
            "f1": 0.757781,
            "f1_weighted": 0.780442
          }
        ],
        "main_score": 0.798672,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.791157,
        "f1": 0.762243,
        "f1_weighted": 0.778113,
        "scores_per_experiment": [
          {
            "accuracy": 0.80733,
            "f1": 0.778406,
            "f1_weighted": 0.796585
          },
          {
            "accuracy": 0.806994,
            "f1": 0.782513,
            "f1_weighted": 0.795958
          },
          {
            "accuracy": 0.79119,
            "f1": 0.752229,
            "f1_weighted": 0.777777
          },
          {
            "accuracy": 0.805985,
            "f1": 0.769614,
            "f1_weighted": 0.792126
          },
          {
            "accuracy": 0.794889,
            "f1": 0.755813,
            "f1_weighted": 0.779727
          },
          {
            "accuracy": 0.761601,
            "f1": 0.739896,
            "f1_weighted": 0.751633
          },
          {
            "accuracy": 0.794889,
            "f1": 0.761699,
            "f1_weighted": 0.779637
          },
          {
            "accuracy": 0.791863,
            "f1": 0.763724,
            "f1_weighted": 0.779951
          },
          {
            "accuracy": 0.772024,
            "f1": 0.746379,
            "f1_weighted": 0.751225
          },
          {
            "accuracy": 0.784802,
            "f1": 0.772154,
            "f1_weighted": 0.776515
          }
        ],
        "main_score": 0.791157,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 47.84240365028381,
  "kg_co2_emissions": null
}
