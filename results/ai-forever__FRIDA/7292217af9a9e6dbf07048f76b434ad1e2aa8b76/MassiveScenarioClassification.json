{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "task_name": "MassiveScenarioClassification",
  "mteb_version": "1.34.7",
  "scores": {
    "validation": [
      {
        "accuracy": 0.886768,
        "f1": 0.873595,
        "f1_weighted": 0.880531,
        "scores_per_experiment": [
          {
            "accuracy": 0.891294,
            "f1": 0.881576,
            "f1_weighted": 0.88662
          },
          {
            "accuracy": 0.892277,
            "f1": 0.879897,
            "f1_weighted": 0.887296
          },
          {
            "accuracy": 0.892769,
            "f1": 0.879433,
            "f1_weighted": 0.886929
          },
          {
            "accuracy": 0.889818,
            "f1": 0.876948,
            "f1_weighted": 0.882282
          },
          {
            "accuracy": 0.885883,
            "f1": 0.87065,
            "f1_weighted": 0.879557
          },
          {
            "accuracy": 0.880964,
            "f1": 0.867622,
            "f1_weighted": 0.875101
          },
          {
            "accuracy": 0.881456,
            "f1": 0.866347,
            "f1_weighted": 0.872921
          },
          {
            "accuracy": 0.881948,
            "f1": 0.868861,
            "f1_weighted": 0.876224
          },
          {
            "accuracy": 0.881456,
            "f1": 0.866161,
            "f1_weighted": 0.873378
          },
          {
            "accuracy": 0.889818,
            "f1": 0.878458,
            "f1_weighted": 0.885007
          }
        ],
        "main_score": 0.886768,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.888063,
        "f1": 0.875314,
        "f1_weighted": 0.882274,
        "scores_per_experiment": [
          {
            "accuracy": 0.887021,
            "f1": 0.877505,
            "f1_weighted": 0.882623
          },
          {
            "accuracy": 0.89072,
            "f1": 0.87698,
            "f1_weighted": 0.884188
          },
          {
            "accuracy": 0.890047,
            "f1": 0.877122,
            "f1_weighted": 0.884289
          },
          {
            "accuracy": 0.889711,
            "f1": 0.875271,
            "f1_weighted": 0.88247
          },
          {
            "accuracy": 0.886012,
            "f1": 0.872725,
            "f1_weighted": 0.880201
          },
          {
            "accuracy": 0.889711,
            "f1": 0.876928,
            "f1_weighted": 0.884147
          },
          {
            "accuracy": 0.878951,
            "f1": 0.863489,
            "f1_weighted": 0.871701
          },
          {
            "accuracy": 0.885676,
            "f1": 0.873718,
            "f1_weighted": 0.881772
          },
          {
            "accuracy": 0.889711,
            "f1": 0.877358,
            "f1_weighted": 0.882561
          },
          {
            "accuracy": 0.893073,
            "f1": 0.882039,
            "f1_weighted": 0.888791
          }
        ],
        "main_score": 0.888063,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 49.21223306655884,
  "kg_co2_emissions": null
}