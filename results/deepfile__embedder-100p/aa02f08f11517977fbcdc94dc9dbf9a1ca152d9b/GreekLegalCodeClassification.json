{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "task_name": "GreekLegalCodeClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.091699,
        "f1": 0.048226,
        "f1_weighted": 0.076592,
        "scores_per_experiment": [
          {
            "accuracy": 0.085938,
            "f1": 0.041669,
            "f1_weighted": 0.071595
          },
          {
            "accuracy": 0.087402,
            "f1": 0.052956,
            "f1_weighted": 0.073275
          },
          {
            "accuracy": 0.090332,
            "f1": 0.050677,
            "f1_weighted": 0.077026
          },
          {
            "accuracy": 0.089844,
            "f1": 0.047637,
            "f1_weighted": 0.075964
          },
          {
            "accuracy": 0.084961,
            "f1": 0.045565,
            "f1_weighted": 0.072788
          },
          {
            "accuracy": 0.09375,
            "f1": 0.048873,
            "f1_weighted": 0.080114
          },
          {
            "accuracy": 0.092285,
            "f1": 0.044467,
            "f1_weighted": 0.073378
          },
          {
            "accuracy": 0.09375,
            "f1": 0.049296,
            "f1_weighted": 0.076415
          },
          {
            "accuracy": 0.099609,
            "f1": 0.049822,
            "f1_weighted": 0.083346
          },
          {
            "accuracy": 0.099121,
            "f1": 0.051298,
            "f1_weighted": 0.082018
          }
        ],
        "main_score": 0.091699,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.080518,
        "f1": 0.04549,
        "f1_weighted": 0.062984,
        "scores_per_experiment": [
          {
            "accuracy": 0.077637,
            "f1": 0.037078,
            "f1_weighted": 0.062077
          },
          {
            "accuracy": 0.07666,
            "f1": 0.041506,
            "f1_weighted": 0.06008
          },
          {
            "accuracy": 0.083008,
            "f1": 0.053095,
            "f1_weighted": 0.064629
          },
          {
            "accuracy": 0.075684,
            "f1": 0.046333,
            "f1_weighted": 0.059993
          },
          {
            "accuracy": 0.071777,
            "f1": 0.046331,
            "f1_weighted": 0.054979
          },
          {
            "accuracy": 0.078613,
            "f1": 0.040271,
            "f1_weighted": 0.060727
          },
          {
            "accuracy": 0.083008,
            "f1": 0.047581,
            "f1_weighted": 0.068697
          },
          {
            "accuracy": 0.086914,
            "f1": 0.045009,
            "f1_weighted": 0.069147
          },
          {
            "accuracy": 0.091309,
            "f1": 0.052434,
            "f1_weighted": 0.068921
          },
          {
            "accuracy": 0.080566,
            "f1": 0.045266,
            "f1_weighted": 0.060585
          }
        ],
        "main_score": 0.080518,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ]
  },
  "evaluation_time": 456.3448598384857,
  "kg_co2_emissions": 0.01807854616164025
}