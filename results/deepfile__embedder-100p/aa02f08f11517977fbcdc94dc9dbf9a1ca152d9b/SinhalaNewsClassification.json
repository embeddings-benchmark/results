{
  "dataset_revision": "7fb2f514ea683c5282dfec0a9672ece8de90ac50",
  "task_name": "SinhalaNewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "train": [
      {
        "accuracy": 0.513232,
        "f1": 0.511968,
        "f1_weighted": 0.52005,
        "scores_per_experiment": [
          {
            "accuracy": 0.530762,
            "f1": 0.535551,
            "f1_weighted": 0.546706
          },
          {
            "accuracy": 0.519043,
            "f1": 0.512023,
            "f1_weighted": 0.521214
          },
          {
            "accuracy": 0.528809,
            "f1": 0.53799,
            "f1_weighted": 0.534972
          },
          {
            "accuracy": 0.499512,
            "f1": 0.477673,
            "f1_weighted": 0.493481
          },
          {
            "accuracy": 0.491699,
            "f1": 0.510906,
            "f1_weighted": 0.509649
          },
          {
            "accuracy": 0.541992,
            "f1": 0.526925,
            "f1_weighted": 0.530501
          },
          {
            "accuracy": 0.480469,
            "f1": 0.49336,
            "f1_weighted": 0.496412
          },
          {
            "accuracy": 0.515625,
            "f1": 0.484499,
            "f1_weighted": 0.509399
          },
          {
            "accuracy": 0.507812,
            "f1": 0.51371,
            "f1_weighted": 0.523754
          },
          {
            "accuracy": 0.516602,
            "f1": 0.527046,
            "f1_weighted": 0.534415
          }
        ],
        "main_score": 0.513232,
        "hf_subset": "default",
        "languages": [
          "sin-Sinh"
        ]
      }
    ]
  },
  "evaluation_time": 8.433570384979248,
  "kg_co2_emissions": 0.00028931165214485656
}