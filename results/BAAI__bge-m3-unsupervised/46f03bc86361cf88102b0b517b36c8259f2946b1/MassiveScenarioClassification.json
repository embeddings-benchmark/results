{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "task_name": "MassiveScenarioClassification",
  "mteb_version": "1.25.8",
  "scores": {
    "validation": [
      {
        "accuracy": 0.719823,
        "f1": 0.707644,
        "f1_weighted": 0.714654,
        "scores_per_experiment": [
          {
            "accuracy": 0.743237,
            "f1": 0.729248,
            "f1_weighted": 0.73993
          },
          {
            "accuracy": 0.740777,
            "f1": 0.737185,
            "f1_weighted": 0.737797
          },
          {
            "accuracy": 0.742745,
            "f1": 0.728209,
            "f1_weighted": 0.739244
          },
          {
            "accuracy": 0.701918,
            "f1": 0.686193,
            "f1_weighted": 0.696683
          },
          {
            "accuracy": 0.723069,
            "f1": 0.704931,
            "f1_weighted": 0.712083
          },
          {
            "accuracy": 0.695524,
            "f1": 0.688024,
            "f1_weighted": 0.688117
          },
          {
            "accuracy": 0.696508,
            "f1": 0.678886,
            "f1_weighted": 0.690546
          },
          {
            "accuracy": 0.711264,
            "f1": 0.70309,
            "f1_weighted": 0.707754
          },
          {
            "accuracy": 0.747664,
            "f1": 0.733771,
            "f1_weighted": 0.744186
          },
          {
            "accuracy": 0.695524,
            "f1": 0.686905,
            "f1_weighted": 0.690199
          }
        ],
        "main_score": 0.719823,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.723705,
        "f1": 0.714042,
        "f1_weighted": 0.718587,
        "scores_per_experiment": [
          {
            "accuracy": 0.746806,
            "f1": 0.737752,
            "f1_weighted": 0.741024
          },
          {
            "accuracy": 0.731675,
            "f1": 0.730761,
            "f1_weighted": 0.727014
          },
          {
            "accuracy": 0.740081,
            "f1": 0.730797,
            "f1_weighted": 0.734682
          },
          {
            "accuracy": 0.723268,
            "f1": 0.710246,
            "f1_weighted": 0.719639
          },
          {
            "accuracy": 0.72764,
            "f1": 0.708438,
            "f1_weighted": 0.716327
          },
          {
            "accuracy": 0.708473,
            "f1": 0.697243,
            "f1_weighted": 0.702056
          },
          {
            "accuracy": 0.718897,
            "f1": 0.704115,
            "f1_weighted": 0.713504
          },
          {
            "accuracy": 0.704102,
            "f1": 0.697196,
            "f1_weighted": 0.701876
          },
          {
            "accuracy": 0.741762,
            "f1": 0.73295,
            "f1_weighted": 0.739729
          },
          {
            "accuracy": 0.694351,
            "f1": 0.690923,
            "f1_weighted": 0.690023
          }
        ],
        "main_score": 0.723705,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 22.194623947143555,
  "kg_co2_emissions": null
}