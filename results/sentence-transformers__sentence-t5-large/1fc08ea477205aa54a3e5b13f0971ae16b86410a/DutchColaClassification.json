{
  "dataset_revision": "2269ed7d95d8abaab829f1592b4b2047372e9f81",
  "task_name": "DutchColaClassification",
  "mteb_version": "2.1.17",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.5375,
            "f1": 0.535686,
            "f1_weighted": 0.535686,
            "precision": 0.538095,
            "precision_weighted": 0.538095,
            "recall": 0.5375,
            "recall_weighted": 0.5375,
            "ap": 0.52,
            "ap_weighted": 0.52
          },
          {
            "accuracy": 0.545417,
            "f1": 0.544996,
            "f1_weighted": 0.544996,
            "precision": 0.545585,
            "precision_weighted": 0.545585,
            "recall": 0.545417,
            "recall_weighted": 0.545417,
            "ap": 0.524653,
            "ap_weighted": 0.524653
          },
          {
            "accuracy": 0.528333,
            "f1": 0.528322,
            "f1_weighted": 0.528322,
            "precision": 0.528336,
            "precision_weighted": 0.528336,
            "recall": 0.528333,
            "recall_weighted": 0.528333,
            "ap": 0.514961,
            "ap_weighted": 0.514961
          },
          {
            "accuracy": 0.56375,
            "f1": 0.563695,
            "f1_weighted": 0.563695,
            "precision": 0.563782,
            "precision_weighted": 0.563782,
            "recall": 0.56375,
            "recall_weighted": 0.56375,
            "ap": 0.536033,
            "ap_weighted": 0.536033
          },
          {
            "accuracy": 0.560833,
            "f1": 0.560643,
            "f1_weighted": 0.560643,
            "precision": 0.560939,
            "precision_weighted": 0.560939,
            "recall": 0.560833,
            "recall_weighted": 0.560833,
            "ap": 0.534278,
            "ap_weighted": 0.534278
          },
          {
            "accuracy": 0.549167,
            "f1": 0.548971,
            "f1_weighted": 0.548971,
            "precision": 0.549252,
            "precision_weighted": 0.549252,
            "recall": 0.549167,
            "recall_weighted": 0.549167,
            "ap": 0.527106,
            "ap_weighted": 0.527106
          },
          {
            "accuracy": 0.556667,
            "f1": 0.556636,
            "f1_weighted": 0.556636,
            "precision": 0.556682,
            "precision_weighted": 0.556682,
            "recall": 0.556667,
            "recall_weighted": 0.556667,
            "ap": 0.531492,
            "ap_weighted": 0.531492
          },
          {
            "accuracy": 0.545,
            "f1": 0.544543,
            "f1_weighted": 0.544543,
            "precision": 0.545181,
            "precision_weighted": 0.545181,
            "recall": 0.545,
            "recall_weighted": 0.545,
            "ap": 0.524662,
            "ap_weighted": 0.524662
          },
          {
            "accuracy": 0.556667,
            "f1": 0.556097,
            "f1_weighted": 0.556097,
            "precision": 0.556959,
            "precision_weighted": 0.556959,
            "recall": 0.556667,
            "recall_weighted": 0.556667,
            "ap": 0.53133,
            "ap_weighted": 0.53133
          },
          {
            "accuracy": 0.564167,
            "f1": 0.562844,
            "f1_weighted": 0.562844,
            "precision": 0.564953,
            "precision_weighted": 0.564953,
            "recall": 0.564167,
            "recall_weighted": 0.564167,
            "ap": 0.535793,
            "ap_weighted": 0.535793
          }
        ],
        "accuracy": 0.55075,
        "f1": 0.550243,
        "f1_weighted": 0.550243,
        "precision": 0.550977,
        "precision_weighted": 0.550977,
        "recall": 0.55075,
        "recall_weighted": 0.55075,
        "ap": 0.528031,
        "ap_weighted": 0.528031,
        "main_score": 0.550243,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 29.44623064994812,
  "kg_co2_emissions": null
}