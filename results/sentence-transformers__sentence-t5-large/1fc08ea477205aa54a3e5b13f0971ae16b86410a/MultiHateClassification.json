{
  "dataset_revision": "8f95949846bb9e33c6aaf730ccfdb8fe6bcfb7a9",
  "task_name": "MultiHateClassification",
  "mteb_version": "2.1.17",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.58,
            "f1": 0.530056,
            "f1_weighted": 0.59195,
            "precision": 0.531864,
            "precision_weighted": 0.610279,
            "recall": 0.535737,
            "recall_weighted": 0.58,
            "ap": 0.314947,
            "ap_weighted": 0.314947
          },
          {
            "accuracy": 0.452,
            "f1": 0.451965,
            "f1_weighted": 0.453736,
            "precision": 0.535759,
            "precision_weighted": 0.622973,
            "recall": 0.536301,
            "recall_weighted": 0.452,
            "ap": 0.314303,
            "ap_weighted": 0.314303
          },
          {
            "accuracy": 0.575,
            "f1": 0.546908,
            "f1_weighted": 0.592487,
            "precision": 0.556511,
            "precision_weighted": 0.635973,
            "recall": 0.566937,
            "recall_weighted": 0.575,
            "ap": 0.331816,
            "ap_weighted": 0.331816
          },
          {
            "accuracy": 0.571,
            "f1": 0.51072,
            "f1_weighted": 0.580102,
            "precision": 0.511857,
            "precision_weighted": 0.592037,
            "recall": 0.512911,
            "recall_weighted": 0.571,
            "ap": 0.303681,
            "ap_weighted": 0.303681
          },
          {
            "accuracy": 0.591,
            "f1": 0.537183,
            "f1_weighted": 0.600943,
            "precision": 0.537762,
            "precision_weighted": 0.615129,
            "recall": 0.54164,
            "recall_weighted": 0.591,
            "ap": 0.318243,
            "ap_weighted": 0.318243
          },
          {
            "accuracy": 0.507,
            "f1": 0.50406,
            "f1_weighted": 0.519487,
            "precision": 0.561333,
            "precision_weighted": 0.649136,
            "recall": 0.568715,
            "recall_weighted": 0.507,
            "ap": 0.331188,
            "ap_weighted": 0.331188
          },
          {
            "accuracy": 0.474,
            "f1": 0.463175,
            "f1_weighted": 0.493972,
            "precision": 0.498214,
            "precision_weighted": 0.579736,
            "recall": 0.497897,
            "recall_weighted": 0.474,
            "ap": 0.297125,
            "ap_weighted": 0.297125
          },
          {
            "accuracy": 0.509,
            "f1": 0.487019,
            "f1_weighted": 0.529919,
            "precision": 0.506981,
            "precision_weighted": 0.58856,
            "recall": 0.508341,
            "recall_weighted": 0.509,
            "ap": 0.301573,
            "ap_weighted": 0.301573
          },
          {
            "accuracy": 0.573,
            "f1": 0.543303,
            "f1_weighted": 0.590352,
            "precision": 0.552174,
            "precision_weighted": 0.631548,
            "recall": 0.56165,
            "recall_weighted": 0.573,
            "ap": 0.328789,
            "ap_weighted": 0.328789
          },
          {
            "accuracy": 0.594,
            "f1": 0.552056,
            "f1_weighted": 0.607433,
            "precision": 0.554335,
            "precision_weighted": 0.631377,
            "recall": 0.562124,
            "recall_weighted": 0.594,
            "ap": 0.329717,
            "ap_weighted": 0.329717
          }
        ],
        "accuracy": 0.5426,
        "f1": 0.512645,
        "f1_weighted": 0.556038,
        "precision": 0.534679,
        "precision_weighted": 0.615675,
        "recall": 0.539225,
        "recall_weighted": 0.5426,
        "ap": 0.317138,
        "ap_weighted": 0.317138,
        "main_score": 0.5426,
        "hf_subset": "nld",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 12.711071729660034,
  "kg_co2_emissions": null
}