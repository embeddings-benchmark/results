{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.352637,
        "f1": 0.339571,
        "f1_weighted": 0.339566,
        "scores_per_experiment": [
          {
            "accuracy": 0.36084,
            "f1": 0.350866,
            "f1_weighted": 0.350838
          },
          {
            "accuracy": 0.34082,
            "f1": 0.32666,
            "f1_weighted": 0.32666
          },
          {
            "accuracy": 0.350586,
            "f1": 0.337534,
            "f1_weighted": 0.337515
          },
          {
            "accuracy": 0.383301,
            "f1": 0.376317,
            "f1_weighted": 0.376325
          },
          {
            "accuracy": 0.342285,
            "f1": 0.33343,
            "f1_weighted": 0.333425
          },
          {
            "accuracy": 0.353027,
            "f1": 0.323742,
            "f1_weighted": 0.323719
          },
          {
            "accuracy": 0.330566,
            "f1": 0.324282,
            "f1_weighted": 0.324263
          },
          {
            "accuracy": 0.375,
            "f1": 0.363066,
            "f1_weighted": 0.363059
          },
          {
            "accuracy": 0.367188,
            "f1": 0.356179,
            "f1_weighted": 0.356156
          },
          {
            "accuracy": 0.322754,
            "f1": 0.303637,
            "f1_weighted": 0.3037
          }
        ],
        "main_score": 0.352637,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.359131,
        "f1": 0.346454,
        "f1_weighted": 0.346461,
        "scores_per_experiment": [
          {
            "accuracy": 0.364746,
            "f1": 0.355654,
            "f1_weighted": 0.355648
          },
          {
            "accuracy": 0.367188,
            "f1": 0.35601,
            "f1_weighted": 0.35602
          },
          {
            "accuracy": 0.362793,
            "f1": 0.349121,
            "f1_weighted": 0.349121
          },
          {
            "accuracy": 0.368164,
            "f1": 0.361121,
            "f1_weighted": 0.361112
          },
          {
            "accuracy": 0.321289,
            "f1": 0.313233,
            "f1_weighted": 0.313248
          },
          {
            "accuracy": 0.372559,
            "f1": 0.341729,
            "f1_weighted": 0.341697
          },
          {
            "accuracy": 0.346191,
            "f1": 0.342214,
            "f1_weighted": 0.342224
          },
          {
            "accuracy": 0.390137,
            "f1": 0.37514,
            "f1_weighted": 0.375176
          },
          {
            "accuracy": 0.378418,
            "f1": 0.368086,
            "f1_weighted": 0.368062
          },
          {
            "accuracy": 0.319824,
            "f1": 0.302233,
            "f1_weighted": 0.302299
          }
        ],
        "main_score": 0.359131,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 20.402092933654785,
  "kg_co2_emissions": 0.0009049806425745501
}