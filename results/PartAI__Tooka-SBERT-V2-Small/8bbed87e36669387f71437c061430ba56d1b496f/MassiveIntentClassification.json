{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.38.27",
  "scores": {
    "validation": [
      {
        "accuracy": 0.667142,
        "f1": 0.644635,
        "f1_weighted": 0.666787,
        "scores_per_experiment": [
          {
            "accuracy": 0.662076,
            "f1": 0.650841,
            "f1_weighted": 0.662269
          },
          {
            "accuracy": 0.699951,
            "f1": 0.665058,
            "f1_weighted": 0.700194
          },
          {
            "accuracy": 0.664535,
            "f1": 0.643576,
            "f1_weighted": 0.662035
          },
          {
            "accuracy": 0.694048,
            "f1": 0.656648,
            "f1_weighted": 0.693891
          },
          {
            "accuracy": 0.671913,
            "f1": 0.649175,
            "f1_weighted": 0.67437
          },
          {
            "accuracy": 0.649287,
            "f1": 0.628773,
            "f1_weighted": 0.644265
          },
          {
            "accuracy": 0.664043,
            "f1": 0.644054,
            "f1_weighted": 0.662264
          },
          {
            "accuracy": 0.669946,
            "f1": 0.641821,
            "f1_weighted": 0.669249
          },
          {
            "accuracy": 0.646827,
            "f1": 0.636963,
            "f1_weighted": 0.650176
          },
          {
            "accuracy": 0.648795,
            "f1": 0.629436,
            "f1_weighted": 0.64916
          }
        ],
        "main_score": 0.667142,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.653295,
        "f1": 0.634384,
        "f1_weighted": 0.655199,
        "scores_per_experiment": [
          {
            "accuracy": 0.645595,
            "f1": 0.633646,
            "f1_weighted": 0.648223
          },
          {
            "accuracy": 0.664425,
            "f1": 0.638227,
            "f1_weighted": 0.663889
          },
          {
            "accuracy": 0.655683,
            "f1": 0.633052,
            "f1_weighted": 0.655328
          },
          {
            "accuracy": 0.677875,
            "f1": 0.651266,
            "f1_weighted": 0.681048
          },
          {
            "accuracy": 0.651311,
            "f1": 0.640571,
            "f1_weighted": 0.651245
          },
          {
            "accuracy": 0.643578,
            "f1": 0.628159,
            "f1_weighted": 0.644279
          },
          {
            "accuracy": 0.647949,
            "f1": 0.633359,
            "f1_weighted": 0.648744
          },
          {
            "accuracy": 0.659045,
            "f1": 0.63436,
            "f1_weighted": 0.663265
          },
          {
            "accuracy": 0.64963,
            "f1": 0.633773,
            "f1_weighted": 0.65533
          },
          {
            "accuracy": 0.637861,
            "f1": 0.617424,
            "f1_weighted": 0.640637
          }
        ],
        "main_score": 0.653295,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 59.27981662750244,
  "kg_co2_emissions": null
}