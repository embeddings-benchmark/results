{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "task_name": "MTOPIntentClassification",
  "mteb_version": "1.36.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.608504,
        "f1": 0.449978,
        "f1_weighted": 0.650051,
        "scores_per_experiment": [
          {
            "accuracy": 0.606019,
            "f1": 0.438302,
            "f1_weighted": 0.650285
          },
          {
            "accuracy": 0.605107,
            "f1": 0.44656,
            "f1_weighted": 0.647652
          },
          {
            "accuracy": 0.624943,
            "f1": 0.471996,
            "f1_weighted": 0.666741
          },
          {
            "accuracy": 0.613087,
            "f1": 0.444812,
            "f1_weighted": 0.650066
          },
          {
            "accuracy": 0.576607,
            "f1": 0.448117,
            "f1_weighted": 0.622168
          },
          {
            "accuracy": 0.609667,
            "f1": 0.444947,
            "f1_weighted": 0.649865
          },
          {
            "accuracy": 0.616051,
            "f1": 0.453175,
            "f1_weighted": 0.660989
          },
          {
            "accuracy": 0.620155,
            "f1": 0.4488,
            "f1_weighted": 0.659289
          },
          {
            "accuracy": 0.622207,
            "f1": 0.468981,
            "f1_weighted": 0.663827
          },
          {
            "accuracy": 0.591199,
            "f1": 0.434088,
            "f1_weighted": 0.629629
          }
        ],
        "main_score": 0.608504,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 18.789658784866333,
  "kg_co2_emissions": 0.00028613274307987436
}