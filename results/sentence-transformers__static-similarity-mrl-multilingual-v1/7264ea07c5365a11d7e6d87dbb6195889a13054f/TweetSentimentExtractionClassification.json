{
  "dataset_revision": "d604517c81ca91fe16a244d1248fc021f9ecee7a",
  "task_name": "TweetSentimentExtractionClassification",
  "mteb_version": "1.36.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.488568,
        "f1": 0.487989,
        "f1_weighted": 0.480805,
        "scores_per_experiment": [
          {
            "accuracy": 0.484154,
            "f1": 0.483631,
            "f1_weighted": 0.477493
          },
          {
            "accuracy": 0.485286,
            "f1": 0.486061,
            "f1_weighted": 0.479359
          },
          {
            "accuracy": 0.506225,
            "f1": 0.502343,
            "f1_weighted": 0.490089
          },
          {
            "accuracy": 0.473401,
            "f1": 0.470874,
            "f1_weighted": 0.461843
          },
          {
            "accuracy": 0.488115,
            "f1": 0.486929,
            "f1_weighted": 0.481607
          },
          {
            "accuracy": 0.502547,
            "f1": 0.501884,
            "f1_weighted": 0.492323
          },
          {
            "accuracy": 0.501415,
            "f1": 0.5022,
            "f1_weighted": 0.491171
          },
          {
            "accuracy": 0.482456,
            "f1": 0.484761,
            "f1_weighted": 0.476419
          },
          {
            "accuracy": 0.494058,
            "f1": 0.491499,
            "f1_weighted": 0.492875
          },
          {
            "accuracy": 0.468025,
            "f1": 0.46971,
            "f1_weighted": 0.46487
          }
        ],
        "main_score": 0.488568,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 8.271572828292847,
  "kg_co2_emissions": 0.00012604745729826656
}