{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "task_name": "MassiveScenarioClassification",
  "mteb_version": "1.36.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.66113,
        "f1": 0.642135,
        "f1_weighted": 0.657391,
        "scores_per_experiment": [
          {
            "accuracy": 0.684936,
            "f1": 0.670569,
            "f1_weighted": 0.684286
          },
          {
            "accuracy": 0.685945,
            "f1": 0.666305,
            "f1_weighted": 0.683809
          },
          {
            "accuracy": 0.64156,
            "f1": 0.619655,
            "f1_weighted": 0.633341
          },
          {
            "accuracy": 0.664425,
            "f1": 0.640493,
            "f1_weighted": 0.659238
          },
          {
            "accuracy": 0.647949,
            "f1": 0.625725,
            "f1_weighted": 0.642569
          },
          {
            "accuracy": 0.651984,
            "f1": 0.629644,
            "f1_weighted": 0.653831
          },
          {
            "accuracy": 0.66308,
            "f1": 0.640315,
            "f1_weighted": 0.660349
          },
          {
            "accuracy": 0.661735,
            "f1": 0.646457,
            "f1_weighted": 0.657287
          },
          {
            "accuracy": 0.663753,
            "f1": 0.639875,
            "f1_weighted": 0.657758
          },
          {
            "accuracy": 0.645931,
            "f1": 0.642311,
            "f1_weighted": 0.641444
          }
        ],
        "main_score": 0.66113,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      },
      {
        "accuracy": 0.562374,
        "f1": 0.54549,
        "f1_weighted": 0.556063,
        "scores_per_experiment": [
          {
            "accuracy": 0.599529,
            "f1": 0.584478,
            "f1_weighted": 0.594294
          },
          {
            "accuracy": 0.561533,
            "f1": 0.54582,
            "f1_weighted": 0.544364
          },
          {
            "accuracy": 0.54842,
            "f1": 0.541602,
            "f1_weighted": 0.540764
          },
          {
            "accuracy": 0.53228,
            "f1": 0.517875,
            "f1_weighted": 0.528823
          },
          {
            "accuracy": 0.574983,
            "f1": 0.553121,
            "f1_weighted": 0.568884
          },
          {
            "accuracy": 0.546066,
            "f1": 0.521011,
            "f1_weighted": 0.538524
          },
          {
            "accuracy": 0.570612,
            "f1": 0.549961,
            "f1_weighted": 0.56902
          },
          {
            "accuracy": 0.571284,
            "f1": 0.550337,
            "f1_weighted": 0.567747
          },
          {
            "accuracy": 0.555145,
            "f1": 0.537826,
            "f1_weighted": 0.54975
          },
          {
            "accuracy": 0.563887,
            "f1": 0.552865,
            "f1_weighted": 0.55846
          }
        ],
        "main_score": 0.562374,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.550615,
        "f1": 0.539691,
        "f1_weighted": 0.545094,
        "scores_per_experiment": [
          {
            "accuracy": 0.593212,
            "f1": 0.58152,
            "f1_weighted": 0.594962
          },
          {
            "accuracy": 0.541072,
            "f1": 0.531674,
            "f1_weighted": 0.52776
          },
          {
            "accuracy": 0.534678,
            "f1": 0.530316,
            "f1_weighted": 0.533102
          },
          {
            "accuracy": 0.499262,
            "f1": 0.496736,
            "f1_weighted": 0.496026
          },
          {
            "accuracy": 0.570093,
            "f1": 0.553832,
            "f1_weighted": 0.562112
          },
          {
            "accuracy": 0.532218,
            "f1": 0.520381,
            "f1_weighted": 0.52322
          },
          {
            "accuracy": 0.568618,
            "f1": 0.55424,
            "f1_weighted": 0.569574
          },
          {
            "accuracy": 0.562715,
            "f1": 0.54726,
            "f1_weighted": 0.556352
          },
          {
            "accuracy": 0.543532,
            "f1": 0.527423,
            "f1_weighted": 0.533463
          },
          {
            "accuracy": 0.560748,
            "f1": 0.553526,
            "f1_weighted": 0.554373
          }
        ],
        "main_score": 0.550615,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 12.790667057037354,
  "kg_co2_emissions": 0.00019477648095016475
}