{
    "dataset_revision": "9080400076fbadbb4c4dcb136ff4eddc40b42553",
    "task_name": "Tatoeba",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "sqi-eng",
                "languages": [
                    "sqi-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.16,
                "f1": 0.12072197229668266,
                "precision": 0.11071252134262681,
                "recall": 0.16,
                "main_score": 0.12072197229668266
            },
            {
                "hf_subset": "fry-eng",
                "languages": [
                    "fry-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.3179190751445087,
                "f1": 0.2533993944398569,
                "precision": 0.23462449892587425,
                "recall": 0.3179190751445087,
                "main_score": 0.2533993944398569
            },
            {
                "hf_subset": "kur-eng",
                "languages": [
                    "kur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.14390243902439023,
                "f1": 0.10647146321087271,
                "precision": 0.09753700307679768,
                "recall": 0.14390243902439023,
                "main_score": 0.10647146321087271
            },
            {
                "hf_subset": "tur-eng",
                "languages": [
                    "tur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.078,
                "f1": 0.05087296515623527,
                "precision": 0.04543963123070674,
                "recall": 0.078,
                "main_score": 0.05087296515623527
            },
            {
                "hf_subset": "deu-eng",
                "languages": [
                    "deu-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.585,
                "f1": 0.5326571428571428,
                "precision": 0.5132397398353281,
                "recall": 0.585,
                "main_score": 0.5326571428571428
            },
            {
                "hf_subset": "nld-eng",
                "languages": [
                    "nld-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.295,
                "f1": 0.2514837668933257,
                "precision": 0.23949224030449837,
                "recall": 0.295,
                "main_score": 0.2514837668933257
            },
            {
                "hf_subset": "ron-eng",
                "languages": [
                    "ron-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.287,
                "f1": 0.23196045369663018,
                "precision": 0.21502155293536873,
                "recall": 0.287,
                "main_score": 0.23196045369663018
            },
            {
                "hf_subset": "ang-eng",
                "languages": [
                    "ang-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.27611940298507465,
                "f1": 0.1943141435678749,
                "precision": 0.17160948504232085,
                "recall": 0.27611940298507465,
                "main_score": 0.1943141435678749
            },
            {
                "hf_subset": "ido-eng",
                "languages": [
                    "ido-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.46,
                "f1": 0.39146820760938406,
                "precision": 0.3689055652165172,
                "recall": 0.46,
                "main_score": 0.39146820760938406
            },
            {
                "hf_subset": "jav-eng",
                "languages": [
                    "jav-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.23414634146341468,
                "f1": 0.18602340748682208,
                "precision": 0.17310239781020476,
                "recall": 0.23414634146341468,
                "main_score": 0.18602340748682208
            },
            {
                "hf_subset": "isl-eng",
                "languages": [
                    "isl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.073,
                "f1": 0.05456411432480631,
                "precision": 0.05073425278627455,
                "recall": 0.073,
                "main_score": 0.05456411432480631
            },
            {
                "hf_subset": "slv-eng",
                "languages": [
                    "slv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.10814094775212636,
                "f1": 0.08096556306772157,
                "precision": 0.07501928709802902,
                "recall": 0.10814094775212636,
                "main_score": 0.08096556306772157
            },
            {
                "hf_subset": "cym-eng",
                "languages": [
                    "cym-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.11304347826086956,
                "f1": 0.07766717493033283,
                "precision": 0.0698093079114751,
                "recall": 0.11304347826086956,
                "main_score": 0.07766717493033283
            },
            {
                "hf_subset": "kaz-eng",
                "languages": [
                    "kaz-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.06260869565217392,
                "f1": 0.04695624631925284,
                "precision": 0.045202426395083976,
                "recall": 0.06260869565217392,
                "main_score": 0.04695624631925284
            },
            {
                "hf_subset": "est-eng",
                "languages": [
                    "est-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.069,
                "f1": 0.04467212205066257,
                "precision": 0.040041427236851085,
                "recall": 0.069,
                "main_score": 0.04467212205066257
            },
            {
                "hf_subset": "heb-eng",
                "languages": [
                    "heb-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.011,
                "f1": 0.006945869191049914,
                "precision": 0.006078431372549019,
                "recall": 0.011,
                "main_score": 0.006945869191049914
            },
            {
                "hf_subset": "gla-eng",
                "languages": [
                    "gla-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.045838359469240045,
                "f1": 0.029858475730729073,
                "precision": 0.02665996515212438,
                "recall": 0.045838359469240045,
                "main_score": 0.029858475730729073
            },
            {
                "hf_subset": "mar-eng",
                "languages": [
                    "mar-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.592,
                "f1": 0.5267345238095238,
                "precision": 0.5013575757575758,
                "recall": 0.592,
                "main_score": 0.5267345238095238
            },
            {
                "hf_subset": "lat-eng",
                "languages": [
                    "lat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.35,
                "f1": 0.2764865301365301,
                "precision": 0.25534839833369244,
                "recall": 0.35,
                "main_score": 0.2764865301365301
            },
            {
                "hf_subset": "bel-eng",
                "languages": [
                    "bel-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.131,
                "f1": 0.0962336638477808,
                "precision": 0.08875194920058407,
                "recall": 0.131,
                "main_score": 0.0962336638477808
            },
            {
                "hf_subset": "pms-eng",
                "languages": [
                    "pms-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.3295238095238095,
                "f1": 0.27600581429152854,
                "precision": 0.26078624096473063,
                "recall": 0.3295238095238095,
                "main_score": 0.27600581429152854
            },
            {
                "hf_subset": "gle-eng",
                "languages": [
                    "gle-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.065,
                "f1": 0.039595645184317046,
                "precision": 0.035893378968989455,
                "recall": 0.065,
                "main_score": 0.039595645184317046
            },
            {
                "hf_subset": "pes-eng",
                "languages": [
                    "pes-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.17800000000000002,
                "f1": 0.13508124743694003,
                "precision": 0.1224545634920635,
                "recall": 0.17800000000000002,
                "main_score": 0.13508124743694003
            },
            {
                "hf_subset": "nob-eng",
                "languages": [
                    "nob-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.217,
                "f1": 0.17670744996104168,
                "precision": 0.1647070885787265,
                "recall": 0.217,
                "main_score": 0.17670744996104168
            },
            {
                "hf_subset": "bul-eng",
                "languages": [
                    "bul-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.193,
                "f1": 0.14249803276788572,
                "precision": 0.12916981621996224,
                "recall": 0.193,
                "main_score": 0.14249803276788572
            },
            {
                "hf_subset": "cbk-eng",
                "languages": [
                    "cbk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.672,
                "f1": 0.6103507936507936,
                "precision": 0.5869699346405229,
                "recall": 0.672,
                "main_score": 0.6103507936507936
            },
            {
                "hf_subset": "hun-eng",
                "languages": [
                    "hun-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.065,
                "f1": 0.042950975721761965,
                "precision": 0.03809609027256814,
                "recall": 0.065,
                "main_score": 0.042950975721761965
            },
            {
                "hf_subset": "uig-eng",
                "languages": [
                    "uig-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.028000000000000004,
                "f1": 0.01678577135635959,
                "precision": 0.01455966810966811,
                "recall": 0.028000000000000004,
                "main_score": 0.01678577135635959
            },
            {
                "hf_subset": "rus-eng",
                "languages": [
                    "rus-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.479,
                "f1": 0.4026661017143776,
                "precision": 0.3768077894327895,
                "recall": 0.479,
                "main_score": 0.4026661017143776
            },
            {
                "hf_subset": "spa-eng",
                "languages": [
                    "spa-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.97,
                "f1": 0.9605,
                "precision": 0.9558333333333334,
                "recall": 0.97,
                "main_score": 0.9605
            },
            {
                "hf_subset": "hye-eng",
                "languages": [
                    "hye-Armn",
                    "eng-Latn"
                ],
                "accuracy": 0.009433962264150943,
                "f1": 0.006457074216068709,
                "precision": 0.006068362258275372,
                "recall": 0.009433962264150943,
                "main_score": 0.006457074216068709
            },
            {
                "hf_subset": "tel-eng",
                "languages": [
                    "tel-Telu",
                    "eng-Latn"
                ],
                "accuracy": 0.7478632478632479,
                "f1": 0.6905372405372405,
                "precision": 0.6682336182336183,
                "recall": 0.7478632478632479,
                "main_score": 0.6905372405372405
            },
            {
                "hf_subset": "afr-eng",
                "languages": [
                    "afr-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.192,
                "f1": 0.1454460169057995,
                "precision": 0.13265236397589336,
                "recall": 0.192,
                "main_score": 0.1454460169057995
            },
            {
                "hf_subset": "mon-eng",
                "languages": [
                    "mon-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.06818181818181818,
                "f1": 0.0478808236251355,
                "precision": 0.044579691142191145,
                "recall": 0.06818181818181818,
                "main_score": 0.0478808236251355
            },
            {
                "hf_subset": "arz-eng",
                "languages": [
                    "arz-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.7253668763102725,
                "f1": 0.6600978336827393,
                "precision": 0.6321104122990915,
                "recall": 0.7253668763102725,
                "main_score": 0.6600978336827393
            },
            {
                "hf_subset": "hrv-eng",
                "languages": [
                    "hrv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.127,
                "f1": 0.09731576351893512,
                "precision": 0.08986658245110662,
                "recall": 0.127,
                "main_score": 0.09731576351893512
            },
            {
                "hf_subset": "nov-eng",
                "languages": [
                    "nov-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.5719844357976653,
                "f1": 0.49138410227904394,
                "precision": 0.45881971465629057,
                "recall": 0.5719844357976653,
                "main_score": 0.49138410227904394
            },
            {
                "hf_subset": "gsw-eng",
                "languages": [
                    "gsw-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.28205128205128205,
                "f1": 0.21863766936230702,
                "precision": 0.2021216437883105,
                "recall": 0.28205128205128205,
                "main_score": 0.21863766936230702
            },
            {
                "hf_subset": "nds-eng",
                "languages": [
                    "nds-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.233,
                "f1": 0.1775959261382939,
                "precision": 0.1618907864830205,
                "recall": 0.233,
                "main_score": 0.1775959261382939
            },
            {
                "hf_subset": "ukr-eng",
                "languages": [
                    "ukr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.191,
                "f1": 0.14320618913993743,
                "precision": 0.12980748202777614,
                "recall": 0.191,
                "main_score": 0.14320618913993743
            },
            {
                "hf_subset": "uzb-eng",
                "languages": [
                    "uzb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.08411214953271028,
                "f1": 0.051523091826830146,
                "precision": 0.04456214003721122,
                "recall": 0.08411214953271028,
                "main_score": 0.051523091826830146
            },
            {
                "hf_subset": "lit-eng",
                "languages": [
                    "lit-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.067,
                "f1": 0.048339305047646455,
                "precision": 0.04475394510103751,
                "recall": 0.067,
                "main_score": 0.048339305047646455
            },
            {
                "hf_subset": "ina-eng",
                "languages": [
                    "ina-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.794,
                "f1": 0.7459166666666667,
                "precision": 0.7259928571428571,
                "recall": 0.794,
                "main_score": 0.7459166666666667
            },
            {
                "hf_subset": "lfn-eng",
                "languages": [
                    "lfn-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.478,
                "f1": 0.41944877899877897,
                "precision": 0.3987211701696996,
                "recall": 0.478,
                "main_score": 0.41944877899877897
            },
            {
                "hf_subset": "zsm-eng",
                "languages": [
                    "zsm-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.85,
                "f1": 0.8147666666666666,
                "precision": 0.7995909090909091,
                "recall": 0.85,
                "main_score": 0.8147666666666666
            },
            {
                "hf_subset": "ita-eng",
                "languages": [
                    "ita-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.626,
                "f1": 0.5596755336167101,
                "precision": 0.5349577131202131,
                "recall": 0.626,
                "main_score": 0.5596755336167101
            },
            {
                "hf_subset": "cmn-eng",
                "languages": [
                    "cmn-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.953,
                "f1": 0.9396666666666669,
                "precision": 0.9333333333333332,
                "recall": 0.953,
                "main_score": 0.9396666666666669
            },
            {
                "hf_subset": "lvs-eng",
                "languages": [
                    "lvs-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.077,
                "f1": 0.05534253062728994,
                "precision": 0.04985756669800787,
                "recall": 0.077,
                "main_score": 0.05534253062728994
            },
            {
                "hf_subset": "glg-eng",
                "languages": [
                    "glg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.805,
                "f1": 0.7591705128205128,
                "precision": 0.7396261904761904,
                "recall": 0.805,
                "main_score": 0.7591705128205128
            },
            {
                "hf_subset": "ceb-eng",
                "languages": [
                    "ceb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.10333333333333333,
                "f1": 0.07753678057001794,
                "precision": 0.0720761422598628,
                "recall": 0.10333333333333333,
                "main_score": 0.07753678057001794
            },
            {
                "hf_subset": "bre-eng",
                "languages": [
                    "bre-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.086,
                "f1": 0.0534568311045007,
                "precision": 0.04569931461907269,
                "recall": 0.086,
                "main_score": 0.0534568311045007
            },
            {
                "hf_subset": "ben-eng",
                "languages": [
                    "ben-Beng",
                    "eng-Latn"
                ],
                "accuracy": 0.828,
                "f1": 0.7875999999999999,
                "precision": 0.7697666666666666,
                "recall": 0.828,
                "main_score": 0.7875999999999999
            },
            {
                "hf_subset": "swg-eng",
                "languages": [
                    "swg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.26785714285714285,
                "f1": 0.21626275510204082,
                "precision": 0.2017219387755102,
                "recall": 0.26785714285714285,
                "main_score": 0.21626275510204082
            },
            {
                "hf_subset": "arq-eng",
                "languages": [
                    "arq-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.3293084522502745,
                "f1": 0.2628151362794163,
                "precision": 0.2405050619189897,
                "recall": 0.3293084522502745,
                "main_score": 0.2628151362794163
            },
            {
                "hf_subset": "kab-eng",
                "languages": [
                    "kab-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.021,
                "f1": 0.01144678201129814,
                "precision": 0.010228433014856975,
                "recall": 0.021,
                "main_score": 0.01144678201129814
            },
            {
                "hf_subset": "fra-eng",
                "languages": [
                    "fra-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.943,
                "f1": 0.9277000000000001,
                "precision": 0.9209166666666667,
                "recall": 0.943,
                "main_score": 0.9277000000000001
            },
            {
                "hf_subset": "por-eng",
                "languages": [
                    "por-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.941,
                "f1": 0.9251666666666667,
                "precision": 0.9175,
                "recall": 0.941,
                "main_score": 0.9251666666666667
            },
            {
                "hf_subset": "tat-eng",
                "languages": [
                    "tat-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.04100000000000001,
                "f1": 0.02856566814643248,
                "precision": 0.026200368188362508,
                "recall": 0.04100000000000001,
                "main_score": 0.02856566814643248
            },
            {
                "hf_subset": "oci-eng",
                "languages": [
                    "oci-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.45899999999999996,
                "f1": 0.3902207792207792,
                "precision": 0.36524158064158063,
                "recall": 0.45899999999999996,
                "main_score": 0.3902207792207792
            },
            {
                "hf_subset": "pol-eng",
                "languages": [
                    "pol-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.134,
                "f1": 0.09610915175295981,
                "precision": 0.08755127233877234,
                "recall": 0.134,
                "main_score": 0.09610915175295981
            },
            {
                "hf_subset": "war-eng",
                "languages": [
                    "war-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.111,
                "f1": 0.08068379205189385,
                "precision": 0.07400827352459544,
                "recall": 0.111,
                "main_score": 0.08068379205189385
            },
            {
                "hf_subset": "aze-eng",
                "languages": [
                    "aze-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.08900000000000001,
                "f1": 0.06632376174517077,
                "precision": 0.0607114926880766,
                "recall": 0.08900000000000001,
                "main_score": 0.06632376174517077
            },
            {
                "hf_subset": "vie-eng",
                "languages": [
                    "vie-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.958,
                "f1": 0.9457333333333334,
                "precision": 0.9399166666666667,
                "recall": 0.958,
                "main_score": 0.9457333333333334
            },
            {
                "hf_subset": "nno-eng",
                "languages": [
                    "nno-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.166,
                "f1": 0.1332894003117462,
                "precision": 0.1247204179664362,
                "recall": 0.166,
                "main_score": 0.1332894003117462
            },
            {
                "hf_subset": "cha-eng",
                "languages": [
                    "cha-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.29927007299270075,
                "f1": 0.22899432278994322,
                "precision": 0.20917701519891302,
                "recall": 0.29927007299270075,
                "main_score": 0.22899432278994322
            },
            {
                "hf_subset": "mhr-eng",
                "languages": [
                    "mhr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.035,
                "f1": 0.02380972267492708,
                "precision": 0.021368238705738705,
                "recall": 0.035,
                "main_score": 0.02380972267492708
            },
            {
                "hf_subset": "dan-eng",
                "languages": [
                    "dan-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.21600000000000003,
                "f1": 0.1754705304666238,
                "precision": 0.1640586970344022,
                "recall": 0.21600000000000003,
                "main_score": 0.1754705304666238
            },
            {
                "hf_subset": "ell-eng",
                "languages": [
                    "ell-Grek",
                    "eng-Latn"
                ],
                "accuracy": 0.036,
                "f1": 0.023374438522182763,
                "precision": 0.02099034070054354,
                "recall": 0.036,
                "main_score": 0.023374438522182763
            },
            {
                "hf_subset": "amh-eng",
                "languages": [
                    "amh-Ethi",
                    "eng-Latn"
                ],
                "accuracy": 0.017857142857142856,
                "f1": 0.0012056962540054328,
                "precision": 0.000628414244485673,
                "recall": 0.017857142857142856,
                "main_score": 0.0012056962540054328
            },
            {
                "hf_subset": "pam-eng",
                "languages": [
                    "pam-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.074,
                "f1": 0.05677284679983816,
                "precision": 0.05314304945764335,
                "recall": 0.074,
                "main_score": 0.05677284679983816
            },
            {
                "hf_subset": "hsb-eng",
                "languages": [
                    "hsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.13043478260869565,
                "f1": 0.09776306477806768,
                "precision": 0.0909389484497104,
                "recall": 0.13043478260869565,
                "main_score": 0.09776306477806768
            },
            {
                "hf_subset": "srp-eng",
                "languages": [
                    "srp-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.12300000000000001,
                "f1": 0.08757454269574472,
                "precision": 0.07882868657107786,
                "recall": 0.12300000000000001,
                "main_score": 0.08757454269574472
            },
            {
                "hf_subset": "epo-eng",
                "languages": [
                    "epo-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.289,
                "f1": 0.23108557220070378,
                "precision": 0.21354333285625132,
                "recall": 0.289,
                "main_score": 0.23108557220070378
            },
            {
                "hf_subset": "kzj-eng",
                "languages": [
                    "kzj-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.064,
                "f1": 0.04781499273475174,
                "precision": 0.04449604005346457,
                "recall": 0.064,
                "main_score": 0.04781499273475174
            },
            {
                "hf_subset": "awa-eng",
                "languages": [
                    "awa-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.5194805194805194,
                "f1": 0.45658020784071207,
                "precision": 0.43541639337093885,
                "recall": 0.5194805194805194,
                "main_score": 0.45658020784071207
            },
            {
                "hf_subset": "fao-eng",
                "languages": [
                    "fao-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.1450381679389313,
                "f1": 0.0941633734873304,
                "precision": 0.08170700850314679,
                "recall": 0.1450381679389313,
                "main_score": 0.0941633734873304
            },
            {
                "hf_subset": "mal-eng",
                "languages": [
                    "mal-Mlym",
                    "eng-Latn"
                ],
                "accuracy": 0.8879184861717613,
                "f1": 0.8556040756914117,
                "precision": 0.8408539543910724,
                "recall": 0.8879184861717613,
                "main_score": 0.8556040756914117
            },
            {
                "hf_subset": "ile-eng",
                "languages": [
                    "ile-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.625,
                "f1": 0.560802331002331,
                "precision": 0.5361378823073945,
                "recall": 0.625,
                "main_score": 0.560802331002331
            },
            {
                "hf_subset": "bos-eng",
                "languages": [
                    "bos-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.16101694915254236,
                "f1": 0.11927172795816864,
                "precision": 0.10939011968423734,
                "recall": 0.16101694915254236,
                "main_score": 0.11927172795816864
            },
            {
                "hf_subset": "cor-eng",
                "languages": [
                    "cor-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.055,
                "f1": 0.031258727724517196,
                "precision": 0.02679506580565404,
                "recall": 0.055,
                "main_score": 0.031258727724517196
            },
            {
                "hf_subset": "cat-eng",
                "languages": [
                    "cat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8759999999999999,
                "f1": 0.8453666666666666,
                "precision": 0.83125,
                "recall": 0.8759999999999999,
                "main_score": 0.8453666666666666
            },
            {
                "hf_subset": "eus-eng",
                "languages": [
                    "eus-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.657,
                "f1": 0.596442857142857,
                "precision": 0.5730171568627451,
                "recall": 0.657,
                "main_score": 0.596442857142857
            },
            {
                "hf_subset": "yue-eng",
                "languages": [
                    "yue-Hant",
                    "eng-Latn"
                ],
                "accuracy": 0.847,
                "f1": 0.8134523809523809,
                "precision": 0.7982777777777779,
                "recall": 0.847,
                "main_score": 0.8134523809523809
            },
            {
                "hf_subset": "swe-eng",
                "languages": [
                    "swe-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.18600000000000003,
                "f1": 0.1493884103295868,
                "precision": 0.1405947808780388,
                "recall": 0.18600000000000003,
                "main_score": 0.1493884103295868
            },
            {
                "hf_subset": "dtp-eng",
                "languages": [
                    "dtp-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.055,
                "f1": 0.03815842342611909,
                "precision": 0.03565130046415928,
                "recall": 0.055,
                "main_score": 0.03815842342611909
            },
            {
                "hf_subset": "kat-eng",
                "languages": [
                    "kat-Geor",
                    "eng-Latn"
                ],
                "accuracy": 0.012064343163538873,
                "f1": 0.009147778048582338,
                "precision": 0.008441848589301672,
                "recall": 0.012064343163538873,
                "main_score": 0.009147778048582338
            },
            {
                "hf_subset": "jpn-eng",
                "languages": [
                    "jpn-Jpan",
                    "eng-Latn"
                ],
                "accuracy": 0.713,
                "f1": 0.6597350649350647,
                "precision": 0.6385277777777777,
                "recall": 0.713,
                "main_score": 0.6597350649350647
            },
            {
                "hf_subset": "csb-eng",
                "languages": [
                    "csb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.13043478260869565,
                "f1": 0.09043759194508344,
                "precision": 0.08097993164155737,
                "recall": 0.13043478260869565,
                "main_score": 0.09043759194508344
            },
            {
                "hf_subset": "xho-eng",
                "languages": [
                    "xho-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.11267605633802819,
                "f1": 0.0830172606520348,
                "precision": 0.0773705901360373,
                "recall": 0.11267605633802819,
                "main_score": 0.0830172606520348
            },
            {
                "hf_subset": "orv-eng",
                "languages": [
                    "orv-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.05029940119760479,
                "f1": 0.0307264903262435,
                "precision": 0.027633481831401782,
                "recall": 0.05029940119760479,
                "main_score": 0.0307264903262435
            },
            {
                "hf_subset": "ind-eng",
                "languages": [
                    "ind-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9060000000000001,
                "f1": 0.8829666666666667,
                "precision": 0.8721666666666666,
                "recall": 0.9060000000000001,
                "main_score": 0.8829666666666667
            },
            {
                "hf_subset": "tuk-eng",
                "languages": [
                    "tuk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.07389162561576355,
                "f1": 0.05142049156827481,
                "precision": 0.04756506859714838,
                "recall": 0.07389162561576355,
                "main_score": 0.05142049156827481
            },
            {
                "hf_subset": "max-eng",
                "languages": [
                    "max-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.44366197183098594,
                "f1": 0.3937867653881126,
                "precision": 0.3771007182068377,
                "recall": 0.44366197183098594,
                "main_score": 0.3937867653881126
            },
            {
                "hf_subset": "swh-eng",
                "languages": [
                    "swh-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.21794871794871795,
                "f1": 0.16314588577641767,
                "precision": 0.14962288221599962,
                "recall": 0.21794871794871795,
                "main_score": 0.16314588577641767
            },
            {
                "hf_subset": "hin-eng",
                "languages": [
                    "hin-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.935,
                "f1": 0.9153333333333333,
                "precision": 0.9058333333333333,
                "recall": 0.935,
                "main_score": 0.9153333333333333
            },
            {
                "hf_subset": "dsb-eng",
                "languages": [
                    "dsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.12526096033402923,
                "f1": 0.0957488704957882,
                "precision": 0.08943001322776725,
                "recall": 0.12526096033402923,
                "main_score": 0.0957488704957882
            },
            {
                "hf_subset": "ber-eng",
                "languages": [
                    "ber-Tfng",
                    "eng-Latn"
                ],
                "accuracy": 0.069,
                "f1": 0.045770099528158,
                "precision": 0.04166915172638407,
                "recall": 0.069,
                "main_score": 0.045770099528158
            },
            {
                "hf_subset": "tam-eng",
                "languages": [
                    "tam-Taml",
                    "eng-Latn"
                ],
                "accuracy": 0.8175895765472313,
                "f1": 0.7729641693811076,
                "precision": 0.7535287730727469,
                "recall": 0.8175895765472313,
                "main_score": 0.7729641693811076
            },
            {
                "hf_subset": "slk-eng",
                "languages": [
                    "slk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.11,
                "f1": 0.08522094712720397,
                "precision": 0.07883076528738328,
                "recall": 0.11,
                "main_score": 0.08522094712720397
            },
            {
                "hf_subset": "tgl-eng",
                "languages": [
                    "tgl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.113,
                "f1": 0.08626190704312432,
                "precision": 0.0799443442063718,
                "recall": 0.113,
                "main_score": 0.08626190704312432
            },
            {
                "hf_subset": "ast-eng",
                "languages": [
                    "ast-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7401574803149606,
                "f1": 0.6816272965879265,
                "precision": 0.6599737532808398,
                "recall": 0.7401574803149606,
                "main_score": 0.6816272965879265
            },
            {
                "hf_subset": "mkd-eng",
                "languages": [
                    "mkd-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.09,
                "f1": 0.06189958106409719,
                "precision": 0.05445330404889228,
                "recall": 0.09,
                "main_score": 0.06189958106409719
            },
            {
                "hf_subset": "khm-eng",
                "languages": [
                    "khm-Khmr",
                    "eng-Latn"
                ],
                "accuracy": 0.002770083102493075,
                "f1": 0.00011664800298618888,
                "precision": 5.9578568115600365e-05,
                "recall": 0.002770083102493075,
                "main_score": 0.00011664800298618888
            },
            {
                "hf_subset": "ces-eng",
                "languages": [
                    "ces-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.088,
                "f1": 0.05636139438882621,
                "precision": 0.049939729145530035,
                "recall": 0.088,
                "main_score": 0.05636139438882621
            },
            {
                "hf_subset": "tzl-eng",
                "languages": [
                    "tzl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.375,
                "f1": 0.3131118881118881,
                "precision": 0.29439102564102565,
                "recall": 0.375,
                "main_score": 0.3131118881118881
            },
            {
                "hf_subset": "urd-eng",
                "languages": [
                    "urd-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.745,
                "f1": 0.6896380952380953,
                "precision": 0.6667968253968255,
                "recall": 0.745,
                "main_score": 0.6896380952380953
            },
            {
                "hf_subset": "ara-eng",
                "languages": [
                    "ara-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.89,
                "f1": 0.8642523809523809,
                "precision": 0.8528333333333332,
                "recall": 0.89,
                "main_score": 0.8642523809523809
            },
            {
                "hf_subset": "kor-eng",
                "languages": [
                    "kor-Hang",
                    "eng-Latn"
                ],
                "accuracy": 0.172,
                "f1": 0.12555081585081584,
                "precision": 0.11292745310245308,
                "recall": 0.172,
                "main_score": 0.12555081585081584
            },
            {
                "hf_subset": "yid-eng",
                "languages": [
                    "yid-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.0035377358490566043,
                "f1": 0.0012010530448397783,
                "precision": 0.0011902214818132154,
                "recall": 0.0035377358490566043,
                "main_score": 0.0012010530448397783
            },
            {
                "hf_subset": "fin-eng",
                "languages": [
                    "fin-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.059,
                "f1": 0.0426942162679512,
                "precision": 0.03967144120536608,
                "recall": 0.059,
                "main_score": 0.0426942162679512
            },
            {
                "hf_subset": "tha-eng",
                "languages": [
                    "tha-Thai",
                    "eng-Latn"
                ],
                "accuracy": 0.027372262773722632,
                "f1": 0.0164474042578532,
                "precision": 0.01567547886228932,
                "recall": 0.027372262773722632,
                "main_score": 0.0164474042578532
            },
            {
                "hf_subset": "wuu-eng",
                "languages": [
                    "wuu-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.8489999999999999,
                "f1": 0.8117555555555555,
                "precision": 0.7956416666666667,
                "recall": 0.8489999999999999,
                "main_score": 0.8117555555555555
            }
        ]
    }
}