{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "task_name": "AmazonCounterfactualClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.652575,
        "f1": 0.627469,
        "f1_weighted": 0.665364,
        "ap": 0.772282,
        "ap_weighted": 0.772282,
        "scores_per_experiment": [
          {
            "accuracy": 0.684549,
            "f1": 0.660927,
            "f1_weighted": 0.696265,
            "ap": 0.792418,
            "ap_weighted": 0.792418
          },
          {
            "accuracy": 0.695279,
            "f1": 0.655698,
            "f1_weighted": 0.701792,
            "ap": 0.777952,
            "ap_weighted": 0.777952
          },
          {
            "accuracy": 0.66309,
            "f1": 0.634181,
            "f1_weighted": 0.674786,
            "ap": 0.772833,
            "ap_weighted": 0.772833
          },
          {
            "accuracy": 0.645923,
            "f1": 0.622101,
            "f1_weighted": 0.659564,
            "ap": 0.769869,
            "ap_weighted": 0.769869
          },
          {
            "accuracy": 0.583691,
            "f1": 0.560975,
            "f1_weighted": 0.600406,
            "ap": 0.736828,
            "ap_weighted": 0.736828
          },
          {
            "accuracy": 0.675966,
            "f1": 0.650843,
            "f1_weighted": 0.687824,
            "ap": 0.785183,
            "ap_weighted": 0.785183
          },
          {
            "accuracy": 0.628755,
            "f1": 0.607268,
            "f1_weighted": 0.64354,
            "ap": 0.763337,
            "ap_weighted": 0.763337
          },
          {
            "accuracy": 0.660944,
            "f1": 0.640937,
            "f1_weighted": 0.674404,
            "ap": 0.784397,
            "ap_weighted": 0.784397
          },
          {
            "accuracy": 0.607296,
            "f1": 0.586292,
            "f1_weighted": 0.623099,
            "ap": 0.75186,
            "ap_weighted": 0.75186
          },
          {
            "accuracy": 0.680258,
            "f1": 0.655467,
            "f1_weighted": 0.691958,
            "ap": 0.788138,
            "ap_weighted": 0.788138
          }
        ],
        "main_score": 0.652575,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.628266,
        "f1": 0.607706,
        "f1_weighted": 0.642361,
        "ap": 0.763177,
        "ap_weighted": 0.763177,
        "scores_per_experiment": [
          {
            "accuracy": 0.6606,
            "f1": 0.638076,
            "f1_weighted": 0.673456,
            "ap": 0.778394,
            "ap_weighted": 0.778394
          },
          {
            "accuracy": 0.67666,
            "f1": 0.641098,
            "f1_weighted": 0.685368,
            "ap": 0.7708,
            "ap_weighted": 0.7708
          },
          {
            "accuracy": 0.632762,
            "f1": 0.607031,
            "f1_weighted": 0.646435,
            "ap": 0.758225,
            "ap_weighted": 0.758225
          },
          {
            "accuracy": 0.652034,
            "f1": 0.633321,
            "f1_weighted": 0.665781,
            "ap": 0.779415,
            "ap_weighted": 0.779415
          },
          {
            "accuracy": 0.584582,
            "f1": 0.570693,
            "f1_weighted": 0.600952,
            "ap": 0.7475,
            "ap_weighted": 0.7475
          },
          {
            "accuracy": 0.669165,
            "f1": 0.648783,
            "f1_weighted": 0.681938,
            "ap": 0.786842,
            "ap_weighted": 0.786842
          },
          {
            "accuracy": 0.584582,
            "f1": 0.568926,
            "f1_weighted": 0.601119,
            "ap": 0.744728,
            "ap_weighted": 0.744728
          },
          {
            "accuracy": 0.609208,
            "f1": 0.595654,
            "f1_weighted": 0.624663,
            "ap": 0.762677,
            "ap_weighted": 0.762677
          },
          {
            "accuracy": 0.561028,
            "f1": 0.543702,
            "f1_weighted": 0.578544,
            "ap": 0.729374,
            "ap_weighted": 0.729374
          },
          {
            "accuracy": 0.652034,
            "f1": 0.629779,
            "f1_weighted": 0.665349,
            "ap": 0.773822,
            "ap_weighted": 0.773822
          }
        ],
        "main_score": 0.628266,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 107.96181273460388,
  "kg_co2_emissions": null
}