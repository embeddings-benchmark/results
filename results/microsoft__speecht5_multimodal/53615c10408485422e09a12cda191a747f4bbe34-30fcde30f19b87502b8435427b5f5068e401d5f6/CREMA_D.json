{
  "dataset_revision": "a9d0821955c418752248b8310438854e56a1cf34",
  "task_name": "CREMA_D",
  "mteb_version": "2.4.2",
  "scores": {
    "train": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.285426,
            "f1": 0.281679,
            "f1_weighted": 0.281803,
            "precision": 0.282093,
            "precision_weighted": 0.283207,
            "recall": 0.286118,
            "recall_weighted": 0.285426,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.377435,
            "f1": 0.369217,
            "f1_weighted": 0.373098,
            "precision": 0.371893,
            "precision_weighted": 0.376763,
            "recall": 0.374329,
            "recall_weighted": 0.377435,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.315188,
            "f1": 0.31597,
            "f1_weighted": 0.317,
            "precision": 0.320172,
            "precision_weighted": 0.321201,
            "recall": 0.314143,
            "recall_weighted": 0.315188,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.363575,
            "f1": 0.358204,
            "f1_weighted": 0.359341,
            "precision": 0.361445,
            "precision_weighted": 0.364891,
            "recall": 0.366025,
            "recall_weighted": 0.363575,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.284274,
            "f1": 0.284267,
            "f1_weighted": 0.284296,
            "precision": 0.292344,
            "precision_weighted": 0.29045,
            "recall": 0.282575,
            "recall_weighted": 0.284274,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.32518,
        "f1": 0.321867,
        "f1_weighted": 0.323107,
        "precision": 0.325589,
        "precision_weighted": 0.327302,
        "recall": 0.324638,
        "recall_weighted": 0.32518,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.32518,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 39.35495352745056,
  "kg_co2_emissions": null
}