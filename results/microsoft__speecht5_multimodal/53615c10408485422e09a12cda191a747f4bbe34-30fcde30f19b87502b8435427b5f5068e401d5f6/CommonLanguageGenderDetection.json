{
  "dataset_revision": "65cdf4a4565f09b1747cd8fb37d18cd9aa1f6dd9",
  "task_name": "CommonLanguageGenderDetection",
  "mteb_version": "2.4.2",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.2995,
            "f1": 0.213882,
            "f1_weighted": 0.389124,
            "precision": 0.309353,
            "precision_weighted": 0.733344,
            "recall": 0.351006,
            "recall_weighted": 0.2995,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.329,
            "f1": 0.223335,
            "f1_weighted": 0.426421,
            "precision": 0.314035,
            "precision_weighted": 0.753053,
            "recall": 0.249745,
            "recall_weighted": 0.329,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.167,
            "f1": 0.145699,
            "f1_weighted": 0.211672,
            "precision": 0.27865,
            "precision_weighted": 0.666569,
            "recall": 0.313788,
            "recall_weighted": 0.167,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.33,
            "f1": 0.193962,
            "f1_weighted": 0.427507,
            "precision": 0.258348,
            "precision_weighted": 0.63737,
            "recall": 0.29776,
            "recall_weighted": 0.33,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.289,
            "f1": 0.184678,
            "f1_weighted": 0.38076,
            "precision": 0.261678,
            "precision_weighted": 0.646626,
            "recall": 0.181813,
            "recall_weighted": 0.289,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.36,
            "f1": 0.225418,
            "f1_weighted": 0.461245,
            "precision": 0.295055,
            "precision_weighted": 0.708288,
            "recall": 0.354559,
            "recall_weighted": 0.36,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.3865,
            "f1": 0.234457,
            "f1_weighted": 0.471181,
            "precision": 0.289467,
            "precision_weighted": 0.695981,
            "recall": 0.36314,
            "recall_weighted": 0.3865,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.197,
            "f1": 0.150545,
            "f1_weighted": 0.279241,
            "precision": 0.261273,
            "precision_weighted": 0.631382,
            "recall": 0.271806,
            "recall_weighted": 0.197,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.391,
            "f1": 0.235654,
            "f1_weighted": 0.482842,
            "precision": 0.287891,
            "precision_weighted": 0.65464,
            "recall": 0.228473,
            "recall_weighted": 0.391,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.249,
            "f1": 0.169064,
            "f1_weighted": 0.33104,
            "precision": 0.270237,
            "precision_weighted": 0.679762,
            "recall": 0.284977,
            "recall_weighted": 0.249,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.2998,
        "f1": 0.197669,
        "f1_weighted": 0.386103,
        "precision": 0.282598,
        "precision_weighted": 0.680701,
        "recall": 0.289707,
        "recall_weighted": 0.2998,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.2998,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 157.3042950630188,
  "kg_co2_emissions": null
}