{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.659364,
        "f1": 0.543448,
        "f1_weighted": 0.713593,
        "ap": 0.181739,
        "ap_weighted": 0.181739,
        "scores_per_experiment": [
          {
            "accuracy": 0.541237,
            "f1": 0.475531,
            "f1_weighted": 0.614599,
            "ap": 0.166072,
            "ap_weighted": 0.166072
          },
          {
            "accuracy": 0.664089,
            "f1": 0.555661,
            "f1_weighted": 0.720095,
            "ap": 0.191913,
            "ap_weighted": 0.191913
          },
          {
            "accuracy": 0.626289,
            "f1": 0.533557,
            "f1_weighted": 0.68936,
            "ap": 0.186553,
            "ap_weighted": 0.186553
          },
          {
            "accuracy": 0.696735,
            "f1": 0.574983,
            "f1_weighted": 0.745397,
            "ap": 0.19789,
            "ap_weighted": 0.19789
          },
          {
            "accuracy": 0.661512,
            "f1": 0.503963,
            "f1_weighted": 0.713388,
            "ap": 0.137934,
            "ap_weighted": 0.137934
          },
          {
            "accuracy": 0.647766,
            "f1": 0.535629,
            "f1_weighted": 0.70658,
            "ap": 0.174424,
            "ap_weighted": 0.174424
          },
          {
            "accuracy": 0.803265,
            "f1": 0.626303,
            "f1_weighted": 0.81895,
            "ap": 0.209559,
            "ap_weighted": 0.209559
          },
          {
            "accuracy": 0.602234,
            "f1": 0.497703,
            "f1_weighted": 0.669362,
            "ap": 0.15402,
            "ap_weighted": 0.15402
          },
          {
            "accuracy": 0.662371,
            "f1": 0.550343,
            "f1_weighted": 0.718482,
            "ap": 0.185268,
            "ap_weighted": 0.185268
          },
          {
            "accuracy": 0.688144,
            "f1": 0.580811,
            "f1_weighted": 0.739715,
            "ap": 0.213756,
            "ap_weighted": 0.213756
          }
        ],
        "main_score": 0.659364,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 9.423626899719238,
  "kg_co2_emissions": 0.0004034566404673393
}