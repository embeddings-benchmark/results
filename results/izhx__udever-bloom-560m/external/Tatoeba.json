{
    "dataset_revision": "9080400076fbadbb4c4dcb136ff4eddc40b42553",
    "task_name": "Tatoeba",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "sqi-eng",
                "languages": [
                    "sqi-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.11,
                "f1": 0.08487309997179562,
                "precision": 0.07935185890268856,
                "recall": 0.11,
                "main_score": 0.08487309997179562
            },
            {
                "hf_subset": "fry-eng",
                "languages": [
                    "fry-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.23699421965317918,
                "f1": 0.18099825672080008,
                "precision": 0.16582017825552964,
                "recall": 0.23699421965317918,
                "main_score": 0.18099825672080008
            },
            {
                "hf_subset": "kur-eng",
                "languages": [
                    "kur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.08780487804878048,
                "f1": 0.06484836753129436,
                "precision": 0.059162208017477236,
                "recall": 0.08780487804878048,
                "main_score": 0.06484836753129436
            },
            {
                "hf_subset": "tur-eng",
                "languages": [
                    "tur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.05,
                "f1": 0.03493223480735001,
                "precision": 0.031492116349139385,
                "recall": 0.05,
                "main_score": 0.03493223480735001
            },
            {
                "hf_subset": "deu-eng",
                "languages": [
                    "deu-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.336,
                "f1": 0.29339340352229065,
                "precision": 0.27997920626374695,
                "recall": 0.336,
                "main_score": 0.29339340352229065
            },
            {
                "hf_subset": "nld-eng",
                "languages": [
                    "nld-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.20200000000000004,
                "f1": 0.16330981736231456,
                "precision": 0.15250949969794045,
                "recall": 0.20200000000000004,
                "main_score": 0.16330981736231456
            },
            {
                "hf_subset": "ron-eng",
                "languages": [
                    "ron-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.196,
                "f1": 0.14951120083366323,
                "precision": 0.13617335362707,
                "recall": 0.196,
                "main_score": 0.14951120083366323
            },
            {
                "hf_subset": "ang-eng",
                "languages": [
                    "ang-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.20149253731343283,
                "f1": 0.13312899786780386,
                "precision": 0.11979388770433545,
                "recall": 0.20149253731343283,
                "main_score": 0.13312899786780386
            },
            {
                "hf_subset": "ido-eng",
                "languages": [
                    "ido-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.314,
                "f1": 0.2621323201417634,
                "precision": 0.24607830064672168,
                "recall": 0.314,
                "main_score": 0.2621323201417634
            },
            {
                "hf_subset": "jav-eng",
                "languages": [
                    "jav-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.18048780487804877,
                "f1": 0.14347798542920492,
                "precision": 0.1330167292057536,
                "recall": 0.18048780487804877,
                "main_score": 0.14347798542920492
            },
            {
                "hf_subset": "isl-eng",
                "languages": [
                    "isl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.052000000000000005,
                "f1": 0.032713297295122505,
                "precision": 0.029785489115857253,
                "recall": 0.052000000000000005,
                "main_score": 0.032713297295122505
            },
            {
                "hf_subset": "slv-eng",
                "languages": [
                    "slv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.07411907654921021,
                "f1": 0.05412915976323278,
                "precision": 0.04975402373122839,
                "recall": 0.07411907654921021,
                "main_score": 0.05412915976323278
            },
            {
                "hf_subset": "cym-eng",
                "languages": [
                    "cym-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.08521739130434783,
                "f1": 0.05871393789897329,
                "precision": 0.05350472658912557,
                "recall": 0.08521739130434783,
                "main_score": 0.05871393789897329
            },
            {
                "hf_subset": "kaz-eng",
                "languages": [
                    "kaz-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.01565217391304348,
                "f1": 0.007422394530145001,
                "precision": 0.007201734373569025,
                "recall": 0.01565217391304348,
                "main_score": 0.007422394530145001
            },
            {
                "hf_subset": "est-eng",
                "languages": [
                    "est-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.053,
                "f1": 0.030838354401589694,
                "precision": 0.027099428390909938,
                "recall": 0.053,
                "main_score": 0.030838354401589694
            },
            {
                "hf_subset": "heb-eng",
                "languages": [
                    "heb-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.008,
                "f1": 0.0024583802742178056,
                "precision": 0.0018710578268453031,
                "recall": 0.008,
                "main_score": 0.0024583802742178056
            },
            {
                "hf_subset": "gla-eng",
                "languages": [
                    "gla-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.04945717732207479,
                "f1": 0.027266734043909436,
                "precision": 0.023247505400014187,
                "recall": 0.04945717732207479,
                "main_score": 0.027266734043909436
            },
            {
                "hf_subset": "mar-eng",
                "languages": [
                    "mar-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.542,
                "f1": 0.4722780366692132,
                "precision": 0.4474017857142856,
                "recall": 0.542,
                "main_score": 0.4722780366692132
            },
            {
                "hf_subset": "lat-eng",
                "languages": [
                    "lat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.258,
                "f1": 0.19547406382656526,
                "precision": 0.1780766233766234,
                "recall": 0.258,
                "main_score": 0.19547406382656526
            },
            {
                "hf_subset": "bel-eng",
                "languages": [
                    "bel-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.049,
                "f1": 0.03283031457969928,
                "precision": 0.030361515007649467,
                "recall": 0.049,
                "main_score": 0.03283031457969928
            },
            {
                "hf_subset": "pms-eng",
                "languages": [
                    "pms-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.22476190476190477,
                "f1": 0.17494204011570957,
                "precision": 0.1616236240785113,
                "recall": 0.22476190476190477,
                "main_score": 0.17494204011570957
            },
            {
                "hf_subset": "gle-eng",
                "languages": [
                    "gle-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.063,
                "f1": 0.03461898170471662,
                "precision": 0.02975546957350575,
                "recall": 0.063,
                "main_score": 0.03461898170471662
            },
            {
                "hf_subset": "pes-eng",
                "languages": [
                    "pes-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.086,
                "f1": 0.05874235156578609,
                "precision": 0.05201352547725499,
                "recall": 0.086,
                "main_score": 0.05874235156578609
            },
            {
                "hf_subset": "nob-eng",
                "languages": [
                    "nob-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.152,
                "f1": 0.11908986787697534,
                "precision": 0.11090628985937809,
                "recall": 0.152,
                "main_score": 0.11908986787697534
            },
            {
                "hf_subset": "bul-eng",
                "languages": [
                    "bul-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.069,
                "f1": 0.0458348360335125,
                "precision": 0.04183620994869927,
                "recall": 0.069,
                "main_score": 0.0458348360335125
            },
            {
                "hf_subset": "cbk-eng",
                "languages": [
                    "cbk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.621,
                "f1": 0.5570845598845598,
                "precision": 0.5322281746031747,
                "recall": 0.621,
                "main_score": 0.5570845598845598
            },
            {
                "hf_subset": "hun-eng",
                "languages": [
                    "hun-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.048,
                "f1": 0.03246932234432234,
                "precision": 0.029738765839703266,
                "recall": 0.048,
                "main_score": 0.03246932234432234
            },
            {
                "hf_subset": "uig-eng",
                "languages": [
                    "uig-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.009,
                "f1": 0.005331481481481481,
                "precision": 0.004918990604783396,
                "recall": 0.009,
                "main_score": 0.005331481481481481
            },
            {
                "hf_subset": "rus-eng",
                "languages": [
                    "rus-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.317,
                "f1": 0.2522406237037816,
                "precision": 0.23272731559290383,
                "recall": 0.317,
                "main_score": 0.2522406237037816
            },
            {
                "hf_subset": "spa-eng",
                "languages": [
                    "spa-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.965,
                "f1": 0.9548333333333333,
                "precision": 0.95,
                "recall": 0.965,
                "main_score": 0.9548333333333333
            },
            {
                "hf_subset": "hye-eng",
                "languages": [
                    "hye-Armn",
                    "eng-Latn"
                ],
                "accuracy": 0.004043126684636119,
                "f1": 0.0022521185350542843,
                "precision": 0.002024538417141191,
                "recall": 0.004043126684636119,
                "main_score": 0.0022521185350542843
            },
            {
                "hf_subset": "tel-eng",
                "languages": [
                    "tel-Telu",
                    "eng-Latn"
                ],
                "accuracy": 0.43162393162393164,
                "f1": 0.3583662064431295,
                "precision": 0.33665901999235337,
                "recall": 0.43162393162393164,
                "main_score": 0.3583662064431295
            },
            {
                "hf_subset": "afr-eng",
                "languages": [
                    "afr-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.122,
                "f1": 0.09007009351120604,
                "precision": 0.0826509907921979,
                "recall": 0.122,
                "main_score": 0.09007009351120604
            },
            {
                "hf_subset": "mon-eng",
                "languages": [
                    "mon-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.020454545454545454,
                "f1": 0.00846869670733307,
                "precision": 0.00719285857023819,
                "recall": 0.020454545454545454,
                "main_score": 0.00846869670733307
            },
            {
                "hf_subset": "arz-eng",
                "languages": [
                    "arz-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.5618448637316562,
                "f1": 0.49418503695233246,
                "precision": 0.46844863731656183,
                "recall": 0.5618448637316562,
                "main_score": 0.49418503695233246
            },
            {
                "hf_subset": "hrv-eng",
                "languages": [
                    "hrv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.084,
                "f1": 0.06274306734742452,
                "precision": 0.058547869151510286,
                "recall": 0.084,
                "main_score": 0.06274306734742452
            },
            {
                "hf_subset": "nov-eng",
                "languages": [
                    "nov-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.45136186770428016,
                "f1": 0.38784818726452974,
                "precision": 0.3665848310789945,
                "recall": 0.45136186770428016,
                "main_score": 0.38784818726452974
            },
            {
                "hf_subset": "gsw-eng",
                "languages": [
                    "gsw-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.23076923076923075,
                "f1": 0.17501757501757503,
                "precision": 0.1606289721674337,
                "recall": 0.23076923076923075,
                "main_score": 0.17501757501757503
            },
            {
                "hf_subset": "nds-eng",
                "languages": [
                    "nds-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.158,
                "f1": 0.11834682187321721,
                "precision": 0.10871016304088595,
                "recall": 0.158,
                "main_score": 0.11834682187321721
            },
            {
                "hf_subset": "ukr-eng",
                "languages": [
                    "ukr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.073,
                "f1": 0.049293149709215396,
                "precision": 0.044277147501285426,
                "recall": 0.073,
                "main_score": 0.049293149709215396
            },
            {
                "hf_subset": "uzb-eng",
                "languages": [
                    "uzb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.051401869158878497,
                "f1": 0.02543797914741945,
                "precision": 0.021476927403586067,
                "recall": 0.051401869158878497,
                "main_score": 0.02543797914741945
            },
            {
                "hf_subset": "lit-eng",
                "languages": [
                    "lit-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.05,
                "f1": 0.03173243817101591,
                "precision": 0.028643206769285487,
                "recall": 0.05,
                "main_score": 0.03173243817101591
            },
            {
                "hf_subset": "ina-eng",
                "languages": [
                    "ina-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.695,
                "f1": 0.6389614902641219,
                "precision": 0.6162865079365079,
                "recall": 0.695,
                "main_score": 0.6389614902641219
            },
            {
                "hf_subset": "lfn-eng",
                "languages": [
                    "lfn-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.418,
                "f1": 0.37523909714712916,
                "precision": 0.36054581750900766,
                "recall": 0.418,
                "main_score": 0.37523909714712916
            },
            {
                "hf_subset": "zsm-eng",
                "languages": [
                    "zsm-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.792,
                "f1": 0.7488805555555554,
                "precision": 0.7305083333333333,
                "recall": 0.792,
                "main_score": 0.7488805555555554
            },
            {
                "hf_subset": "ita-eng",
                "languages": [
                    "ita-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.435,
                "f1": 0.37286600195906044,
                "precision": 0.35180674474335194,
                "recall": 0.435,
                "main_score": 0.37286600195906044
            },
            {
                "hf_subset": "cmn-eng",
                "languages": [
                    "cmn-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.945,
                "f1": 0.9295,
                "precision": 0.922,
                "recall": 0.945,
                "main_score": 0.9295
            },
            {
                "hf_subset": "lvs-eng",
                "languages": [
                    "lvs-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.052000000000000005,
                "f1": 0.03529775565148403,
                "precision": 0.031900137226905836,
                "recall": 0.052000000000000005,
                "main_score": 0.03529775565148403
            },
            {
                "hf_subset": "glg-eng",
                "languages": [
                    "glg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.747,
                "f1": 0.6926023809523809,
                "precision": 0.6703261904761905,
                "recall": 0.747,
                "main_score": 0.6926023809523809
            },
            {
                "hf_subset": "ceb-eng",
                "languages": [
                    "ceb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.08,
                "f1": 0.05639611303143687,
                "precision": 0.052098568242774285,
                "recall": 0.08,
                "main_score": 0.05639611303143687
            },
            {
                "hf_subset": "bre-eng",
                "languages": [
                    "bre-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.061,
                "f1": 0.03847611167634209,
                "precision": 0.033324923687423694,
                "recall": 0.061,
                "main_score": 0.03847611167634209
            },
            {
                "hf_subset": "ben-eng",
                "languages": [
                    "ben-Beng",
                    "eng-Latn"
                ],
                "accuracy": 0.755,
                "f1": 0.7014214285714285,
                "precision": 0.6788761904761904,
                "recall": 0.755,
                "main_score": 0.7014214285714285
            },
            {
                "hf_subset": "swg-eng",
                "languages": [
                    "swg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.20535714285714285,
                "f1": 0.16437074829931972,
                "precision": 0.15459837781266353,
                "recall": 0.20535714285714285,
                "main_score": 0.16437074829931972
            },
            {
                "hf_subset": "arq-eng",
                "languages": [
                    "arq-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.21405049396267833,
                "f1": 0.16162968480476714,
                "precision": 0.14506603642481392,
                "recall": 0.21405049396267833,
                "main_score": 0.16162968480476714
            },
            {
                "hf_subset": "kab-eng",
                "languages": [
                    "kab-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.014000000000000002,
                "f1": 0.008861559696342305,
                "precision": 0.007898232323232323,
                "recall": 0.014000000000000002,
                "main_score": 0.008861559696342305
            },
            {
                "hf_subset": "fra-eng",
                "languages": [
                    "fra-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.935,
                "f1": 0.9165333333333333,
                "precision": 0.9080833333333332,
                "recall": 0.935,
                "main_score": 0.9165333333333333
            },
            {
                "hf_subset": "por-eng",
                "languages": [
                    "por-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.938,
                "f1": 0.9208333333333333,
                "precision": 0.9123333333333333,
                "recall": 0.938,
                "main_score": 0.9208333333333333
            },
            {
                "hf_subset": "tat-eng",
                "languages": [
                    "tat-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.013000000000000001,
                "f1": 0.009654912597950574,
                "precision": 0.00911237853823405,
                "recall": 0.013000000000000001,
                "main_score": 0.009654912597950574
            },
            {
                "hf_subset": "oci-eng",
                "languages": [
                    "oci-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.355,
                "f1": 0.2938586802086802,
                "precision": 0.2738218614718615,
                "recall": 0.355,
                "main_score": 0.2938586802086802
            },
            {
                "hf_subset": "pol-eng",
                "languages": [
                    "pol-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.083,
                "f1": 0.05625495291471218,
                "precision": 0.05006352187769519,
                "recall": 0.083,
                "main_score": 0.05625495291471218
            },
            {
                "hf_subset": "war-eng",
                "languages": [
                    "war-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.09300000000000001,
                "f1": 0.07188871139201601,
                "precision": 0.0668110313042221,
                "recall": 0.09300000000000001,
                "main_score": 0.07188871139201601
            },
            {
                "hf_subset": "aze-eng",
                "languages": [
                    "aze-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.049,
                "f1": 0.03436819671181639,
                "precision": 0.03151657575547619,
                "recall": 0.049,
                "main_score": 0.03436819671181639
            },
            {
                "hf_subset": "vie-eng",
                "languages": [
                    "vie-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.945,
                "f1": 0.9285666666666667,
                "precision": 0.9207499999999998,
                "recall": 0.945,
                "main_score": 0.9285666666666667
            },
            {
                "hf_subset": "nno-eng",
                "languages": [
                    "nno-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.109,
                "f1": 0.08052880589619718,
                "precision": 0.07283302043868081,
                "recall": 0.109,
                "main_score": 0.08052880589619718
            },
            {
                "hf_subset": "cha-eng",
                "languages": [
                    "cha-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.21897810218978106,
                "f1": 0.16459096459096456,
                "precision": 0.1499391727493917,
                "recall": 0.21897810218978106,
                "main_score": 0.16459096459096456
            },
            {
                "hf_subset": "mhr-eng",
                "languages": [
                    "mhr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.008,
                "f1": 0.0043900258600589265,
                "precision": 0.004215147327778906,
                "recall": 0.008,
                "main_score": 0.0043900258600589265
            },
            {
                "hf_subset": "dan-eng",
                "languages": [
                    "dan-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.149,
                "f1": 0.11403181682754628,
                "precision": 0.10506373051667312,
                "recall": 0.149,
                "main_score": 0.11403181682754628
            },
            {
                "hf_subset": "ell-eng",
                "languages": [
                    "ell-Grek",
                    "eng-Latn"
                ],
                "accuracy": 0.019,
                "f1": 0.008872641689515834,
                "precision": 0.007857231069685399,
                "recall": 0.019,
                "main_score": 0.008872641689515834
            },
            {
                "hf_subset": "amh-eng",
                "languages": [
                    "amh-Ethi",
                    "eng-Latn"
                ],
                "accuracy": 0.011904761904761904,
                "f1": 0.002084704849681808,
                "precision": 0.0011904761904761904,
                "recall": 0.011904761904761904,
                "main_score": 0.002084704849681808
            },
            {
                "hf_subset": "pam-eng",
                "languages": [
                    "pam-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.053,
                "f1": 0.03784571880595977,
                "precision": 0.03455647702071978,
                "recall": 0.053,
                "main_score": 0.03784571880595977
            },
            {
                "hf_subset": "hsb-eng",
                "languages": [
                    "hsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.09316770186335405,
                "f1": 0.0680343720685027,
                "precision": 0.06316650292717499,
                "recall": 0.09316770186335405,
                "main_score": 0.0680343720685027
            },
            {
                "hf_subset": "srp-eng",
                "languages": [
                    "srp-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.059,
                "f1": 0.0454869262283137,
                "precision": 0.043111219136124264,
                "recall": 0.059,
                "main_score": 0.0454869262283137
            },
            {
                "hf_subset": "epo-eng",
                "languages": [
                    "epo-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.18099999999999997,
                "f1": 0.134170874831821,
                "precision": 0.12178193046524806,
                "recall": 0.18099999999999997,
                "main_score": 0.134170874831821
            },
            {
                "hf_subset": "kzj-eng",
                "languages": [
                    "kzj-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.044,
                "f1": 0.033905735425765526,
                "precision": 0.032588935800436626,
                "recall": 0.044,
                "main_score": 0.033905735425765526
            },
            {
                "hf_subset": "awa-eng",
                "languages": [
                    "awa-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.37662337662337664,
                "f1": 0.30539579468150896,
                "precision": 0.2860288100547841,
                "recall": 0.37662337662337664,
                "main_score": 0.30539579468150896
            },
            {
                "hf_subset": "fao-eng",
                "languages": [
                    "fao-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.12213740458015267,
                "f1": 0.08297822182308039,
                "precision": 0.07463649581970193,
                "recall": 0.12213740458015267,
                "main_score": 0.08297822182308039
            },
            {
                "hf_subset": "mal-eng",
                "languages": [
                    "mal-Mlym",
                    "eng-Latn"
                ],
                "accuracy": 0.7831149927219796,
                "f1": 0.7335759340126152,
                "precision": 0.7126394953905871,
                "recall": 0.7831149927219796,
                "main_score": 0.7335759340126152
            },
            {
                "hf_subset": "ile-eng",
                "languages": [
                    "ile-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.518,
                "f1": 0.4424010323010323,
                "precision": 0.41450707972582973,
                "recall": 0.518,
                "main_score": 0.4424010323010323
            },
            {
                "hf_subset": "bos-eng",
                "languages": [
                    "bos-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.1327683615819209,
                "f1": 0.09167320569156727,
                "precision": 0.08200402665583079,
                "recall": 0.1327683615819209,
                "main_score": 0.09167320569156727
            },
            {
                "hf_subset": "cor-eng",
                "languages": [
                    "cor-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.048,
                "f1": 0.03126876335279028,
                "precision": 0.028439371869960102,
                "recall": 0.048,
                "main_score": 0.03126876335279028
            },
            {
                "hf_subset": "cat-eng",
                "languages": [
                    "cat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.851,
                "f1": 0.8155,
                "precision": 0.7998166666666665,
                "recall": 0.851,
                "main_score": 0.8155
            },
            {
                "hf_subset": "eus-eng",
                "languages": [
                    "eus-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.483,
                "f1": 0.42347894491129784,
                "precision": 0.4036040404040404,
                "recall": 0.483,
                "main_score": 0.42347894491129784
            },
            {
                "hf_subset": "yue-eng",
                "languages": [
                    "yue-Hant",
                    "eng-Latn"
                ],
                "accuracy": 0.7879999999999999,
                "f1": 0.7435484848484848,
                "precision": 0.7243277777777777,
                "recall": 0.7879999999999999,
                "main_score": 0.7435484848484848
            },
            {
                "hf_subset": "swe-eng",
                "languages": [
                    "swe-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.139,
                "f1": 0.10718252991153887,
                "precision": 0.09835761434404196,
                "recall": 0.139,
                "main_score": 0.10718252991153887
            },
            {
                "hf_subset": "dtp-eng",
                "languages": [
                    "dtp-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.049,
                "f1": 0.03371714825002496,
                "precision": 0.030859282540034793,
                "recall": 0.049,
                "main_score": 0.03371714825002496
            },
            {
                "hf_subset": "kat-eng",
                "languages": [
                    "kat-Geor",
                    "eng-Latn"
                ],
                "accuracy": 0.005361930294906166,
                "f1": 0.004038970369202193,
                "precision": 0.004030266685480457,
                "recall": 0.005361930294906166,
                "main_score": 0.004038970369202193
            },
            {
                "hf_subset": "jpn-eng",
                "languages": [
                    "jpn-Jpan",
                    "eng-Latn"
                ],
                "accuracy": 0.553,
                "f1": 0.48833531135531133,
                "precision": 0.4648630659536542,
                "recall": 0.553,
                "main_score": 0.48833531135531133
            },
            {
                "hf_subset": "csb-eng",
                "languages": [
                    "csb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.08300395256916997,
                "f1": 0.05261552988548537,
                "precision": 0.04724388115499655,
                "recall": 0.08300395256916997,
                "main_score": 0.05261552988548537
            },
            {
                "hf_subset": "xho-eng",
                "languages": [
                    "xho-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.08450704225352113,
                "f1": 0.048299744704787866,
                "precision": 0.043375857984788165,
                "recall": 0.08450704225352113,
                "main_score": 0.048299744704787866
            },
            {
                "hf_subset": "orv-eng",
                "languages": [
                    "orv-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.010778443113772455,
                "f1": 0.005373251562068135,
                "precision": 0.005107640721914694,
                "recall": 0.010778443113772455,
                "main_score": 0.005373251562068135
            },
            {
                "hf_subset": "ind-eng",
                "languages": [
                    "ind-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.885,
                "f1": 0.8546333333333334,
                "precision": 0.841,
                "recall": 0.885,
                "main_score": 0.8546333333333334
            },
            {
                "hf_subset": "tuk-eng",
                "languages": [
                    "tuk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.054187192118226604,
                "f1": 0.028063639248802966,
                "precision": 0.022699550039451514,
                "recall": 0.054187192118226604,
                "main_score": 0.028063639248802966
            },
            {
                "hf_subset": "max-eng",
                "languages": [
                    "max-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.40492957746478875,
                "f1": 0.3345545495193382,
                "precision": 0.314339393461183,
                "recall": 0.40492957746478875,
                "main_score": 0.3345545495193382
            },
            {
                "hf_subset": "swh-eng",
                "languages": [
                    "swh-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.18974358974358974,
                "f1": 0.14517578026097205,
                "precision": 0.133510327465177,
                "recall": 0.18974358974358974,
                "main_score": 0.14517578026097205
            },
            {
                "hf_subset": "hin-eng",
                "languages": [
                    "hin-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.885,
                "f1": 0.8534666666666666,
                "precision": 0.839,
                "recall": 0.885,
                "main_score": 0.8534666666666666
            },
            {
                "hf_subset": "dsb-eng",
                "languages": [
                    "dsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.081419624217119,
                "f1": 0.058307830127637315,
                "precision": 0.05440871422311654,
                "recall": 0.081419624217119,
                "main_score": 0.058307830127637315
            },
            {
                "hf_subset": "ber-eng",
                "languages": [
                    "ber-Tfng",
                    "eng-Latn"
                ],
                "accuracy": 0.05800000000000001,
                "f1": 0.039245687335866405,
                "precision": 0.035535667824951586,
                "recall": 0.05800000000000001,
                "main_score": 0.039245687335866405
            },
            {
                "hf_subset": "tam-eng",
                "languages": [
                    "tam-Taml",
                    "eng-Latn"
                ],
                "accuracy": 0.6840390879478826,
                "f1": 0.6225738069386277,
                "precision": 0.6010935318752908,
                "recall": 0.6840390879478826,
                "main_score": 0.6225738069386277
            },
            {
                "hf_subset": "slk-eng",
                "languages": [
                    "slk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.071,
                "f1": 0.05487678783376213,
                "precision": 0.05126663482701374,
                "recall": 0.071,
                "main_score": 0.05487678783376213
            },
            {
                "hf_subset": "tgl-eng",
                "languages": [
                    "tgl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.08900000000000001,
                "f1": 0.06519531004112515,
                "precision": 0.05987707404636394,
                "recall": 0.08900000000000001,
                "main_score": 0.06519531004112515
            },
            {
                "hf_subset": "ast-eng",
                "languages": [
                    "ast-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.6692913385826772,
                "f1": 0.5996062992125983,
                "precision": 0.5713348331458566,
                "recall": 0.6692913385826772,
                "main_score": 0.5996062992125983
            },
            {
                "hf_subset": "mkd-eng",
                "languages": [
                    "mkd-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.043,
                "f1": 0.027658053436072014,
                "precision": 0.025247851243177143,
                "recall": 0.043,
                "main_score": 0.027658053436072014
            },
            {
                "hf_subset": "khm-eng",
                "languages": [
                    "khm-Khmr",
                    "eng-Latn"
                ],
                "accuracy": 0.004155124653739612,
                "f1": 0.001497838495760933,
                "precision": 0.0014429034844729552,
                "recall": 0.004155124653739612,
                "main_score": 0.001497838495760933
            },
            {
                "hf_subset": "ces-eng",
                "languages": [
                    "ces-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.05800000000000001,
                "f1": 0.03761224995516873,
                "precision": 0.032689210175496086,
                "recall": 0.05800000000000001,
                "main_score": 0.03761224995516873
            },
            {
                "hf_subset": "tzl-eng",
                "languages": [
                    "tzl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.16346153846153846,
                "f1": 0.14524291497975708,
                "precision": 0.13995726495726496,
                "recall": 0.16346153846153846,
                "main_score": 0.14524291497975708
            },
            {
                "hf_subset": "urd-eng",
                "languages": [
                    "urd-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.6780000000000002,
                "f1": 0.6161580086580086,
                "precision": 0.5912333333333334,
                "recall": 0.6780000000000002,
                "main_score": 0.6161580086580086
            },
            {
                "hf_subset": "ara-eng",
                "languages": [
                    "ara-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.838,
                "f1": 0.8008857142857143,
                "precision": 0.7846666666666667,
                "recall": 0.838,
                "main_score": 0.8008857142857143
            },
            {
                "hf_subset": "kor-eng",
                "languages": [
                    "kor-Hang",
                    "eng-Latn"
                ],
                "accuracy": 0.042,
                "f1": 0.026507751588440254,
                "precision": 0.02335273168189835,
                "recall": 0.042,
                "main_score": 0.026507751588440254
            },
            {
                "hf_subset": "yid-eng",
                "languages": [
                    "yid-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.0047169811320754715,
                "f1": 0.0019293763102725366,
                "precision": 0.001622040325564188,
                "recall": 0.0047169811320754715,
                "main_score": 0.0019293763102725366
            },
            {
                "hf_subset": "fin-eng",
                "languages": [
                    "fin-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.049,
                "f1": 0.035001791555125235,
                "precision": 0.03277940522301425,
                "recall": 0.049,
                "main_score": 0.035001791555125235
            },
            {
                "hf_subset": "tha-eng",
                "languages": [
                    "tha-Thai",
                    "eng-Latn"
                ],
                "accuracy": 0.009124087591240875,
                "f1": 0.005083420229405631,
                "precision": 0.004674562188049969,
                "recall": 0.009124087591240875,
                "main_score": 0.005083420229405631
            },
            {
                "hf_subset": "wuu-eng",
                "languages": [
                    "wuu-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.794,
                "f1": 0.7462333333333333,
                "precision": 0.7252333333333334,
                "recall": 0.794,
                "main_score": 0.7462333333333333
            }
        ]
    }
}