{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 0.4742098184263618,
                "f1": 0.45225418545577434,
                "main_score": 0.4742098184263618
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 0.24707464694014797,
                "f1": 0.2403350608188247,
                "main_score": 0.24707464694014797
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 0.6209145931405515,
                "f1": 0.6222048940230962,
                "main_score": 0.6209145931405515
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 0.3925016812373907,
                "f1": 0.3835431952425269,
                "main_score": 0.3925016812373907
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 0.6337256220578346,
                "f1": 0.6312728180326932,
                "main_score": 0.6337256220578346
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 0.3917283120376597,
                "f1": 0.37078841372640237,
                "main_score": 0.3917283120376597
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 0.4911230665770006,
                "f1": 0.46489580286547244,
                "main_score": 0.4911230665770006
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 0.507128446536651,
                "f1": 0.4827782602378952,
                "main_score": 0.507128446536651
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 0.39465366509751176,
                "f1": 0.374365280056047,
                "main_score": 0.39465366509751176
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 0.7326160053799597,
                "f1": 0.7344782499678171,
                "main_score": 0.7326160053799597
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 0.6831203765971756,
                "f1": 0.6870554437788069,
                "main_score": 0.6831203765971756
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 0.4565232010759919,
                "f1": 0.44553577452655213,
                "main_score": 0.4565232010759919
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 0.3894754539340955,
                "f1": 0.3648927336173062,
                "main_score": 0.3894754539340955
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 0.6869872225958304,
                "f1": 0.6881347966311543,
                "main_score": 0.6869872225958304
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 0.32131809011432416,
                "f1": 0.30212230946937474,
                "main_score": 0.32131809011432416
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 0.6557498318762609,
                "f1": 0.6516084751135228,
                "main_score": 0.6557498318762609
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 0.4296570275722932,
                "f1": 0.40575896627739105,
                "main_score": 0.4296570275722932
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 0.32125084061869535,
                "f1": 0.30708056882129475,
                "main_score": 0.32125084061869535
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 0.6510759919300607,
                "f1": 0.6450078001193149,
                "main_score": 0.6510759919300607
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 0.4083725622057834,
                "f1": 0.37855774705520884,
                "main_score": 0.4083725622057834
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 0.5455279085406859,
                "f1": 0.5273318944173822,
                "main_score": 0.5455279085406859
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 0.5714525891055817,
                "f1": 0.5596714177558203,
                "main_score": 0.5714525891055817
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 0.4930060524546065,
                "f1": 0.4782999154670342,
                "main_score": 0.4930060524546065
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 0.2585743106926698,
                "f1": 0.24974946990729716,
                "main_score": 0.2585743106926698
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 0.31180228648285135,
                "f1": 0.2822387838219335,
                "main_score": 0.31180228648285135
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 0.5300941492938802,
                "f1": 0.5239610045092559,
                "main_score": 0.5300941492938802
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 0.4024546065904505,
                "f1": 0.3899779773215032,
                "main_score": 0.4024546065904505
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 0.41882985877605916,
                "f1": 0.3953867071594289,
                "main_score": 0.41882985877605916
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 0.5907868190988568,
                "f1": 0.5847368723772022,
                "main_score": 0.5907868190988568
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 0.33338937457969064,
                "f1": 0.32113466354321224,
                "main_score": 0.33338937457969064
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 0.5745460659045057,
                "f1": 0.5613075383338251,
                "main_score": 0.5745460659045057
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 0.2719569603227976,
                "f1": 0.26300773160344015,
                "main_score": 0.2719569603227976
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 0.4678547410894418,
                "f1": 0.44233771335183014,
                "main_score": 0.4678547410894418
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 0.48419636852723597,
                "f1": 0.4555838648206857,
                "main_score": 0.48419636852723597
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 0.4163080026899798,
                "f1": 0.4077775839499525,
                "main_score": 0.4163080026899798
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 0.66408876933423,
                "f1": 0.6673586938710421,
                "main_score": 0.66408876933423
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 0.500773369199731,
                "f1": 0.48572749739090015,
                "main_score": 0.500773369199731
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 0.49942837928715533,
                "f1": 0.4934771836662566,
                "main_score": 0.49942837928715533
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 0.4343308675184936,
                "f1": 0.41818008297000986,
                "main_score": 0.4343308675184936
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 0.44082044384667113,
                "f1": 0.43250027464321295,
                "main_score": 0.44082044384667113
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 0.4645258910558171,
                "f1": 0.4400958237591922,
                "main_score": 0.4645258910558171
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 0.4953261600537996,
                "f1": 0.48019696996346717,
                "main_score": 0.4953261600537996
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 0.5679219905850706,
                "f1": 0.5654421925671813,
                "main_score": 0.5679219905850706
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 0.540114324142569,
                "f1": 0.5229830350891558,
                "main_score": 0.540114324142569
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 0.38584398117014124,
                "f1": 0.36551426239639573,
                "main_score": 0.38584398117014124
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 0.4807330195023538,
                "f1": 0.46463553675519975,
                "main_score": 0.4807330195023538
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 0.40645595158036313,
                "f1": 0.40212806766079856,
                "main_score": 0.40645595158036313
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 0.5774714189643577,
                "f1": 0.568673027258351,
                "main_score": 0.5774714189643577
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 0.6583389374579692,
                "f1": 0.6611273939782248,
                "main_score": 0.6583389374579692
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 0.7238735709482181,
                "f1": 0.7289481650271512,
                "main_score": 0.7238735709482181
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 0.6963685272360458,
                "f1": 0.7072285841806938,
                "main_score": 0.6963685272360458
            }
        ]
    }
}