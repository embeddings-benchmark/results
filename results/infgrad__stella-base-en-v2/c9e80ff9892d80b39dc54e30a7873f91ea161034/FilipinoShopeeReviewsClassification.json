{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.291406,
        "f1": 0.285772,
        "f1_weighted": 0.285763,
        "scores_per_experiment": [
          {
            "accuracy": 0.297363,
            "f1": 0.295253,
            "f1_weighted": 0.295243
          },
          {
            "accuracy": 0.283203,
            "f1": 0.276643,
            "f1_weighted": 0.27664
          },
          {
            "accuracy": 0.28418,
            "f1": 0.281631,
            "f1_weighted": 0.281613
          },
          {
            "accuracy": 0.312012,
            "f1": 0.310868,
            "f1_weighted": 0.310866
          },
          {
            "accuracy": 0.268066,
            "f1": 0.264452,
            "f1_weighted": 0.264427
          },
          {
            "accuracy": 0.311035,
            "f1": 0.299659,
            "f1_weighted": 0.299649
          },
          {
            "accuracy": 0.238281,
            "f1": 0.238318,
            "f1_weighted": 0.238291
          },
          {
            "accuracy": 0.312988,
            "f1": 0.303348,
            "f1_weighted": 0.303334
          },
          {
            "accuracy": 0.317871,
            "f1": 0.30679,
            "f1_weighted": 0.306797
          },
          {
            "accuracy": 0.289062,
            "f1": 0.280757,
            "f1_weighted": 0.280769
          }
        ],
        "main_score": 0.291406,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.296729,
        "f1": 0.291077,
        "f1_weighted": 0.291076,
        "scores_per_experiment": [
          {
            "accuracy": 0.281738,
            "f1": 0.2819,
            "f1_weighted": 0.281895
          },
          {
            "accuracy": 0.310547,
            "f1": 0.30333,
            "f1_weighted": 0.303315
          },
          {
            "accuracy": 0.299316,
            "f1": 0.297416,
            "f1_weighted": 0.297423
          },
          {
            "accuracy": 0.303223,
            "f1": 0.304097,
            "f1_weighted": 0.304091
          },
          {
            "accuracy": 0.291504,
            "f1": 0.287273,
            "f1_weighted": 0.287244
          },
          {
            "accuracy": 0.291016,
            "f1": 0.274769,
            "f1_weighted": 0.274775
          },
          {
            "accuracy": 0.268066,
            "f1": 0.268514,
            "f1_weighted": 0.268491
          },
          {
            "accuracy": 0.323242,
            "f1": 0.315944,
            "f1_weighted": 0.315962
          },
          {
            "accuracy": 0.325195,
            "f1": 0.312731,
            "f1_weighted": 0.312744
          },
          {
            "accuracy": 0.273438,
            "f1": 0.264796,
            "f1_weighted": 0.26482
          }
        ],
        "main_score": 0.296729,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 14.635732650756836,
  "kg_co2_emissions": 0.0005191286297472524
}