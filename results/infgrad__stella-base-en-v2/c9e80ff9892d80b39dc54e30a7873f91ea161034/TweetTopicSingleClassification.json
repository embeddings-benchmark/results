{
  "dataset_revision": "87b7a0d1c402dbb481db649569c556d9aa27ac05",
  "task_name": "TweetTopicSingleClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test_2021": [
      {
        "accuracy": 0.726816,
        "f1": 0.569192,
        "f1_weighted": 0.749711,
        "scores_per_experiment": [
          {
            "accuracy": 0.702894,
            "f1": 0.528628,
            "f1_weighted": 0.728929
          },
          {
            "accuracy": 0.721205,
            "f1": 0.597737,
            "f1_weighted": 0.745878
          },
          {
            "accuracy": 0.731837,
            "f1": 0.583699,
            "f1_weighted": 0.753155
          },
          {
            "accuracy": 0.695216,
            "f1": 0.559926,
            "f1_weighted": 0.732748
          },
          {
            "accuracy": 0.748376,
            "f1": 0.585701,
            "f1_weighted": 0.765976
          },
          {
            "accuracy": 0.739516,
            "f1": 0.577974,
            "f1_weighted": 0.758534
          },
          {
            "accuracy": 0.720024,
            "f1": 0.542882,
            "f1_weighted": 0.744528
          },
          {
            "accuracy": 0.744241,
            "f1": 0.584587,
            "f1_weighted": 0.769261
          },
          {
            "accuracy": 0.709982,
            "f1": 0.534202,
            "f1_weighted": 0.72495
          },
          {
            "accuracy": 0.754873,
            "f1": 0.596582,
            "f1_weighted": 0.773155
          }
        ],
        "main_score": 0.726816,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 8.039294719696045,
  "kg_co2_emissions": 0.00028313787459883355
}