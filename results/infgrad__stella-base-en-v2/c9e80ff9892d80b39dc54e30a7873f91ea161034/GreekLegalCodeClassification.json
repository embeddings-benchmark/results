{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "task_name": "GreekLegalCodeClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.040332,
        "f1": 0.018685,
        "f1_weighted": 0.029262,
        "scores_per_experiment": [
          {
            "accuracy": 0.038086,
            "f1": 0.015737,
            "f1_weighted": 0.027047
          },
          {
            "accuracy": 0.035645,
            "f1": 0.015553,
            "f1_weighted": 0.02293
          },
          {
            "accuracy": 0.035156,
            "f1": 0.021236,
            "f1_weighted": 0.025012
          },
          {
            "accuracy": 0.035645,
            "f1": 0.018102,
            "f1_weighted": 0.02607
          },
          {
            "accuracy": 0.041992,
            "f1": 0.019306,
            "f1_weighted": 0.030887
          },
          {
            "accuracy": 0.044434,
            "f1": 0.018124,
            "f1_weighted": 0.031773
          },
          {
            "accuracy": 0.050293,
            "f1": 0.020802,
            "f1_weighted": 0.035101
          },
          {
            "accuracy": 0.037109,
            "f1": 0.016605,
            "f1_weighted": 0.025542
          },
          {
            "accuracy": 0.04248,
            "f1": 0.018637,
            "f1_weighted": 0.032355
          },
          {
            "accuracy": 0.04248,
            "f1": 0.022752,
            "f1_weighted": 0.0359
          }
        ],
        "main_score": 0.040332,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.036914,
        "f1": 0.018061,
        "f1_weighted": 0.026391,
        "scores_per_experiment": [
          {
            "accuracy": 0.030273,
            "f1": 0.018539,
            "f1_weighted": 0.019479
          },
          {
            "accuracy": 0.039062,
            "f1": 0.021055,
            "f1_weighted": 0.02804
          },
          {
            "accuracy": 0.038086,
            "f1": 0.01821,
            "f1_weighted": 0.027848
          },
          {
            "accuracy": 0.028809,
            "f1": 0.016407,
            "f1_weighted": 0.020327
          },
          {
            "accuracy": 0.035156,
            "f1": 0.015476,
            "f1_weighted": 0.024801
          },
          {
            "accuracy": 0.043457,
            "f1": 0.019319,
            "f1_weighted": 0.031401
          },
          {
            "accuracy": 0.043945,
            "f1": 0.017225,
            "f1_weighted": 0.032477
          },
          {
            "accuracy": 0.03418,
            "f1": 0.016702,
            "f1_weighted": 0.024569
          },
          {
            "accuracy": 0.039551,
            "f1": 0.018874,
            "f1_weighted": 0.026923
          },
          {
            "accuracy": 0.036621,
            "f1": 0.018803,
            "f1_weighted": 0.02804
          }
        ],
        "main_score": 0.036914,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ]
  },
  "evaluation_time": 329.4223585128784,
  "kg_co2_emissions": 0.01621222001774126
}