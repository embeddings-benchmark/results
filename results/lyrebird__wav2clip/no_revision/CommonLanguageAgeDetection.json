{
  "dataset_revision": "a9c585af68d65a29c4ad12121f83853fa1cdda92",
  "task_name": "CommonLanguageAgeDetection",
  "mteb_version": "2.4.2",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.1195,
            "f1": 0.063787,
            "f1_weighted": 0.156407,
            "precision": 0.098596,
            "precision_weighted": 0.279428,
            "recall": 0.075693,
            "recall_weighted": 0.1195,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.167,
            "f1": 0.093821,
            "f1_weighted": 0.204933,
            "precision": 0.119427,
            "precision_weighted": 0.300768,
            "recall": 0.102854,
            "recall_weighted": 0.167,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.1305,
            "f1": 0.075394,
            "f1_weighted": 0.155819,
            "precision": 0.092725,
            "precision_weighted": 0.230993,
            "recall": 0.091988,
            "recall_weighted": 0.1305,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.1005,
            "f1": 0.080868,
            "f1_weighted": 0.131157,
            "precision": 0.133142,
            "precision_weighted": 0.307312,
            "recall": 0.091503,
            "recall_weighted": 0.1005,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.1945,
            "f1": 0.091737,
            "f1_weighted": 0.236838,
            "precision": 0.113523,
            "precision_weighted": 0.31403,
            "recall": 0.0857,
            "recall_weighted": 0.1945,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.0925,
            "f1": 0.05812,
            "f1_weighted": 0.116466,
            "precision": 0.092037,
            "precision_weighted": 0.24454,
            "recall": 0.082842,
            "recall_weighted": 0.0925,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.0815,
            "f1": 0.045065,
            "f1_weighted": 0.100844,
            "precision": 0.07088,
            "precision_weighted": 0.197319,
            "recall": 0.055351,
            "recall_weighted": 0.0815,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.1625,
            "f1": 0.077969,
            "f1_weighted": 0.186555,
            "precision": 0.088404,
            "precision_weighted": 0.233365,
            "recall": 0.07783,
            "recall_weighted": 0.1625,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.1255,
            "f1": 0.0717,
            "f1_weighted": 0.14456,
            "precision": 0.085298,
            "precision_weighted": 0.209517,
            "recall": 0.088477,
            "recall_weighted": 0.1255,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.1365,
            "f1": 0.077904,
            "f1_weighted": 0.175253,
            "precision": 0.108275,
            "precision_weighted": 0.2902,
            "recall": 0.084439,
            "recall_weighted": 0.1365,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.13105,
        "f1": 0.073636,
        "f1_weighted": 0.160883,
        "precision": 0.100231,
        "precision_weighted": 0.260747,
        "recall": 0.083668,
        "recall_weighted": 0.13105,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.13105,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 226.70709896087646,
  "kg_co2_emissions": null
}