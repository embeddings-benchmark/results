{
  "dataset_revision": "445bd662ffd3883ac662e2f1df18724c10688304",
  "task_name": "CommonLanguageLanguageDetection",
  "mteb_version": "2.4.2",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.0815,
            "f1": 0.061776,
            "f1_weighted": 0.069363,
            "precision": 0.068729,
            "precision_weighted": 0.075642,
            "recall": 0.084187,
            "recall_weighted": 0.0815,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.0845,
            "f1": 0.052344,
            "f1_weighted": 0.07034,
            "precision": 0.066827,
            "precision_weighted": 0.08418,
            "recall": 0.066414,
            "recall_weighted": 0.0845,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.0495,
            "f1": 0.037003,
            "f1_weighted": 0.040301,
            "precision": 0.043876,
            "precision_weighted": 0.050537,
            "recall": 0.05271,
            "recall_weighted": 0.0495,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.094,
            "f1": 0.047284,
            "f1_weighted": 0.074165,
            "precision": 0.045962,
            "precision_weighted": 0.069287,
            "recall": 0.058344,
            "recall_weighted": 0.094,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.0735,
            "f1": 0.040623,
            "f1_weighted": 0.059094,
            "precision": 0.039487,
            "precision_weighted": 0.06098,
            "recall": 0.054711,
            "recall_weighted": 0.0735,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.075,
            "f1": 0.048608,
            "f1_weighted": 0.061308,
            "precision": 0.047701,
            "precision_weighted": 0.059175,
            "recall": 0.060108,
            "recall_weighted": 0.075,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.0795,
            "f1": 0.054066,
            "f1_weighted": 0.066101,
            "precision": 0.055575,
            "precision_weighted": 0.069542,
            "recall": 0.074825,
            "recall_weighted": 0.0795,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.05,
            "f1": 0.041757,
            "f1_weighted": 0.051649,
            "precision": 0.05025,
            "precision_weighted": 0.071541,
            "recall": 0.052507,
            "recall_weighted": 0.05,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.038,
            "f1": 0.036824,
            "f1_weighted": 0.033176,
            "precision": 0.063562,
            "precision_weighted": 0.063947,
            "recall": 0.045993,
            "recall_weighted": 0.038,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.0335,
            "f1": 0.030847,
            "f1_weighted": 0.029352,
            "precision": 0.029766,
            "precision_weighted": 0.03464,
            "recall": 0.041468,
            "recall_weighted": 0.0335,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.0659,
        "f1": 0.045113,
        "f1_weighted": 0.055485,
        "precision": 0.051173,
        "precision_weighted": 0.063947,
        "recall": 0.059127,
        "recall_weighted": 0.0659,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.0659,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 188.67694425582886,
  "kg_co2_emissions": null
}