{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 2040.9782483577728,
  "kg_co2_emissions": 0.04626674092603899,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.7298828125,
        "ap": 0.6673086606017231,
        "ap_weighted": 0.6673086606017231,
        "f1": 0.7281481664616588,
        "f1_weighted": 0.728144034377426,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7298828125,
        "scores_per_experiment": [
          {
            "accuracy": 0.741058349609375,
            "ap": 0.6821510527522547,
            "ap_weighted": 0.6821510527522547,
            "f1": 0.7408212959012948,
            "f1_weighted": 0.7408246447932354
          },
          {
            "accuracy": 0.681365966796875,
            "ap": 0.6161907494589958,
            "ap_weighted": 0.6161907494589958,
            "f1": 0.6749348868467224,
            "f1_weighted": 0.6749153522146325
          },
          {
            "accuracy": 0.757232666015625,
            "ap": 0.6993580495841614,
            "ap_weighted": 0.6993580495841614,
            "f1": 0.7569542775711543,
            "f1_weighted": 0.7569577919428608
          },
          {
            "accuracy": 0.739349365234375,
            "ap": 0.6779838330934196,
            "ap_weighted": 0.6779838330934196,
            "f1": 0.7393201326931431,
            "f1_weighted": 0.7393213121040285
          },
          {
            "accuracy": 0.73040771484375,
            "ap": 0.6608596181197847,
            "ap_weighted": 0.6608596181197847,
            "f1": 0.7287102392959794,
            "f1_weighted": 0.7287010708323727
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}