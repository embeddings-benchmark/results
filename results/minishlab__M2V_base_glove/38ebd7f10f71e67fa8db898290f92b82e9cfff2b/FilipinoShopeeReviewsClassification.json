{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.2646,
        "f1": 0.259794,
        "f1_weighted": 0.25978,
        "scores_per_experiment": [
          {
            "accuracy": 0.240234,
            "f1": 0.237256,
            "f1_weighted": 0.237237
          },
          {
            "accuracy": 0.259277,
            "f1": 0.25402,
            "f1_weighted": 0.254008
          },
          {
            "accuracy": 0.259766,
            "f1": 0.254131,
            "f1_weighted": 0.254136
          },
          {
            "accuracy": 0.270996,
            "f1": 0.270568,
            "f1_weighted": 0.270547
          },
          {
            "accuracy": 0.240723,
            "f1": 0.241801,
            "f1_weighted": 0.241792
          },
          {
            "accuracy": 0.265625,
            "f1": 0.249775,
            "f1_weighted": 0.24976
          },
          {
            "accuracy": 0.260742,
            "f1": 0.259376,
            "f1_weighted": 0.259349
          },
          {
            "accuracy": 0.317871,
            "f1": 0.310245,
            "f1_weighted": 0.310197
          },
          {
            "accuracy": 0.262207,
            "f1": 0.263259,
            "f1_weighted": 0.263256
          },
          {
            "accuracy": 0.268555,
            "f1": 0.257505,
            "f1_weighted": 0.257513
          }
        ],
        "main_score": 0.2646,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.269531,
        "f1": 0.264073,
        "f1_weighted": 0.26406,
        "scores_per_experiment": [
          {
            "accuracy": 0.261719,
            "f1": 0.256837,
            "f1_weighted": 0.256794
          },
          {
            "accuracy": 0.265625,
            "f1": 0.25967,
            "f1_weighted": 0.259651
          },
          {
            "accuracy": 0.260742,
            "f1": 0.256008,
            "f1_weighted": 0.256
          },
          {
            "accuracy": 0.260254,
            "f1": 0.260625,
            "f1_weighted": 0.26062
          },
          {
            "accuracy": 0.262207,
            "f1": 0.263726,
            "f1_weighted": 0.263718
          },
          {
            "accuracy": 0.26123,
            "f1": 0.244694,
            "f1_weighted": 0.244696
          },
          {
            "accuracy": 0.260742,
            "f1": 0.256757,
            "f1_weighted": 0.256741
          },
          {
            "accuracy": 0.324219,
            "f1": 0.317973,
            "f1_weighted": 0.317941
          },
          {
            "accuracy": 0.272461,
            "f1": 0.273703,
            "f1_weighted": 0.273678
          },
          {
            "accuracy": 0.266113,
            "f1": 0.250738,
            "f1_weighted": 0.25076
          }
        ],
        "main_score": 0.269531,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 13.645562648773193,
  "kg_co2_emissions": 0.0003122733498568058
}