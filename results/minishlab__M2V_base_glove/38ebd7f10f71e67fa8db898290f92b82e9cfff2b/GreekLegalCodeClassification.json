{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "task_name": "GreekLegalCodeClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.046094,
        "f1": 0.038065,
        "f1_weighted": 0.048857,
        "scores_per_experiment": [
          {
            "accuracy": 0.041504,
            "f1": 0.036052,
            "f1_weighted": 0.04623
          },
          {
            "accuracy": 0.050781,
            "f1": 0.040087,
            "f1_weighted": 0.05552
          },
          {
            "accuracy": 0.046387,
            "f1": 0.038079,
            "f1_weighted": 0.049363
          },
          {
            "accuracy": 0.048828,
            "f1": 0.040736,
            "f1_weighted": 0.048645
          },
          {
            "accuracy": 0.046387,
            "f1": 0.036175,
            "f1_weighted": 0.050154
          },
          {
            "accuracy": 0.046875,
            "f1": 0.039433,
            "f1_weighted": 0.049118
          },
          {
            "accuracy": 0.049316,
            "f1": 0.04046,
            "f1_weighted": 0.052589
          },
          {
            "accuracy": 0.04834,
            "f1": 0.041367,
            "f1_weighted": 0.048136
          },
          {
            "accuracy": 0.041016,
            "f1": 0.033427,
            "f1_weighted": 0.043575
          },
          {
            "accuracy": 0.041504,
            "f1": 0.034837,
            "f1_weighted": 0.045246
          }
        ],
        "main_score": 0.046094,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.048682,
        "f1": 0.042559,
        "f1_weighted": 0.049498,
        "scores_per_experiment": [
          {
            "accuracy": 0.041016,
            "f1": 0.034385,
            "f1_weighted": 0.041832
          },
          {
            "accuracy": 0.054199,
            "f1": 0.051408,
            "f1_weighted": 0.055621
          },
          {
            "accuracy": 0.05127,
            "f1": 0.048423,
            "f1_weighted": 0.050984
          },
          {
            "accuracy": 0.04834,
            "f1": 0.04107,
            "f1_weighted": 0.049546
          },
          {
            "accuracy": 0.056152,
            "f1": 0.048262,
            "f1_weighted": 0.058799
          },
          {
            "accuracy": 0.050293,
            "f1": 0.043357,
            "f1_weighted": 0.049773
          },
          {
            "accuracy": 0.045898,
            "f1": 0.037718,
            "f1_weighted": 0.046762
          },
          {
            "accuracy": 0.045898,
            "f1": 0.040768,
            "f1_weighted": 0.045227
          },
          {
            "accuracy": 0.044434,
            "f1": 0.038421,
            "f1_weighted": 0.048686
          },
          {
            "accuracy": 0.049316,
            "f1": 0.041781,
            "f1_weighted": 0.047754
          }
        ],
        "main_score": 0.048682,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ]
  },
  "evaluation_time": 440.5828523635864,
  "kg_co2_emissions": 0.010121517608079892
}