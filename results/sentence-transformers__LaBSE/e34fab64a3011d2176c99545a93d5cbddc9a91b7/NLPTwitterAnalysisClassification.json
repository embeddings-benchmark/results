{
  "dataset_revision": "4ceb1312583fd2c7c73ad2d550b726124dcd39a0",
  "task_name": "NLPTwitterAnalysisClassification",
  "mteb_version": "1.25.8",
  "scores": {
    "test": [
      {
        "accuracy": 0.749338,
        "f1": 0.771719,
        "f1_weighted": 0.756408,
        "scores_per_experiment": [
          {
            "accuracy": 0.743382,
            "f1": 0.765533,
            "f1_weighted": 0.750715
          },
          {
            "accuracy": 0.744853,
            "f1": 0.766228,
            "f1_weighted": 0.751023
          },
          {
            "accuracy": 0.745588,
            "f1": 0.76834,
            "f1_weighted": 0.751461
          },
          {
            "accuracy": 0.758088,
            "f1": 0.77948,
            "f1_weighted": 0.765457
          },
          {
            "accuracy": 0.741176,
            "f1": 0.765761,
            "f1_weighted": 0.748785
          },
          {
            "accuracy": 0.760294,
            "f1": 0.780374,
            "f1_weighted": 0.766526
          },
          {
            "accuracy": 0.751471,
            "f1": 0.771124,
            "f1_weighted": 0.756878
          },
          {
            "accuracy": 0.751471,
            "f1": 0.775092,
            "f1_weighted": 0.761144
          },
          {
            "accuracy": 0.748529,
            "f1": 0.773394,
            "f1_weighted": 0.755204
          },
          {
            "accuracy": 0.748529,
            "f1": 0.771869,
            "f1_weighted": 0.756885
          }
        ],
        "main_score": 0.749338,
        "hf_subset": "default",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 9.063534498214722,
  "kg_co2_emissions": null
}