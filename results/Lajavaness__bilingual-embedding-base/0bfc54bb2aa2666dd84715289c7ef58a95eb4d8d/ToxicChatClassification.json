{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.769759,
        "f1": 0.648125,
        "f1_weighted": 0.801582,
        "ap": 0.269084,
        "ap_weighted": 0.269084,
        "scores_per_experiment": [
          {
            "accuracy": 0.599656,
            "f1": 0.523244,
            "f1_weighted": 0.66623,
            "ap": 0.192738,
            "ap_weighted": 0.192738
          },
          {
            "accuracy": 0.790378,
            "f1": 0.655701,
            "f1_weighted": 0.817017,
            "ap": 0.260622,
            "ap_weighted": 0.260622
          },
          {
            "accuracy": 0.793814,
            "f1": 0.683104,
            "f1_weighted": 0.823423,
            "ap": 0.313055,
            "ap_weighted": 0.313055
          },
          {
            "accuracy": 0.737973,
            "f1": 0.633719,
            "f1_weighted": 0.780111,
            "ap": 0.267511,
            "ap_weighted": 0.267511
          },
          {
            "accuracy": 0.728522,
            "f1": 0.577447,
            "f1_weighted": 0.766725,
            "ap": 0.183,
            "ap_weighted": 0.183
          },
          {
            "accuracy": 0.813574,
            "f1": 0.68063,
            "f1_weighted": 0.834994,
            "ap": 0.287607,
            "ap_weighted": 0.287607
          },
          {
            "accuracy": 0.839347,
            "f1": 0.68923,
            "f1_weighted": 0.851037,
            "ap": 0.284503,
            "ap_weighted": 0.284503
          },
          {
            "accuracy": 0.7689,
            "f1": 0.648632,
            "f1_weighted": 0.802632,
            "ap": 0.266199,
            "ap_weighted": 0.266199
          },
          {
            "accuracy": 0.82732,
            "f1": 0.705296,
            "f1_weighted": 0.847358,
            "ap": 0.325021,
            "ap_weighted": 0.325021
          },
          {
            "accuracy": 0.79811,
            "f1": 0.684242,
            "f1_weighted": 0.826292,
            "ap": 0.310589,
            "ap_weighted": 0.310589
          }
        ],
        "main_score": 0.769759,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 8.13745403289795,
  "kg_co2_emissions": 0.00027026578985635247
}