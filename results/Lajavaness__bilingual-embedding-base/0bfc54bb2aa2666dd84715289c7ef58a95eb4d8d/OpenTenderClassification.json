{
  "dataset_revision": "9af5657575a669dc18c7f897a67287ff7d1a0c65",
  "task_name": "OpenTenderClassification",
  "mteb_version": "2.1.8",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.479153,
            "f1": 0.459074,
            "f1_weighted": 0.459077,
            "precision": 0.473181,
            "precision_weighted": 0.47314,
            "recall": 0.479139,
            "recall_weighted": 0.479153,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.470903,
            "f1": 0.450867,
            "f1_weighted": 0.450774,
            "precision": 0.466726,
            "precision_weighted": 0.466708,
            "recall": 0.471082,
            "recall_weighted": 0.470903,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.487625,
            "f1": 0.461513,
            "f1_weighted": 0.461436,
            "precision": 0.468747,
            "precision_weighted": 0.468708,
            "recall": 0.487731,
            "recall_weighted": 0.487625,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.480936,
            "f1": 0.461619,
            "f1_weighted": 0.461562,
            "precision": 0.467255,
            "precision_weighted": 0.467155,
            "recall": 0.480972,
            "recall_weighted": 0.480936,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.458417,
            "f1": 0.438664,
            "f1_weighted": 0.438611,
            "precision": 0.455034,
            "precision_weighted": 0.455049,
            "recall": 0.458495,
            "recall_weighted": 0.458417,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.474916,
            "f1": 0.45641,
            "f1_weighted": 0.456301,
            "precision": 0.466766,
            "precision_weighted": 0.46673,
            "recall": 0.475103,
            "recall_weighted": 0.474916,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.467559,
            "f1": 0.446054,
            "f1_weighted": 0.445966,
            "precision": 0.4626,
            "precision_weighted": 0.462504,
            "recall": 0.467588,
            "recall_weighted": 0.467559,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.472018,
            "f1": 0.448716,
            "f1_weighted": 0.448656,
            "precision": 0.461295,
            "precision_weighted": 0.461239,
            "recall": 0.472095,
            "recall_weighted": 0.472018,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.442586,
            "f1": 0.41831,
            "f1_weighted": 0.418138,
            "precision": 0.429607,
            "precision_weighted": 0.429393,
            "recall": 0.442694,
            "recall_weighted": 0.442586,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.461761,
            "f1": 0.438227,
            "f1_weighted": 0.438122,
            "precision": 0.446079,
            "precision_weighted": 0.446007,
            "recall": 0.461892,
            "recall_weighted": 0.461761,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.469588,
        "f1": 0.447945,
        "f1_weighted": 0.447864,
        "precision": 0.459729,
        "precision_weighted": 0.459663,
        "recall": 0.469679,
        "recall_weighted": 0.469588,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.447945,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 145.16642808914185,
  "kg_co2_emissions": null
}