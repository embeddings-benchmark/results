{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "2.7.26",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.687626092804304,
            "f1": 0.6668741404039565,
            "f1_weighted": 0.6735867127387486,
            "precision": 0.6569902492065306,
            "precision_weighted": 0.7509033476763121,
            "recall": 0.7724354841756429,
            "recall_weighted": 0.687626092804304,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.7131809011432414,
            "f1": 0.6907502304931056,
            "f1_weighted": 0.7013848652349156,
            "precision": 0.6765778943435573,
            "precision_weighted": 0.7696597960116767,
            "recall": 0.790462519636977,
            "recall_weighted": 0.7131809011432414,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.6657700067249496,
            "f1": 0.6536344374304908,
            "f1_weighted": 0.649926477194968,
            "precision": 0.6366432721833728,
            "precision_weighted": 0.719198975897045,
            "recall": 0.7732445724579734,
            "recall_weighted": 0.6657700067249496,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.7024209818426361,
            "f1": 0.6816537014368961,
            "f1_weighted": 0.6912928324529997,
            "precision": 0.6659598976289206,
            "precision_weighted": 0.7614964375743727,
            "recall": 0.7746850875293079,
            "recall_weighted": 0.7024209818426361,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.6708137188971083,
            "f1": 0.6628848738292493,
            "f1_weighted": 0.6541632397663729,
            "precision": 0.656237736565662,
            "precision_weighted": 0.7490290068097271,
            "recall": 0.7680500447837851,
            "recall_weighted": 0.6708137188971083,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.6355077336919973,
            "f1": 0.6425730246418457,
            "f1_weighted": 0.6155591480442221,
            "precision": 0.6332929629837467,
            "precision_weighted": 0.7099738659066585,
            "recall": 0.757031132456155,
            "recall_weighted": 0.6355077336919973,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.6694687289845326,
            "f1": 0.6600872384655679,
            "f1_weighted": 0.6542596059890996,
            "precision": 0.6530459126954117,
            "precision_weighted": 0.7269561244491297,
            "recall": 0.7675818517887493,
            "recall_weighted": 0.6694687289845326,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.6684599865501009,
            "f1": 0.6506763849161457,
            "f1_weighted": 0.651422252803103,
            "precision": 0.6443558852998378,
            "precision_weighted": 0.7404599063013284,
            "recall": 0.7646938980062469,
            "recall_weighted": 0.6684599865501009,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.6570275722932079,
            "f1": 0.6591588322565117,
            "f1_weighted": 0.6228055521161865,
            "precision": 0.6600873464009993,
            "precision_weighted": 0.7325507969345693,
            "recall": 0.7626833323322619,
            "recall_weighted": 0.6570275722932079,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.6859448554135844,
            "f1": 0.6775454438316922,
            "f1_weighted": 0.6788626609233184,
            "precision": 0.6661566823196613,
            "precision_weighted": 0.7437839564462015,
            "recall": 0.7772679946511888,
            "recall_weighted": 0.6859448554135844,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.6756220578345663,
        "f1": 0.6645838307705462,
        "f1_weighted": 0.6593263347263935,
        "precision": 0.6549347839627699,
        "precision_weighted": 0.7404012214007021,
        "recall": 0.7708135917818288,
        "recall_weighted": 0.6756220578345663,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.6756220578345663,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 22.0889151096344,
  "kg_co2_emissions": null
}
