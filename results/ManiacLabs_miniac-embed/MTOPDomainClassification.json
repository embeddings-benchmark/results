{
  "dataset_revision": "a76d16fae880597b9c73047b50159220a441cb54",
  "task_name": "MTOPDomainClassification",
  "mteb_version": "2.7.26",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.9186046511627907,
            "f1": 0.915837910414562,
            "f1_weighted": 0.9183100203286777,
            "precision": 0.9125487359821632,
            "precision_weighted": 0.9213576286481093,
            "recall": 0.9222033651451164,
            "recall_weighted": 0.9186046511627907,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.9129046967624259,
            "f1": 0.9104279387811464,
            "f1_weighted": 0.9123180045731375,
            "precision": 0.9062500138578126,
            "precision_weighted": 0.914128048836406,
            "recall": 0.9167062437559665,
            "recall_weighted": 0.9129046967624259,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.9101687186502508,
            "f1": 0.9091833042519132,
            "f1_weighted": 0.9102523632201934,
            "precision": 0.903563494173078,
            "precision_weighted": 0.9135676577828281,
            "recall": 0.9177986656880439,
            "recall_weighted": 0.9101687186502508,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.9129046967624259,
            "f1": 0.911429147650129,
            "f1_weighted": 0.9133252581056288,
            "precision": 0.9065706667181866,
            "precision_weighted": 0.9200718950932154,
            "recall": 0.9219407153101883,
            "recall_weighted": 0.9129046967624259,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.9035567715458276,
            "f1": 0.9019378587992897,
            "f1_weighted": 0.9043033932510367,
            "precision": 0.8975249854222767,
            "precision_weighted": 0.9113748647725171,
            "recall": 0.9125007939877734,
            "recall_weighted": 0.9035567715458276,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.9122207022343821,
            "f1": 0.909097087755221,
            "f1_weighted": 0.9119627937850725,
            "precision": 0.9033623419709772,
            "precision_weighted": 0.9142619369745255,
            "recall": 0.9170527218227694,
            "recall_weighted": 0.9122207022343821,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.8962608299133606,
            "f1": 0.8944394545126838,
            "f1_weighted": 0.8957238523081785,
            "precision": 0.8875009858044218,
            "precision_weighted": 0.9033977018418533,
            "recall": 0.9083490453071872,
            "recall_weighted": 0.8962608299133606,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.9224806201550387,
            "f1": 0.9173724450594576,
            "f1_weighted": 0.9229906165363093,
            "precision": 0.9111719810729881,
            "precision_weighted": 0.9260308757842199,
            "recall": 0.9268578589833442,
            "recall_weighted": 0.9224806201550387,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.9122207022343821,
            "f1": 0.9100248571335778,
            "f1_weighted": 0.9121324395726255,
            "precision": 0.903311302421844,
            "precision_weighted": 0.9185743063910978,
            "recall": 0.9230917654146793,
            "recall_weighted": 0.9122207022343821,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.9165526675786594,
            "f1": 0.9126928954457602,
            "f1_weighted": 0.9170403420610299,
            "precision": 0.9071088838645774,
            "precision_weighted": 0.9200966602039798,
            "recall": 0.9210137480308731,
            "recall_weighted": 0.9165526675786594,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.9117875056999545,
        "f1": 0.9092442899803741,
        "f1_weighted": 0.9118359083741889,
        "precision": 0.9038913391288326,
        "precision_weighted": 0.9162861576328754,
        "recall": 0.9187514923445942,
        "recall_weighted": 0.9117875056999545,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.9117875056999545,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 31.03924798965454,
  "kg_co2_emissions": null
}
