{
  "dataset_revision": "edfaf9da55d3dd50d43143d90c1ac476895ae6de",
  "task_name": "ToxicConversationsClassification",
  "mteb_version": "2.7.26",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.693359375,
            "f1": 0.5322262229496272,
            "f1_weighted": 0.76333566898527,
            "precision": 0.5602502100804309,
            "precision_weighted": 0.8977210929059268,
            "recall": 0.6839807286961759,
            "recall_weighted": 0.693359375,
            "ap": 0.13310040650268934,
            "ap_weighted": 0.13310040650268934
          },
          {
            "accuracy": 0.751953125,
            "f1": 0.5603515889114267,
            "f1_weighted": 0.804671890846856,
            "precision": 0.5655579139022061,
            "precision_weighted": 0.8934613260783252,
            "recall": 0.6734744642132412,
            "recall_weighted": 0.751953125,
            "ap": 0.13534396746543675,
            "ap_weighted": 0.13534396746543675
          },
          {
            "accuracy": 0.73828125,
            "f1": 0.5607506702412869,
            "f1_weighted": 0.7958218833780161,
            "precision": 0.5704300471592185,
            "precision_weighted": 0.8992658127245677,
            "recall": 0.6970857389733318,
            "recall_weighted": 0.73828125,
            "ap": 0.14436551679033485,
            "ap_weighted": 0.14436551679033485
          },
          {
            "accuracy": 0.70556640625,
            "f1": 0.5347248516797571,
            "f1_weighted": 0.7720583247491356,
            "precision": 0.5576870161945333,
            "precision_weighted": 0.8940248373760961,
            "recall": 0.6708593535210714,
            "recall_weighted": 0.70556640625,
            "ap": 0.1288662117786391,
            "ap_weighted": 0.1288662117786391
          },
          {
            "accuracy": 0.541015625,
            "f1": 0.44275399571135476,
            "f1_weighted": 0.6397342851225459,
            "precision": 0.5418378518594813,
            "precision_weighted": 0.896217628295911,
            "recall": 0.6435856146001073,
            "recall_weighted": 0.541015625,
            "ap": 0.11106305031375592,
            "ap_weighted": 0.11106305031375592
          },
          {
            "accuracy": 0.45849609375,
            "f1": 0.39337772895381307,
            "f1_weighted": 0.5606863145106332,
            "precision": 0.5383373102327351,
            "precision_weighted": 0.8986678265056022,
            "recall": 0.6269948810599217,
            "recall_weighted": 0.45849609375,
            "ap": 0.10489780600645227,
            "ap_weighted": 0.10489780600645227
          },
          {
            "accuracy": 0.7294921875,
            "f1": 0.5469431826168069,
            "f1_weighted": 0.7890312475542106,
            "precision": 0.5605346829580312,
            "precision_weighted": 0.893049223860262,
            "recall": 0.669743267480984,
            "recall_weighted": 0.7294921875,
            "ap": 0.13085133059058063,
            "ap_weighted": 0.13085133059058063
          },
          {
            "accuracy": 0.60693359375,
            "f1": 0.47556986575283294,
            "f1_weighted": 0.6965172482573364,
            "precision": 0.5409733005742622,
            "precision_weighted": 0.8897939920925811,
            "recall": 0.6370560203186573,
            "recall_weighted": 0.60693359375,
            "ap": 0.11105835592809465,
            "ap_weighted": 0.11105835592809465
          },
          {
            "accuracy": 0.66015625,
            "f1": 0.5106533739803906,
            "f1_weighted": 0.7383414996498311,
            "precision": 0.5528515613789824,
            "precision_weighted": 0.8954317154208978,
            "recall": 0.668774465522433,
            "recall_weighted": 0.66015625,
            "ap": 0.1244507815314209,
            "ap_weighted": 0.1244507815314209
          },
          {
            "accuracy": 0.73388671875,
            "f1": 0.5628632737552299,
            "f1_weighted": 0.793030657963648,
            "precision": 0.574064019518565,
            "precision_weighted": 0.9028639401331163,
            "recall": 0.7116275872903657,
            "recall_weighted": 0.73388671875,
            "ap": 0.15061400582529846,
            "ap_weighted": 0.15061400582529846
          }
        ],
        "accuracy": 0.6619140625,
        "f1": 0.5120214754552526,
        "f1_weighted": 0.7353229021017482,
        "precision": 0.5562523913858446,
        "precision_weighted": 0.8960497395393286,
        "recall": 0.6683182121676289,
        "recall_weighted": 0.6619140625,
        "ap": 0.12746114327327027,
        "ap_weighted": 0.12746114327327027,
        "main_score": 0.6619140625,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 52.15334463119507,
  "kg_co2_emissions": null
}
