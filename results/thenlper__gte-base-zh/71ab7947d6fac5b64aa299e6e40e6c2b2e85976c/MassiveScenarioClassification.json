{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "task_name": "MassiveScenarioClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.460108,
        "f1": 0.42003,
        "f1_weighted": 0.452905,
        "scores_per_experiment": [
          {
            "accuracy": 0.468273,
            "f1": 0.421359,
            "f1_weighted": 0.462009
          },
          {
            "accuracy": 0.462863,
            "f1": 0.428451,
            "f1_weighted": 0.461913
          },
          {
            "accuracy": 0.448106,
            "f1": 0.410714,
            "f1_weighted": 0.44112
          },
          {
            "accuracy": 0.462863,
            "f1": 0.420116,
            "f1_weighted": 0.459899
          },
          {
            "accuracy": 0.472209,
            "f1": 0.428982,
            "f1_weighted": 0.469587
          },
          {
            "accuracy": 0.45696,
            "f1": 0.426528,
            "f1_weighted": 0.45001
          },
          {
            "accuracy": 0.453025,
            "f1": 0.41581,
            "f1_weighted": 0.441277
          },
          {
            "accuracy": 0.488441,
            "f1": 0.449563,
            "f1_weighted": 0.480059
          },
          {
            "accuracy": 0.418101,
            "f1": 0.372471,
            "f1_weighted": 0.396115
          },
          {
            "accuracy": 0.470241,
            "f1": 0.426308,
            "f1_weighted": 0.467063
          }
        ],
        "main_score": 0.460108,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.474714,
        "f1": 0.425349,
        "f1_weighted": 0.468671,
        "scores_per_experiment": [
          {
            "accuracy": 0.48117,
            "f1": 0.425397,
            "f1_weighted": 0.472733
          },
          {
            "accuracy": 0.475118,
            "f1": 0.430251,
            "f1_weighted": 0.474449
          },
          {
            "accuracy": 0.463685,
            "f1": 0.411647,
            "f1_weighted": 0.459396
          },
          {
            "accuracy": 0.493611,
            "f1": 0.430392,
            "f1_weighted": 0.489022
          },
          {
            "accuracy": 0.48655,
            "f1": 0.436819,
            "f1_weighted": 0.485294
          },
          {
            "accuracy": 0.451917,
            "f1": 0.414352,
            "f1_weighted": 0.451099
          },
          {
            "accuracy": 0.469065,
            "f1": 0.420721,
            "f1_weighted": 0.459303
          },
          {
            "accuracy": 0.504035,
            "f1": 0.464585,
            "f1_weighted": 0.495352
          },
          {
            "accuracy": 0.432078,
            "f1": 0.385324,
            "f1_weighted": 0.413395
          },
          {
            "accuracy": 0.489913,
            "f1": 0.434005,
            "f1_weighted": 0.48667
          }
        ],
        "main_score": 0.474714,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 10.51525354385376,
  "kg_co2_emissions": null
}