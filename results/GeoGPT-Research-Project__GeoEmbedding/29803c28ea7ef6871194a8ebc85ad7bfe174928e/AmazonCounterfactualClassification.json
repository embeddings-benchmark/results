{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "task_name": "AmazonCounterfactualClassification",
  "mteb_version": "1.31.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.969552,
        "f1": 0.953324,
        "f1_weighted": 0.970082,
        "ap": 0.865487,
        "ap_weighted": 0.865487,
        "scores_per_experiment": [
          {
            "accuracy": 0.974627,
            "f1": 0.960898,
            "f1_weighted": 0.975007,
            "ap": 0.885243,
            "ap_weighted": 0.885243
          },
          {
            "accuracy": 0.968657,
            "f1": 0.951959,
            "f1_weighted": 0.969206,
            "ap": 0.860985,
            "ap_weighted": 0.860985
          },
          {
            "accuracy": 0.970149,
            "f1": 0.953358,
            "f1_weighted": 0.9704,
            "ap": 0.867193,
            "ap_weighted": 0.867193
          },
          {
            "accuracy": 0.973134,
            "f1": 0.958711,
            "f1_weighted": 0.973571,
            "ap": 0.879084,
            "ap_weighted": 0.879084
          },
          {
            "accuracy": 0.968657,
            "f1": 0.951432,
            "f1_weighted": 0.969045,
            "ap": 0.860918,
            "ap_weighted": 0.860918
          },
          {
            "accuracy": 0.974627,
            "f1": 0.960683,
            "f1_weighted": 0.974941,
            "ap": 0.885414,
            "ap_weighted": 0.885414
          },
          {
            "accuracy": 0.973134,
            "f1": 0.958255,
            "f1_weighted": 0.973432,
            "ap": 0.879339,
            "ap_weighted": 0.879339
          },
          {
            "accuracy": 0.977612,
            "f1": 0.965116,
            "f1_weighted": 0.97783,
            "ap": 0.89816,
            "ap_weighted": 0.89816
          },
          {
            "accuracy": 0.956716,
            "f1": 0.935379,
            "f1_weighted": 0.957991,
            "ap": 0.816612,
            "ap_weighted": 0.816612
          },
          {
            "accuracy": 0.958209,
            "f1": 0.937447,
            "f1_weighted": 0.959392,
            "ap": 0.82192,
            "ap_weighted": 0.82192
          }
        ],
        "main_score": 0.969552,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 41.224671602249146,
  "kg_co2_emissions": null
}