{
  "dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300",
  "task_name": "Banking77Classification",
  "mteb_version": "2.3.9",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.679545,
            "f1": 0.665146,
            "f1_weighted": 0.665146,
            "precision": 0.7149,
            "precision_weighted": 0.7149,
            "recall": 0.679545,
            "recall_weighted": 0.679545,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.691883,
            "f1": 0.68005,
            "f1_weighted": 0.68005,
            "precision": 0.725775,
            "precision_weighted": 0.725775,
            "recall": 0.691883,
            "recall_weighted": 0.691883,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.677597,
            "f1": 0.66742,
            "f1_weighted": 0.66742,
            "precision": 0.718008,
            "precision_weighted": 0.718008,
            "recall": 0.677597,
            "recall_weighted": 0.677597,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.686039,
            "f1": 0.675531,
            "f1_weighted": 0.675531,
            "precision": 0.721819,
            "precision_weighted": 0.721819,
            "recall": 0.686039,
            "recall_weighted": 0.686039,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.684416,
            "f1": 0.670737,
            "f1_weighted": 0.670737,
            "precision": 0.716672,
            "precision_weighted": 0.716672,
            "recall": 0.684416,
            "recall_weighted": 0.684416,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.692532,
            "f1": 0.685122,
            "f1_weighted": 0.685122,
            "precision": 0.717875,
            "precision_weighted": 0.717875,
            "recall": 0.692532,
            "recall_weighted": 0.692532,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.678896,
            "f1": 0.66496,
            "f1_weighted": 0.66496,
            "precision": 0.706972,
            "precision_weighted": 0.706972,
            "recall": 0.678896,
            "recall_weighted": 0.678896,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.673701,
            "f1": 0.66191,
            "f1_weighted": 0.66191,
            "precision": 0.712352,
            "precision_weighted": 0.712352,
            "recall": 0.673701,
            "recall_weighted": 0.673701,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.683117,
            "f1": 0.67018,
            "f1_weighted": 0.67018,
            "precision": 0.723063,
            "precision_weighted": 0.723063,
            "recall": 0.683117,
            "recall_weighted": 0.683117,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.687987,
            "f1": 0.672963,
            "f1_weighted": 0.672963,
            "precision": 0.712897,
            "precision_weighted": 0.712897,
            "recall": 0.687987,
            "recall_weighted": 0.687987,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.683571,
        "f1": 0.671402,
        "f1_weighted": 0.671402,
        "precision": 0.717033,
        "precision_weighted": 0.717033,
        "recall": 0.683571,
        "recall_weighted": 0.683571,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.683571,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 44.603633880615234,
  "kg_co2_emissions": null
}