{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.428923,
        "f1": 0.383042,
        "f1_weighted": 0.421228,
        "scores_per_experiment": [
          {
            "accuracy": 0.428923,
            "f1": 0.378729,
            "f1_weighted": 0.421252
          },
          {
            "accuracy": 0.404329,
            "f1": 0.351491,
            "f1_weighted": 0.399726
          },
          {
            "accuracy": 0.424004,
            "f1": 0.376256,
            "f1_weighted": 0.413916
          },
          {
            "accuracy": 0.447614,
            "f1": 0.403522,
            "f1_weighted": 0.43374
          },
          {
            "accuracy": 0.448598,
            "f1": 0.389003,
            "f1_weighted": 0.441165
          },
          {
            "accuracy": 0.423512,
            "f1": 0.381501,
            "f1_weighted": 0.419359
          },
          {
            "accuracy": 0.403837,
            "f1": 0.365713,
            "f1_weighted": 0.394924
          },
          {
            "accuracy": 0.445647,
            "f1": 0.392219,
            "f1_weighted": 0.437649
          },
          {
            "accuracy": 0.437777,
            "f1": 0.408817,
            "f1_weighted": 0.431296
          },
          {
            "accuracy": 0.424988,
            "f1": 0.383173,
            "f1_weighted": 0.419254
          }
        ],
        "main_score": 0.428923,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.445293,
        "f1": 0.402904,
        "f1_weighted": 0.434475,
        "scores_per_experiment": [
          {
            "accuracy": 0.445192,
            "f1": 0.402401,
            "f1_weighted": 0.436365
          },
          {
            "accuracy": 0.431069,
            "f1": 0.380509,
            "f1_weighted": 0.425124
          },
          {
            "accuracy": 0.452253,
            "f1": 0.408647,
            "f1_weighted": 0.437826
          },
          {
            "accuracy": 0.45965,
            "f1": 0.416085,
            "f1_weighted": 0.443014
          },
          {
            "accuracy": 0.453262,
            "f1": 0.388509,
            "f1_weighted": 0.43715
          },
          {
            "accuracy": 0.440148,
            "f1": 0.407796,
            "f1_weighted": 0.432489
          },
          {
            "accuracy": 0.437458,
            "f1": 0.411517,
            "f1_weighted": 0.426855
          },
          {
            "accuracy": 0.469738,
            "f1": 0.421273,
            "f1_weighted": 0.459336
          },
          {
            "accuracy": 0.435777,
            "f1": 0.399521,
            "f1_weighted": 0.423047
          },
          {
            "accuracy": 0.428379,
            "f1": 0.392779,
            "f1_weighted": 0.423541
          }
        ],
        "main_score": 0.445293,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 18.03706932067871,
  "kg_co2_emissions": null
}