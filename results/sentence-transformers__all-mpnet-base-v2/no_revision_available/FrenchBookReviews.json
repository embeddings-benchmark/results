{
  "dataset_revision": "534725e03fec6f560dbe8166e8ae3825314a6290",
  "evaluation_time": 11.003267765045166,
  "kg_co2_emissions": 0.00036852561884807,
  "mteb_version": "1.12.41",
  "scores": {
    "train": [
      {
        "accuracy": 0.3462890625,
        "f1": 0.29853792060522705,
        "f1_weighted": 0.38450545109826545,
        "hf_subset": "default",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.3462890625,
        "scores_per_experiment": [
          {
            "accuracy": 0.31982421875,
            "f1": 0.2837739322791779,
            "f1_weighted": 0.37403501573629394
          },
          {
            "accuracy": 0.36181640625,
            "f1": 0.3130189903728185,
            "f1_weighted": 0.40474734040074317
          },
          {
            "accuracy": 0.41943359375,
            "f1": 0.3287858457841874,
            "f1_weighted": 0.4577331504973568
          },
          {
            "accuracy": 0.2958984375,
            "f1": 0.26668576828349627,
            "f1_weighted": 0.30274341155284445
          },
          {
            "accuracy": 0.32080078125,
            "f1": 0.2853355043602111,
            "f1_weighted": 0.3497020246904145
          },
          {
            "accuracy": 0.3291015625,
            "f1": 0.29455748377331686,
            "f1_weighted": 0.3727521907740884
          },
          {
            "accuracy": 0.4130859375,
            "f1": 0.31435607774443186,
            "f1_weighted": 0.45102417138625867
          },
          {
            "accuracy": 0.3349609375,
            "f1": 0.2939802530752862,
            "f1_weighted": 0.37479338121163996
          },
          {
            "accuracy": 0.33544921875,
            "f1": 0.30075359864468926,
            "f1_weighted": 0.3828439211923531
          },
          {
            "accuracy": 0.33251953125,
            "f1": 0.30413175173465556,
            "f1_weighted": 0.37467990354066216
          }
        ]
      }
    ]
  },
  "task_name": "FrenchBookReviews"
}