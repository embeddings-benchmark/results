{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 16.840473175048828,
  "kg_co2_emissions": 0.0005270041816239305,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.259033203125,
        "f1": 0.2542494196942523,
        "f1_weighted": 0.25425190182115587,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.259033203125,
        "scores_per_experiment": [
          {
            "accuracy": 0.2451171875,
            "f1": 0.24512085829712707,
            "f1_weighted": 0.24511304177987986
          },
          {
            "accuracy": 0.25634765625,
            "f1": 0.2452976201696042,
            "f1_weighted": 0.24534059645660566
          },
          {
            "accuracy": 0.28076171875,
            "f1": 0.2769540794758139,
            "f1_weighted": 0.2769348214741716
          },
          {
            "accuracy": 0.29296875,
            "f1": 0.2918720574590841,
            "f1_weighted": 0.29185961595335164
          },
          {
            "accuracy": 0.22705078125,
            "f1": 0.22321338348050485,
            "f1_weighted": 0.22317058476472162
          },
          {
            "accuracy": 0.24365234375,
            "f1": 0.2363152392870135,
            "f1_weighted": 0.2363112395346938
          },
          {
            "accuracy": 0.23046875,
            "f1": 0.22641991930221775,
            "f1_weighted": 0.22641135990897188
          },
          {
            "accuracy": 0.29638671875,
            "f1": 0.2902843666743613,
            "f1_weighted": 0.29031484507768424
          },
          {
            "accuracy": 0.275390625,
            "f1": 0.26840173151254587,
            "f1_weighted": 0.2684161502381061
          },
          {
            "accuracy": 0.2421875,
            "f1": 0.2386149412842506,
            "f1_weighted": 0.23864676302337176
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.260205078125,
        "f1": 0.254364471880201,
        "f1_weighted": 0.2543652827204548,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.260205078125,
        "scores_per_experiment": [
          {
            "accuracy": 0.2744140625,
            "f1": 0.27172355087408284,
            "f1_weighted": 0.2717260902810058
          },
          {
            "accuracy": 0.22900390625,
            "f1": 0.21825397003864627,
            "f1_weighted": 0.2183031152311359
          },
          {
            "accuracy": 0.26416015625,
            "f1": 0.2604932738453368,
            "f1_weighted": 0.2604693856990599
          },
          {
            "accuracy": 0.29833984375,
            "f1": 0.2952348888584813,
            "f1_weighted": 0.295221207976437
          },
          {
            "accuracy": 0.2451171875,
            "f1": 0.239478294022893,
            "f1_weighted": 0.23942501179631448
          },
          {
            "accuracy": 0.2451171875,
            "f1": 0.2374962350834545,
            "f1_weighted": 0.2375184399510799
          },
          {
            "accuracy": 0.22607421875,
            "f1": 0.22085775397983648,
            "f1_weighted": 0.22084006026008007
          },
          {
            "accuracy": 0.2822265625,
            "f1": 0.2743020373173935,
            "f1_weighted": 0.27432449880725274
          },
          {
            "accuracy": 0.28564453125,
            "f1": 0.27844133578853353,
            "f1_weighted": 0.27845954174901955
          },
          {
            "accuracy": 0.251953125,
            "f1": 0.2473633789933518,
            "f1_weighted": 0.24736547545316254
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}