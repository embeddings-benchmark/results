{
  "dataset_revision": "b07c6ce548f6a7ac8d546e1bbe197a0086409190",
  "evaluation_time": 9.530439853668213,
  "kg_co2_emissions": 0.0003378303207162391,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.52646484375,
        "f1": 0.47331856430423674,
        "f1_weighted": 0.5543914110838942,
        "hf_subset": "default",
        "languages": [
          "swe-Latn"
        ],
        "main_score": 0.52646484375,
        "scores_per_experiment": [
          {
            "accuracy": 0.50439453125,
            "f1": 0.4418327769811614,
            "f1_weighted": 0.5267547687408722
          },
          {
            "accuracy": 0.578125,
            "f1": 0.5124326196511543,
            "f1_weighted": 0.608353511076875
          },
          {
            "accuracy": 0.54248046875,
            "f1": 0.49795812543226764,
            "f1_weighted": 0.577353421795205
          },
          {
            "accuracy": 0.45068359375,
            "f1": 0.4275766260129055,
            "f1_weighted": 0.5053434344616892
          },
          {
            "accuracy": 0.49609375,
            "f1": 0.4514206838818051,
            "f1_weighted": 0.5357404302768161
          },
          {
            "accuracy": 0.515625,
            "f1": 0.46696004087866827,
            "f1_weighted": 0.541434296164071
          },
          {
            "accuracy": 0.55712890625,
            "f1": 0.4960397152156875,
            "f1_weighted": 0.5726294198447315
          },
          {
            "accuracy": 0.61962890625,
            "f1": 0.5475145725194559,
            "f1_weighted": 0.6395844240520755
          },
          {
            "accuracy": 0.458984375,
            "f1": 0.43378738603430844,
            "f1_weighted": 0.4859145126118367
          },
          {
            "accuracy": 0.54150390625,
            "f1": 0.4576630964349539,
            "f1_weighted": 0.5508058918147697
          }
        ]
      }
    ]
  },
  "task_name": "SweRecClassification"
}