{
  "dataset_revision": "566be6449bb30b9b9f2b59173391647fe0ca3224",
  "evaluation_time": 7.917516231536865,
  "kg_co2_emissions": 0.0002260323440649872,
  "mteb_version": "1.12.41",
  "scores": {
    "train": [
      {
        "accuracy": 0.41875,
        "f1": 0.40445545975167485,
        "f1_weighted": 0.41838133981844355,
        "hf_subset": "default",
        "languages": [
          "urd-Latn"
        ],
        "main_score": 0.40445545975167485,
        "scores_per_experiment": [
          {
            "accuracy": 0.4736328125,
            "f1": 0.4508249111614309,
            "f1_weighted": 0.4692961282059004
          },
          {
            "accuracy": 0.4140625,
            "f1": 0.4094057112698546,
            "f1_weighted": 0.41388840345346933
          },
          {
            "accuracy": 0.43212890625,
            "f1": 0.422298107218138,
            "f1_weighted": 0.4365604734477334
          },
          {
            "accuracy": 0.4248046875,
            "f1": 0.4026125742372033,
            "f1_weighted": 0.42132180894832477
          },
          {
            "accuracy": 0.37744140625,
            "f1": 0.3598996139618389,
            "f1_weighted": 0.37504582617183957
          },
          {
            "accuracy": 0.435546875,
            "f1": 0.4228581207837019,
            "f1_weighted": 0.4421032293369843
          },
          {
            "accuracy": 0.4287109375,
            "f1": 0.4136735301360657,
            "f1_weighted": 0.4307058755497283
          },
          {
            "accuracy": 0.37548828125,
            "f1": 0.3483928294381751,
            "f1_weighted": 0.3663149517519119
          },
          {
            "accuracy": 0.40087890625,
            "f1": 0.39346123691038953,
            "f1_weighted": 0.40133571894869213
          },
          {
            "accuracy": 0.4248046875,
            "f1": 0.42112796239995154,
            "f1_weighted": 0.42724098236985164
          }
        ]
      }
    ]
  },
  "task_name": "UrduRomanSentimentClassification"
}