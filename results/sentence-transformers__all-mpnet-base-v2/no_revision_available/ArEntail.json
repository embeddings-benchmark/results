{
  "dataset_revision": "4da4316c6e3287746ab74ff67dd252ad128fceff",
  "evaluation_time": 1.3493225574493408,
  "kg_co2_emissions": 5.822576292781262e-05,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.582,
          "accuracy_threshold": 0.861383318901062,
          "ap": 0.5949660656657413,
          "f1": 0.6684491978609626,
          "f1_threshold": 0.4907738268375397,
          "precision": 0.5020080321285141,
          "recall": 1.0
        },
        "dot": {
          "accuracy": 0.582,
          "accuracy_threshold": 0.8613836169242859,
          "ap": 0.5949660656657413,
          "f1": 0.6684491978609626,
          "f1_threshold": 0.49077361822128296,
          "precision": 0.5020080321285141,
          "recall": 1.0
        },
        "euclidean": {
          "accuracy": 0.582,
          "accuracy_threshold": 0.5265294313430786,
          "ap": 0.5949660656657413,
          "f1": 0.6684491978609626,
          "f1_threshold": 1.0091756582260132,
          "precision": 0.5020080321285141,
          "recall": 1.0
        },
        "hf_subset": "default",
        "languages": [
          "ara-Arab"
        ],
        "main_score": 0.5949660656657413,
        "manhattan": {
          "accuracy": 0.576,
          "accuracy_threshold": 11.326497077941895,
          "ap": 0.5943525826882127,
          "f1": 0.6675585284280937,
          "f1_threshold": 21.760969161987305,
          "precision": 0.5015075376884423,
          "recall": 0.998
        },
        "max": {
          "accuracy": 0.582,
          "ap": 0.5949660656657413,
          "f1": 0.6684491978609626
        },
        "similarity": {
          "accuracy": 0.582,
          "accuracy_threshold": 0.8613833785057068,
          "ap": 0.5949660656657413,
          "f1": 0.6684491978609626,
          "f1_threshold": 0.4907737374305725,
          "precision": 0.5020080321285141,
          "recall": 1.0
        }
      }
    ]
  },
  "task_name": "ArEntail"
}