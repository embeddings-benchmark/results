{
  "dataset_revision": "358bcc95aeddd5d07a4524ee416f03d993099b23",
  "evaluation_time": 8.469050168991089,
  "kg_co2_emissions": 0.0003000702087152178,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.356005859375,
        "f1": 0.30238757740465505,
        "f1_weighted": 0.38093834918711417,
        "hf_subset": "default",
        "languages": [
          "ron-Latn"
        ],
        "main_score": 0.356005859375,
        "scores_per_experiment": [
          {
            "accuracy": 0.4189453125,
            "f1": 0.3456496787452657,
            "f1_weighted": 0.4529938761179527
          },
          {
            "accuracy": 0.380859375,
            "f1": 0.3181254337607703,
            "f1_weighted": 0.40436280486452103
          },
          {
            "accuracy": 0.3818359375,
            "f1": 0.32560298753480543,
            "f1_weighted": 0.41710548067937586
          },
          {
            "accuracy": 0.40283203125,
            "f1": 0.3424789071915407,
            "f1_weighted": 0.436848576761201
          },
          {
            "accuracy": 0.32666015625,
            "f1": 0.29506816458668556,
            "f1_weighted": 0.3532921434104128
          },
          {
            "accuracy": 0.3134765625,
            "f1": 0.26632475834501046,
            "f1_weighted": 0.3425285982497698
          },
          {
            "accuracy": 0.28515625,
            "f1": 0.25788404617757626,
            "f1_weighted": 0.31560049545131497
          },
          {
            "accuracy": 0.3828125,
            "f1": 0.3204837970238287,
            "f1_weighted": 0.40722653639317197
          },
          {
            "accuracy": 0.3515625,
            "f1": 0.2908055102382175,
            "f1_weighted": 0.36070638654859305
          },
          {
            "accuracy": 0.31591796875,
            "f1": 0.2614524904428497,
            "f1_weighted": 0.3187185933948285
          }
        ]
      }
    ]
  },
  "task_name": "RomanianReviewsSentiment"
}