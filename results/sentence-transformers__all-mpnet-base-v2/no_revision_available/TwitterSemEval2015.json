{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "evaluation_time": 8.408659934997559,
  "kg_co2_emissions": 0.000307120741134198,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.8607617571675508,
          "accuracy_threshold": 0.725288987159729,
          "ap": 0.7385399413971873,
          "f1": 0.6850702798531088,
          "f1_threshold": 0.6927633285522461,
          "precision": 0.6586316045775505,
          "recall": 0.7137203166226913
        },
        "dot": {
          "accuracy": 0.8607617571675508,
          "accuracy_threshold": 0.7252891063690186,
          "ap": 0.7385398230889887,
          "f1": 0.6850702798531088,
          "f1_threshold": 0.69276362657547,
          "precision": 0.6586316045775505,
          "recall": 0.7137203166226913
        },
        "euclidean": {
          "accuracy": 0.8607617571675508,
          "accuracy_threshold": 0.7412300705909729,
          "ap": 0.7385398848546609,
          "f1": 0.6850702798531088,
          "f1_threshold": 0.7838834524154663,
          "precision": 0.6586316045775505,
          "recall": 0.7137203166226913
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7386875133476073,
        "manhattan": {
          "accuracy": 0.8598676759849795,
          "accuracy_threshold": 16.310483932495117,
          "ap": 0.7386875133476073,
          "f1": 0.6855096559662361,
          "f1_threshold": 16.945289611816406,
          "precision": 0.6651774633904195,
          "recall": 0.7071240105540897
        },
        "max": {
          "accuracy": 0.8607617571675508,
          "ap": 0.7386875133476073,
          "f1": 0.6855096559662361
        },
        "similarity": {
          "accuracy": 0.8607617571675508,
          "accuracy_threshold": 0.725288987159729,
          "ap": 0.7385396239722836,
          "f1": 0.6850702798531088,
          "f1_threshold": 0.6927634477615356,
          "precision": 0.6586316045775505,
          "recall": 0.7137203166226913
        }
      }
    ]
  },
  "task_name": "TwitterSemEval2015"
}