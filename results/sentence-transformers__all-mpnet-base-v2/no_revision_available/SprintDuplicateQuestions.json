{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 43.49668765068054,
  "kg_co2_emissions": 0.0012163048592133707,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.9965940594059406,
          "accuracy_threshold": 0.7384017705917358,
          "ap": 0.9014551416831162,
          "f1": 0.8226044226044226,
          "f1_threshold": 0.7027579545974731,
          "precision": 0.808695652173913,
          "recall": 0.837
        },
        "dot": {
          "accuracy": 0.9965940594059406,
          "accuracy_threshold": 0.7384023070335388,
          "ap": 0.9014551416831161,
          "f1": 0.8226044226044226,
          "f1_threshold": 0.7027579545974731,
          "precision": 0.808695652173913,
          "recall": 0.837
        },
        "euclidean": {
          "accuracy": 0.9965940594059406,
          "accuracy_threshold": 0.7233232259750366,
          "ap": 0.9014551416831162,
          "f1": 0.8226044226044226,
          "f1_threshold": 0.7710279226303101,
          "precision": 0.808695652173913,
          "recall": 0.837
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.9014551416831162,
        "manhattan": {
          "accuracy": 0.9964950495049505,
          "accuracy_threshold": 15.732294082641602,
          "ap": 0.8954927521951052,
          "f1": 0.8158280410356619,
          "f1_threshold": 16.84906005859375,
          "precision": 0.7975167144221585,
          "recall": 0.835
        },
        "max": {
          "accuracy": 0.9965940594059406,
          "ap": 0.9014551416831162,
          "f1": 0.8226044226044226
        },
        "similarity": {
          "accuracy": 0.9965940594059406,
          "accuracy_threshold": 0.7384018301963806,
          "ap": 0.9014551416831162,
          "f1": 0.8226044226044226,
          "f1_threshold": 0.7027580738067627,
          "precision": 0.808695652173913,
          "recall": 0.837
        }
      }
    ],
    "validation": [
      {
        "cosine": {
          "accuracy": 0.9970891089108911,
          "accuracy_threshold": 0.6985461711883545,
          "ap": 0.9180912172105131,
          "f1": 0.853731343283582,
          "f1_threshold": 0.6985461711883545,
          "precision": 0.8495049504950495,
          "recall": 0.858
        },
        "dot": {
          "accuracy": 0.9970891089108911,
          "accuracy_threshold": 0.6985460519790649,
          "ap": 0.9180912172105132,
          "f1": 0.853731343283582,
          "f1_threshold": 0.6985460519790649,
          "precision": 0.8495049504950495,
          "recall": 0.858
        },
        "euclidean": {
          "accuracy": 0.9970891089108911,
          "accuracy_threshold": 0.7764711380004883,
          "ap": 0.9180912172105131,
          "f1": 0.853731343283582,
          "f1_threshold": 0.7764711380004883,
          "precision": 0.8495049504950495,
          "recall": 0.858
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.9180912172105132,
        "manhattan": {
          "accuracy": 0.9968910891089109,
          "accuracy_threshold": 16.26099967956543,
          "ap": 0.9132665797992368,
          "f1": 0.8457760314341848,
          "f1_threshold": 17.13541030883789,
          "precision": 0.831081081081081,
          "recall": 0.861
        },
        "max": {
          "accuracy": 0.9970891089108911,
          "ap": 0.9180912172105132,
          "f1": 0.853731343283582
        },
        "similarity": {
          "accuracy": 0.9970891089108911,
          "accuracy_threshold": 0.698546290397644,
          "ap": 0.9180912172105132,
          "f1": 0.853731343283582,
          "f1_threshold": 0.698546290397644,
          "precision": 0.8495049504950495,
          "recall": 0.858
        }
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}