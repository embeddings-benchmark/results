{
  "dataset_revision": "3c976110fc13596004dc36279fc4c453ff2c18aa",
  "evaluation_time": 2.1042802333831787,
  "kg_co2_emissions": 8.544644436874878e-05,
  "mteb_version": "1.12.41",
  "scores": {
    "test_expert": [
      {
        "cosine": {
          "accuracy": 0.5147058823529411,
          "accuracy_threshold": 0.49039745330810547,
          "ap": 0.4854164662428644,
          "f1": 0.6755440077947387,
          "f1_threshold": 0.2506084442138672,
          "precision": 0.5103042198233563,
          "recall": 0.9990393852065321
        },
        "dot": {
          "accuracy": 0.5147058823529411,
          "accuracy_threshold": 0.4903973340988159,
          "ap": 0.485416375385411,
          "f1": 0.6755440077947387,
          "f1_threshold": 0.2506084740161896,
          "precision": 0.5103042198233563,
          "recall": 0.9990393852065321
        },
        "euclidean": {
          "accuracy": 0.5147058823529411,
          "accuracy_threshold": 1.0095570087432861,
          "ap": 0.48541598157344285,
          "f1": 0.6755440077947387,
          "f1_threshold": 1.224227786064148,
          "precision": 0.5103042198233563,
          "recall": 0.9990393852065321
        },
        "hf_subset": "default",
        "languages": [
          "ind-Latn"
        ],
        "main_score": 0.4854164662428644,
        "manhattan": {
          "accuracy": 0.5127450980392156,
          "accuracy_threshold": 21.732181549072266,
          "ap": 0.48534225945490944,
          "f1": 0.6753246753246754,
          "f1_threshold": 26.974857330322266,
          "precision": 0.5100539480137323,
          "recall": 0.9990393852065321
        },
        "max": {
          "accuracy": 0.5147058823529411,
          "ap": 0.4854164662428644,
          "f1": 0.6755440077947387
        },
        "similarity": {
          "accuracy": 0.5147058823529411,
          "accuracy_threshold": 0.49039751291275024,
          "ap": 0.48541598157344285,
          "f1": 0.6755440077947387,
          "f1_threshold": 0.25060850381851196,
          "precision": 0.5103042198233563,
          "recall": 0.9990393852065321
        }
      }
    ]
  },
  "task_name": "indonli"
}