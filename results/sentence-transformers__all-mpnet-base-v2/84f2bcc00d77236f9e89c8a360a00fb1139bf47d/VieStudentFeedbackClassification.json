{
  "dataset_revision": "7b56c6cb1c9c8523249f407044c838660df3811a",
  "evaluation_time": 10.190817832946777,
  "kg_co2_emissions": 0.0015298326265330401,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.52431640625,
        "f1": 0.43741197205231896,
        "f1_weighted": 0.5630947925227956,
        "hf_subset": "default",
        "languages": [
          "vie-Latn"
        ],
        "main_score": 0.52431640625,
        "scores_per_experiment": [
          {
            "accuracy": 0.57666015625,
            "f1": 0.4797283286471861,
            "f1_weighted": 0.620362600879832
          },
          {
            "accuracy": 0.572265625,
            "f1": 0.46523872567803704,
            "f1_weighted": 0.5913923792434508
          },
          {
            "accuracy": 0.59912109375,
            "f1": 0.48566507063513525,
            "f1_weighted": 0.6142045913058192
          },
          {
            "accuracy": 0.5244140625,
            "f1": 0.4363959890814275,
            "f1_weighted": 0.5672168812721222
          },
          {
            "accuracy": 0.47509765625,
            "f1": 0.38777212524195104,
            "f1_weighted": 0.5133311040959587
          },
          {
            "accuracy": 0.505859375,
            "f1": 0.4374526295539209,
            "f1_weighted": 0.5570657689134021
          },
          {
            "accuracy": 0.59033203125,
            "f1": 0.47680902016175414,
            "f1_weighted": 0.6186016266065895
          },
          {
            "accuracy": 0.38818359375,
            "f1": 0.33912318433020544,
            "f1_weighted": 0.4281489621803531
          },
          {
            "accuracy": 0.513671875,
            "f1": 0.44792238557742,
            "f1_weighted": 0.5802554280773728
          },
          {
            "accuracy": 0.49755859375,
            "f1": 0.4180122616161525,
            "f1_weighted": 0.5403685826530553
          }
        ]
      }
    ]
  },
  "task_name": "VieStudentFeedbackClassification"
}