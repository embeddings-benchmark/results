{
  "dataset_revision": "12ca3b695563788fead87a982ad1a068284413f4",
  "evaluation_time": 1.7305107116699219,
  "kg_co2_emissions": 8.074448959108573e-05,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.6796875,
          "accuracy_threshold": 0.19848576188087463,
          "ap": 0.7633105905078273,
          "f1": 0.7648725212464589,
          "f1_threshold": 0.18419542908668518,
          "precision": 0.6737367436057392,
          "recall": 0.8845208845208845
        },
        "dot": {
          "accuracy": 0.6796875,
          "accuracy_threshold": 0.19848577678203583,
          "ap": 0.7633105905078273,
          "f1": 0.7648725212464589,
          "f1_threshold": 0.18419548869132996,
          "precision": 0.6737367436057392,
          "recall": 0.8845208845208845
        },
        "euclidean": {
          "accuracy": 0.6796875,
          "accuracy_threshold": 1.2661075592041016,
          "ap": 0.7633105905078273,
          "f1": 0.7648725212464589,
          "f1_threshold": 1.2773444652557373,
          "precision": 0.6737367436057392,
          "recall": 0.8845208845208845
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.68212890625,
        "manhattan": {
          "accuracy": 0.68212890625,
          "accuracy_threshold": 27.30790138244629,
          "ap": 0.7641446177382872,
          "f1": 0.7660148347943359,
          "f1_threshold": 28.43294906616211,
          "precision": 0.6510028653295129,
          "recall": 0.9303849303849304
        },
        "max": {
          "accuracy": 0.68212890625,
          "ap": 0.7641446177382872,
          "f1": 0.7660148347943359
        },
        "similarity": {
          "accuracy": 0.6796875,
          "accuracy_threshold": 0.19848576188087463,
          "ap": 0.7633105905078273,
          "f1": 0.7648725212464589,
          "f1_threshold": 0.18419548869132996,
          "precision": 0.6737367436057392,
          "recall": 0.8845208845208845
        }
      }
    ]
  },
  "task_name": "LegalBenchPC"
}