{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.673625,
        "f1": 0.558984,
        "f1_weighted": 0.724466,
        "ap": 0.193388,
        "ap_weighted": 0.193388,
        "scores_per_experiment": [
          {
            "accuracy": 0.600515,
            "f1": 0.508788,
            "f1_weighted": 0.667807,
            "ap": 0.168967,
            "ap_weighted": 0.168967
          },
          {
            "accuracy": 0.77921,
            "f1": 0.623189,
            "f1_weighted": 0.804831,
            "ap": 0.216469,
            "ap_weighted": 0.216469
          },
          {
            "accuracy": 0.706186,
            "f1": 0.588757,
            "f1_weighted": 0.753383,
            "ap": 0.212481,
            "ap_weighted": 0.212481
          },
          {
            "accuracy": 0.640893,
            "f1": 0.534738,
            "f1_weighted": 0.701226,
            "ap": 0.177613,
            "ap_weighted": 0.177613
          },
          {
            "accuracy": 0.567869,
            "f1": 0.493119,
            "f1_weighted": 0.638941,
            "ap": 0.170839,
            "ap_weighted": 0.170839
          },
          {
            "accuracy": 0.792955,
            "f1": 0.636969,
            "f1_weighted": 0.815239,
            "ap": 0.228605,
            "ap_weighted": 0.228605
          },
          {
            "accuracy": 0.734536,
            "f1": 0.586145,
            "f1_weighted": 0.771794,
            "ap": 0.190727,
            "ap_weighted": 0.190727
          },
          {
            "accuracy": 0.575601,
            "f1": 0.508532,
            "f1_weighted": 0.644543,
            "ap": 0.190075,
            "ap_weighted": 0.190075
          },
          {
            "accuracy": 0.681271,
            "f1": 0.548627,
            "f1_weighted": 0.731933,
            "ap": 0.171962,
            "ap_weighted": 0.171962
          },
          {
            "accuracy": 0.657216,
            "f1": 0.560971,
            "f1_weighted": 0.714964,
            "ap": 0.206143,
            "ap_weighted": 0.206143
          }
        ],
        "main_score": 0.673625,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 7.354158878326416,
  "kg_co2_emissions": 0.00021499198318943713
}