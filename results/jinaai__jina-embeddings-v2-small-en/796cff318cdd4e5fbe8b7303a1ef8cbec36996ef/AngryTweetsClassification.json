{
  "dataset_revision": "20b0e6081892e78179356fada741b7afa381443d",
  "task_name": "AngryTweetsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.40554,
        "f1": 0.398124,
        "f1_weighted": 0.405367,
        "scores_per_experiment": [
          {
            "accuracy": 0.436485,
            "f1": 0.432853,
            "f1_weighted": 0.438933
          },
          {
            "accuracy": 0.393505,
            "f1": 0.385987,
            "f1_weighted": 0.393664
          },
          {
            "accuracy": 0.393505,
            "f1": 0.382734,
            "f1_weighted": 0.391062
          },
          {
            "accuracy": 0.39446,
            "f1": 0.389199,
            "f1_weighted": 0.396273
          },
          {
            "accuracy": 0.420248,
            "f1": 0.413458,
            "f1_weighted": 0.420598
          },
          {
            "accuracy": 0.440306,
            "f1": 0.424259,
            "f1_weighted": 0.435096
          },
          {
            "accuracy": 0.381089,
            "f1": 0.381341,
            "f1_weighted": 0.385891
          },
          {
            "accuracy": 0.39446,
            "f1": 0.394464,
            "f1_weighted": 0.397373
          },
          {
            "accuracy": 0.346705,
            "f1": 0.342671,
            "f1_weighted": 0.346832
          },
          {
            "accuracy": 0.454632,
            "f1": 0.434274,
            "f1_weighted": 0.447952
          }
        ],
        "main_score": 0.40554,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 9.258466958999634,
  "kg_co2_emissions": 0.0002644814114994775
}