{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.282764,
        "f1": 0.278278,
        "f1_weighted": 0.278271,
        "scores_per_experiment": [
          {
            "accuracy": 0.288086,
            "f1": 0.280682,
            "f1_weighted": 0.280679
          },
          {
            "accuracy": 0.26416,
            "f1": 0.252951,
            "f1_weighted": 0.252907
          },
          {
            "accuracy": 0.291992,
            "f1": 0.28986,
            "f1_weighted": 0.289845
          },
          {
            "accuracy": 0.293457,
            "f1": 0.288392,
            "f1_weighted": 0.288409
          },
          {
            "accuracy": 0.259766,
            "f1": 0.256937,
            "f1_weighted": 0.256928
          },
          {
            "accuracy": 0.275391,
            "f1": 0.268793,
            "f1_weighted": 0.268796
          },
          {
            "accuracy": 0.244629,
            "f1": 0.24388,
            "f1_weighted": 0.243854
          },
          {
            "accuracy": 0.328125,
            "f1": 0.326967,
            "f1_weighted": 0.326956
          },
          {
            "accuracy": 0.311523,
            "f1": 0.3099,
            "f1_weighted": 0.309903
          },
          {
            "accuracy": 0.270508,
            "f1": 0.26442,
            "f1_weighted": 0.264432
          }
        ],
        "main_score": 0.282764,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.286523,
        "f1": 0.281968,
        "f1_weighted": 0.281971,
        "scores_per_experiment": [
          {
            "accuracy": 0.277832,
            "f1": 0.269254,
            "f1_weighted": 0.269249
          },
          {
            "accuracy": 0.286133,
            "f1": 0.276834,
            "f1_weighted": 0.276815
          },
          {
            "accuracy": 0.286133,
            "f1": 0.286236,
            "f1_weighted": 0.286242
          },
          {
            "accuracy": 0.288086,
            "f1": 0.285329,
            "f1_weighted": 0.285333
          },
          {
            "accuracy": 0.289062,
            "f1": 0.287143,
            "f1_weighted": 0.287128
          },
          {
            "accuracy": 0.27832,
            "f1": 0.266966,
            "f1_weighted": 0.26699
          },
          {
            "accuracy": 0.237793,
            "f1": 0.23537,
            "f1_weighted": 0.235365
          },
          {
            "accuracy": 0.33252,
            "f1": 0.331089,
            "f1_weighted": 0.331107
          },
          {
            "accuracy": 0.330078,
            "f1": 0.325383,
            "f1_weighted": 0.325389
          },
          {
            "accuracy": 0.259277,
            "f1": 0.256079,
            "f1_weighted": 0.256091
          }
        ],
        "main_score": 0.286523,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 16.993562698364258,
  "kg_co2_emissions": 0.0006074235923427764
}