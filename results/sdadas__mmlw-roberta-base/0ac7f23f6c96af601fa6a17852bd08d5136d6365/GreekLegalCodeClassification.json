{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "task_name": "GreekLegalCodeClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.022949,
        "f1": 0.018971,
        "f1_weighted": 0.023119,
        "scores_per_experiment": [
          {
            "accuracy": 0.022949,
            "f1": 0.018004,
            "f1_weighted": 0.023976
          },
          {
            "accuracy": 0.020996,
            "f1": 0.017355,
            "f1_weighted": 0.020822
          },
          {
            "accuracy": 0.024902,
            "f1": 0.022303,
            "f1_weighted": 0.026446
          },
          {
            "accuracy": 0.02002,
            "f1": 0.017697,
            "f1_weighted": 0.021372
          },
          {
            "accuracy": 0.020508,
            "f1": 0.017566,
            "f1_weighted": 0.021652
          },
          {
            "accuracy": 0.026367,
            "f1": 0.018186,
            "f1_weighted": 0.027313
          },
          {
            "accuracy": 0.019531,
            "f1": 0.019346,
            "f1_weighted": 0.018887
          },
          {
            "accuracy": 0.021973,
            "f1": 0.015711,
            "f1_weighted": 0.021641
          },
          {
            "accuracy": 0.02002,
            "f1": 0.014548,
            "f1_weighted": 0.019913
          },
          {
            "accuracy": 0.032227,
            "f1": 0.028996,
            "f1_weighted": 0.029168
          }
        ],
        "main_score": 0.022949,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.025586,
        "f1": 0.022098,
        "f1_weighted": 0.024041,
        "scores_per_experiment": [
          {
            "accuracy": 0.018555,
            "f1": 0.019275,
            "f1_weighted": 0.01604
          },
          {
            "accuracy": 0.024902,
            "f1": 0.023037,
            "f1_weighted": 0.024274
          },
          {
            "accuracy": 0.019531,
            "f1": 0.02132,
            "f1_weighted": 0.019096
          },
          {
            "accuracy": 0.024902,
            "f1": 0.019954,
            "f1_weighted": 0.022736
          },
          {
            "accuracy": 0.024902,
            "f1": 0.01969,
            "f1_weighted": 0.021354
          },
          {
            "accuracy": 0.027344,
            "f1": 0.022882,
            "f1_weighted": 0.027104
          },
          {
            "accuracy": 0.028809,
            "f1": 0.022282,
            "f1_weighted": 0.027595
          },
          {
            "accuracy": 0.032715,
            "f1": 0.028371,
            "f1_weighted": 0.031167
          },
          {
            "accuracy": 0.027832,
            "f1": 0.022331,
            "f1_weighted": 0.027771
          },
          {
            "accuracy": 0.026367,
            "f1": 0.021836,
            "f1_weighted": 0.023269
          }
        ],
        "main_score": 0.025586,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ]
  },
  "evaluation_time": 1395.815221786499,
  "kg_co2_emissions": 0.04445482397543116
}