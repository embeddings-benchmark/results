{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.25.8",
  "scores": {
    "validation": [
      {
        "accuracy": 0.527398,
        "f1": 0.482312,
        "f1_weighted": 0.533484,
        "scores_per_experiment": [
          {
            "accuracy": 0.545991,
            "f1": 0.505597,
            "f1_weighted": 0.555735
          },
          {
            "accuracy": 0.542056,
            "f1": 0.488751,
            "f1_weighted": 0.552015
          },
          {
            "accuracy": 0.517462,
            "f1": 0.4696,
            "f1_weighted": 0.52463
          },
          {
            "accuracy": 0.532218,
            "f1": 0.488731,
            "f1_weighted": 0.540425
          },
          {
            "accuracy": 0.530251,
            "f1": 0.48917,
            "f1_weighted": 0.534406
          },
          {
            "accuracy": 0.523364,
            "f1": 0.477626,
            "f1_weighted": 0.52873
          },
          {
            "accuracy": 0.538613,
            "f1": 0.499353,
            "f1_weighted": 0.544008
          },
          {
            "accuracy": 0.5091,
            "f1": 0.461564,
            "f1_weighted": 0.508376
          },
          {
            "accuracy": 0.519921,
            "f1": 0.478182,
            "f1_weighted": 0.527377
          },
          {
            "accuracy": 0.515002,
            "f1": 0.464542,
            "f1_weighted": 0.519138
          }
        ],
        "main_score": 0.527398,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.514257,
        "f1": 0.473227,
        "f1_weighted": 0.521566,
        "scores_per_experiment": [
          {
            "accuracy": 0.52152,
            "f1": 0.478613,
            "f1_weighted": 0.53088
          },
          {
            "accuracy": 0.532952,
            "f1": 0.484956,
            "f1_weighted": 0.541799
          },
          {
            "accuracy": 0.512105,
            "f1": 0.471016,
            "f1_weighted": 0.518644
          },
          {
            "accuracy": 0.508742,
            "f1": 0.461321,
            "f1_weighted": 0.520834
          },
          {
            "accuracy": 0.517821,
            "f1": 0.482204,
            "f1_weighted": 0.522125
          },
          {
            "accuracy": 0.507734,
            "f1": 0.46648,
            "f1_weighted": 0.511279
          },
          {
            "accuracy": 0.51345,
            "f1": 0.476604,
            "f1_weighted": 0.523808
          },
          {
            "accuracy": 0.515804,
            "f1": 0.478555,
            "f1_weighted": 0.518211
          },
          {
            "accuracy": 0.50807,
            "f1": 0.467916,
            "f1_weighted": 0.517352
          },
          {
            "accuracy": 0.504371,
            "f1": 0.464602,
            "f1_weighted": 0.510723
          }
        ],
        "main_score": 0.514257,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 24.518524408340454,
  "kg_co2_emissions": null
}