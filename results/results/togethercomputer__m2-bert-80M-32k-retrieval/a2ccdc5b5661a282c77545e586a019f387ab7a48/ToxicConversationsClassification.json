{
  "dataset_revision": "edfaf9da55d3dd50d43143d90c1ac476895ae6de",
  "task_name": "ToxicConversationsClassification",
  "mteb_version": "1.38.39",
  "scores": {
    "test": [
      {
        "accuracy": 0.531104,
        "f1": 0.401719,
        "f1_weighted": 0.623477,
        "ap": 0.080732,
        "ap_weighted": 0.080732,
        "scores_per_experiment": [
          {
            "accuracy": 0.476562,
            "f1": 0.383418,
            "f1_weighted": 0.585153,
            "ap": 0.081028,
            "ap_weighted": 0.081028
          },
          {
            "accuracy": 0.481934,
            "f1": 0.383312,
            "f1_weighted": 0.590911,
            "ap": 0.078907,
            "ap_weighted": 0.078907
          },
          {
            "accuracy": 0.682129,
            "f1": 0.473604,
            "f1_weighted": 0.7525,
            "ap": 0.082885,
            "ap_weighted": 0.082885
          },
          {
            "accuracy": 0.37793,
            "f1": 0.329956,
            "f1_weighted": 0.480881,
            "ap": 0.082835,
            "ap_weighted": 0.082835
          },
          {
            "accuracy": 0.305664,
            "f1": 0.281176,
            "f1_weighted": 0.392861,
            "ap": 0.081495,
            "ap_weighted": 0.081495
          },
          {
            "accuracy": 0.527344,
            "f1": 0.394952,
            "f1_weighted": 0.633202,
            "ap": 0.074068,
            "ap_weighted": 0.074068
          },
          {
            "accuracy": 0.688477,
            "f1": 0.485684,
            "f1_weighted": 0.757546,
            "ap": 0.088375,
            "ap_weighted": 0.088375
          },
          {
            "accuracy": 0.693848,
            "f1": 0.461658,
            "f1_weighted": 0.759275,
            "ap": 0.077023,
            "ap_weighted": 0.077023
          },
          {
            "accuracy": 0.568848,
            "f1": 0.429924,
            "f1_weighted": 0.666822,
            "ap": 0.083331,
            "ap_weighted": 0.083331
          },
          {
            "accuracy": 0.508301,
            "f1": 0.393506,
            "f1_weighted": 0.615623,
            "ap": 0.077377,
            "ap_weighted": 0.077377
          }
        ],
        "main_score": 0.531104,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 20.251418113708496,
  "kg_co2_emissions": null
}