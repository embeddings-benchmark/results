{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.777921,
        "f1": 0.657646,
        "f1_weighted": 0.808428,
        "ap": 0.278501,
        "ap_weighted": 0.278501,
        "scores_per_experiment": [
          {
            "accuracy": 0.706186,
            "f1": 0.584458,
            "f1_weighted": 0.752945,
            "ap": 0.205644,
            "ap_weighted": 0.205644
          },
          {
            "accuracy": 0.811856,
            "f1": 0.676456,
            "f1_weighted": 0.833253,
            "ap": 0.281156,
            "ap_weighted": 0.281156
          },
          {
            "accuracy": 0.833333,
            "f1": 0.718209,
            "f1_weighted": 0.85314,
            "ap": 0.347544,
            "ap_weighted": 0.347544
          },
          {
            "accuracy": 0.823883,
            "f1": 0.707096,
            "f1_weighted": 0.845652,
            "ap": 0.332806,
            "ap_weighted": 0.332806
          },
          {
            "accuracy": 0.798969,
            "f1": 0.658839,
            "f1_weighted": 0.822637,
            "ap": 0.259526,
            "ap_weighted": 0.259526
          },
          {
            "accuracy": 0.761168,
            "f1": 0.645961,
            "f1_weighted": 0.797258,
            "ap": 0.26846,
            "ap_weighted": 0.26846
          },
          {
            "accuracy": 0.845361,
            "f1": 0.702918,
            "f1_weighted": 0.857025,
            "ap": 0.304881,
            "ap_weighted": 0.304881
          },
          {
            "accuracy": 0.652921,
            "f1": 0.575156,
            "f1_weighted": 0.711322,
            "ap": 0.238927,
            "ap_weighted": 0.238927
          },
          {
            "accuracy": 0.74055,
            "f1": 0.627118,
            "f1_weighted": 0.781187,
            "ap": 0.251345,
            "ap_weighted": 0.251345
          },
          {
            "accuracy": 0.804983,
            "f1": 0.680253,
            "f1_weighted": 0.82986,
            "ap": 0.294721,
            "ap_weighted": 0.294721
          }
        ],
        "main_score": 0.777921,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 7.211871385574341,
  "kg_co2_emissions": 0.0002191511354769642
}