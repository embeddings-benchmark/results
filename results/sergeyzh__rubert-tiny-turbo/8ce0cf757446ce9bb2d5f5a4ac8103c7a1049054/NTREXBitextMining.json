{
  "dataset_revision": "ed9a4403ed4adbfaf4aab56d5b2709e9f6c3ba33",
  "evaluation_time": 22.243237733840942,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test": [
      {
        "accuracy": 0.011016524787180772,
        "f1": 0.01026607708172428,
        "hf_subset": "arb_Arab-rus_Cyrl",
        "languages": [
          "arb-Arab",
          "rus-Cyrl"
        ],
        "main_score": 0.01026607708172428,
        "precision": 0.010182279298640406,
        "recall": 0.011016524787180772
      },
      {
        "accuracy": 0.4041061592388583,
        "f1": 0.3601826179025213,
        "hf_subset": "bel_Cyrl-rus_Cyrl",
        "languages": [
          "bel-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.3601826179025213,
        "precision": 0.3469362649220603,
        "recall": 0.4041061592388583
      },
      {
        "accuracy": 0.004506760140210316,
        "f1": 0.003154522233882181,
        "hf_subset": "ben_Beng-rus_Cyrl",
        "languages": [
          "ben-Beng",
          "rus-Cyrl"
        ],
        "main_score": 0.003154522233882181,
        "precision": 0.002956197116187101,
        "recall": 0.004506760140210316
      },
      {
        "accuracy": 0.10665998998497747,
        "f1": 0.09642511942512368,
        "hf_subset": "bos_Latn-rus_Cyrl",
        "languages": [
          "bos-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.09642511942512368,
        "precision": 0.09394038067089021,
        "recall": 0.10665998998497747
      },
      {
        "accuracy": 0.7536304456685028,
        "f1": 0.7131041950871695,
        "hf_subset": "bul_Cyrl-rus_Cyrl",
        "languages": [
          "bul-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.7131041950871695,
        "precision": 0.6984429853336154,
        "recall": 0.7536304456685028
      },
      {
        "accuracy": 0.14171256885327993,
        "f1": 0.12486120915469716,
        "hf_subset": "ces_Latn-rus_Cyrl",
        "languages": [
          "ces-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.12486120915469716,
        "precision": 0.1205728723322858,
        "recall": 0.14171256885327993
      },
      {
        "accuracy": 0.15923885828743115,
        "f1": 0.13857691224889288,
        "hf_subset": "deu_Latn-rus_Cyrl",
        "languages": [
          "deu-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.13857691224889288,
        "precision": 0.13314504129391372,
        "recall": 0.15923885828743115
      },
      {
        "accuracy": 0.06760140210315473,
        "f1": 0.06082906579570555,
        "hf_subset": "ell_Grek-rus_Cyrl",
        "languages": [
          "ell-Grek",
          "rus-Cyrl"
        ],
        "main_score": 0.06082906579570555,
        "precision": 0.058795502984080095,
        "recall": 0.06760140210315473
      },
      {
        "accuracy": 0.9344016024036054,
        "f1": 0.9155328230440899,
        "hf_subset": "eng_Latn-rus_Cyrl",
        "languages": [
          "eng-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.9155328230440899,
        "precision": 0.9070689367384409,
        "recall": 0.9344016024036054
      },
      {
        "accuracy": 0.07260891337005508,
        "f1": 0.06735866803301364,
        "hf_subset": "fas_Arab-rus_Cyrl",
        "languages": [
          "fas-Arab",
          "rus-Cyrl"
        ],
        "main_score": 0.06735866803301364,
        "precision": 0.06594560009130215,
        "recall": 0.07260891337005508
      },
      {
        "accuracy": 0.10115172759138708,
        "f1": 0.08801048036533808,
        "hf_subset": "fin_Latn-rus_Cyrl",
        "languages": [
          "fin-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.08801048036533808,
        "precision": 0.08483486467399382,
        "recall": 0.10115172759138708
      },
      {
        "accuracy": 0.11467200801201803,
        "f1": 0.10765706607076264,
        "hf_subset": "fra_Latn-rus_Cyrl",
        "languages": [
          "fra-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.10765706607076264,
        "precision": 0.1055064814529095,
        "recall": 0.11467200801201803
      },
      {
        "accuracy": 0.03605408112168253,
        "f1": 0.031943739119601924,
        "hf_subset": "heb_Hebr-rus_Cyrl",
        "languages": [
          "heb-Hebr",
          "rus-Cyrl"
        ],
        "main_score": 0.031943739119601924,
        "precision": 0.03072095911074528,
        "recall": 0.03605408112168253
      },
      {
        "accuracy": 0.02754131196795193,
        "f1": 0.023069581278549398,
        "hf_subset": "hin_Deva-rus_Cyrl",
        "languages": [
          "hin-Deva",
          "rus-Cyrl"
        ],
        "main_score": 0.023069581278549398,
        "precision": 0.02216055160830302,
        "recall": 0.02754131196795193
      },
      {
        "accuracy": 0.11316975463194792,
        "f1": 0.10028803430667248,
        "hf_subset": "hrv_Latn-rus_Cyrl",
        "languages": [
          "hrv-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.10028803430667248,
        "precision": 0.09706919946106933,
        "recall": 0.11316975463194792
      },
      {
        "accuracy": 0.09163745618427642,
        "f1": 0.07842539552996414,
        "hf_subset": "hun_Latn-rus_Cyrl",
        "languages": [
          "hun-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.07842539552996414,
        "precision": 0.07500808378787363,
        "recall": 0.09163745618427642
      },
      {
        "accuracy": 0.128693039559339,
        "f1": 0.10817804239045138,
        "hf_subset": "ind_Latn-rus_Cyrl",
        "languages": [
          "ind-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.10817804239045138,
        "precision": 0.10395048562637482,
        "recall": 0.128693039559339
      },
      {
        "accuracy": 0.013520280420630946,
        "f1": 0.01034804810040204,
        "hf_subset": "jpn_Jpan-rus_Cyrl",
        "languages": [
          "jpn-Jpan",
          "rus-Cyrl"
        ],
        "main_score": 0.01034804810040204,
        "precision": 0.009725295940034257,
        "recall": 0.013520280420630946
      },
      {
        "accuracy": 0.03655483224837256,
        "f1": 0.0332818486083712,
        "hf_subset": "kor_Hang-rus_Cyrl",
        "languages": [
          "kor-Hang",
          "rus-Cyrl"
        ],
        "main_score": 0.0332818486083712,
        "precision": 0.032640975213318185,
        "recall": 0.03655483224837256
      },
      {
        "accuracy": 0.11066599899849774,
        "f1": 0.09427303078663489,
        "hf_subset": "lit_Latn-rus_Cyrl",
        "languages": [
          "lit-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.09427303078663489,
        "precision": 0.09020428774163286,
        "recall": 0.11066599899849774
      },
      {
        "accuracy": 0.6169253880821232,
        "f1": 0.5702078038250892,
        "hf_subset": "mkd_Cyrl-rus_Cyrl",
        "languages": [
          "mkd-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.5702078038250892,
        "precision": 0.5553097275356242,
        "recall": 0.6169253880821232
      },
      {
        "accuracy": 0.19829744616925388,
        "f1": 0.1728935579921588,
        "hf_subset": "nld_Latn-rus_Cyrl",
        "languages": [
          "nld-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.1728935579921588,
        "precision": 0.16559673740902522,
        "recall": 0.19829744616925388
      },
      {
        "accuracy": 0.15072608913370056,
        "f1": 0.13134835090659477,
        "hf_subset": "pol_Latn-rus_Cyrl",
        "languages": [
          "pol-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.13134835090659477,
        "precision": 0.1260518878209358,
        "recall": 0.15072608913370056
      },
      {
        "accuracy": 0.16925388082123186,
        "f1": 0.1521436248814931,
        "hf_subset": "por_Latn-rus_Cyrl",
        "languages": [
          "por-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.1521436248814931,
        "precision": 0.14691124945434886,
        "recall": 0.16925388082123186
      },
      {
        "accuracy": 0.0485728592889334,
        "f1": 0.0315142942044222,
        "hf_subset": "rus_Cyrl-arb_Arab",
        "languages": [
          "rus-Cyrl",
          "arb-Arab"
        ],
        "main_score": 0.0315142942044222,
        "precision": 0.0284017312000739,
        "recall": 0.0485728592889334
      },
      {
        "accuracy": 0.5267901852779169,
        "f1": 0.44837869385941487,
        "hf_subset": "rus_Cyrl-bel_Cyrl",
        "languages": [
          "rus-Cyrl",
          "bel-Cyrl"
        ],
        "main_score": 0.44837869385941487,
        "precision": 0.4211334859432005,
        "recall": 0.5267901852779169
      },
      {
        "accuracy": 0.03755633450175263,
        "f1": 0.02014103656399535,
        "hf_subset": "rus_Cyrl-ben_Beng",
        "languages": [
          "rus-Cyrl",
          "ben-Beng"
        ],
        "main_score": 0.02014103656399535,
        "precision": 0.017085584026485863,
        "recall": 0.03755633450175263
      },
      {
        "accuracy": 0.2684026039058588,
        "f1": 0.20743703184564577,
        "hf_subset": "rus_Cyrl-bos_Latn",
        "languages": [
          "rus-Cyrl",
          "bos-Latn"
        ],
        "main_score": 0.20743703184564577,
        "precision": 0.19063132536651675,
        "recall": 0.2684026039058588
      },
      {
        "accuracy": 0.7986980470706059,
        "f1": 0.752680210792379,
        "hf_subset": "rus_Cyrl-bul_Cyrl",
        "languages": [
          "rus-Cyrl",
          "bul-Cyrl"
        ],
        "main_score": 0.752680210792379,
        "precision": 0.7339886994994657,
        "recall": 0.7986980470706059
      },
      {
        "accuracy": 0.29744616925388084,
        "f1": 0.2289004056344866,
        "hf_subset": "rus_Cyrl-ces_Latn",
        "languages": [
          "rus-Cyrl",
          "ces-Latn"
        ],
        "main_score": 0.2289004056344866,
        "precision": 0.20966447084793088,
        "recall": 0.29744616925388084
      },
      {
        "accuracy": 0.33199799699549326,
        "f1": 0.2550472812570392,
        "hf_subset": "rus_Cyrl-deu_Latn",
        "languages": [
          "rus-Cyrl",
          "deu-Latn"
        ],
        "main_score": 0.2550472812570392,
        "precision": 0.23397983542128606,
        "recall": 0.33199799699549326
      },
      {
        "accuracy": 0.10165247871807712,
        "f1": 0.06291604887123822,
        "hf_subset": "rus_Cyrl-ell_Grek",
        "languages": [
          "rus-Cyrl",
          "ell-Grek"
        ],
        "main_score": 0.06291604887123822,
        "precision": 0.056145076309965965,
        "recall": 0.10165247871807712
      },
      {
        "accuracy": 0.9609414121181773,
        "f1": 0.9488232348522784,
        "hf_subset": "rus_Cyrl-eng_Latn",
        "languages": [
          "rus-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.9488232348522784,
        "precision": 0.9430812885995661,
        "recall": 0.9609414121181773
      },
      {
        "accuracy": 0.12468703054581873,
        "f1": 0.07725440527290363,
        "hf_subset": "rus_Cyrl-fas_Arab",
        "languages": [
          "rus-Cyrl",
          "fas-Arab"
        ],
        "main_score": 0.07725440527290363,
        "precision": 0.06855489818689751,
        "recall": 0.12468703054581873
      },
      {
        "accuracy": 0.20681021532298446,
        "f1": 0.1541174070237473,
        "hf_subset": "rus_Cyrl-fin_Latn",
        "languages": [
          "rus-Cyrl",
          "fin-Latn"
        ],
        "main_score": 0.1541174070237473,
        "precision": 0.1408831754937445,
        "recall": 0.20681021532298446
      },
      {
        "accuracy": 0.38507761642463695,
        "f1": 0.3166680668114315,
        "hf_subset": "rus_Cyrl-fra_Latn",
        "languages": [
          "rus-Cyrl",
          "fra-Latn"
        ],
        "main_score": 0.3166680668114315,
        "precision": 0.29562382221605965,
        "recall": 0.38507761642463695
      },
      {
        "accuracy": 0.0756134201301953,
        "f1": 0.048891336274641324,
        "hf_subset": "rus_Cyrl-heb_Hebr",
        "languages": [
          "rus-Cyrl",
          "heb-Hebr"
        ],
        "main_score": 0.048891336274641324,
        "precision": 0.04458490061822524,
        "recall": 0.0756134201301953
      },
      {
        "accuracy": 0.06059088632949424,
        "f1": 0.038499970182610664,
        "hf_subset": "rus_Cyrl-hin_Deva",
        "languages": [
          "rus-Cyrl",
          "hin-Deva"
        ],
        "main_score": 0.038499970182610664,
        "precision": 0.03470860909459424,
        "recall": 0.06059088632949424
      },
      {
        "accuracy": 0.27441161742613923,
        "f1": 0.211611483622573,
        "hf_subset": "rus_Cyrl-hrv_Latn",
        "languages": [
          "rus-Cyrl",
          "hrv-Latn"
        ],
        "main_score": 0.211611483622573,
        "precision": 0.19493061409486745,
        "recall": 0.27441161742613923
      },
      {
        "accuracy": 0.214321482223335,
        "f1": 0.15736743771874123,
        "hf_subset": "rus_Cyrl-hun_Latn",
        "languages": [
          "rus-Cyrl",
          "hun-Latn"
        ],
        "main_score": 0.15736743771874123,
        "precision": 0.14357373841331858,
        "recall": 0.214321482223335
      },
      {
        "accuracy": 0.21782674011016526,
        "f1": 0.1575490993225256,
        "hf_subset": "rus_Cyrl-ind_Latn",
        "languages": [
          "rus-Cyrl",
          "ind-Latn"
        ],
        "main_score": 0.1575490993225256,
        "precision": 0.142137341011753,
        "recall": 0.21782674011016526
      },
      {
        "accuracy": 0.04807210816224337,
        "f1": 0.03126130669193548,
        "hf_subset": "rus_Cyrl-jpn_Jpan",
        "languages": [
          "rus-Cyrl",
          "jpn-Jpan"
        ],
        "main_score": 0.03126130669193548,
        "precision": 0.028076232792486878,
        "recall": 0.04807210816224337
      },
      {
        "accuracy": 0.08412618928392589,
        "f1": 0.042104578480222185,
        "hf_subset": "rus_Cyrl-kor_Hang",
        "languages": [
          "rus-Cyrl",
          "kor-Hang"
        ],
        "main_score": 0.042104578480222185,
        "precision": 0.03410036482401689,
        "recall": 0.08412618928392589
      },
      {
        "accuracy": 0.22934401602403606,
        "f1": 0.1728942237863367,
        "hf_subset": "rus_Cyrl-lit_Latn",
        "languages": [
          "rus-Cyrl",
          "lit-Latn"
        ],
        "main_score": 0.1728942237863367,
        "precision": 0.15832396964439358,
        "recall": 0.22934401602403606
      },
      {
        "accuracy": 0.6960440660991487,
        "f1": 0.6309529770846746,
        "hf_subset": "rus_Cyrl-mkd_Cyrl",
        "languages": [
          "rus-Cyrl",
          "mkd-Cyrl"
        ],
        "main_score": 0.6309529770846746,
        "precision": 0.6063335081988062,
        "recall": 0.6960440660991487
      },
      {
        "accuracy": 0.37356034051076614,
        "f1": 0.29477275939969205,
        "hf_subset": "rus_Cyrl-nld_Latn",
        "languages": [
          "rus-Cyrl",
          "nld-Latn"
        ],
        "main_score": 0.29477275939969205,
        "precision": 0.2714967968623553,
        "recall": 0.37356034051076614
      },
      {
        "accuracy": 0.27641462193289934,
        "f1": 0.2091567172814688,
        "hf_subset": "rus_Cyrl-pol_Latn",
        "languages": [
          "rus-Cyrl",
          "pol-Latn"
        ],
        "main_score": 0.2091567172814688,
        "precision": 0.19165822940014396,
        "recall": 0.27641462193289934
      },
      {
        "accuracy": 0.3600400600901352,
        "f1": 0.28412785210613534,
        "hf_subset": "rus_Cyrl-por_Latn",
        "languages": [
          "rus-Cyrl",
          "por-Latn"
        ],
        "main_score": 0.28412785210613534,
        "precision": 0.26167506671262303,
        "recall": 0.3600400600901352
      },
      {
        "accuracy": 0.27541311967951926,
        "f1": 0.2111793007729722,
        "hf_subset": "rus_Cyrl-slk_Latn",
        "languages": [
          "rus-Cyrl",
          "slk-Latn"
        ],
        "main_score": 0.2111793007729722,
        "precision": 0.19424331163986922,
        "recall": 0.27541311967951926
      },
      {
        "accuracy": 0.2583875813720581,
        "f1": 0.19315111562633386,
        "hf_subset": "rus_Cyrl-slv_Latn",
        "languages": [
          "rus-Cyrl",
          "slv-Latn"
        ],
        "main_score": 0.19315111562633386,
        "precision": 0.17560976061679787,
        "recall": 0.2583875813720581
      },
      {
        "accuracy": 0.3925888833249875,
        "f1": 0.32378067874798744,
        "hf_subset": "rus_Cyrl-spa_Latn",
        "languages": [
          "rus-Cyrl",
          "spa-Latn"
        ],
        "main_score": 0.32378067874798744,
        "precision": 0.3030668458813967,
        "recall": 0.3925888833249875
      },
      {
        "accuracy": 0.513770655983976,
        "f1": 0.4377609565641613,
        "hf_subset": "rus_Cyrl-srp_Cyrl",
        "languages": [
          "rus-Cyrl",
          "srp-Cyrl"
        ],
        "main_score": 0.4377609565641613,
        "precision": 0.41179524726094585,
        "recall": 0.513770655983976
      },
      {
        "accuracy": 0.24486730095142714,
        "f1": 0.18801417385016175,
        "hf_subset": "rus_Cyrl-srp_Latn",
        "languages": [
          "rus-Cyrl",
          "srp-Latn"
        ],
        "main_score": 0.18801417385016175,
        "precision": 0.17244632616396702,
        "recall": 0.24486730095142714
      },
      {
        "accuracy": 0.14872308462694042,
        "f1": 0.09936884000411651,
        "hf_subset": "rus_Cyrl-swa_Latn",
        "languages": [
          "rus-Cyrl",
          "swa-Latn"
        ],
        "main_score": 0.09936884000411651,
        "precision": 0.0884676827028202,
        "recall": 0.14872308462694042
      },
      {
        "accuracy": 0.32098147220831247,
        "f1": 0.24881982251796936,
        "hf_subset": "rus_Cyrl-swe_Latn",
        "languages": [
          "rus-Cyrl",
          "swe-Latn"
        ],
        "main_score": 0.24881982251796936,
        "precision": 0.22851100230075366,
        "recall": 0.32098147220831247
      },
      {
        "accuracy": 0.07661492238357537,
        "f1": 0.04825253356631871,
        "hf_subset": "rus_Cyrl-tam_Taml",
        "languages": [
          "rus-Cyrl",
          "tam-Taml"
        ],
        "main_score": 0.04825253356631871,
        "precision": 0.043359881256298784,
        "recall": 0.07661492238357537
      },
      {
        "accuracy": 0.17826740110165248,
        "f1": 0.12749597958022,
        "hf_subset": "rus_Cyrl-tur_Latn",
        "languages": [
          "rus-Cyrl",
          "tur-Latn"
        ],
        "main_score": 0.12749597958022,
        "precision": 0.11650519852320672,
        "recall": 0.17826740110165248
      },
      {
        "accuracy": 0.7601402103154732,
        "f1": 0.7085346273378321,
        "hf_subset": "rus_Cyrl-ukr_Cyrl",
        "languages": [
          "rus-Cyrl",
          "ukr-Cyrl"
        ],
        "main_score": 0.7085346273378321,
        "precision": 0.6871380483423548,
        "recall": 0.7601402103154732
      },
      {
        "accuracy": 0.2513770655983976,
        "f1": 0.18455622013978587,
        "hf_subset": "rus_Cyrl-vie_Latn",
        "languages": [
          "rus-Cyrl",
          "vie-Latn"
        ],
        "main_score": 0.18455622013978587,
        "precision": 0.16712039917217053,
        "recall": 0.2513770655983976
      },
      {
        "accuracy": 0.2028042063094642,
        "f1": 0.1486836582311921,
        "hf_subset": "rus_Cyrl-zho_Hant",
        "languages": [
          "rus-Cyrl",
          "zho-Hant"
        ],
        "main_score": 0.1486836582311921,
        "precision": 0.13574339449711736,
        "recall": 0.2028042063094642
      },
      {
        "accuracy": 0.18377566349524285,
        "f1": 0.12318562378586237,
        "hf_subset": "rus_Cyrl-zul_Latn",
        "languages": [
          "rus-Cyrl",
          "zul-Latn"
        ],
        "main_score": 0.12318562378586237,
        "precision": 0.1098467335201691,
        "recall": 0.18377566349524285
      },
      {
        "accuracy": 0.12819228843264896,
        "f1": 0.11424635622540097,
        "hf_subset": "slk_Latn-rus_Cyrl",
        "languages": [
          "slk-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.11424635622540097,
        "precision": 0.11055182432472174,
        "recall": 0.12819228843264896
      },
      {
        "accuracy": 0.10565848773159739,
        "f1": 0.09494709997726607,
        "hf_subset": "slv_Latn-rus_Cyrl",
        "languages": [
          "slv-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.09494709997726607,
        "precision": 0.09234398859530481,
        "recall": 0.10565848773159739
      },
      {
        "accuracy": 0.15322984476715074,
        "f1": 0.14089912891290243,
        "hf_subset": "spa_Latn-rus_Cyrl",
        "languages": [
          "spa-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.14089912891290243,
        "precision": 0.13717651677883547,
        "recall": 0.15322984476715074
      },
      {
        "accuracy": 0.4646970455683525,
        "f1": 0.42406403198609227,
        "hf_subset": "srp_Cyrl-rus_Cyrl",
        "languages": [
          "srp-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.42406403198609227,
        "precision": 0.41172516588601427,
        "recall": 0.4646970455683525
      },
      {
        "accuracy": 0.07511266900350526,
        "f1": 0.06803844602116818,
        "hf_subset": "srp_Latn-rus_Cyrl",
        "languages": [
          "srp-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.06803844602116818,
        "precision": 0.06657294606430234,
        "recall": 0.07511266900350526
      },
      {
        "accuracy": 0.06359539308963445,
        "f1": 0.05183370982249626,
        "hf_subset": "swa_Latn-rus_Cyrl",
        "languages": [
          "swa-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.05183370982249626,
        "precision": 0.049190545407064676,
        "recall": 0.06359539308963445
      },
      {
        "accuracy": 0.1757636454682023,
        "f1": 0.1562604569717593,
        "hf_subset": "swe_Latn-rus_Cyrl",
        "languages": [
          "swe-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.1562604569717593,
        "precision": 0.1507788635413041,
        "recall": 0.1757636454682023
      },
      {
        "accuracy": 0.03104656985478217,
        "f1": 0.026527481823910716,
        "hf_subset": "tam_Taml-rus_Cyrl",
        "languages": [
          "tam-Taml",
          "rus-Cyrl"
        ],
        "main_score": 0.026527481823910716,
        "precision": 0.025405543403502324,
        "recall": 0.03104656985478217
      },
      {
        "accuracy": 0.0756134201301953,
        "f1": 0.06490872264639297,
        "hf_subset": "tur_Latn-rus_Cyrl",
        "languages": [
          "tur-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.06490872264639297,
        "precision": 0.06205791353784073,
        "recall": 0.0756134201301953
      },
      {
        "accuracy": 0.6745117676514772,
        "f1": 0.6355606562854506,
        "hf_subset": "ukr_Cyrl-rus_Cyrl",
        "languages": [
          "ukr-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.6355606562854506,
        "precision": 0.6215354748650382,
        "recall": 0.6745117676514772
      },
      {
        "accuracy": 0.13620430645968953,
        "f1": 0.11953952396808747,
        "hf_subset": "vie_Latn-rus_Cyrl",
        "languages": [
          "vie-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.11953952396808747,
        "precision": 0.11534645528155403,
        "recall": 0.13620430645968953
      },
      {
        "accuracy": 0.12018027040560841,
        "f1": 0.10892789905238733,
        "hf_subset": "zho_Hant-rus_Cyrl",
        "languages": [
          "zho-Hant",
          "rus-Cyrl"
        ],
        "main_score": 0.10892789905238733,
        "precision": 0.1053954167582944,
        "recall": 0.12018027040560841
      },
      {
        "accuracy": 0.10115172759138708,
        "f1": 0.0865281753915575,
        "hf_subset": "zul_Latn-rus_Cyrl",
        "languages": [
          "zul-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0865281753915575,
        "precision": 0.0834218625192423,
        "recall": 0.10115172759138708
      }
    ]
  },
  "task_name": "NTREXBitextMining"
}