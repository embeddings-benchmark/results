{
  "dataset_revision": "424d0f767aaa5c411e3a529eec04658e5726a39e",
  "task_name": "RuNLUIntentClassification",
  "mteb_version": "1.38.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.4724,
        "f1": 0.392756,
        "f1_weighted": 0.455779,
        "scores_per_experiment": [
          {
            "accuracy": 0.4464,
            "f1": 0.368174,
            "f1_weighted": 0.434366
          },
          {
            "accuracy": 0.4618,
            "f1": 0.388816,
            "f1_weighted": 0.449327
          },
          {
            "accuracy": 0.4648,
            "f1": 0.377578,
            "f1_weighted": 0.448245
          },
          {
            "accuracy": 0.4958,
            "f1": 0.419202,
            "f1_weighted": 0.486174
          },
          {
            "accuracy": 0.4756,
            "f1": 0.404469,
            "f1_weighted": 0.452901
          },
          {
            "accuracy": 0.4906,
            "f1": 0.398144,
            "f1_weighted": 0.481159
          },
          {
            "accuracy": 0.5142,
            "f1": 0.419423,
            "f1_weighted": 0.50309
          },
          {
            "accuracy": 0.4686,
            "f1": 0.393683,
            "f1_weighted": 0.444535
          },
          {
            "accuracy": 0.4524,
            "f1": 0.391621,
            "f1_weighted": 0.432915
          },
          {
            "accuracy": 0.4538,
            "f1": 0.366447,
            "f1_weighted": 0.425076
          }
        ],
        "main_score": 0.4724,
        "hf_subset": "rus-eng",
        "languages": [
          "rus-Cyrl",
          "rus-Latn"
        ]
      },
      {
        "accuracy": 0.56762,
        "f1": 0.500122,
        "f1_weighted": 0.545324,
        "scores_per_experiment": [
          {
            "accuracy": 0.556,
            "f1": 0.487812,
            "f1_weighted": 0.533283
          },
          {
            "accuracy": 0.5606,
            "f1": 0.489978,
            "f1_weighted": 0.533402
          },
          {
            "accuracy": 0.5554,
            "f1": 0.496122,
            "f1_weighted": 0.530499
          },
          {
            "accuracy": 0.5734,
            "f1": 0.491798,
            "f1_weighted": 0.552881
          },
          {
            "accuracy": 0.5622,
            "f1": 0.50382,
            "f1_weighted": 0.539404
          },
          {
            "accuracy": 0.574,
            "f1": 0.499504,
            "f1_weighted": 0.556207
          },
          {
            "accuracy": 0.6064,
            "f1": 0.527689,
            "f1_weighted": 0.593617
          },
          {
            "accuracy": 0.5708,
            "f1": 0.508466,
            "f1_weighted": 0.548251
          },
          {
            "accuracy": 0.5522,
            "f1": 0.493897,
            "f1_weighted": 0.521955
          },
          {
            "accuracy": 0.5652,
            "f1": 0.502135,
            "f1_weighted": 0.543745
          }
        ],
        "main_score": 0.56762,
        "hf_subset": "rus",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 28.75963258743286,
  "kg_co2_emissions": null
}