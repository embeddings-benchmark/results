{
  "dataset_revision": "9af5657575a669dc18c7f897a67287ff7d1a0c65",
  "task_name": "OpenTenderClassification",
  "mteb_version": "2.1.17",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.16767,
            "f1": 0.163797,
            "f1_weighted": 0.163818,
            "precision": 0.168036,
            "precision_weighted": 0.16807,
            "recall": 0.167659,
            "recall_weighted": 0.16767,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.153177,
            "f1": 0.155477,
            "f1_weighted": 0.155447,
            "precision": 0.164587,
            "precision_weighted": 0.164545,
            "recall": 0.153202,
            "recall_weighted": 0.153177,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.15942,
            "f1": 0.158133,
            "f1_weighted": 0.158137,
            "precision": 0.163312,
            "precision_weighted": 0.163327,
            "recall": 0.159434,
            "recall_weighted": 0.15942,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.153846,
            "f1": 0.15378,
            "f1_weighted": 0.153819,
            "precision": 0.157764,
            "precision_weighted": 0.157803,
            "recall": 0.153808,
            "recall_weighted": 0.153846,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.152508,
            "f1": 0.150663,
            "f1_weighted": 0.150711,
            "precision": 0.154282,
            "precision_weighted": 0.154301,
            "recall": 0.152446,
            "recall_weighted": 0.152508,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.162542,
            "f1": 0.158658,
            "f1_weighted": 0.158612,
            "precision": 0.159777,
            "precision_weighted": 0.15976,
            "recall": 0.162616,
            "recall_weighted": 0.162542,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.154069,
            "f1": 0.153713,
            "f1_weighted": 0.153703,
            "precision": 0.161037,
            "precision_weighted": 0.160988,
            "recall": 0.154054,
            "recall_weighted": 0.154069,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.142252,
            "f1": 0.142358,
            "f1_weighted": 0.142332,
            "precision": 0.148576,
            "precision_weighted": 0.148572,
            "recall": 0.142301,
            "recall_weighted": 0.142252,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.152062,
            "f1": 0.150801,
            "f1_weighted": 0.150789,
            "precision": 0.152958,
            "precision_weighted": 0.152925,
            "recall": 0.152055,
            "recall_weighted": 0.152062,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.171014,
            "f1": 0.168855,
            "f1_weighted": 0.168866,
            "precision": 0.175761,
            "precision_weighted": 0.175771,
            "recall": 0.170999,
            "recall_weighted": 0.171014,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.156856,
        "f1": 0.155624,
        "f1_weighted": 0.155623,
        "precision": 0.160609,
        "precision_weighted": 0.160606,
        "recall": 0.156857,
        "recall_weighted": 0.156856,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.155624,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 29.050243139266968,
  "kg_co2_emissions": null
}