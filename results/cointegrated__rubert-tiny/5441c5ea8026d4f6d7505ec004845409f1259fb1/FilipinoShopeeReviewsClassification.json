{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.246582,
        "f1": 0.242494,
        "f1_weighted": 0.242485,
        "scores_per_experiment": [
          {
            "accuracy": 0.243652,
            "f1": 0.240956,
            "f1_weighted": 0.240914
          },
          {
            "accuracy": 0.24707,
            "f1": 0.237742,
            "f1_weighted": 0.237704
          },
          {
            "accuracy": 0.228516,
            "f1": 0.227379,
            "f1_weighted": 0.227355
          },
          {
            "accuracy": 0.258789,
            "f1": 0.257332,
            "f1_weighted": 0.257312
          },
          {
            "accuracy": 0.216797,
            "f1": 0.21271,
            "f1_weighted": 0.212704
          },
          {
            "accuracy": 0.23584,
            "f1": 0.232064,
            "f1_weighted": 0.232117
          },
          {
            "accuracy": 0.235352,
            "f1": 0.230321,
            "f1_weighted": 0.230326
          },
          {
            "accuracy": 0.285645,
            "f1": 0.285841,
            "f1_weighted": 0.285836
          },
          {
            "accuracy": 0.25293,
            "f1": 0.25075,
            "f1_weighted": 0.250734
          },
          {
            "accuracy": 0.26123,
            "f1": 0.249847,
            "f1_weighted": 0.249849
          }
        ],
        "main_score": 0.246582,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.254688,
        "f1": 0.251306,
        "f1_weighted": 0.251302,
        "scores_per_experiment": [
          {
            "accuracy": 0.245117,
            "f1": 0.243573,
            "f1_weighted": 0.243544
          },
          {
            "accuracy": 0.26123,
            "f1": 0.255196,
            "f1_weighted": 0.255185
          },
          {
            "accuracy": 0.227051,
            "f1": 0.226632,
            "f1_weighted": 0.226603
          },
          {
            "accuracy": 0.249512,
            "f1": 0.249148,
            "f1_weighted": 0.24913
          },
          {
            "accuracy": 0.258789,
            "f1": 0.254839,
            "f1_weighted": 0.254824
          },
          {
            "accuracy": 0.242188,
            "f1": 0.238341,
            "f1_weighted": 0.238393
          },
          {
            "accuracy": 0.260742,
            "f1": 0.259694,
            "f1_weighted": 0.259698
          },
          {
            "accuracy": 0.27002,
            "f1": 0.269659,
            "f1_weighted": 0.269672
          },
          {
            "accuracy": 0.27002,
            "f1": 0.266401,
            "f1_weighted": 0.266371
          },
          {
            "accuracy": 0.262207,
            "f1": 0.249577,
            "f1_weighted": 0.249596
          }
        ],
        "main_score": 0.254688,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 14.345828294754028,
  "kg_co2_emissions": 0.00041315734297063913
}