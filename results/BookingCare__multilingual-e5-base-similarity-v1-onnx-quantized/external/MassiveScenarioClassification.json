{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 0.6508742434431742,
                "f1": 0.6304406004231176,
                "main_score": 0.6508742434431742
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 0.5852387357094821,
                "f1": 0.5682398588814533,
                "main_score": 0.5852387357094821
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 0.6223940820443847,
                "f1": 0.6192570286170469,
                "main_score": 0.6223940820443847
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 0.6374915938130463,
                "f1": 0.6213074068939628,
                "main_score": 0.6374915938130463
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 0.6500336247478145,
                "f1": 0.6371080635228055,
                "main_score": 0.6500336247478145
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 0.5283792871553464,
                "f1": 0.5039074168032084,
                "main_score": 0.5283792871553464
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 0.7242098184263618,
                "f1": 0.7141355113538995,
                "main_score": 0.7242098184263618
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 0.7195359784801613,
                "f1": 0.7142699340156742,
                "main_score": 0.7195359784801613
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 0.7018157363819771,
                "f1": 0.6974836113037671,
                "main_score": 0.7018157363819771
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 0.7708137188971083,
                "f1": 0.7678000685068261,
                "main_score": 0.7708137188971083
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 0.7150302622730329,
                "f1": 0.7171620130425673,
                "main_score": 0.7150302622730329
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 0.7024546065904506,
                "f1": 0.690763831173036,
                "main_score": 0.7024546065904506
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 0.6912911903160726,
                "f1": 0.6832651736539814,
                "main_score": 0.6912911903160726
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 0.7189307330195025,
                "f1": 0.7133986549860186,
                "main_score": 0.7189307330195025
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 0.6744451916610626,
                "f1": 0.6690192664503866,
                "main_score": 0.6744451916610626
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 0.6916274377942166,
                "f1": 0.6801090953775066,
                "main_score": 0.6916274377942166
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 0.7075319435104237,
                "f1": 0.7018035309201403,
                "main_score": 0.7075319435104237
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 0.6314391392064559,
                "f1": 0.6148286540778145,
                "main_score": 0.6314391392064559
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 0.7070275722932078,
                "f1": 0.7026164779846495,
                "main_score": 0.7070275722932078
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 0.6093813046402153,
                "f1": 0.588852862116525,
                "main_score": 0.6093813046402153
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 0.72320107599193,
                "f1": 0.7219836409602923,
                "main_score": 0.72320107599193
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 0.7465366509751177,
                "f1": 0.7455188288799579,
                "main_score": 0.7465366509751177
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 0.5969401479488904,
                "f1": 0.5811353311721067,
                "main_score": 0.5969401479488904
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 0.5437457969065231,
                "f1": 0.5281306134311697,
                "main_score": 0.5437457969065231
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 0.483086751849361,
                "f1": 0.4539644976541938,
                "main_score": 0.483086751849361
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 0.6215198386012106,
                "f1": 0.6031762544281696,
                "main_score": 0.6215198386012106
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 0.7244788164088769,
                "f1": 0.7168150151736367,
                "main_score": 0.7244788164088769
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 0.6281439139206455,
                "f1": 0.6206735559105593,
                "main_score": 0.6281439139206455
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 0.6804303967720241,
                "f1": 0.6668298851670132,
                "main_score": 0.6804303967720241
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 0.6143913920645595,
                "f1": 0.6025605977560783,
                "main_score": 0.6143913920645595
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 0.6690316072629456,
                "f1": 0.651325924692381,
                "main_score": 0.6690316072629456
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 0.6163752521856086,
                "f1": 0.5914284778039585,
                "main_score": 0.6163752521856086
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 0.7163080026899797,
                "f1": 0.7089771864626877,
                "main_score": 0.7163080026899797
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 0.7210827168796234,
                "f1": 0.7171954219691159,
                "main_score": 0.7210827168796234
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 0.7059515803631471,
                "f1": 0.7005040128099003,
                "main_score": 0.7059515803631471
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 0.7083389374579692,
                "f1": 0.7084877936562735,
                "main_score": 0.7083389374579692
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 0.6918628110289172,
                "f1": 0.6897232927921841,
                "main_score": 0.6918628110289172
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 0.7299260255548083,
                "f1": 0.7285139492157733,
                "main_score": 0.7299260255548083
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 0.6526227303295226,
                "f1": 0.650883365546943,
                "main_score": 0.6526227303295226
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 0.664862138533961,
                "f1": 0.6443483199071298,
                "main_score": 0.664862138533961
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 0.7314391392064559,
                "f1": 0.722580822579741,
                "main_score": 0.7314391392064559
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 0.5988567585743106,
                "f1": 0.583073765932569,
                "main_score": 0.5988567585743106
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 0.6238399462004034,
                "f1": 0.6082139544252606,
                "main_score": 0.6238399462004034
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 0.6258574310692671,
                "f1": 0.6071443370385374,
                "main_score": 0.6258574310692671
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 0.7161398789509079,
                "f1": 0.7099761812049401,
                "main_score": 0.7161398789509079
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 0.6273705447209146,
                "f1": 0.616808493317948,
                "main_score": 0.6273705447209146
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 0.7166778749159382,
                "f1": 0.7117320646080115,
                "main_score": 0.7166778749159382
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 0.64640215198386,
                "f1": 0.6330180515701544,
                "main_score": 0.64640215198386
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 0.7000672494956288,
                "f1": 0.7026005548582106,
                "main_score": 0.7000672494956288
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 0.7542030934767989,
                "f1": 0.7520748428825981,
                "main_score": 0.7542030934767989
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 0.7069266980497646,
                "f1": 0.7094103167391193,
                "main_score": 0.7069266980497646
            }
        ]
    }
}