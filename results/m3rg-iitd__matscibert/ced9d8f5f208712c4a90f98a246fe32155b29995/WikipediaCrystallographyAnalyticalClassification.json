{
  "dataset_revision": "740565a6a853aaed1114a13bdfd5fd46857b4f11",
  "task_name": "WikipediaCrystallographyAnalyticalClassification",
  "mteb_version": "1.26.4",
  "scores": {
    "test": [
      {
        "accuracy": 0.875601,
        "f1": 0.874138,
        "f1_weighted": 0.875363,
        "ap": 0.869365,
        "ap_weighted": 0.869365,
        "scores_per_experiment": [
          {
            "accuracy": 0.876289,
            "f1": 0.875214,
            "f1_weighted": 0.876607,
            "ap": 0.8625,
            "ap_weighted": 0.8625
          },
          {
            "accuracy": 0.896907,
            "f1": 0.896141,
            "f1_weighted": 0.897214,
            "ap": 0.888689,
            "ap_weighted": 0.888689
          },
          {
            "accuracy": 0.841924,
            "f1": 0.833076,
            "f1_weighted": 0.837698,
            "ap": 0.789699,
            "ap_weighted": 0.789699
          },
          {
            "accuracy": 0.862543,
            "f1": 0.861954,
            "f1_weighted": 0.863039,
            "ap": 0.856831,
            "ap_weighted": 0.856831
          },
          {
            "accuracy": 0.876289,
            "f1": 0.875214,
            "f1_weighted": 0.876607,
            "ap": 0.8625,
            "ap_weighted": 0.8625
          },
          {
            "accuracy": 0.838488,
            "f1": 0.838457,
            "f1_weighted": 0.83819,
            "ap": 0.860214,
            "ap_weighted": 0.860214
          },
          {
            "accuracy": 0.883162,
            "f1": 0.882147,
            "f1_weighted": 0.883462,
            "ap": 0.870265,
            "ap_weighted": 0.870265
          },
          {
            "accuracy": 0.914089,
            "f1": 0.913759,
            "f1_weighted": 0.914401,
            "ap": 0.919463,
            "ap_weighted": 0.919463
          },
          {
            "accuracy": 0.896907,
            "f1": 0.896012,
            "f1_weighted": 0.897172,
            "ap": 0.886032,
            "ap_weighted": 0.886032
          },
          {
            "accuracy": 0.869416,
            "f1": 0.869402,
            "f1_weighted": 0.86924,
            "ap": 0.897455,
            "ap_weighted": 0.897455
          }
        ],
        "main_score": 0.875601,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 3.053042411804199,
  "kg_co2_emissions": null
}