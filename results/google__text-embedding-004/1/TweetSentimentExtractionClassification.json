{
  "dataset_revision": "d604517c81ca91fe16a244d1248fc021f9ecee7a",
  "task_name": "TweetSentimentExtractionClassification",
  "mteb_version": "1.34.7",
  "scores": {
    "test": [
      {
        "accuracy": 0.737889,
        "f1": 0.739402,
        "f1_weighted": 0.73056,
        "scores_per_experiment": [
          {
            "accuracy": 0.730617,
            "f1": 0.72991,
            "f1_weighted": 0.719251
          },
          {
            "accuracy": 0.733164,
            "f1": 0.734655,
            "f1_weighted": 0.72501
          },
          {
            "accuracy": 0.739955,
            "f1": 0.741685,
            "f1_weighted": 0.733664
          },
          {
            "accuracy": 0.745331,
            "f1": 0.747465,
            "f1_weighted": 0.739505
          },
          {
            "accuracy": 0.734861,
            "f1": 0.736479,
            "f1_weighted": 0.72753
          },
          {
            "accuracy": 0.75382,
            "f1": 0.756329,
            "f1_weighted": 0.749307
          },
          {
            "accuracy": 0.72326,
            "f1": 0.725159,
            "f1_weighted": 0.715917
          },
          {
            "accuracy": 0.741087,
            "f1": 0.742321,
            "f1_weighted": 0.733567
          },
          {
            "accuracy": 0.732598,
            "f1": 0.732689,
            "f1_weighted": 0.722371
          },
          {
            "accuracy": 0.744199,
            "f1": 0.747325,
            "f1_weighted": 0.739482
          }
        ],
        "main_score": 0.737889,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 17.416389226913452,
  "kg_co2_emissions": null
}