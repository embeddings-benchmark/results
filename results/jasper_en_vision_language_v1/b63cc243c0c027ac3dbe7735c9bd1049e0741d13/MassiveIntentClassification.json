{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.21.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.85306,
        "f1": 0.831076,
        "f1_weighted": 0.844685,
        "scores_per_experiment": [
          {
            "accuracy": 0.855077,
            "f1": 0.821182,
            "f1_weighted": 0.844661
          },
          {
            "accuracy": 0.849697,
            "f1": 0.828731,
            "f1_weighted": 0.840694
          },
          {
            "accuracy": 0.841291,
            "f1": 0.823174,
            "f1_weighted": 0.831337
          },
          {
            "accuracy": 0.85844,
            "f1": 0.84116,
            "f1_weighted": 0.849079
          },
          {
            "accuracy": 0.844317,
            "f1": 0.817532,
            "f1_weighted": 0.830675
          },
          {
            "accuracy": 0.85575,
            "f1": 0.843015,
            "f1_weighted": 0.852594
          },
          {
            "accuracy": 0.846335,
            "f1": 0.829128,
            "f1_weighted": 0.835505
          },
          {
            "accuracy": 0.859785,
            "f1": 0.833447,
            "f1_weighted": 0.85091
          },
          {
            "accuracy": 0.852724,
            "f1": 0.828656,
            "f1_weighted": 0.848367
          },
          {
            "accuracy": 0.867182,
            "f1": 0.844731,
            "f1_weighted": 0.863026
          }
        ],
        "main_score": 0.85306,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 37.63224411010742,
  "kg_co2_emissions": null
}