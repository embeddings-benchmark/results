{
    "dataset_revision": "9215a3c954078fd15c2bbecca914477d53944de1",
    "mteb_dataset_name": "TwitterSemEval2015-VN",
    "mteb_version": null,
    "task_name": "TwitterSemEval2015-VN",
    "scores": {
        "test": [
            {
                "evaluation_time": 8.0,
                "hf_subset": "default",
                "dot_accuracy": 0.8061659910390442,
                "dot_ap": 0.586756295714989,
                "dot_f1": 0.5577111503037427,
                "dot_accuracy_threshold": 38356.171875,
                "dot_f1_threshold": 33922.60546875,
                "dot_precision": 0.49035148173673326,
                "dot_recall": 0.6465243071331213,
                "euclidean_accuracy": 0.8240879027096224,
                "euclidean_ap": 0.6551201638295799,
                "euclidean_f1": 0.6036184210526315,
                "euclidean_accuracy_threshold": 124.77159118652344,
                "euclidean_f1_threshold": 143.89285278320312,
                "euclidean_precision": 0.5512579797221179,
                "euclidean_recall": 0.6669695592912313,
                "manhattan_accuracy": 0.8248346490292298,
                "manhattan_ap": 0.6543764611143896,
                "manhattan_f1": 0.6029503428215249,
                "manhattan_accuracy_threshold": 5698.9267578125,
                "manhattan_f1_threshold": 6580.51953125,
                "manhattan_precision": 0.5555130168453293,
                "manhattan_recall": 0.6592457973648341,
                "max_accuracy": 0.8248346490292298,
                "max_ap": 0.6551201638295799,
                "max_f1": 0.6036184210526315,
                "main_score": 0.6551201638295799,
                "languages": [
                    "vie-Latn"
                ]
            }
        ]
    },
    "evaluation_time": null
}