{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "evaluation_time": 147.0128161907196,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.7640625,
        "ap": 0.7114187629824069,
        "ap_weighted": 0.7114187629824069,
        "f1": 0.763516672336194,
        "f1_weighted": 0.763516672336194,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.7640625,
        "scores_per_experiment": [
          {
            "accuracy": 0.7802734375,
            "ap": 0.7301126433165548,
            "ap_weighted": 0.7301126433165548,
            "f1": 0.7793845172743646,
            "f1_weighted": 0.7793845172743646
          },
          {
            "accuracy": 0.775390625,
            "ap": 0.7217431006493507,
            "ap_weighted": 0.7217431006493507,
            "f1": 0.7748538347118182,
            "f1_weighted": 0.7748538347118182
          },
          {
            "accuracy": 0.7763671875,
            "ap": 0.7256689562360179,
            "ap_weighted": 0.7256689562360179,
            "f1": 0.7754624642481311,
            "f1_weighted": 0.7754624642481311
          },
          {
            "accuracy": 0.6171875,
            "ap": 0.5721938467117988,
            "ap_weighted": 0.5721938467117988,
            "f1": 0.61717837282116,
            "f1_weighted": 0.61717837282116
          },
          {
            "accuracy": 0.76318359375,
            "ap": 0.7132120253164557,
            "ap_weighted": 0.7132120253164557,
            "f1": 0.7618192931456145,
            "f1_weighted": 0.7618192931456145
          },
          {
            "accuracy": 0.7744140625,
            "ap": 0.7223177945640177,
            "ap_weighted": 0.7223177945640177,
            "f1": 0.7736626813830322,
            "f1_weighted": 0.7736626813830322
          },
          {
            "accuracy": 0.783203125,
            "ap": 0.7248965992647058,
            "ap_weighted": 0.7248965992647058,
            "f1": 0.783128461241253,
            "f1_weighted": 0.783128461241253
          },
          {
            "accuracy": 0.80224609375,
            "ap": 0.74220889499148,
            "ap_weighted": 0.74220889499148,
            "f1": 0.8022456694152414,
            "f1_weighted": 0.8022456694152414
          },
          {
            "accuracy": 0.7880859375,
            "ap": 0.7336899640690928,
            "ap_weighted": 0.7336899640690928,
            "f1": 0.7877937070016006,
            "f1_weighted": 0.7877937070016006
          },
          {
            "accuracy": 0.7802734375,
            "ap": 0.7281438047045952,
            "ap_weighted": 0.7281438047045952,
            "f1": 0.7796377221197244,
            "f1_weighted": 0.7796377221197244
          }
        ]
      }
    ]
  },
  "task_name": "InappropriatenessClassification"
}