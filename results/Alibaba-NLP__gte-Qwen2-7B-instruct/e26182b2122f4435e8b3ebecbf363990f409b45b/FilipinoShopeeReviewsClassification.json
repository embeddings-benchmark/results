{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 158.7307903766632,
  "kg_co2_emissions": 0.014064496452949604,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.386279296875,
        "f1": 0.3659579620828562,
        "f1_weighted": 0.3659591194401009,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.386279296875,
        "scores_per_experiment": [
          {
            "accuracy": 0.3994140625,
            "f1": 0.37993560946391136,
            "f1_weighted": 0.3799146589957229
          },
          {
            "accuracy": 0.3984375,
            "f1": 0.3654706231199803,
            "f1_weighted": 0.36549225183583356
          },
          {
            "accuracy": 0.37744140625,
            "f1": 0.34991155680030983,
            "f1_weighted": 0.34992571145551216
          },
          {
            "accuracy": 0.39599609375,
            "f1": 0.3833277612931965,
            "f1_weighted": 0.38329917794063384
          },
          {
            "accuracy": 0.38037109375,
            "f1": 0.3375058137144582,
            "f1_weighted": 0.33748644335245814
          },
          {
            "accuracy": 0.38818359375,
            "f1": 0.35346782609037397,
            "f1_weighted": 0.353422544485466
          },
          {
            "accuracy": 0.4296875,
            "f1": 0.40843045888290624,
            "f1_weighted": 0.40840821882878375
          },
          {
            "accuracy": 0.37548828125,
            "f1": 0.3706605273457944,
            "f1_weighted": 0.3706769246070689
          },
          {
            "accuracy": 0.390625,
            "f1": 0.3901959439419795,
            "f1_weighted": 0.3902392968704814
          },
          {
            "accuracy": 0.3271484375,
            "f1": 0.3206735001756513,
            "f1_weighted": 0.320725966029048
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.386083984375,
        "f1": 0.36491845787714405,
        "f1_weighted": 0.36491763360979307,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.386083984375,
        "scores_per_experiment": [
          {
            "accuracy": 0.41162109375,
            "f1": 0.39470372509180346,
            "f1_weighted": 0.39466558315293887
          },
          {
            "accuracy": 0.37841796875,
            "f1": 0.34145713357823604,
            "f1_weighted": 0.3414980439229637
          },
          {
            "accuracy": 0.37841796875,
            "f1": 0.3500778112300642,
            "f1_weighted": 0.3500970762883929
          },
          {
            "accuracy": 0.39306640625,
            "f1": 0.378891649251826,
            "f1_weighted": 0.3788550092597086
          },
          {
            "accuracy": 0.359375,
            "f1": 0.3141044625065105,
            "f1_weighted": 0.3140812297800874
          },
          {
            "accuracy": 0.4033203125,
            "f1": 0.3671046794337968,
            "f1_weighted": 0.3670722897662193
          },
          {
            "accuracy": 0.41552734375,
            "f1": 0.3972485002249882,
            "f1_weighted": 0.3972441202541492
          },
          {
            "accuracy": 0.39306640625,
            "f1": 0.38896088326189043,
            "f1_weighted": 0.3889475948684062
          },
          {
            "accuracy": 0.39306640625,
            "f1": 0.39150101210842514,
            "f1_weighted": 0.3915205036240777
          },
          {
            "accuracy": 0.3349609375,
            "f1": 0.3251347220838998,
            "f1_weighted": 0.32519488518098666
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}