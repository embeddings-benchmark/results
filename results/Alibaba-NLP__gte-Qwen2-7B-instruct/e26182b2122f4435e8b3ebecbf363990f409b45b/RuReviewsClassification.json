{
  "dataset_revision": "f6d2c31f4dc6b88f468552750bfec05b4b41b05a",
  "evaluation_time": 191.76958346366882,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.748486328125,
        "f1": 0.7454639412847763,
        "f1_weighted": 0.7454682073224734,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.748486328125,
        "scores_per_experiment": [
          {
            "accuracy": 0.74951171875,
            "f1": 0.7456733319247455,
            "f1_weighted": 0.7456723362771012
          },
          {
            "accuracy": 0.74169921875,
            "f1": 0.742974691093561,
            "f1_weighted": 0.7430017044526305
          },
          {
            "accuracy": 0.748046875,
            "f1": 0.7412380269844728,
            "f1_weighted": 0.7412380304288668
          },
          {
            "accuracy": 0.7509765625,
            "f1": 0.7484609393238836,
            "f1_weighted": 0.7484685534871924
          },
          {
            "accuracy": 0.7578125,
            "f1": 0.7581913452770745,
            "f1_weighted": 0.7581967751402336
          },
          {
            "accuracy": 0.75,
            "f1": 0.7477398602792508,
            "f1_weighted": 0.7477424508205288
          },
          {
            "accuracy": 0.7451171875,
            "f1": 0.7410496116626378,
            "f1_weighted": 0.7410517362292125
          },
          {
            "accuracy": 0.7568359375,
            "f1": 0.7557545747176411,
            "f1_weighted": 0.7557582060377059
          },
          {
            "accuracy": 0.7314453125,
            "f1": 0.7190902047007594,
            "f1_weighted": 0.7190816360684438
          },
          {
            "accuracy": 0.75341796875,
            "f1": 0.7544668268837377,
            "f1_weighted": 0.7544706442828187
          }
        ]
      }
    ]
  },
  "task_name": "RuReviewsClassification"
}