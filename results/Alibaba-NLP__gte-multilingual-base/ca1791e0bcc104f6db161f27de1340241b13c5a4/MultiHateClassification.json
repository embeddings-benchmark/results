{
  "dataset_revision": "8f95949846bb9e33c6aaf730ccfdb8fe6bcfb7a9",
  "task_name": "MultiHateClassification",
  "mteb_version": "2.1.1",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.644,
            "f1": 0.596845,
            "f1_weighted": 0.652548,
            "precision": 0.594878,
            "precision_weighted": 0.665754,
            "recall": 0.604495,
            "recall_weighted": 0.644,
            "ap": 0.359537,
            "ap_weighted": 0.359537
          },
          {
            "accuracy": 0.49,
            "f1": 0.487769,
            "f1_weighted": 0.501427,
            "precision": 0.548535,
            "precision_weighted": 0.635476,
            "recall": 0.55371,
            "recall_weighted": 0.49,
            "ap": 0.323136,
            "ap_weighted": 0.323136
          },
          {
            "accuracy": 0.598,
            "f1": 0.554029,
            "f1_weighted": 0.610603,
            "precision": 0.555539,
            "precision_weighted": 0.632121,
            "recall": 0.563041,
            "recall_weighted": 0.598,
            "ap": 0.330393,
            "ap_weighted": 0.330393
          },
          {
            "accuracy": 0.59,
            "f1": 0.56625,
            "f1_weighted": 0.607255,
            "precision": 0.577389,
            "precision_weighted": 0.656996,
            "recall": 0.592105,
            "recall_weighted": 0.59,
            "ap": 0.347184,
            "ap_weighted": 0.347184
          },
          {
            "accuracy": 0.523,
            "f1": 0.518656,
            "f1_weighted": 0.53713,
            "precision": 0.569407,
            "precision_weighted": 0.657015,
            "recall": 0.579146,
            "recall_weighted": 0.523,
            "ap": 0.337176,
            "ap_weighted": 0.337176
          },
          {
            "accuracy": 0.587,
            "f1": 0.57813,
            "f1_weighted": 0.602843,
            "precision": 0.611477,
            "precision_weighted": 0.698219,
            "recall": 0.631489,
            "recall_weighted": 0.587,
            "ap": 0.371248,
            "ap_weighted": 0.371248
          },
          {
            "accuracy": 0.527,
            "f1": 0.517406,
            "f1_weighted": 0.544896,
            "precision": 0.552424,
            "precision_weighted": 0.636616,
            "recall": 0.561717,
            "recall_weighted": 0.527,
            "ap": 0.32781,
            "ap_weighted": 0.32781
          },
          {
            "accuracy": 0.582,
            "f1": 0.543038,
            "f1_weighted": 0.596944,
            "precision": 0.547168,
            "precision_weighted": 0.625346,
            "recall": 0.554542,
            "recall_weighted": 0.582,
            "ap": 0.325082,
            "ap_weighted": 0.325082
          },
          {
            "accuracy": 0.664,
            "f1": 0.6262,
            "f1_weighted": 0.674223,
            "precision": 0.623799,
            "precision_weighted": 0.693804,
            "recall": 0.639984,
            "recall_weighted": 0.664,
            "ap": 0.386544,
            "ap_weighted": 0.386544
          },
          {
            "accuracy": 0.619,
            "f1": 0.582141,
            "f1_weighted": 0.632279,
            "precision": 0.583639,
            "precision_weighted": 0.658827,
            "recall": 0.596345,
            "recall_weighted": 0.619,
            "ap": 0.351773,
            "ap_weighted": 0.351773
          }
        ],
        "accuracy": 0.5824,
        "f1": 0.557046,
        "f1_weighted": 0.596015,
        "precision": 0.576426,
        "precision_weighted": 0.656017,
        "recall": 0.587658,
        "recall_weighted": 0.5824,
        "ap": 0.345988,
        "ap_weighted": 0.345988,
        "main_score": 0.5824,
        "hf_subset": "nld",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 9.70898151397705,
  "kg_co2_emissions": null
}