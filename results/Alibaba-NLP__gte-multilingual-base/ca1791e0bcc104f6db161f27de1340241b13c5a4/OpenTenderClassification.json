{
  "dataset_revision": "9af5657575a669dc18c7f897a67287ff7d1a0c65",
  "task_name": "OpenTenderClassification",
  "mteb_version": "2.1.1",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.459532,
            "f1": 0.43737,
            "f1_weighted": 0.437389,
            "precision": 0.444957,
            "precision_weighted": 0.444899,
            "recall": 0.459479,
            "recall_weighted": 0.459532,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.466444,
            "f1": 0.44709,
            "f1_weighted": 0.447018,
            "precision": 0.454594,
            "precision_weighted": 0.454543,
            "recall": 0.466514,
            "recall_weighted": 0.466444,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.463545,
            "f1": 0.441064,
            "f1_weighted": 0.441033,
            "precision": 0.459865,
            "precision_weighted": 0.459859,
            "recall": 0.463589,
            "recall_weighted": 0.463545,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.469565,
            "f1": 0.448647,
            "f1_weighted": 0.448606,
            "precision": 0.458002,
            "precision_weighted": 0.457909,
            "recall": 0.469577,
            "recall_weighted": 0.469565,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.452843,
            "f1": 0.434404,
            "f1_weighted": 0.43438,
            "precision": 0.45043,
            "precision_weighted": 0.450433,
            "recall": 0.452865,
            "recall_weighted": 0.452843,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.435452,
            "f1": 0.415326,
            "f1_weighted": 0.415247,
            "precision": 0.425398,
            "precision_weighted": 0.425366,
            "recall": 0.435585,
            "recall_weighted": 0.435452,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.460647,
            "f1": 0.441486,
            "f1_weighted": 0.441439,
            "precision": 0.456769,
            "precision_weighted": 0.45676,
            "recall": 0.460688,
            "recall_weighted": 0.460647,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.455518,
            "f1": 0.432338,
            "f1_weighted": 0.432284,
            "precision": 0.449145,
            "precision_weighted": 0.449053,
            "recall": 0.45554,
            "recall_weighted": 0.455518,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.439019,
            "f1": 0.416471,
            "f1_weighted": 0.416351,
            "precision": 0.427066,
            "precision_weighted": 0.426911,
            "recall": 0.439085,
            "recall_weighted": 0.439019,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.44214,
            "f1": 0.421755,
            "f1_weighted": 0.421696,
            "precision": 0.434418,
            "precision_weighted": 0.434379,
            "recall": 0.442176,
            "recall_weighted": 0.44214,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.45447,
        "f1": 0.433595,
        "f1_weighted": 0.433544,
        "precision": 0.446064,
        "precision_weighted": 0.446011,
        "recall": 0.45451,
        "recall_weighted": 0.45447,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.433595,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 31.865575551986694,
  "kg_co2_emissions": null
}