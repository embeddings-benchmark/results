{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 2901.2339668273926,
  "kg_co2_emissions": 0.22259174365931111,
  "mteb_version": "1.12.75",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08664047173176385,
        "map": 0.10213991758569457,
        "mrr": 0.08664047173176385,
        "nAUC_map_diff1": 0.04303103680951931,
        "nAUC_map_max": 0.05299774994685352,
        "nAUC_map_std": 0.13275789175064706,
        "nAUC_mrr_diff1": 0.03540013705787619,
        "nAUC_mrr_max": 0.044574590683799376,
        "nAUC_mrr_std": 0.09658404437388554
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.11886712645459595,
        "map": 0.13779955270815328,
        "mrr": 0.11886712645459595,
        "nAUC_map_diff1": 0.029665471123561885,
        "nAUC_map_max": 0.024674718868589903,
        "nAUC_map_std": 0.005747581955652207,
        "nAUC_mrr_diff1": 0.025001346461212482,
        "nAUC_mrr_max": 0.022258055706492846,
        "nAUC_mrr_std": -0.005109969347358856
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.10361254608472968,
        "map": 0.11754037396361312,
        "mrr": 0.10361254608472968,
        "nAUC_map_diff1": 0.12122888927372757,
        "nAUC_map_max": 0.12192146217104521,
        "nAUC_map_std": 0.13180913260695531,
        "nAUC_mrr_diff1": 0.12045830251440273,
        "nAUC_mrr_max": 0.12187573366942887,
        "nAUC_mrr_std": 0.10632490209611176
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.10395230005932159,
        "map": 0.12086521558608294,
        "mrr": 0.10395230005932159,
        "nAUC_map_diff1": 0.041173444560934025,
        "nAUC_map_max": 0.07398449402498594,
        "nAUC_map_std": 0.15148939973957298,
        "nAUC_mrr_diff1": 0.04543746334574915,
        "nAUC_mrr_max": 0.0809252882747115,
        "nAUC_mrr_std": 0.14003681433740595
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08734475342299769,
        "map": 0.10385801791815001,
        "mrr": 0.08734475342299769,
        "nAUC_map_diff1": 0.023597358141846696,
        "nAUC_map_max": 0.11516028550090407,
        "nAUC_map_std": 0.1996219526003295,
        "nAUC_mrr_diff1": 0.019072041049098116,
        "nAUC_mrr_max": 0.12614116745858917,
        "nAUC_mrr_std": 0.19687859034943575
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.14423459365811403,
        "map": 0.1614725627672879,
        "mrr": 0.14423459365811403,
        "nAUC_map_diff1": 0.17222081383782295,
        "nAUC_map_max": 0.08302993161476281,
        "nAUC_map_std": 0.07731621931943876,
        "nAUC_mrr_diff1": 0.1822993586285031,
        "nAUC_mrr_max": 0.08547736850951536,
        "nAUC_mrr_std": 0.07150316062532727
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}