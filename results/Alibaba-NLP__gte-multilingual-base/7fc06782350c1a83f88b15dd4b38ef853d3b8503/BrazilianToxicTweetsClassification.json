{
  "dataset_revision": "fb4f11a5bc68b99891852d20f1ec074be6289768",
  "evaluation_time": 4.471953868865967,
  "kg_co2_emissions": 0.00016421731814662404,
  "mteb_version": "1.12.75",
  "scores": {
    "test": [
      {
        "accuracy": 0.186962890625,
        "f1": 0.16201349864446146,
        "hf_subset": "default",
        "languages": [
          "por-Latn"
        ],
        "lrap": 0.8017557779947927,
        "main_score": 0.186962890625,
        "scores_per_experiment": [
          {
            "accuracy": 0.14697265625,
            "f1": 0.16083358302455805,
            "lrap": 0.8210177951388892
          },
          {
            "accuracy": 0.1435546875,
            "f1": 0.17536624167400347,
            "lrap": 0.8049858940972238
          },
          {
            "accuracy": 0.17529296875,
            "f1": 0.16184159281409177,
            "lrap": 0.8106146918402786
          },
          {
            "accuracy": 0.21142578125,
            "f1": 0.15388391893712672,
            "lrap": 0.8200005425347228
          },
          {
            "accuracy": 0.1953125,
            "f1": 0.163601717551508,
            "lrap": 0.8025309244791679
          },
          {
            "accuracy": 0.158203125,
            "f1": 0.15752479740350503,
            "lrap": 0.8051893446180571
          },
          {
            "accuracy": 0.24755859375,
            "f1": 0.14696684345300104,
            "lrap": 0.7555338541666676
          },
          {
            "accuracy": 0.11474609375,
            "f1": 0.17149828203791276,
            "lrap": 0.79766845703125
          },
          {
            "accuracy": 0.23388671875,
            "f1": 0.1613543005843391,
            "lrap": 0.7918701171875013
          },
          {
            "accuracy": 0.24267578125,
            "f1": 0.1672637089645688,
            "lrap": 0.8081461588541681
          }
        ]
      }
    ]
  },
  "task_name": "BrazilianToxicTweetsClassification"
}