{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.406788,
        "f1": 0.391274,
        "f1_weighted": 0.415487,
        "scores_per_experiment": [
          {
            "accuracy": 0.398918,
            "f1": 0.388849,
            "f1_weighted": 0.406561
          },
          {
            "accuracy": 0.402361,
            "f1": 0.383147,
            "f1_weighted": 0.410881
          },
          {
            "accuracy": 0.411707,
            "f1": 0.390807,
            "f1_weighted": 0.418569
          },
          {
            "accuracy": 0.425971,
            "f1": 0.405994,
            "f1_weighted": 0.438192
          },
          {
            "accuracy": 0.419577,
            "f1": 0.404888,
            "f1_weighted": 0.42639
          },
          {
            "accuracy": 0.416626,
            "f1": 0.393731,
            "f1_weighted": 0.426522
          },
          {
            "accuracy": 0.367437,
            "f1": 0.364362,
            "f1_weighted": 0.376934
          },
          {
            "accuracy": 0.403837,
            "f1": 0.388809,
            "f1_weighted": 0.412088
          },
          {
            "accuracy": 0.409739,
            "f1": 0.394338,
            "f1_weighted": 0.418618
          },
          {
            "accuracy": 0.411707,
            "f1": 0.397814,
            "f1_weighted": 0.420115
          }
        ],
        "main_score": 0.406788,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.418191,
        "f1": 0.393224,
        "f1_weighted": 0.425495,
        "scores_per_experiment": [
          {
            "accuracy": 0.429724,
            "f1": 0.407759,
            "f1_weighted": 0.436652
          },
          {
            "accuracy": 0.411567,
            "f1": 0.378506,
            "f1_weighted": 0.417666
          },
          {
            "accuracy": 0.431069,
            "f1": 0.403346,
            "f1_weighted": 0.433258
          },
          {
            "accuracy": 0.435104,
            "f1": 0.403533,
            "f1_weighted": 0.446502
          },
          {
            "accuracy": 0.416947,
            "f1": 0.385615,
            "f1_weighted": 0.422453
          },
          {
            "accuracy": 0.413921,
            "f1": 0.393654,
            "f1_weighted": 0.425708
          },
          {
            "accuracy": 0.391392,
            "f1": 0.375678,
            "f1_weighted": 0.398473
          },
          {
            "accuracy": 0.414257,
            "f1": 0.389102,
            "f1_weighted": 0.424147
          },
          {
            "accuracy": 0.414593,
            "f1": 0.391304,
            "f1_weighted": 0.419876
          },
          {
            "accuracy": 0.423336,
            "f1": 0.403749,
            "f1_weighted": 0.430218
          }
        ],
        "main_score": 0.418191,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 18.0628821849823,
  "kg_co2_emissions": null
}