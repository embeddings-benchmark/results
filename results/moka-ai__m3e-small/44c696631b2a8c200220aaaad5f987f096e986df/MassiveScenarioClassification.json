{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "task_name": "MassiveScenarioClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.441761,
        "f1": 0.427509,
        "f1_weighted": 0.452332,
        "scores_per_experiment": [
          {
            "accuracy": 0.448598,
            "f1": 0.43132,
            "f1_weighted": 0.451213
          },
          {
            "accuracy": 0.455485,
            "f1": 0.438983,
            "f1_weighted": 0.470756
          },
          {
            "accuracy": 0.440728,
            "f1": 0.426499,
            "f1_weighted": 0.452705
          },
          {
            "accuracy": 0.438269,
            "f1": 0.413718,
            "f1_weighted": 0.448665
          },
          {
            "accuracy": 0.447614,
            "f1": 0.422919,
            "f1_weighted": 0.458031
          },
          {
            "accuracy": 0.419085,
            "f1": 0.411472,
            "f1_weighted": 0.427652
          },
          {
            "accuracy": 0.436301,
            "f1": 0.431598,
            "f1_weighted": 0.451113
          },
          {
            "accuracy": 0.453517,
            "f1": 0.436985,
            "f1_weighted": 0.46927
          },
          {
            "accuracy": 0.42548,
            "f1": 0.407423,
            "f1_weighted": 0.432162
          },
          {
            "accuracy": 0.452533,
            "f1": 0.454168,
            "f1_weighted": 0.461749
          }
        ],
        "main_score": 0.441761,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.455615,
        "f1": 0.43308,
        "f1_weighted": 0.466249,
        "scores_per_experiment": [
          {
            "accuracy": 0.468056,
            "f1": 0.449604,
            "f1_weighted": 0.475366
          },
          {
            "accuracy": 0.46234,
            "f1": 0.444065,
            "f1_weighted": 0.475216
          },
          {
            "accuracy": 0.452253,
            "f1": 0.43426,
            "f1_weighted": 0.463407
          },
          {
            "accuracy": 0.460323,
            "f1": 0.423279,
            "f1_weighted": 0.467794
          },
          {
            "accuracy": 0.474109,
            "f1": 0.433177,
            "f1_weighted": 0.485579
          },
          {
            "accuracy": 0.429052,
            "f1": 0.404033,
            "f1_weighted": 0.434756
          },
          {
            "accuracy": 0.459987,
            "f1": 0.447734,
            "f1_weighted": 0.476784
          },
          {
            "accuracy": 0.464022,
            "f1": 0.443335,
            "f1_weighted": 0.478762
          },
          {
            "accuracy": 0.434095,
            "f1": 0.409793,
            "f1_weighted": 0.443686
          },
          {
            "accuracy": 0.451917,
            "f1": 0.441518,
            "f1_weighted": 0.461143
          }
        ],
        "main_score": 0.455615,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 8.484380722045898,
  "kg_co2_emissions": null
}