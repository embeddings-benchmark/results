{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "task_name": "WebLINXCandidatesReranking",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "map": 0.140654,
        "mrr": 0.123015,
        "nAUC_map_max": -0.047891,
        "nAUC_map_std": -0.05237,
        "nAUC_map_diff1": 0.135621,
        "nAUC_mrr_max": -0.041124,
        "nAUC_mrr_std": -0.051449,
        "nAUC_mrr_diff1": 0.14156,
        "main_score": 0.123015,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_iid": [
      {
        "map": 0.105777,
        "mrr": 0.089368,
        "nAUC_map_max": 0.030109,
        "nAUC_map_std": 0.125114,
        "nAUC_map_diff1": 0.146219,
        "nAUC_mrr_max": 0.044359,
        "nAUC_mrr_std": 0.115093,
        "nAUC_mrr_diff1": 0.143284,
        "main_score": 0.089368,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_cat": [
      {
        "map": 0.084614,
        "mrr": 0.0684,
        "nAUC_map_max": 0.028068,
        "nAUC_map_std": 0.264797,
        "nAUC_map_diff1": 0.054438,
        "nAUC_mrr_max": 0.020524,
        "nAUC_mrr_std": 0.241039,
        "nAUC_mrr_diff1": 0.051595,
        "main_score": 0.0684,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_geo": [
      {
        "map": 0.104822,
        "mrr": 0.0852,
        "nAUC_map_max": -0.054561,
        "nAUC_map_std": -0.057324,
        "nAUC_map_diff1": 0.147267,
        "nAUC_mrr_max": -0.042545,
        "nAUC_mrr_std": -0.064957,
        "nAUC_mrr_diff1": 0.15445,
        "main_score": 0.0852,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_vis": [
      {
        "map": 0.109845,
        "mrr": 0.091663,
        "nAUC_map_max": 0.04778,
        "nAUC_map_std": 0.127786,
        "nAUC_map_diff1": 0.134513,
        "nAUC_mrr_max": 0.057736,
        "nAUC_mrr_std": 0.111995,
        "nAUC_mrr_diff1": 0.137296,
        "main_score": 0.091663,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_web": [
      {
        "map": 0.091964,
        "mrr": 0.074631,
        "nAUC_map_max": 0.010641,
        "nAUC_map_std": 0.062636,
        "nAUC_map_diff1": 0.141086,
        "nAUC_mrr_max": 0.004111,
        "nAUC_mrr_std": 0.050203,
        "nAUC_mrr_diff1": 0.141737,
        "main_score": 0.074631,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 6463.099184989929,
  "kg_co2_emissions": 0.5672701802641639
}