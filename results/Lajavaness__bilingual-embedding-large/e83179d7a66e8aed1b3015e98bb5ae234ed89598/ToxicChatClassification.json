{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.800515,
        "f1": 0.676475,
        "f1_weighted": 0.826079,
        "ap": 0.294699,
        "ap_weighted": 0.294699,
        "scores_per_experiment": [
          {
            "accuracy": 0.750859,
            "f1": 0.630678,
            "f1_weighted": 0.788506,
            "ap": 0.248539,
            "ap_weighted": 0.248539
          },
          {
            "accuracy": 0.82732,
            "f1": 0.691082,
            "f1_weighted": 0.844768,
            "ap": 0.296121,
            "ap_weighted": 0.296121
          },
          {
            "accuracy": 0.818729,
            "f1": 0.70174,
            "f1_weighted": 0.841677,
            "ap": 0.326503,
            "ap_weighted": 0.326503
          },
          {
            "accuracy": 0.810137,
            "f1": 0.694051,
            "f1_weighted": 0.835233,
            "ap": 0.318912,
            "ap_weighted": 0.318912
          },
          {
            "accuracy": 0.762887,
            "f1": 0.607771,
            "f1_weighted": 0.792554,
            "ap": 0.204123,
            "ap_weighted": 0.204123
          },
          {
            "accuracy": 0.841924,
            "f1": 0.720058,
            "f1_weighted": 0.858427,
            "ap": 0.3422,
            "ap_weighted": 0.3422
          },
          {
            "accuracy": 0.805842,
            "f1": 0.671728,
            "f1_weighted": 0.828915,
            "ap": 0.277298,
            "ap_weighted": 0.277298
          },
          {
            "accuracy": 0.715636,
            "f1": 0.617764,
            "f1_weighted": 0.762661,
            "ap": 0.258253,
            "ap_weighted": 0.258253
          },
          {
            "accuracy": 0.839347,
            "f1": 0.712599,
            "f1_weighted": 0.85558,
            "ap": 0.328659,
            "ap_weighted": 0.328659
          },
          {
            "accuracy": 0.832474,
            "f1": 0.717281,
            "f1_weighted": 0.852474,
            "ap": 0.346383,
            "ap_weighted": 0.346383
          }
        ],
        "main_score": 0.800515,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 9.770459175109863,
  "kg_co2_emissions": 0.000401717833327171
}