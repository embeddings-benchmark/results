{
  "dataset_revision": "bd27d0058bea2ad52470d9072a3b5da6b97c1ac3",
  "task_name": "VaccinChatNLClassification",
  "mteb_version": "2.1.8",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.487179,
            "f1": 0.530872,
            "f1_weighted": 0.459054,
            "precision": 0.527073,
            "precision_weighted": 0.627114,
            "recall": 0.690706,
            "recall_weighted": 0.487179,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.487179,
            "f1": 0.537684,
            "f1_weighted": 0.453806,
            "precision": 0.546379,
            "precision_weighted": 0.685861,
            "recall": 0.698634,
            "recall_weighted": 0.487179,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.498291,
            "f1": 0.54217,
            "f1_weighted": 0.466675,
            "precision": 0.550785,
            "precision_weighted": 0.66037,
            "recall": 0.693694,
            "recall_weighted": 0.498291,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.487179,
            "f1": 0.544499,
            "f1_weighted": 0.457241,
            "precision": 0.555516,
            "precision_weighted": 0.664758,
            "recall": 0.69618,
            "recall_weighted": 0.487179,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.51453,
            "f1": 0.557433,
            "f1_weighted": 0.489455,
            "precision": 0.555154,
            "precision_weighted": 0.685492,
            "recall": 0.702925,
            "recall_weighted": 0.51453,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.493162,
            "f1": 0.535025,
            "f1_weighted": 0.473419,
            "precision": 0.537212,
            "precision_weighted": 0.668669,
            "recall": 0.69334,
            "recall_weighted": 0.493162,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.504274,
            "f1": 0.551819,
            "f1_weighted": 0.469569,
            "precision": 0.547208,
            "precision_weighted": 0.612912,
            "recall": 0.708394,
            "recall_weighted": 0.504274,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.496581,
            "f1": 0.533919,
            "f1_weighted": 0.462109,
            "precision": 0.545965,
            "precision_weighted": 0.666812,
            "recall": 0.687114,
            "recall_weighted": 0.496581,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.503419,
            "f1": 0.547312,
            "f1_weighted": 0.474704,
            "precision": 0.549028,
            "precision_weighted": 0.596267,
            "recall": 0.688384,
            "recall_weighted": 0.503419,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.528205,
            "f1": 0.555433,
            "f1_weighted": 0.505418,
            "precision": 0.557296,
            "precision_weighted": 0.67045,
            "recall": 0.712301,
            "recall_weighted": 0.528205,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.5,
        "f1": 0.543617,
        "f1_weighted": 0.471145,
        "precision": 0.547162,
        "precision_weighted": 0.65387,
        "recall": 0.697167,
        "recall_weighted": 0.5,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.543617,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 52.277167558670044,
  "kg_co2_emissions": null
}