{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "task_name": "InappropriatenessClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.51499,
        "f1": 0.514162,
        "f1_weighted": 0.514162,
        "ap": 0.508209,
        "ap_weighted": 0.508209,
        "scores_per_experiment": [
          {
            "accuracy": 0.540039,
            "f1": 0.539927,
            "f1_weighted": 0.539927,
            "ap": 0.521574,
            "ap_weighted": 0.521574
          },
          {
            "accuracy": 0.519043,
            "f1": 0.518006,
            "f1_weighted": 0.518006,
            "ap": 0.509853,
            "ap_weighted": 0.509853
          },
          {
            "accuracy": 0.507324,
            "f1": 0.506434,
            "f1_weighted": 0.506434,
            "ap": 0.503712,
            "ap_weighted": 0.503712
          },
          {
            "accuracy": 0.514648,
            "f1": 0.514259,
            "f1_weighted": 0.514259,
            "ap": 0.507552,
            "ap_weighted": 0.507552
          },
          {
            "accuracy": 0.547363,
            "f1": 0.543307,
            "f1_weighted": 0.543307,
            "ap": 0.525569,
            "ap_weighted": 0.525569
          },
          {
            "accuracy": 0.507812,
            "f1": 0.50772,
            "f1_weighted": 0.50772,
            "ap": 0.503966,
            "ap_weighted": 0.503966
          },
          {
            "accuracy": 0.54541,
            "f1": 0.545401,
            "f1_weighted": 0.545401,
            "ap": 0.524749,
            "ap_weighted": 0.524749
          },
          {
            "accuracy": 0.469727,
            "f1": 0.469714,
            "f1_weighted": 0.469714,
            "ap": 0.485789,
            "ap_weighted": 0.485789
          },
          {
            "accuracy": 0.504395,
            "f1": 0.504176,
            "f1_weighted": 0.504176,
            "ap": 0.502216,
            "ap_weighted": 0.502216
          },
          {
            "accuracy": 0.494141,
            "f1": 0.492677,
            "f1_weighted": 0.492677,
            "ap": 0.497109,
            "ap_weighted": 0.497109
          }
        ],
        "main_score": 0.51499,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 8.128708124160767,
  "kg_co2_emissions": 0.0002432111599988614
}