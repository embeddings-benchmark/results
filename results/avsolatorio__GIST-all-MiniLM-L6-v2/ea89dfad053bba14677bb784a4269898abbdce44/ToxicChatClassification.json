{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.784622,
        "f1": 0.662706,
        "f1_weighted": 0.813618,
        "ap": 0.283726,
        "ap_weighted": 0.283726,
        "scores_per_experiment": [
          {
            "accuracy": 0.655498,
            "f1": 0.540137,
            "f1_weighted": 0.712684,
            "ap": 0.175649,
            "ap_weighted": 0.175649
          },
          {
            "accuracy": 0.849656,
            "f1": 0.711829,
            "f1_weighted": 0.861128,
            "ap": 0.318564,
            "ap_weighted": 0.318564
          },
          {
            "accuracy": 0.862543,
            "f1": 0.749466,
            "f1_weighted": 0.875557,
            "ap": 0.387655,
            "ap_weighted": 0.387655
          },
          {
            "accuracy": 0.784364,
            "f1": 0.665964,
            "f1_weighted": 0.814947,
            "ap": 0.285933,
            "ap_weighted": 0.285933
          },
          {
            "accuracy": 0.808419,
            "f1": 0.658662,
            "f1_weighted": 0.828037,
            "ap": 0.252755,
            "ap_weighted": 0.252755
          },
          {
            "accuracy": 0.736254,
            "f1": 0.629411,
            "f1_weighted": 0.778479,
            "ap": 0.260288,
            "ap_weighted": 0.260288
          },
          {
            "accuracy": 0.824742,
            "f1": 0.676469,
            "f1_weighted": 0.840548,
            "ap": 0.271797,
            "ap_weighted": 0.271797
          },
          {
            "accuracy": 0.728522,
            "f1": 0.630289,
            "f1_weighted": 0.773055,
            "ap": 0.27085,
            "ap_weighted": 0.27085
          },
          {
            "accuracy": 0.774055,
            "f1": 0.660595,
            "f1_weighted": 0.807604,
            "ap": 0.285502,
            "ap_weighted": 0.285502
          },
          {
            "accuracy": 0.822165,
            "f1": 0.704239,
            "f1_weighted": 0.844146,
            "ap": 0.328268,
            "ap_weighted": 0.328268
          }
        ],
        "main_score": 0.784622,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 8.424375534057617,
  "kg_co2_emissions": 0.0002501237908909513
}