{
  "dataset_revision": "20b0e6081892e78179356fada741b7afa381443d",
  "task_name": "AngryTweetsClassification",
  "mteb_version": "2.3.2",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.547278,
            "f1": 0.545186,
            "f1_weighted": 0.549933,
            "precision": 0.552617,
            "precision_weighted": 0.565701,
            "recall": 0.552103,
            "recall_weighted": 0.547278,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.540592,
            "f1": 0.538297,
            "f1_weighted": 0.542034,
            "precision": 0.538961,
            "precision_weighted": 0.547537,
            "recall": 0.542064,
            "recall_weighted": 0.540592,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.554919,
            "f1": 0.541255,
            "f1_weighted": 0.546979,
            "precision": 0.547279,
            "precision_weighted": 0.549823,
            "recall": 0.545136,
            "recall_weighted": 0.554919,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.52149,
            "f1": 0.515229,
            "f1_weighted": 0.518647,
            "precision": 0.519818,
            "precision_weighted": 0.528505,
            "recall": 0.524486,
            "recall_weighted": 0.52149,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.51958,
            "f1": 0.504692,
            "f1_weighted": 0.5163,
            "precision": 0.506706,
            "precision_weighted": 0.514977,
            "recall": 0.504864,
            "recall_weighted": 0.51958,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.56447,
            "f1": 0.545932,
            "f1_weighted": 0.557983,
            "precision": 0.54762,
            "precision_weighted": 0.556704,
            "recall": 0.549033,
            "recall_weighted": 0.56447,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.531041,
            "f1": 0.527856,
            "f1_weighted": 0.537868,
            "precision": 0.537333,
            "precision_weighted": 0.554555,
            "recall": 0.528754,
            "recall_weighted": 0.531041,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.502388,
            "f1": 0.500238,
            "f1_weighted": 0.504419,
            "precision": 0.502865,
            "precision_weighted": 0.511497,
            "recall": 0.502474,
            "recall_weighted": 0.502388,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.528176,
            "f1": 0.525517,
            "f1_weighted": 0.53207,
            "precision": 0.538327,
            "precision_weighted": 0.549392,
            "recall": 0.524794,
            "recall_weighted": 0.528176,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.574021,
            "f1": 0.561838,
            "f1_weighted": 0.570152,
            "precision": 0.564901,
            "precision_weighted": 0.569998,
            "recall": 0.562279,
            "recall_weighted": 0.574021,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.538395,
        "f1": 0.530604,
        "f1_weighted": 0.537639,
        "precision": 0.535643,
        "precision_weighted": 0.544869,
        "recall": 0.533599,
        "recall_weighted": 0.538395,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.538395,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 74.03551530838013,
  "kg_co2_emissions": null
}