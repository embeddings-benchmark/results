{
  "dataset_revision": "24fcf066e6b96f9e0d743e8b79184e0c599f73c3",
  "task_name": "SwahiliNewsClassification",
  "mteb_version": "2.3.11",
  "scores": {
    "train": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.552734,
            "f1": 0.537236,
            "f1_weighted": 0.540658,
            "precision": 0.543183,
            "precision_weighted": 0.745715,
            "recall": 0.704925,
            "recall_weighted": 0.552734,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.577148,
            "f1": 0.549242,
            "f1_weighted": 0.569019,
            "precision": 0.551334,
            "precision_weighted": 0.774643,
            "recall": 0.725068,
            "recall_weighted": 0.577148,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.592285,
            "f1": 0.548762,
            "f1_weighted": 0.593397,
            "precision": 0.5454,
            "precision_weighted": 0.763591,
            "recall": 0.69794,
            "recall_weighted": 0.592285,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.558105,
            "f1": 0.535288,
            "f1_weighted": 0.548473,
            "precision": 0.535742,
            "precision_weighted": 0.743209,
            "recall": 0.709494,
            "recall_weighted": 0.558105,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.547852,
            "f1": 0.525413,
            "f1_weighted": 0.534278,
            "precision": 0.537048,
            "precision_weighted": 0.757683,
            "recall": 0.708839,
            "recall_weighted": 0.547852,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.512207,
            "f1": 0.495841,
            "f1_weighted": 0.474458,
            "precision": 0.519346,
            "precision_weighted": 0.740379,
            "recall": 0.688539,
            "recall_weighted": 0.512207,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.482422,
            "f1": 0.467086,
            "f1_weighted": 0.416823,
            "precision": 0.503242,
            "precision_weighted": 0.733245,
            "recall": 0.695771,
            "recall_weighted": 0.482422,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.557129,
            "f1": 0.543603,
            "f1_weighted": 0.543179,
            "precision": 0.558141,
            "precision_weighted": 0.771729,
            "recall": 0.719427,
            "recall_weighted": 0.557129,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.53125,
            "f1": 0.509948,
            "f1_weighted": 0.516434,
            "precision": 0.503282,
            "precision_weighted": 0.694801,
            "recall": 0.695182,
            "recall_weighted": 0.53125,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.549805,
            "f1": 0.526092,
            "f1_weighted": 0.546984,
            "precision": 0.529824,
            "precision_weighted": 0.735235,
            "recall": 0.691857,
            "recall_weighted": 0.549805,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.546094,
        "f1": 0.523851,
        "f1_weighted": 0.52837,
        "precision": 0.532654,
        "precision_weighted": 0.746023,
        "recall": 0.703704,
        "recall_weighted": 0.546094,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.546094,
        "hf_subset": "default",
        "languages": [
          "swa-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 116.50370407104492,
  "kg_co2_emissions": null
}