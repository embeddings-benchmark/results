{
  "dataset_revision": "b4280e921a2760ce34d2dd80a9e5dc8bcbf61785",
  "task_name": "TweetTopicSingleClassification",
  "mteb_version": "2.3.11",
  "scores": {
    "test_2021": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.866509,
            "f1": 0.728673,
            "f1_weighted": 0.868116,
            "precision": 0.733684,
            "precision_weighted": 0.871727,
            "recall": 0.728562,
            "recall_weighted": 0.866509,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.861784,
            "f1": 0.723215,
            "f1_weighted": 0.863327,
            "precision": 0.719301,
            "precision_weighted": 0.865485,
            "recall": 0.728984,
            "recall_weighted": 0.861784,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.860012,
            "f1": 0.717094,
            "f1_weighted": 0.861408,
            "precision": 0.715961,
            "precision_weighted": 0.863751,
            "recall": 0.72054,
            "recall_weighted": 0.860012,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.865328,
            "f1": 0.720799,
            "f1_weighted": 0.866077,
            "precision": 0.72911,
            "precision_weighted": 0.868822,
            "recall": 0.719279,
            "recall_weighted": 0.865328,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.85824,
            "f1": 0.696577,
            "f1_weighted": 0.857557,
            "precision": 0.718526,
            "precision_weighted": 0.860693,
            "recall": 0.691682,
            "recall_weighted": 0.85824,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.861784,
            "f1": 0.719639,
            "f1_weighted": 0.862991,
            "precision": 0.72792,
            "precision_weighted": 0.866844,
            "recall": 0.720243,
            "recall_weighted": 0.861784,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.861193,
            "f1": 0.720721,
            "f1_weighted": 0.86327,
            "precision": 0.729557,
            "precision_weighted": 0.869268,
            "recall": 0.723718,
            "recall_weighted": 0.861193,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.869462,
            "f1": 0.725256,
            "f1_weighted": 0.868399,
            "precision": 0.757754,
            "precision_weighted": 0.873033,
            "recall": 0.720845,
            "recall_weighted": 0.869462,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.868281,
            "f1": 0.735095,
            "f1_weighted": 0.869954,
            "precision": 0.736808,
            "precision_weighted": 0.873207,
            "recall": 0.737007,
            "recall_weighted": 0.868281,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.85883,
            "f1": 0.708373,
            "f1_weighted": 0.861342,
            "precision": 0.710429,
            "precision_weighted": 0.865721,
            "recall": 0.709393,
            "recall_weighted": 0.85883,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.863142,
        "f1": 0.719544,
        "f1_weighted": 0.864244,
        "precision": 0.727905,
        "precision_weighted": 0.867855,
        "recall": 0.720025,
        "recall_weighted": 0.863142,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.863142,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 25.794650316238403,
  "kg_co2_emissions": null
}