{
  "dataset_revision": "6bb0321659c4f07c4c2176c30c98c971be6571b4",
  "task_name": "MalteseNewsClassification",
  "mteb_version": "2.3.11",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.592077,
            "lrap": 0.692161,
            "f1": 0.394279,
            "hamming": 0.664461
          },
          {
            "accuracy": 0.58424,
            "lrap": 0.699894,
            "f1": 0.416199,
            "hamming": 0.675258
          },
          {
            "accuracy": 0.59643,
            "lrap": 0.699288,
            "f1": 0.38666,
            "hamming": 0.673161
          },
          {
            "accuracy": 0.520244,
            "lrap": 0.671005,
            "f1": 0.394221,
            "hamming": 0.649209
          },
          {
            "accuracy": 0.585111,
            "lrap": 0.700082,
            "f1": 0.400124,
            "hamming": 0.675192
          },
          {
            "accuracy": 0.560731,
            "lrap": 0.684684,
            "f1": 0.392399,
            "hamming": 0.659063
          },
          {
            "accuracy": 0.574227,
            "lrap": 0.692439,
            "f1": 0.392004,
            "hamming": 0.667842
          },
          {
            "accuracy": 0.541576,
            "lrap": 0.680959,
            "f1": 0.383331,
            "hamming": 0.65796
          },
          {
            "accuracy": 0.513714,
            "lrap": 0.653702,
            "f1": 0.38977,
            "hamming": 0.628849
          },
          {
            "accuracy": 0.627775,
            "lrap": 0.72713,
            "f1": 0.413516,
            "hamming": 0.702191
          }
        ],
        "accuracy": 0.569613,
        "lrap": 0.690134,
        "f1": 0.39625,
        "hamming": 0.665319,
        "main_score": 0.569613,
        "hf_subset": "default",
        "languages": [
          "mlt-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 112.30863738059998,
  "kg_co2_emissions": null
}