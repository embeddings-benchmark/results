{
  "dataset_revision": "1a5f2fa2914bfeff4fcdc6fff4194fa8ec8fa19e",
  "task_name": "GujaratiNewsClassification",
  "mteb_version": "2.3.11",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.932473,
            "f1": 0.914651,
            "f1_weighted": 0.932673,
            "precision": 0.912928,
            "precision_weighted": 0.932971,
            "recall": 0.916518,
            "recall_weighted": 0.932473,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.930956,
            "f1": 0.912528,
            "f1_weighted": 0.930875,
            "precision": 0.912259,
            "precision_weighted": 0.930867,
            "recall": 0.912866,
            "recall_weighted": 0.930956,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.924886,
            "f1": 0.905872,
            "f1_weighted": 0.925283,
            "precision": 0.902976,
            "precision_weighted": 0.926011,
            "recall": 0.909253,
            "recall_weighted": 0.924886,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.930956,
            "f1": 0.912288,
            "f1_weighted": 0.931235,
            "precision": 0.910679,
            "precision_weighted": 0.931574,
            "recall": 0.913991,
            "recall_weighted": 0.930956,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.930956,
            "f1": 0.912851,
            "f1_weighted": 0.93102,
            "precision": 0.913013,
            "precision_weighted": 0.931113,
            "recall": 0.912713,
            "recall_weighted": 0.930956,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.930956,
            "f1": 0.91347,
            "f1_weighted": 0.931167,
            "precision": 0.912105,
            "precision_weighted": 0.931418,
            "recall": 0.9149,
            "recall_weighted": 0.930956,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.924886,
            "f1": 0.906053,
            "f1_weighted": 0.925414,
            "precision": 0.902646,
            "precision_weighted": 0.926326,
            "recall": 0.910054,
            "recall_weighted": 0.924886,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.925645,
            "f1": 0.907128,
            "f1_weighted": 0.925873,
            "precision": 0.905589,
            "precision_weighted": 0.92616,
            "recall": 0.908762,
            "recall_weighted": 0.925645,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.930956,
            "f1": 0.912988,
            "f1_weighted": 0.93091,
            "precision": 0.914029,
            "precision_weighted": 0.930925,
            "recall": 0.912021,
            "recall_weighted": 0.930956,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.933991,
            "f1": 0.916514,
            "f1_weighted": 0.933953,
            "precision": 0.917031,
            "precision_weighted": 0.933928,
            "recall": 0.916014,
            "recall_weighted": 0.933991,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.929666,
        "f1": 0.911434,
        "f1_weighted": 0.92984,
        "precision": 0.910325,
        "precision_weighted": 0.930129,
        "recall": 0.912709,
        "recall_weighted": 0.929666,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.929666,
        "hf_subset": "default",
        "languages": [
          "guj-Gujr"
        ]
      }
    ]
  },
  "evaluation_time": 13.775839567184448,
  "kg_co2_emissions": null
}