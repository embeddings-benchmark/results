{
  "dataset_revision": "d604517c81ca91fe16a244d1248fc021f9ecee7a",
  "task_name": "TweetSentimentExtractionClassification",
  "mteb_version": "2.3.11",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.719015,
            "f1": 0.721285,
            "f1_weighted": 0.713807,
            "precision": 0.718791,
            "precision_weighted": 0.720869,
            "recall": 0.735113,
            "recall_weighted": 0.719015,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.723826,
            "f1": 0.727834,
            "f1_weighted": 0.721192,
            "precision": 0.723825,
            "precision_weighted": 0.723668,
            "recall": 0.736757,
            "recall_weighted": 0.723826,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.722977,
            "f1": 0.726275,
            "f1_weighted": 0.719776,
            "precision": 0.722176,
            "precision_weighted": 0.722373,
            "recall": 0.735795,
            "recall_weighted": 0.722977,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.720713,
            "f1": 0.724806,
            "f1_weighted": 0.717345,
            "precision": 0.722511,
            "precision_weighted": 0.723717,
            "recall": 0.736768,
            "recall_weighted": 0.720713,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.726938,
            "f1": 0.730318,
            "f1_weighted": 0.724114,
            "precision": 0.726254,
            "precision_weighted": 0.726377,
            "recall": 0.739151,
            "recall_weighted": 0.726938,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.716752,
            "f1": 0.721134,
            "f1_weighted": 0.713913,
            "precision": 0.718154,
            "precision_weighted": 0.71839,
            "recall": 0.731415,
            "recall_weighted": 0.716752,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.706848,
            "f1": 0.708731,
            "f1_weighted": 0.700577,
            "precision": 0.706284,
            "precision_weighted": 0.707148,
            "recall": 0.723067,
            "recall_weighted": 0.706848,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.725241,
            "f1": 0.728241,
            "f1_weighted": 0.721646,
            "precision": 0.724458,
            "precision_weighted": 0.725003,
            "recall": 0.738482,
            "recall_weighted": 0.725241,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.724109,
            "f1": 0.727349,
            "f1_weighted": 0.720291,
            "precision": 0.723413,
            "precision_weighted": 0.724188,
            "recall": 0.738495,
            "recall_weighted": 0.724109,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.72043,
            "f1": 0.723938,
            "f1_weighted": 0.717242,
            "precision": 0.719862,
            "precision_weighted": 0.720195,
            "recall": 0.733798,
            "recall_weighted": 0.72043,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.720685,
        "f1": 0.723991,
        "f1_weighted": 0.71699,
        "precision": 0.720573,
        "precision_weighted": 0.721193,
        "recall": 0.734884,
        "recall_weighted": 0.720685,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.720685,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 76.62735557556152,
  "kg_co2_emissions": null
}