{
  "dataset_revision": "23a20c659d868740ef9c54854de631fe19cd5c17",
  "task_name": "CSFDSKMovieReviewSentimentClassification",
  "mteb_version": "2.3.11",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.47998,
            "f1": 0.457713,
            "f1_weighted": 0.460988,
            "precision": 0.460717,
            "precision_weighted": 0.462135,
            "recall": 0.474659,
            "recall_weighted": 0.47998,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.437988,
            "f1": 0.404019,
            "f1_weighted": 0.407289,
            "precision": 0.41579,
            "precision_weighted": 0.41716,
            "recall": 0.432015,
            "recall_weighted": 0.437988,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.479004,
            "f1": 0.446505,
            "f1_weighted": 0.45002,
            "precision": 0.467954,
            "precision_weighted": 0.468629,
            "recall": 0.473037,
            "recall_weighted": 0.479004,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.478027,
            "f1": 0.473324,
            "f1_weighted": 0.47549,
            "precision": 0.474203,
            "precision_weighted": 0.475649,
            "recall": 0.475108,
            "recall_weighted": 0.478027,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.463867,
            "f1": 0.447614,
            "f1_weighted": 0.450563,
            "precision": 0.453555,
            "precision_weighted": 0.454704,
            "recall": 0.458902,
            "recall_weighted": 0.463867,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.458496,
            "f1": 0.422351,
            "f1_weighted": 0.425141,
            "precision": 0.459301,
            "precision_weighted": 0.460703,
            "recall": 0.454283,
            "recall_weighted": 0.458496,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.467285,
            "f1": 0.447384,
            "f1_weighted": 0.450783,
            "precision": 0.467936,
            "precision_weighted": 0.468569,
            "recall": 0.461312,
            "recall_weighted": 0.467285,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.473633,
            "f1": 0.468144,
            "f1_weighted": 0.470928,
            "precision": 0.475652,
            "precision_weighted": 0.477519,
            "recall": 0.470041,
            "recall_weighted": 0.473633,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.466309,
            "f1": 0.439862,
            "f1_weighted": 0.442809,
            "precision": 0.454783,
            "precision_weighted": 0.455906,
            "recall": 0.461264,
            "recall_weighted": 0.466309,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.460449,
            "f1": 0.420585,
            "f1_weighted": 0.423517,
            "precision": 0.44553,
            "precision_weighted": 0.446823,
            "recall": 0.455696,
            "recall_weighted": 0.460449,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.466504,
        "f1": 0.44275,
        "f1_weighted": 0.445753,
        "precision": 0.457542,
        "precision_weighted": 0.45878,
        "recall": 0.461632,
        "recall_weighted": 0.466504,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.466504,
        "hf_subset": "default",
        "languages": [
          "slk-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 117.66086149215698,
  "kg_co2_emissions": null
}