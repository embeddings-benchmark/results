{
  "dataset_revision": "1beac1b941da76a9c51e3e5b39d230fde9a80983",
  "task_name": "SentimentAnalysisHindi",
  "mteb_version": "2.3.11",
  "scores": {
    "train": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.462402,
            "f1": 0.44221,
            "f1_weighted": 0.472925,
            "precision": 0.467311,
            "precision_weighted": 0.537675,
            "recall": 0.488545,
            "recall_weighted": 0.462402,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.465332,
            "f1": 0.446424,
            "f1_weighted": 0.485851,
            "precision": 0.469984,
            "precision_weighted": 0.54465,
            "recall": 0.481143,
            "recall_weighted": 0.465332,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.481934,
            "f1": 0.453961,
            "f1_weighted": 0.487731,
            "precision": 0.461769,
            "precision_weighted": 0.520797,
            "recall": 0.474931,
            "recall_weighted": 0.481934,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.45166,
            "f1": 0.440818,
            "f1_weighted": 0.464834,
            "precision": 0.473876,
            "precision_weighted": 0.544025,
            "recall": 0.500614,
            "recall_weighted": 0.45166,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.494141,
            "f1": 0.460102,
            "f1_weighted": 0.502346,
            "precision": 0.460082,
            "precision_weighted": 0.520233,
            "recall": 0.477455,
            "recall_weighted": 0.494141,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.461914,
            "f1": 0.426698,
            "f1_weighted": 0.468407,
            "precision": 0.431279,
            "precision_weighted": 0.490438,
            "recall": 0.445604,
            "recall_weighted": 0.461914,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.468262,
            "f1": 0.450492,
            "f1_weighted": 0.487512,
            "precision": 0.47504,
            "precision_weighted": 0.543859,
            "recall": 0.484751,
            "recall_weighted": 0.468262,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.504883,
            "f1": 0.480187,
            "f1_weighted": 0.515559,
            "precision": 0.486185,
            "precision_weighted": 0.549776,
            "recall": 0.505468,
            "recall_weighted": 0.504883,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.45166,
            "f1": 0.423936,
            "f1_weighted": 0.459104,
            "precision": 0.435022,
            "precision_weighted": 0.497076,
            "recall": 0.447094,
            "recall_weighted": 0.45166,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.410156,
            "f1": 0.404112,
            "f1_weighted": 0.433273,
            "precision": 0.455961,
            "precision_weighted": 0.533237,
            "recall": 0.460988,
            "recall_weighted": 0.410156,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.465234,
        "f1": 0.442894,
        "f1_weighted": 0.477754,
        "precision": 0.461651,
        "precision_weighted": 0.528177,
        "recall": 0.476659,
        "recall_weighted": 0.465234,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.442894,
        "hf_subset": "default",
        "languages": [
          "hin-Deva"
        ]
      }
    ]
  },
  "evaluation_time": 19.15088438987732,
  "kg_co2_emissions": null
}