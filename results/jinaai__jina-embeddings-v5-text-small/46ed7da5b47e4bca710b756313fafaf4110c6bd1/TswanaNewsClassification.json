{
  "dataset_revision": "061ca1525717eebaaa9bada240f6cbb31eb3aa87",
  "task_name": "TswanaNewsClassification",
  "mteb_version": "2.3.11",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.537988,
            "f1": 0.564634,
            "f1_weighted": 0.5274,
            "precision": 0.542514,
            "precision_weighted": 0.532835,
            "recall": 0.604367,
            "recall_weighted": 0.537988,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.540041,
            "f1": 0.554517,
            "f1_weighted": 0.524891,
            "precision": 0.543093,
            "precision_weighted": 0.533494,
            "recall": 0.584532,
            "recall_weighted": 0.540041,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.537988,
            "f1": 0.538379,
            "f1_weighted": 0.498015,
            "precision": 0.523503,
            "precision_weighted": 0.522738,
            "recall": 0.5902,
            "recall_weighted": 0.537988,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.527721,
            "f1": 0.521468,
            "f1_weighted": 0.476226,
            "precision": 0.509339,
            "precision_weighted": 0.493455,
            "recall": 0.572251,
            "recall_weighted": 0.527721,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.542094,
            "f1": 0.559334,
            "f1_weighted": 0.523828,
            "precision": 0.547896,
            "precision_weighted": 0.54742,
            "recall": 0.595768,
            "recall_weighted": 0.542094,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.529774,
            "f1": 0.538655,
            "f1_weighted": 0.488211,
            "precision": 0.524345,
            "precision_weighted": 0.505377,
            "recall": 0.585988,
            "recall_weighted": 0.529774,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.513347,
            "f1": 0.524111,
            "f1_weighted": 0.486682,
            "precision": 0.507669,
            "precision_weighted": 0.501726,
            "recall": 0.570171,
            "recall_weighted": 0.513347,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.570842,
            "f1": 0.566148,
            "f1_weighted": 0.546503,
            "precision": 0.56694,
            "precision_weighted": 0.584116,
            "recall": 0.59944,
            "recall_weighted": 0.570842,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.544148,
            "f1": 0.546652,
            "f1_weighted": 0.535059,
            "precision": 0.534606,
            "precision_weighted": 0.539413,
            "recall": 0.572125,
            "recall_weighted": 0.544148,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.533881,
            "f1": 0.532803,
            "f1_weighted": 0.509007,
            "precision": 0.51721,
            "precision_weighted": 0.531319,
            "recall": 0.57739,
            "recall_weighted": 0.533881,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.537782,
        "f1": 0.54467,
        "f1_weighted": 0.511582,
        "precision": 0.531712,
        "precision_weighted": 0.529189,
        "recall": 0.585223,
        "recall_weighted": 0.537782,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.537782,
        "hf_subset": "default",
        "languages": [
          "tsn-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 41.19702625274658,
  "kg_co2_emissions": null
}