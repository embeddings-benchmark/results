{
  "dataset_revision": "55caf0e52693a1ea63b15a4980a73fc137fb862b",
  "task_name": "IsiZuluNewsClassification",
  "mteb_version": "2.3.11",
  "scores": {
    "train": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.191489,
            "f1": 0.158437,
            "f1_weighted": 0.184066,
            "precision": 0.166324,
            "precision_weighted": 0.335302,
            "recall": 0.228701,
            "recall_weighted": 0.191489,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.166223,
            "f1": 0.128262,
            "f1_weighted": 0.165227,
            "precision": 0.14805,
            "precision_weighted": 0.375553,
            "recall": 0.193965,
            "recall_weighted": 0.166223,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.214096,
            "f1": 0.166916,
            "f1_weighted": 0.214517,
            "precision": 0.172483,
            "precision_weighted": 0.377172,
            "recall": 0.243263,
            "recall_weighted": 0.214096,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.191489,
            "f1": 0.149038,
            "f1_weighted": 0.188652,
            "precision": 0.145813,
            "precision_weighted": 0.324146,
            "recall": 0.213906,
            "recall_weighted": 0.191489,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.226064,
            "f1": 0.160365,
            "f1_weighted": 0.235209,
            "precision": 0.161295,
            "precision_weighted": 0.3724,
            "recall": 0.227038,
            "recall_weighted": 0.226064,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.210106,
            "f1": 0.163148,
            "f1_weighted": 0.211071,
            "precision": 0.15954,
            "precision_weighted": 0.330666,
            "recall": 0.222665,
            "recall_weighted": 0.210106,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.232713,
            "f1": 0.171648,
            "f1_weighted": 0.238708,
            "precision": 0.174828,
            "precision_weighted": 0.373492,
            "recall": 0.240596,
            "recall_weighted": 0.232713,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.196809,
            "f1": 0.153416,
            "f1_weighted": 0.194834,
            "precision": 0.154306,
            "precision_weighted": 0.333078,
            "recall": 0.223455,
            "recall_weighted": 0.196809,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.18617,
            "f1": 0.146976,
            "f1_weighted": 0.176817,
            "precision": 0.148453,
            "precision_weighted": 0.292609,
            "recall": 0.220002,
            "recall_weighted": 0.18617,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.175532,
            "f1": 0.134091,
            "f1_weighted": 0.167877,
            "precision": 0.139821,
            "precision_weighted": 0.315645,
            "recall": 0.203525,
            "recall_weighted": 0.175532,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.199069,
        "f1": 0.15323,
        "f1_weighted": 0.197698,
        "precision": 0.157091,
        "precision_weighted": 0.343006,
        "recall": 0.221712,
        "recall_weighted": 0.199069,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.199069,
        "hf_subset": "default",
        "languages": [
          "zul-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 79.00327563285828,
  "kg_co2_emissions": null
}