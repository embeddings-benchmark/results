{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "task_name": "MassiveScenarioClassification",
  "mteb_version": "2.3.11",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.920309,
            "f1": 0.916309,
            "f1_weighted": 0.918943,
            "precision": 0.920859,
            "precision_weighted": 0.920575,
            "recall": 0.914616,
            "recall_weighted": 0.920309,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.920309,
            "f1": 0.916175,
            "f1_weighted": 0.918962,
            "precision": 0.920366,
            "precision_weighted": 0.920377,
            "recall": 0.914676,
            "recall_weighted": 0.920309,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.920646,
            "f1": 0.916688,
            "f1_weighted": 0.919421,
            "precision": 0.921104,
            "precision_weighted": 0.921024,
            "recall": 0.915011,
            "recall_weighted": 0.920646,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.919973,
            "f1": 0.915268,
            "f1_weighted": 0.918844,
            "precision": 0.91896,
            "precision_weighted": 0.920375,
            "recall": 0.91413,
            "recall_weighted": 0.919973,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.920309,
            "f1": 0.916069,
            "f1_weighted": 0.91904,
            "precision": 0.920025,
            "precision_weighted": 0.92041,
            "recall": 0.914676,
            "recall_weighted": 0.920309,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.920646,
            "f1": 0.916771,
            "f1_weighted": 0.919472,
            "precision": 0.92114,
            "precision_weighted": 0.921028,
            "recall": 0.915046,
            "recall_weighted": 0.920646,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.921318,
            "f1": 0.917277,
            "f1_weighted": 0.920119,
            "precision": 0.921643,
            "precision_weighted": 0.921624,
            "recall": 0.915546,
            "recall_weighted": 0.921318,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.921318,
            "f1": 0.917093,
            "f1_weighted": 0.920002,
            "precision": 0.921253,
            "precision_weighted": 0.921643,
            "recall": 0.915749,
            "recall_weighted": 0.921318,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.920646,
            "f1": 0.916077,
            "f1_weighted": 0.919414,
            "precision": 0.920266,
            "precision_weighted": 0.92084,
            "recall": 0.914478,
            "recall_weighted": 0.920646,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.919973,
            "f1": 0.915641,
            "f1_weighted": 0.918827,
            "precision": 0.920097,
            "precision_weighted": 0.920193,
            "recall": 0.913624,
            "recall_weighted": 0.919973,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.920545,
        "f1": 0.916337,
        "f1_weighted": 0.919304,
        "precision": 0.920571,
        "precision_weighted": 0.920809,
        "recall": 0.914755,
        "recall_weighted": 0.920545,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.920545,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 74.93175315856934,
  "kg_co2_emissions": null
}