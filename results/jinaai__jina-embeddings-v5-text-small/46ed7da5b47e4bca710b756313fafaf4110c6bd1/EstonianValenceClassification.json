{
  "dataset_revision": "9157397f05a127b3ac93b93dd88abf1bdf710c22",
  "task_name": "EstonianValenceClassification",
  "mteb_version": "2.3.11",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.575795,
            "f1": 0.474586,
            "f1_weighted": 0.563181,
            "precision": 0.491615,
            "precision_weighted": 0.559523,
            "recall": 0.465538,
            "recall_weighted": 0.575795,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.574572,
            "f1": 0.481299,
            "f1_weighted": 0.569209,
            "precision": 0.492401,
            "precision_weighted": 0.570061,
            "recall": 0.476154,
            "recall_weighted": 0.574572,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.556235,
            "f1": 0.467186,
            "f1_weighted": 0.549971,
            "precision": 0.481442,
            "precision_weighted": 0.556162,
            "recall": 0.466077,
            "recall_weighted": 0.556235,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.536675,
            "f1": 0.455294,
            "f1_weighted": 0.539688,
            "precision": 0.466926,
            "precision_weighted": 0.548048,
            "recall": 0.450561,
            "recall_weighted": 0.536675,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.551345,
            "f1": 0.458029,
            "f1_weighted": 0.547606,
            "precision": 0.466164,
            "precision_weighted": 0.550085,
            "recall": 0.456504,
            "recall_weighted": 0.551345,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.579462,
            "f1": 0.479685,
            "f1_weighted": 0.570255,
            "precision": 0.492455,
            "precision_weighted": 0.567778,
            "recall": 0.472718,
            "recall_weighted": 0.579462,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.547677,
            "f1": 0.455256,
            "f1_weighted": 0.545732,
            "precision": 0.469562,
            "precision_weighted": 0.55415,
            "recall": 0.452534,
            "recall_weighted": 0.547677,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.564792,
            "f1": 0.474516,
            "f1_weighted": 0.559654,
            "precision": 0.492829,
            "precision_weighted": 0.566979,
            "recall": 0.46878,
            "recall_weighted": 0.564792,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.562347,
            "f1": 0.469517,
            "f1_weighted": 0.556242,
            "precision": 0.48098,
            "precision_weighted": 0.555299,
            "recall": 0.464266,
            "recall_weighted": 0.562347,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.556235,
            "f1": 0.470624,
            "f1_weighted": 0.556764,
            "precision": 0.47973,
            "precision_weighted": 0.564285,
            "recall": 0.46993,
            "recall_weighted": 0.556235,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.560513,
        "f1": 0.468599,
        "f1_weighted": 0.55583,
        "precision": 0.48141,
        "precision_weighted": 0.559237,
        "recall": 0.464306,
        "recall_weighted": 0.560513,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.560513,
        "hf_subset": "default",
        "languages": [
          "est-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 50.67542219161987,
  "kg_co2_emissions": null
}