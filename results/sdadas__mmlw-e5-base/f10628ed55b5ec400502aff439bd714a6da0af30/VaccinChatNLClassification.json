{
  "dataset_revision": "bd27d0058bea2ad52470d9072a3b5da6b97c1ac3",
  "task_name": "VaccinChatNLClassification",
  "mteb_version": "2.1.17",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.368376,
            "f1": 0.413524,
            "f1_weighted": 0.363293,
            "precision": 0.412629,
            "precision_weighted": 0.489226,
            "recall": 0.542347,
            "recall_weighted": 0.368376,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.353846,
            "f1": 0.406981,
            "f1_weighted": 0.347921,
            "precision": 0.400524,
            "precision_weighted": 0.450554,
            "recall": 0.531479,
            "recall_weighted": 0.353846,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.379487,
            "f1": 0.432063,
            "f1_weighted": 0.370177,
            "precision": 0.419993,
            "precision_weighted": 0.487762,
            "recall": 0.555401,
            "recall_weighted": 0.379487,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.394872,
            "f1": 0.422571,
            "f1_weighted": 0.387293,
            "precision": 0.406838,
            "precision_weighted": 0.495737,
            "recall": 0.550344,
            "recall_weighted": 0.394872,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.387179,
            "f1": 0.423038,
            "f1_weighted": 0.378037,
            "precision": 0.417013,
            "precision_weighted": 0.493329,
            "recall": 0.531646,
            "recall_weighted": 0.387179,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.364103,
            "f1": 0.415338,
            "f1_weighted": 0.353915,
            "precision": 0.409846,
            "precision_weighted": 0.47836,
            "recall": 0.538789,
            "recall_weighted": 0.364103,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.396581,
            "f1": 0.458566,
            "f1_weighted": 0.390478,
            "precision": 0.449549,
            "precision_weighted": 0.492889,
            "recall": 0.567915,
            "recall_weighted": 0.396581,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.367521,
            "f1": 0.403482,
            "f1_weighted": 0.355766,
            "precision": 0.398438,
            "precision_weighted": 0.472975,
            "recall": 0.535343,
            "recall_weighted": 0.367521,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.380342,
            "f1": 0.429513,
            "f1_weighted": 0.37084,
            "precision": 0.422366,
            "precision_weighted": 0.493072,
            "recall": 0.566874,
            "recall_weighted": 0.380342,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.376923,
            "f1": 0.423762,
            "f1_weighted": 0.370879,
            "precision": 0.410511,
            "precision_weighted": 0.491167,
            "recall": 0.543343,
            "recall_weighted": 0.376923,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.376923,
        "f1": 0.422884,
        "f1_weighted": 0.36886,
        "precision": 0.414771,
        "precision_weighted": 0.484507,
        "recall": 0.546348,
        "recall_weighted": 0.376923,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.422884,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 57.27643036842346,
  "kg_co2_emissions": null
}