{
  "dataset_revision": "7fb2f514ea683c5282dfec0a9672ece8de90ac50",
  "task_name": "SinhalaNewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "train": [
      {
        "accuracy": 0.612988,
        "f1": 0.591265,
        "f1_weighted": 0.612318,
        "scores_per_experiment": [
          {
            "accuracy": 0.613281,
            "f1": 0.595488,
            "f1_weighted": 0.620682
          },
          {
            "accuracy": 0.636719,
            "f1": 0.607733,
            "f1_weighted": 0.634547
          },
          {
            "accuracy": 0.572266,
            "f1": 0.546995,
            "f1_weighted": 0.568533
          },
          {
            "accuracy": 0.589844,
            "f1": 0.561332,
            "f1_weighted": 0.582283
          },
          {
            "accuracy": 0.60498,
            "f1": 0.598848,
            "f1_weighted": 0.606189
          },
          {
            "accuracy": 0.615723,
            "f1": 0.585931,
            "f1_weighted": 0.611145
          },
          {
            "accuracy": 0.615723,
            "f1": 0.594831,
            "f1_weighted": 0.611673
          },
          {
            "accuracy": 0.592285,
            "f1": 0.566946,
            "f1_weighted": 0.593217
          },
          {
            "accuracy": 0.635254,
            "f1": 0.614514,
            "f1_weighted": 0.634992
          },
          {
            "accuracy": 0.653809,
            "f1": 0.640031,
            "f1_weighted": 0.65992
          }
        ],
        "main_score": 0.612988,
        "hf_subset": "default",
        "languages": [
          "sin-Sinh"
        ]
      }
    ]
  },
  "evaluation_time": 21.302574396133423,
  "kg_co2_emissions": 0.0014030894294716257
}