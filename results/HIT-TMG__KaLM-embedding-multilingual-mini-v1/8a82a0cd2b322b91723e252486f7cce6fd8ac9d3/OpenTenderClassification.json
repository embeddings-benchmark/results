{
  "dataset_revision": "9af5657575a669dc18c7f897a67287ff7d1a0c65",
  "task_name": "OpenTenderClassification",
  "mteb_version": "2.1.8",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.472018,
            "f1": 0.449712,
            "f1_weighted": 0.449745,
            "precision": 0.473268,
            "precision_weighted": 0.473236,
            "recall": 0.471978,
            "recall_weighted": 0.472018,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.45262,
            "f1": 0.433246,
            "f1_weighted": 0.433215,
            "precision": 0.455244,
            "precision_weighted": 0.455267,
            "recall": 0.452673,
            "recall_weighted": 0.45262,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.480713,
            "f1": 0.454823,
            "f1_weighted": 0.454817,
            "precision": 0.484135,
            "precision_weighted": 0.484168,
            "recall": 0.48075,
            "recall_weighted": 0.480713,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.480713,
            "f1": 0.459235,
            "f1_weighted": 0.459237,
            "precision": 0.473904,
            "precision_weighted": 0.473875,
            "recall": 0.480729,
            "recall_weighted": 0.480713,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.453066,
            "f1": 0.4334,
            "f1_weighted": 0.433407,
            "precision": 0.451796,
            "precision_weighted": 0.451768,
            "recall": 0.453014,
            "recall_weighted": 0.453066,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.470903,
            "f1": 0.448621,
            "f1_weighted": 0.448507,
            "precision": 0.462572,
            "precision_weighted": 0.462522,
            "recall": 0.471047,
            "recall_weighted": 0.470903,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.47068,
            "f1": 0.453021,
            "f1_weighted": 0.45302,
            "precision": 0.471539,
            "precision_weighted": 0.471525,
            "recall": 0.470637,
            "recall_weighted": 0.47068,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.459532,
            "f1": 0.438461,
            "f1_weighted": 0.43841,
            "precision": 0.461049,
            "precision_weighted": 0.460963,
            "recall": 0.459556,
            "recall_weighted": 0.459532,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.441026,
            "f1": 0.414872,
            "f1_weighted": 0.414746,
            "precision": 0.433561,
            "precision_weighted": 0.433376,
            "recall": 0.441095,
            "recall_weighted": 0.441026,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.470903,
            "f1": 0.445337,
            "f1_weighted": 0.445264,
            "precision": 0.458144,
            "precision_weighted": 0.458058,
            "recall": 0.470946,
            "recall_weighted": 0.470903,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.465217,
        "f1": 0.443073,
        "f1_weighted": 0.443037,
        "precision": 0.462521,
        "precision_weighted": 0.462476,
        "recall": 0.465242,
        "recall_weighted": 0.465217,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.443073,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 75.83653426170349,
  "kg_co2_emissions": null
}