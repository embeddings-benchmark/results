{
  "dataset_revision": "d743dcd5abb03d5ab357757a0e83522fc6696fcd",
  "task_name": "DanishPoliticalCommentsClassification",
  "mteb_version": "2.3.3",
  "scores": {
    "train": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.351651,
            "f1": 0.332769,
            "f1_weighted": 0.370468,
            "precision": 0.342075,
            "precision_weighted": 0.458276,
            "recall": 0.394756,
            "recall_weighted": 0.351651,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.361088,
            "f1": 0.327247,
            "f1_weighted": 0.379563,
            "precision": 0.329478,
            "precision_weighted": 0.457674,
            "recall": 0.389732,
            "recall_weighted": 0.361088,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.362476,
            "f1": 0.328852,
            "f1_weighted": 0.382801,
            "precision": 0.328024,
            "precision_weighted": 0.45126,
            "recall": 0.38718,
            "recall_weighted": 0.362476,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.3801,
            "f1": 0.352965,
            "f1_weighted": 0.3947,
            "precision": 0.345554,
            "precision_weighted": 0.45461,
            "recall": 0.427349,
            "recall_weighted": 0.3801,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.384957,
            "f1": 0.34624,
            "f1_weighted": 0.402093,
            "precision": 0.339174,
            "precision_weighted": 0.456576,
            "recall": 0.402903,
            "recall_weighted": 0.384957,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.339162,
            "f1": 0.30597,
            "f1_weighted": 0.35523,
            "precision": 0.305511,
            "precision_weighted": 0.422166,
            "recall": 0.364536,
            "recall_weighted": 0.339162,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.369969,
            "f1": 0.356397,
            "f1_weighted": 0.380563,
            "precision": 0.362016,
            "precision_weighted": 0.476569,
            "recall": 0.419537,
            "recall_weighted": 0.369969,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.346656,
            "f1": 0.326967,
            "f1_weighted": 0.35926,
            "precision": 0.329226,
            "precision_weighted": 0.448927,
            "recall": 0.40429,
            "recall_weighted": 0.346656,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.388426,
            "f1": 0.35205,
            "f1_weighted": 0.400285,
            "precision": 0.340051,
            "precision_weighted": 0.446177,
            "recall": 0.417897,
            "recall_weighted": 0.388426,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.359978,
            "f1": 0.325341,
            "f1_weighted": 0.378628,
            "precision": 0.324393,
            "precision_weighted": 0.442653,
            "recall": 0.388213,
            "recall_weighted": 0.359978,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.364446,
        "f1": 0.33548,
        "f1_weighted": 0.380359,
        "precision": 0.33455,
        "precision_weighted": 0.451489,
        "recall": 0.399639,
        "recall_weighted": 0.364446,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.364446,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 108.05260396003723,
  "kg_co2_emissions": null
}