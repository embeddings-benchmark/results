{
  "dataset_revision": "b07c6ce548f6a7ac8d546e1bbe197a0086409190",
  "task_name": "SweRecClassification",
  "mteb_version": "2.3.3",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.435059,
            "f1": 0.397371,
            "f1_weighted": 0.451105,
            "precision": 0.406654,
            "precision_weighted": 0.481546,
            "recall": 0.414775,
            "recall_weighted": 0.435059,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.470215,
            "f1": 0.455512,
            "f1_weighted": 0.509553,
            "precision": 0.517956,
            "precision_weighted": 0.624693,
            "recall": 0.507734,
            "recall_weighted": 0.470215,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.479492,
            "f1": 0.446962,
            "f1_weighted": 0.516148,
            "precision": 0.485596,
            "precision_weighted": 0.591414,
            "recall": 0.468265,
            "recall_weighted": 0.479492,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.35498,
            "f1": 0.338996,
            "f1_weighted": 0.385945,
            "precision": 0.381393,
            "precision_weighted": 0.462496,
            "recall": 0.366551,
            "recall_weighted": 0.35498,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.440918,
            "f1": 0.419257,
            "f1_weighted": 0.482306,
            "precision": 0.476324,
            "precision_weighted": 0.585641,
            "recall": 0.447288,
            "recall_weighted": 0.440918,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.477539,
            "f1": 0.41618,
            "f1_weighted": 0.48989,
            "precision": 0.427852,
            "precision_weighted": 0.516199,
            "recall": 0.424413,
            "recall_weighted": 0.477539,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.529785,
            "f1": 0.482505,
            "f1_weighted": 0.561923,
            "precision": 0.514415,
            "precision_weighted": 0.625266,
            "recall": 0.501566,
            "recall_weighted": 0.529785,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.480957,
            "f1": 0.431411,
            "f1_weighted": 0.510096,
            "precision": 0.460947,
            "precision_weighted": 0.563059,
            "recall": 0.437808,
            "recall_weighted": 0.480957,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.461914,
            "f1": 0.437612,
            "f1_weighted": 0.490408,
            "precision": 0.468744,
            "precision_weighted": 0.557764,
            "recall": 0.47233,
            "recall_weighted": 0.461914,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.437012,
            "f1": 0.404835,
            "f1_weighted": 0.471293,
            "precision": 0.449277,
            "precision_weighted": 0.549074,
            "recall": 0.42649,
            "recall_weighted": 0.437012,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.456787,
        "f1": 0.423064,
        "f1_weighted": 0.486867,
        "precision": 0.458916,
        "precision_weighted": 0.555715,
        "recall": 0.446722,
        "recall_weighted": 0.456787,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.456787,
        "hf_subset": "default",
        "languages": [
          "swe-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 23.40658140182495,
  "kg_co2_emissions": null
}