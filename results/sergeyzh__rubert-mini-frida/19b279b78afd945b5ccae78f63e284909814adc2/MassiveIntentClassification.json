{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.36.16",
  "scores": {
    "validation": [
      {
        "accuracy": 0.647664,
        "f1": 0.608599,
        "f1_weighted": 0.633221,
        "scores_per_experiment": [
          {
            "accuracy": 0.659616,
            "f1": 0.625382,
            "f1_weighted": 0.649549
          },
          {
            "accuracy": 0.664535,
            "f1": 0.621902,
            "f1_weighted": 0.657157
          },
          {
            "accuracy": 0.648795,
            "f1": 0.60208,
            "f1_weighted": 0.638126
          },
          {
            "accuracy": 0.665027,
            "f1": 0.620407,
            "f1_weighted": 0.653705
          },
          {
            "accuracy": 0.648303,
            "f1": 0.604417,
            "f1_weighted": 0.630373
          },
          {
            "accuracy": 0.625676,
            "f1": 0.592523,
            "f1_weighted": 0.604256
          },
          {
            "accuracy": 0.647319,
            "f1": 0.602269,
            "f1_weighted": 0.633531
          },
          {
            "accuracy": 0.620758,
            "f1": 0.581883,
            "f1_weighted": 0.604521
          },
          {
            "accuracy": 0.632563,
            "f1": 0.613927,
            "f1_weighted": 0.606582
          },
          {
            "accuracy": 0.664043,
            "f1": 0.621196,
            "f1_weighted": 0.65441
          }
        ],
        "main_score": 0.647664,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.645595,
        "f1": 0.625037,
        "f1_weighted": 0.63329,
        "scores_per_experiment": [
          {
            "accuracy": 0.655346,
            "f1": 0.638349,
            "f1_weighted": 0.647148
          },
          {
            "accuracy": 0.670141,
            "f1": 0.63709,
            "f1_weighted": 0.66458
          },
          {
            "accuracy": 0.635844,
            "f1": 0.611981,
            "f1_weighted": 0.628635
          },
          {
            "accuracy": 0.659381,
            "f1": 0.626729,
            "f1_weighted": 0.651412
          },
          {
            "accuracy": 0.640888,
            "f1": 0.624626,
            "f1_weighted": 0.62564
          },
          {
            "accuracy": 0.621385,
            "f1": 0.608287,
            "f1_weighted": 0.602768
          },
          {
            "accuracy": 0.648285,
            "f1": 0.628397,
            "f1_weighted": 0.637509
          },
          {
            "accuracy": 0.63349,
            "f1": 0.618394,
            "f1_weighted": 0.619325
          },
          {
            "accuracy": 0.629455,
            "f1": 0.614532,
            "f1_weighted": 0.603522
          },
          {
            "accuracy": 0.661735,
            "f1": 0.641986,
            "f1_weighted": 0.652357
          }
        ],
        "main_score": 0.645595,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 32.8644540309906,
  "kg_co2_emissions": null
}