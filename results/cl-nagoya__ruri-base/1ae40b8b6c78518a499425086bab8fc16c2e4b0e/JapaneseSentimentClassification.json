{
  "dataset_revision": "113a72dd629207b956dd4db3c2d11445853f3b1f",
  "task_name": "JapaneseSentimentClassification",
  "mteb_version": "2.2.0",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.887539,
            "f1": 0.881743,
            "f1_weighted": 0.88876,
            "precision": 0.87559,
            "precision_weighted": 0.893916,
            "recall": 0.8923,
            "recall_weighted": 0.887539,
            "ap": 0.905075,
            "ap_weighted": 0.905075
          },
          {
            "accuracy": 0.890674,
            "f1": 0.884523,
            "f1_weighted": 0.891666,
            "precision": 0.878932,
            "precision_weighted": 0.89519,
            "recall": 0.892962,
            "recall_weighted": 0.890674,
            "ap": 0.904276,
            "ap_weighted": 0.904276
          },
          {
            "accuracy": 0.848746,
            "f1": 0.844779,
            "f1_weighted": 0.85143,
            "precision": 0.842731,
            "precision_weighted": 0.873899,
            "recall": 0.86827,
            "recall_weighted": 0.848746,
            "ap": 0.892529,
            "ap_weighted": 0.892529
          },
          {
            "accuracy": 0.791536,
            "f1": 0.787632,
            "f1_weighted": 0.79535,
            "precision": 0.791119,
            "precision_weighted": 0.826793,
            "recall": 0.813648,
            "recall_weighted": 0.791536,
            "ap": 0.846197,
            "ap_weighted": 0.846197
          },
          {
            "accuracy": 0.805643,
            "f1": 0.801044,
            "f1_weighted": 0.809151,
            "precision": 0.801203,
            "precision_weighted": 0.834211,
            "recall": 0.824094,
            "recall_weighted": 0.805643,
            "ap": 0.853296,
            "ap_weighted": 0.853296
          },
          {
            "accuracy": 0.876176,
            "f1": 0.86814,
            "f1_weighted": 0.876864,
            "precision": 0.864497,
            "precision_weighted": 0.878365,
            "recall": 0.872702,
            "recall_weighted": 0.876176,
            "ap": 0.883971,
            "ap_weighted": 0.883971
          },
          {
            "accuracy": 0.713166,
            "f1": 0.713166,
            "f1_weighted": 0.713166,
            "precision": 0.768363,
            "precision_weighted": 0.823561,
            "recall": 0.768363,
            "recall_weighted": 0.713166,
            "ap": 0.8254,
            "ap_weighted": 0.8254
          },
          {
            "accuracy": 0.765282,
            "f1": 0.751981,
            "f1_weighted": 0.767375,
            "precision": 0.748597,
            "precision_weighted": 0.771514,
            "recall": 0.757639,
            "recall_weighted": 0.765282,
            "ap": 0.791311,
            "ap_weighted": 0.791311
          },
          {
            "accuracy": 0.794671,
            "f1": 0.7929,
            "f1_weighted": 0.798033,
            "precision": 0.807929,
            "precision_weighted": 0.850703,
            "recall": 0.829472,
            "recall_weighted": 0.794671,
            "ap": 0.867345,
            "ap_weighted": 0.867345
          },
          {
            "accuracy": 0.881661,
            "f1": 0.872845,
            "f1_weighted": 0.881819,
            "precision": 0.871876,
            "precision_weighted": 0.882017,
            "recall": 0.87386,
            "recall_weighted": 0.881661,
            "ap": 0.882961,
            "ap_weighted": 0.882961
          }
        ],
        "accuracy": 0.825509,
        "f1": 0.819875,
        "f1_weighted": 0.827361,
        "precision": 0.825084,
        "precision_weighted": 0.853017,
        "recall": 0.839331,
        "recall_weighted": 0.825509,
        "ap": 0.865236,
        "ap_weighted": 0.865236,
        "main_score": 0.825509,
        "hf_subset": "default",
        "languages": [
          "jpn-Jpan"
        ]
      }
    ]
  },
  "evaluation_time": 4.264770269393921,
  "kg_co2_emissions": null
}