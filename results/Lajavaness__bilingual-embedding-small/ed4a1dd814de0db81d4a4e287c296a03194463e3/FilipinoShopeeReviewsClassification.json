{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.323682,
        "f1": 0.319643,
        "f1_weighted": 0.319641,
        "scores_per_experiment": [
          {
            "accuracy": 0.324219,
            "f1": 0.320408,
            "f1_weighted": 0.320408
          },
          {
            "accuracy": 0.321289,
            "f1": 0.317867,
            "f1_weighted": 0.317863
          },
          {
            "accuracy": 0.347656,
            "f1": 0.349331,
            "f1_weighted": 0.349335
          },
          {
            "accuracy": 0.354492,
            "f1": 0.348982,
            "f1_weighted": 0.348984
          },
          {
            "accuracy": 0.310547,
            "f1": 0.302824,
            "f1_weighted": 0.302804
          },
          {
            "accuracy": 0.337402,
            "f1": 0.330965,
            "f1_weighted": 0.330944
          },
          {
            "accuracy": 0.272461,
            "f1": 0.277332,
            "f1_weighted": 0.277312
          },
          {
            "accuracy": 0.359375,
            "f1": 0.353172,
            "f1_weighted": 0.353151
          },
          {
            "accuracy": 0.317383,
            "f1": 0.312813,
            "f1_weighted": 0.312814
          },
          {
            "accuracy": 0.291992,
            "f1": 0.282736,
            "f1_weighted": 0.2828
          }
        ],
        "main_score": 0.323682,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.323682,
        "f1": 0.319214,
        "f1_weighted": 0.319217,
        "scores_per_experiment": [
          {
            "accuracy": 0.323242,
            "f1": 0.318543,
            "f1_weighted": 0.318559
          },
          {
            "accuracy": 0.316895,
            "f1": 0.313949,
            "f1_weighted": 0.313946
          },
          {
            "accuracy": 0.341797,
            "f1": 0.342331,
            "f1_weighted": 0.342337
          },
          {
            "accuracy": 0.347656,
            "f1": 0.339482,
            "f1_weighted": 0.339472
          },
          {
            "accuracy": 0.310059,
            "f1": 0.302817,
            "f1_weighted": 0.302804
          },
          {
            "accuracy": 0.326172,
            "f1": 0.315938,
            "f1_weighted": 0.315909
          },
          {
            "accuracy": 0.28418,
            "f1": 0.289285,
            "f1_weighted": 0.289272
          },
          {
            "accuracy": 0.353516,
            "f1": 0.345305,
            "f1_weighted": 0.345309
          },
          {
            "accuracy": 0.330566,
            "f1": 0.328884,
            "f1_weighted": 0.32888
          },
          {
            "accuracy": 0.302734,
            "f1": 0.295608,
            "f1_weighted": 0.295679
          }
        ],
        "main_score": 0.323682,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 14.346263408660889,
  "kg_co2_emissions": 0.0004334615817603919
}