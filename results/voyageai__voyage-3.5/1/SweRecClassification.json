{
  "dataset_revision": "b07c6ce548f6a7ac8d546e1bbe197a0086409190",
  "task_name": "SweRecClassification",
  "mteb_version": "2.1.11",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.76416,
            "f1": 0.677071,
            "f1_weighted": 0.776184,
            "precision": 0.676639,
            "precision_weighted": 0.793665,
            "recall": 0.686733,
            "recall_weighted": 0.76416,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.778809,
            "f1": 0.703791,
            "f1_weighted": 0.794642,
            "precision": 0.706749,
            "precision_weighted": 0.820522,
            "recall": 0.719402,
            "recall_weighted": 0.778809,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.751953,
            "f1": 0.684172,
            "f1_weighted": 0.773017,
            "precision": 0.697464,
            "precision_weighted": 0.824598,
            "recall": 0.711607,
            "recall_weighted": 0.751953,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.805176,
            "f1": 0.707275,
            "f1_weighted": 0.809724,
            "precision": 0.707792,
            "precision_weighted": 0.816854,
            "recall": 0.709867,
            "recall_weighted": 0.805176,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.787109,
            "f1": 0.644061,
            "f1_weighted": 0.775095,
            "precision": 0.649354,
            "precision_weighted": 0.765888,
            "recall": 0.644452,
            "recall_weighted": 0.787109,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.802734,
            "f1": 0.680943,
            "f1_weighted": 0.794997,
            "precision": 0.687451,
            "precision_weighted": 0.788959,
            "recall": 0.678235,
            "recall_weighted": 0.802734,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.799316,
            "f1": 0.698671,
            "f1_weighted": 0.800817,
            "precision": 0.697613,
            "precision_weighted": 0.802473,
            "recall": 0.699938,
            "recall_weighted": 0.799316,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.705078,
            "f1": 0.659855,
            "f1_weighted": 0.743231,
            "precision": 0.695759,
            "precision_weighted": 0.830646,
            "recall": 0.703577,
            "recall_weighted": 0.705078,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.69873,
            "f1": 0.640229,
            "f1_weighted": 0.732945,
            "precision": 0.67674,
            "precision_weighted": 0.81699,
            "recall": 0.670715,
            "recall_weighted": 0.69873,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.787598,
            "f1": 0.689585,
            "f1_weighted": 0.792702,
            "precision": 0.688414,
            "precision_weighted": 0.799023,
            "recall": 0.69291,
            "recall_weighted": 0.787598,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.768066,
        "f1": 0.678565,
        "f1_weighted": 0.779335,
        "precision": 0.688398,
        "precision_weighted": 0.805962,
        "recall": 0.691744,
        "recall_weighted": 0.768066,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.768066,
        "hf_subset": "default",
        "languages": [
          "swe-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 215.13013815879822,
  "kg_co2_emissions": null
}