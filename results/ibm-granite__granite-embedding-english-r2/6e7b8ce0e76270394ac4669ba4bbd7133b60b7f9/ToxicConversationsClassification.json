{
  "dataset_revision": "edfaf9da55d3dd50d43143d90c1ac476895ae6de",
  "task_name": "ToxicConversationsClassification",
  "mteb_version": "1.36.8",
  "scores": {
    "test": [
      {
        "accuracy": 0.620801,
        "f1": 0.473896,
        "f1_weighted": 0.704236,
        "ap": 0.103738,
        "ap_weighted": 0.103738,
        "scores_per_experiment": [
          {
            "accuracy": 0.647949,
            "f1": 0.482078,
            "f1_weighted": 0.72881,
            "ap": 0.097997,
            "ap_weighted": 0.097997
          },
          {
            "accuracy": 0.710938,
            "f1": 0.52373,
            "f1_weighted": 0.77509,
            "ap": 0.112947,
            "ap_weighted": 0.112947
          },
          {
            "accuracy": 0.717773,
            "f1": 0.515123,
            "f1_weighted": 0.778997,
            "ap": 0.101989,
            "ap_weighted": 0.101989
          },
          {
            "accuracy": 0.677246,
            "f1": 0.509784,
            "f1_weighted": 0.750974,
            "ap": 0.113571,
            "ap_weighted": 0.113571
          },
          {
            "accuracy": 0.479492,
            "f1": 0.399371,
            "f1_weighted": 0.584036,
            "ap": 0.096226,
            "ap_weighted": 0.096226
          },
          {
            "accuracy": 0.525391,
            "f1": 0.426622,
            "f1_weighted": 0.626948,
            "ap": 0.100008,
            "ap_weighted": 0.100008
          },
          {
            "accuracy": 0.697266,
            "f1": 0.501204,
            "f1_weighted": 0.764452,
            "ap": 0.097101,
            "ap_weighted": 0.097101
          },
          {
            "accuracy": 0.570801,
            "f1": 0.455322,
            "f1_weighted": 0.666442,
            "ap": 0.107511,
            "ap_weighted": 0.107511
          },
          {
            "accuracy": 0.585449,
            "f1": 0.46201,
            "f1_weighted": 0.678941,
            "ap": 0.106917,
            "ap_weighted": 0.106917
          },
          {
            "accuracy": 0.595703,
            "f1": 0.46372,
            "f1_weighted": 0.687676,
            "ap": 0.103116,
            "ap_weighted": 0.103116
          }
        ],
        "main_score": 0.620801,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 13.800911664962769,
  "kg_co2_emissions": null
}