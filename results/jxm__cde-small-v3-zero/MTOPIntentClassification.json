{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "task_name": "MTOPIntentClassification",
  "mteb_version": "1.36.33",
  "scores": {
    "test": [
      {
        "accuracy": 0.779298,
        "f1": 0.588292,
        "f1_weighted": 0.807528,
        "scores_per_experiment": [
          {
            "accuracy": 0.77565,
            "f1": 0.570253,
            "f1_weighted": 0.80874
          },
          {
            "accuracy": 0.795942,
            "f1": 0.596583,
            "f1_weighted": 0.8208
          },
          {
            "accuracy": 0.769494,
            "f1": 0.584919,
            "f1_weighted": 0.800729
          },
          {
            "accuracy": 0.796626,
            "f1": 0.605124,
            "f1_weighted": 0.819885
          },
          {
            "accuracy": 0.772686,
            "f1": 0.582288,
            "f1_weighted": 0.798839
          },
          {
            "accuracy": 0.761058,
            "f1": 0.567021,
            "f1_weighted": 0.792073
          },
          {
            "accuracy": 0.779982,
            "f1": 0.589645,
            "f1_weighted": 0.812854
          },
          {
            "accuracy": 0.805746,
            "f1": 0.629763,
            "f1_weighted": 0.832427
          },
          {
            "accuracy": 0.776334,
            "f1": 0.595001,
            "f1_weighted": 0.804891
          },
          {
            "accuracy": 0.759462,
            "f1": 0.562323,
            "f1_weighted": 0.784039
          }
        ],
        "main_score": 0.779298,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 121.36161017417908,
  "kg_co2_emissions": null
}