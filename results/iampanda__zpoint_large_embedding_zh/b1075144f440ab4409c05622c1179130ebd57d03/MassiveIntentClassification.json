{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.430989,
        "f1": 0.411702,
        "f1_weighted": 0.434324,
        "scores_per_experiment": [
          {
            "accuracy": 0.409739,
            "f1": 0.406496,
            "f1_weighted": 0.416449
          },
          {
            "accuracy": 0.436301,
            "f1": 0.407571,
            "f1_weighted": 0.440839
          },
          {
            "accuracy": 0.426955,
            "f1": 0.409599,
            "f1_weighted": 0.429312
          },
          {
            "accuracy": 0.434825,
            "f1": 0.418507,
            "f1_weighted": 0.435925
          },
          {
            "accuracy": 0.468273,
            "f1": 0.435343,
            "f1_weighted": 0.471667
          },
          {
            "accuracy": 0.435809,
            "f1": 0.41612,
            "f1_weighted": 0.443298
          },
          {
            "accuracy": 0.41515,
            "f1": 0.390907,
            "f1_weighted": 0.416744
          },
          {
            "accuracy": 0.43089,
            "f1": 0.413255,
            "f1_weighted": 0.433552
          },
          {
            "accuracy": 0.43335,
            "f1": 0.426593,
            "f1_weighted": 0.436039
          },
          {
            "accuracy": 0.418593,
            "f1": 0.392628,
            "f1_weighted": 0.419414
          }
        ],
        "main_score": 0.430989,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.439341,
        "f1": 0.418825,
        "f1_weighted": 0.442294,
        "scores_per_experiment": [
          {
            "accuracy": 0.430397,
            "f1": 0.423115,
            "f1_weighted": 0.434038
          },
          {
            "accuracy": 0.440484,
            "f1": 0.410998,
            "f1_weighted": 0.442245
          },
          {
            "accuracy": 0.444183,
            "f1": 0.424836,
            "f1_weighted": 0.448527
          },
          {
            "accuracy": 0.437122,
            "f1": 0.414146,
            "f1_weighted": 0.440884
          },
          {
            "accuracy": 0.453598,
            "f1": 0.421425,
            "f1_weighted": 0.453871
          },
          {
            "accuracy": 0.444855,
            "f1": 0.431923,
            "f1_weighted": 0.45146
          },
          {
            "accuracy": 0.434432,
            "f1": 0.41924,
            "f1_weighted": 0.437378
          },
          {
            "accuracy": 0.433423,
            "f1": 0.411887,
            "f1_weighted": 0.43604
          },
          {
            "accuracy": 0.43813,
            "f1": 0.416989,
            "f1_weighted": 0.439526
          },
          {
            "accuracy": 0.436785,
            "f1": 0.413692,
            "f1_weighted": 0.438975
          }
        ],
        "main_score": 0.439341,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 90.107834815979,
  "kg_co2_emissions": null
}