{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "task_name": "MassiveScenarioClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.45391,
        "f1": 0.434305,
        "f1_weighted": 0.459953,
        "scores_per_experiment": [
          {
            "accuracy": 0.458436,
            "f1": 0.445346,
            "f1_weighted": 0.459369
          },
          {
            "accuracy": 0.447614,
            "f1": 0.42482,
            "f1_weighted": 0.461034
          },
          {
            "accuracy": 0.441712,
            "f1": 0.431951,
            "f1_weighted": 0.452385
          },
          {
            "accuracy": 0.445647,
            "f1": 0.412649,
            "f1_weighted": 0.45155
          },
          {
            "accuracy": 0.46483,
            "f1": 0.445316,
            "f1_weighted": 0.473876
          },
          {
            "accuracy": 0.461387,
            "f1": 0.447035,
            "f1_weighted": 0.470334
          },
          {
            "accuracy": 0.438269,
            "f1": 0.426468,
            "f1_weighted": 0.443727
          },
          {
            "accuracy": 0.492376,
            "f1": 0.46245,
            "f1_weighted": 0.49283
          },
          {
            "accuracy": 0.430398,
            "f1": 0.409702,
            "f1_weighted": 0.42846
          },
          {
            "accuracy": 0.458436,
            "f1": 0.437318,
            "f1_weighted": 0.465969
          }
        ],
        "main_score": 0.45391,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.469166,
        "f1": 0.438219,
        "f1_weighted": 0.474935,
        "scores_per_experiment": [
          {
            "accuracy": 0.472091,
            "f1": 0.438393,
            "f1_weighted": 0.474293
          },
          {
            "accuracy": 0.470074,
            "f1": 0.441383,
            "f1_weighted": 0.479317
          },
          {
            "accuracy": 0.474109,
            "f1": 0.447063,
            "f1_weighted": 0.483347
          },
          {
            "accuracy": 0.481843,
            "f1": 0.435269,
            "f1_weighted": 0.488888
          },
          {
            "accuracy": 0.465703,
            "f1": 0.430577,
            "f1_weighted": 0.474146
          },
          {
            "accuracy": 0.468729,
            "f1": 0.443832,
            "f1_weighted": 0.477408
          },
          {
            "accuracy": 0.457297,
            "f1": 0.434396,
            "f1_weighted": 0.466562
          },
          {
            "accuracy": 0.506389,
            "f1": 0.47371,
            "f1_weighted": 0.50583
          },
          {
            "accuracy": 0.44889,
            "f1": 0.418741,
            "f1_weighted": 0.449689
          },
          {
            "accuracy": 0.446537,
            "f1": 0.418822,
            "f1_weighted": 0.449867
          }
        ],
        "main_score": 0.469166,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 44.56996512413025,
  "kg_co2_emissions": null
}