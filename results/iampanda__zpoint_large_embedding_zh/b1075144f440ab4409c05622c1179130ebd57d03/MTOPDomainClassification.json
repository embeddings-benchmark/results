{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "task_name": "MTOPDomainClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.623912,
        "f1": 0.607167,
        "f1_weighted": 0.627291,
        "scores_per_experiment": [
          {
            "accuracy": 0.643526,
            "f1": 0.62991,
            "f1_weighted": 0.645676
          },
          {
            "accuracy": 0.572452,
            "f1": 0.562496,
            "f1_weighted": 0.577913
          },
          {
            "accuracy": 0.604959,
            "f1": 0.586238,
            "f1_weighted": 0.605694
          },
          {
            "accuracy": 0.634711,
            "f1": 0.618919,
            "f1_weighted": 0.640943
          },
          {
            "accuracy": 0.649587,
            "f1": 0.631188,
            "f1_weighted": 0.654447
          },
          {
            "accuracy": 0.573554,
            "f1": 0.549663,
            "f1_weighted": 0.571014
          },
          {
            "accuracy": 0.643526,
            "f1": 0.624996,
            "f1_weighted": 0.649877
          },
          {
            "accuracy": 0.641322,
            "f1": 0.631013,
            "f1_weighted": 0.646969
          },
          {
            "accuracy": 0.656198,
            "f1": 0.637635,
            "f1_weighted": 0.661447
          },
          {
            "accuracy": 0.619284,
            "f1": 0.599611,
            "f1_weighted": 0.618931
          }
        ],
        "main_score": 0.623912,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.63246,
        "f1": 0.611913,
        "f1_weighted": 0.635732,
        "scores_per_experiment": [
          {
            "accuracy": 0.653705,
            "f1": 0.637195,
            "f1_weighted": 0.657278
          },
          {
            "accuracy": 0.567484,
            "f1": 0.551743,
            "f1_weighted": 0.574128
          },
          {
            "accuracy": 0.629191,
            "f1": 0.610265,
            "f1_weighted": 0.631185
          },
          {
            "accuracy": 0.647506,
            "f1": 0.622865,
            "f1_weighted": 0.653378
          },
          {
            "accuracy": 0.646379,
            "f1": 0.624131,
            "f1_weighted": 0.649452
          },
          {
            "accuracy": 0.607495,
            "f1": 0.581301,
            "f1_weighted": 0.604041
          },
          {
            "accuracy": 0.655678,
            "f1": 0.632642,
            "f1_weighted": 0.661189
          },
          {
            "accuracy": 0.640744,
            "f1": 0.624585,
            "f1_weighted": 0.647511
          },
          {
            "accuracy": 0.661031,
            "f1": 0.640342,
            "f1_weighted": 0.665088
          },
          {
            "accuracy": 0.615385,
            "f1": 0.594066,
            "f1_weighted": 0.614071
          }
        ],
        "main_score": 0.63246,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 42.767937898635864,
  "kg_co2_emissions": null
}