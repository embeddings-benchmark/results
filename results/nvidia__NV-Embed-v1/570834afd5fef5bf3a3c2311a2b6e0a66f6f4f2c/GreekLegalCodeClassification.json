{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "task_name": "GreekLegalCodeClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.124756,
        "f1": 0.091007,
        "f1_weighted": 0.115495,
        "scores_per_experiment": [
          {
            "accuracy": 0.11377,
            "f1": 0.09161,
            "f1_weighted": 0.105777
          },
          {
            "accuracy": 0.120605,
            "f1": 0.085535,
            "f1_weighted": 0.110575
          },
          {
            "accuracy": 0.122559,
            "f1": 0.092696,
            "f1_weighted": 0.115662
          },
          {
            "accuracy": 0.126953,
            "f1": 0.089421,
            "f1_weighted": 0.123803
          },
          {
            "accuracy": 0.124023,
            "f1": 0.087294,
            "f1_weighted": 0.11463
          },
          {
            "accuracy": 0.117188,
            "f1": 0.082213,
            "f1_weighted": 0.11065
          },
          {
            "accuracy": 0.126465,
            "f1": 0.088138,
            "f1_weighted": 0.114665
          },
          {
            "accuracy": 0.130371,
            "f1": 0.09363,
            "f1_weighted": 0.117447
          },
          {
            "accuracy": 0.138672,
            "f1": 0.09945,
            "f1_weighted": 0.130084
          },
          {
            "accuracy": 0.126953,
            "f1": 0.100081,
            "f1_weighted": 0.111656
          }
        ],
        "main_score": 0.124756,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.127393,
        "f1": 0.096612,
        "f1_weighted": 0.116943,
        "scores_per_experiment": [
          {
            "accuracy": 0.117188,
            "f1": 0.093686,
            "f1_weighted": 0.105443
          },
          {
            "accuracy": 0.124023,
            "f1": 0.091908,
            "f1_weighted": 0.111159
          },
          {
            "accuracy": 0.12793,
            "f1": 0.107422,
            "f1_weighted": 0.114697
          },
          {
            "accuracy": 0.130859,
            "f1": 0.099624,
            "f1_weighted": 0.123596
          },
          {
            "accuracy": 0.117188,
            "f1": 0.085941,
            "f1_weighted": 0.105043
          },
          {
            "accuracy": 0.125977,
            "f1": 0.096448,
            "f1_weighted": 0.118915
          },
          {
            "accuracy": 0.137695,
            "f1": 0.101911,
            "f1_weighted": 0.12726
          },
          {
            "accuracy": 0.129883,
            "f1": 0.100775,
            "f1_weighted": 0.123742
          },
          {
            "accuracy": 0.144043,
            "f1": 0.102132,
            "f1_weighted": 0.139087
          },
          {
            "accuracy": 0.119141,
            "f1": 0.086274,
            "f1_weighted": 0.100483
          }
        ],
        "main_score": 0.127393,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ]
  },
  "evaluation_time": 27502.2389793396,
  "kg_co2_emissions": 2.4801948836497134
}