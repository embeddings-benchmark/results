{
  "dataset_revision": "b89853e6de927b0e3bfa8ecc0e56fe4e02ceafc6",
  "task_name": "AllegroReviews",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.40994,
        "f1": 0.359215,
        "f1_weighted": 0.417636,
        "scores_per_experiment": [
          {
            "accuracy": 0.431412,
            "f1": 0.379623,
            "f1_weighted": 0.446438
          },
          {
            "accuracy": 0.477137,
            "f1": 0.418316,
            "f1_weighted": 0.484364
          },
          {
            "accuracy": 0.324056,
            "f1": 0.311167,
            "f1_weighted": 0.341692
          },
          {
            "accuracy": 0.387674,
            "f1": 0.348065,
            "f1_weighted": 0.398724
          },
          {
            "accuracy": 0.467197,
            "f1": 0.387049,
            "f1_weighted": 0.471818
          },
          {
            "accuracy": 0.380716,
            "f1": 0.34327,
            "f1_weighted": 0.392751
          },
          {
            "accuracy": 0.373757,
            "f1": 0.332119,
            "f1_weighted": 0.385112
          },
          {
            "accuracy": 0.370775,
            "f1": 0.353166,
            "f1_weighted": 0.390895
          },
          {
            "accuracy": 0.478131,
            "f1": 0.358702,
            "f1_weighted": 0.439705
          },
          {
            "accuracy": 0.408549,
            "f1": 0.36067,
            "f1_weighted": 0.424864
          }
        ],
        "main_score": 0.40994,
        "hf_subset": "default",
        "languages": [
          "pol-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 31.81968379020691,
  "kg_co2_emissions": 0.0021345762761542887
}