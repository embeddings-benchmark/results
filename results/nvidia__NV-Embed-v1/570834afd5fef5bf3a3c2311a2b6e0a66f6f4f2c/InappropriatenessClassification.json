{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "evaluation_time": 75.5907051563263,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.5970703125,
        "ap": 0.557315117150581,
        "ap_weighted": 0.557315117150581,
        "f1": 0.5905441943515085,
        "f1_weighted": 0.5905441943515085,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.5970703125,
        "scores_per_experiment": [
          {
            "accuracy": 0.60595703125,
            "ap": 0.5627793151907503,
            "ap_weighted": 0.5627793151907503,
            "f1": 0.6038602115048454,
            "f1_weighted": 0.6038602115048454
          },
          {
            "accuracy": 0.62353515625,
            "ap": 0.5740242034313725,
            "ap_weighted": 0.5740242034313725,
            "f1": 0.6177941922923591,
            "f1_weighted": 0.6177941922923591
          },
          {
            "accuracy": 0.6015625,
            "ap": 0.5632370283018868,
            "ap_weighted": 0.5632370283018868,
            "f1": 0.59859804464121,
            "f1_weighted": 0.59859804464121
          },
          {
            "accuracy": 0.5546875,
            "ap": 0.5301899395910781,
            "ap_weighted": 0.5301899395910781,
            "f1": 0.5544002290294876,
            "f1_weighted": 0.5544002290294876
          },
          {
            "accuracy": 0.57275390625,
            "ap": 0.5430933918060719,
            "ap_weighted": 0.5430933918060719,
            "f1": 0.567902797419473,
            "f1_weighted": 0.567902797419473
          },
          {
            "accuracy": 0.58447265625,
            "ap": 0.5470657298207204,
            "ap_weighted": 0.5470657298207204,
            "f1": 0.5593508568377241,
            "f1_weighted": 0.5593508568377241
          },
          {
            "accuracy": 0.623046875,
            "ap": 0.5738086247028527,
            "ap_weighted": 0.5738086247028527,
            "f1": 0.6178864382283706,
            "f1_weighted": 0.6178864382283706
          },
          {
            "accuracy": 0.630859375,
            "ap": 0.5773909127728513,
            "ap_weighted": 0.5773909127728513,
            "f1": 0.6128253987306636,
            "f1_weighted": 0.6128253987306636
          },
          {
            "accuracy": 0.57373046875,
            "ap": 0.5423931890826713,
            "ap_weighted": 0.5423931890826713,
            "f1": 0.5737010954896442,
            "f1_weighted": 0.5737010954896442
          },
          {
            "accuracy": 0.60009765625,
            "ap": 0.5591688368055556,
            "ap_weighted": 0.5591688368055556,
            "f1": 0.5991226793413069,
            "f1_weighted": 0.5991226793413069
          }
        ]
      }
    ]
  },
  "task_name": "InappropriatenessClassification"
}