{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.782216,
        "f1": 0.655282,
        "f1_weighted": 0.810366,
        "ap": 0.273129,
        "ap_weighted": 0.273129,
        "scores_per_experiment": [
          {
            "accuracy": 0.64433,
            "f1": 0.537215,
            "f1_weighted": 0.704008,
            "ap": 0.178782,
            "ap_weighted": 0.178782
          },
          {
            "accuracy": 0.87457,
            "f1": 0.723801,
            "f1_weighted": 0.876674,
            "ap": 0.328108,
            "ap_weighted": 0.328108
          },
          {
            "accuracy": 0.830756,
            "f1": 0.695987,
            "f1_weighted": 0.847624,
            "ap": 0.302722,
            "ap_weighted": 0.302722
          },
          {
            "accuracy": 0.793814,
            "f1": 0.668313,
            "f1_weighted": 0.821158,
            "ap": 0.281388,
            "ap_weighted": 0.281388
          },
          {
            "accuracy": 0.705326,
            "f1": 0.574753,
            "f1_weighted": 0.75128,
            "ap": 0.19203,
            "ap_weighted": 0.19203
          },
          {
            "accuracy": 0.853093,
            "f1": 0.719691,
            "f1_weighted": 0.864556,
            "ap": 0.331326,
            "ap_weighted": 0.331326
          },
          {
            "accuracy": 0.859107,
            "f1": 0.701342,
            "f1_weighted": 0.863955,
            "ap": 0.295381,
            "ap_weighted": 0.295381
          },
          {
            "accuracy": 0.712199,
            "f1": 0.622666,
            "f1_weighted": 0.760361,
            "ap": 0.27351,
            "ap_weighted": 0.27351
          },
          {
            "accuracy": 0.754296,
            "f1": 0.628128,
            "f1_weighted": 0.790396,
            "ap": 0.240969,
            "ap_weighted": 0.240969
          },
          {
            "accuracy": 0.794674,
            "f1": 0.680921,
            "f1_weighted": 0.823644,
            "ap": 0.307073,
            "ap_weighted": 0.307073
          }
        ],
        "main_score": 0.782216,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 15.359938383102417,
  "kg_co2_emissions": 0.0009332410749627706
}