{
  "dataset_revision": "f6d2c31f4dc6b88f468552750bfec05b4b41b05a",
  "evaluation_time": 78.14594507217407,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.62529296875,
        "f1": 0.6167672496731613,
        "f1_weighted": 0.6167681451340391,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.62529296875,
        "scores_per_experiment": [
          {
            "accuracy": 0.65625,
            "f1": 0.6497163146483244,
            "f1_weighted": 0.6497193465252993
          },
          {
            "accuracy": 0.6044921875,
            "f1": 0.5866212731441977,
            "f1_weighted": 0.5865952400222377
          },
          {
            "accuracy": 0.6123046875,
            "f1": 0.6074455877047897,
            "f1_weighted": 0.6074475464008124
          },
          {
            "accuracy": 0.64306640625,
            "f1": 0.6421343552870485,
            "f1_weighted": 0.6421550808152551
          },
          {
            "accuracy": 0.66455078125,
            "f1": 0.6673582735217226,
            "f1_weighted": 0.6673609632495248
          },
          {
            "accuracy": 0.6083984375,
            "f1": 0.5906677915359994,
            "f1_weighted": 0.5906797612358929
          },
          {
            "accuracy": 0.603515625,
            "f1": 0.5913842360589063,
            "f1_weighted": 0.5913952265786976
          },
          {
            "accuracy": 0.58740234375,
            "f1": 0.5835766437687119,
            "f1_weighted": 0.5835720031041085
          },
          {
            "accuracy": 0.6015625,
            "f1": 0.5820806374461871,
            "f1_weighted": 0.5820429382762148
          },
          {
            "accuracy": 0.67138671875,
            "f1": 0.6666873836157258,
            "f1_weighted": 0.6667133451323478
          }
        ]
      }
    ]
  },
  "task_name": "RuReviewsClassification"
}