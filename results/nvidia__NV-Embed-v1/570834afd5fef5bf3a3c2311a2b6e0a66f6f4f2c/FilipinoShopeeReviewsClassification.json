{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.342041,
        "f1": 0.325325,
        "f1_weighted": 0.325324,
        "scores_per_experiment": [
          {
            "accuracy": 0.368164,
            "f1": 0.361002,
            "f1_weighted": 0.360987
          },
          {
            "accuracy": 0.299316,
            "f1": 0.281419,
            "f1_weighted": 0.281464
          },
          {
            "accuracy": 0.347168,
            "f1": 0.327652,
            "f1_weighted": 0.327673
          },
          {
            "accuracy": 0.361328,
            "f1": 0.354047,
            "f1_weighted": 0.354004
          },
          {
            "accuracy": 0.352051,
            "f1": 0.318768,
            "f1_weighted": 0.318739
          },
          {
            "accuracy": 0.354492,
            "f1": 0.329471,
            "f1_weighted": 0.32946
          },
          {
            "accuracy": 0.336426,
            "f1": 0.322151,
            "f1_weighted": 0.322153
          },
          {
            "accuracy": 0.365723,
            "f1": 0.359302,
            "f1_weighted": 0.359298
          },
          {
            "accuracy": 0.333496,
            "f1": 0.320456,
            "f1_weighted": 0.320414
          },
          {
            "accuracy": 0.302246,
            "f1": 0.278985,
            "f1_weighted": 0.279046
          }
        ],
        "main_score": 0.342041,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.337939,
        "f1": 0.320829,
        "f1_weighted": 0.32084,
        "scores_per_experiment": [
          {
            "accuracy": 0.346191,
            "f1": 0.342807,
            "f1_weighted": 0.342801
          },
          {
            "accuracy": 0.307129,
            "f1": 0.287335,
            "f1_weighted": 0.287385
          },
          {
            "accuracy": 0.342285,
            "f1": 0.324242,
            "f1_weighted": 0.324261
          },
          {
            "accuracy": 0.351562,
            "f1": 0.345119,
            "f1_weighted": 0.345102
          },
          {
            "accuracy": 0.359863,
            "f1": 0.326054,
            "f1_weighted": 0.32603
          },
          {
            "accuracy": 0.333008,
            "f1": 0.305902,
            "f1_weighted": 0.305904
          },
          {
            "accuracy": 0.344238,
            "f1": 0.331529,
            "f1_weighted": 0.331532
          },
          {
            "accuracy": 0.351562,
            "f1": 0.344427,
            "f1_weighted": 0.344448
          },
          {
            "accuracy": 0.347168,
            "f1": 0.33095,
            "f1_weighted": 0.330941
          },
          {
            "accuracy": 0.296387,
            "f1": 0.269926,
            "f1_weighted": 0.269995
          }
        ],
        "main_score": 0.337939,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 50.81986999511719,
  "kg_co2_emissions": 0.002958667490488511
}