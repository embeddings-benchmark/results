{
  "dataset_revision": "edfaf9da55d3dd50d43143d90c1ac476895ae6de",
  "task_name": "ToxicConversationsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.591699,
        "f1": 0.464639,
        "f1_weighted": 0.67807,
        "ap": 0.110099,
        "ap_weighted": 0.110099,
        "scores_per_experiment": [
          {
            "accuracy": 0.533203,
            "f1": 0.435507,
            "f1_weighted": 0.633193,
            "ap": 0.106474,
            "ap_weighted": 0.106474
          },
          {
            "accuracy": 0.606445,
            "f1": 0.473888,
            "f1_weighted": 0.696193,
            "ap": 0.109225,
            "ap_weighted": 0.109225
          },
          {
            "accuracy": 0.740723,
            "f1": 0.542014,
            "f1_weighted": 0.79596,
            "ap": 0.119195,
            "ap_weighted": 0.119195
          },
          {
            "accuracy": 0.608398,
            "f1": 0.485805,
            "f1_weighted": 0.697156,
            "ap": 0.124448,
            "ap_weighted": 0.124448
          },
          {
            "accuracy": 0.45459,
            "f1": 0.390307,
            "f1_weighted": 0.556959,
            "ap": 0.103588,
            "ap_weighted": 0.103588
          },
          {
            "accuracy": 0.480957,
            "f1": 0.400319,
            "f1_weighted": 0.585432,
            "ap": 0.096422,
            "ap_weighted": 0.096422
          },
          {
            "accuracy": 0.695312,
            "f1": 0.50298,
            "f1_weighted": 0.763248,
            "ap": 0.099216,
            "ap_weighted": 0.099216
          },
          {
            "accuracy": 0.479004,
            "f1": 0.402937,
            "f1_weighted": 0.582334,
            "ap": 0.101573,
            "ap_weighted": 0.101573
          },
          {
            "accuracy": 0.727051,
            "f1": 0.540464,
            "f1_weighted": 0.786959,
            "ap": 0.124106,
            "ap_weighted": 0.124106
          },
          {
            "accuracy": 0.591309,
            "f1": 0.472167,
            "f1_weighted": 0.683267,
            "ap": 0.116739,
            "ap_weighted": 0.116739
          }
        ],
        "main_score": 0.591699,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 28.54499125480652,
  "kg_co2_emissions": 0.0017943556289414916
}