{
  "dataset_revision": "b48bc27d383cfca5b6a47135a52390fa5f66b253",
  "task_name": "AmazonCounterfactualVNClassification",
  "mteb_version": "1.38.42",
  "scores": {
    "test": [
      {
        "accuracy": 0.554292,
        "f1": 0.49716,
        "f1_weighted": 0.599259,
        "ap": 0.215074,
        "ap_weighted": 0.215074,
        "scores_per_experiment": [
          {
            "accuracy": 0.502146,
            "f1": 0.482856,
            "f1_weighted": 0.546298,
            "ap": 0.237479,
            "ap_weighted": 0.237479
          },
          {
            "accuracy": 0.55794,
            "f1": 0.497171,
            "f1_weighted": 0.608205,
            "ap": 0.206235,
            "ap_weighted": 0.206235
          },
          {
            "accuracy": 0.645923,
            "f1": 0.564991,
            "f1_weighted": 0.684174,
            "ap": 0.238506,
            "ap_weighted": 0.238506
          },
          {
            "accuracy": 0.506438,
            "f1": 0.463381,
            "f1_weighted": 0.559933,
            "ap": 0.198574,
            "ap_weighted": 0.198574
          },
          {
            "accuracy": 0.55794,
            "f1": 0.514082,
            "f1_weighted": 0.60681,
            "ap": 0.229386,
            "ap_weighted": 0.229386
          },
          {
            "accuracy": 0.493562,
            "f1": 0.468078,
            "f1_weighted": 0.542033,
            "ap": 0.217782,
            "ap_weighted": 0.217782
          },
          {
            "accuracy": 0.594421,
            "f1": 0.539446,
            "f1_weighted": 0.640517,
            "ap": 0.237833,
            "ap_weighted": 0.237833
          },
          {
            "accuracy": 0.626609,
            "f1": 0.529162,
            "f1_weighted": 0.665221,
            "ap": 0.207948,
            "ap_weighted": 0.207948
          },
          {
            "accuracy": 0.409871,
            "f1": 0.388559,
            "f1_weighted": 0.461069,
            "ap": 0.179019,
            "ap_weighted": 0.179019
          },
          {
            "accuracy": 0.648069,
            "f1": 0.523874,
            "f1_weighted": 0.678335,
            "ap": 0.197979,
            "ap_weighted": 0.197979
          }
        ],
        "main_score": 0.554292,
        "hf_subset": "default",
        "languages": [
          "vie-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 2.0781779289245605,
  "kg_co2_emissions": null
}