{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "task_name": "GreekLegalCodeClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.187109,
        "f1": 0.143907,
        "f1_weighted": 0.171539,
        "scores_per_experiment": [
          {
            "accuracy": 0.177734,
            "f1": 0.140626,
            "f1_weighted": 0.16884
          },
          {
            "accuracy": 0.185059,
            "f1": 0.135204,
            "f1_weighted": 0.17247
          },
          {
            "accuracy": 0.176758,
            "f1": 0.139103,
            "f1_weighted": 0.160502
          },
          {
            "accuracy": 0.178711,
            "f1": 0.132343,
            "f1_weighted": 0.15975
          },
          {
            "accuracy": 0.193848,
            "f1": 0.143547,
            "f1_weighted": 0.176149
          },
          {
            "accuracy": 0.181152,
            "f1": 0.145226,
            "f1_weighted": 0.166525
          },
          {
            "accuracy": 0.202637,
            "f1": 0.147665,
            "f1_weighted": 0.184809
          },
          {
            "accuracy": 0.185059,
            "f1": 0.150161,
            "f1_weighted": 0.161702
          },
          {
            "accuracy": 0.191895,
            "f1": 0.143794,
            "f1_weighted": 0.176685
          },
          {
            "accuracy": 0.198242,
            "f1": 0.161396,
            "f1_weighted": 0.187959
          }
        ],
        "main_score": 0.187109,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.170557,
        "f1": 0.140347,
        "f1_weighted": 0.158613,
        "scores_per_experiment": [
          {
            "accuracy": 0.175781,
            "f1": 0.14345,
            "f1_weighted": 0.162297
          },
          {
            "accuracy": 0.181152,
            "f1": 0.148819,
            "f1_weighted": 0.167842
          },
          {
            "accuracy": 0.16748,
            "f1": 0.13679,
            "f1_weighted": 0.154372
          },
          {
            "accuracy": 0.18457,
            "f1": 0.153701,
            "f1_weighted": 0.170783
          },
          {
            "accuracy": 0.16748,
            "f1": 0.135444,
            "f1_weighted": 0.157364
          },
          {
            "accuracy": 0.154297,
            "f1": 0.12536,
            "f1_weighted": 0.144849
          },
          {
            "accuracy": 0.166992,
            "f1": 0.124524,
            "f1_weighted": 0.155006
          },
          {
            "accuracy": 0.15625,
            "f1": 0.131967,
            "f1_weighted": 0.142624
          },
          {
            "accuracy": 0.172852,
            "f1": 0.15105,
            "f1_weighted": 0.163673
          },
          {
            "accuracy": 0.178711,
            "f1": 0.152369,
            "f1_weighted": 0.16732
          }
        ],
        "main_score": 0.170557,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ]
  },
  "evaluation_time": 715.0391471385956,
  "kg_co2_emissions": 0.16299090873510927
}