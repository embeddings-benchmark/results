{
  "dataset_revision": "673a610d6d3dd91a547a0d57ae1b56f37ebbf6a1",
  "evaluation_time": 65.67443513870239,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.6673828125,
        "f1": 0.6564611808368113,
        "f1_weighted": 0.6565678384778197,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6673828125,
        "scores_per_experiment": [
          {
            "accuracy": 0.68798828125,
            "f1": 0.6810050944295518,
            "f1_weighted": 0.6811133014806365
          },
          {
            "accuracy": 0.66357421875,
            "f1": 0.6511976944015947,
            "f1_weighted": 0.6513117011798647
          },
          {
            "accuracy": 0.65478515625,
            "f1": 0.6411231743506376,
            "f1_weighted": 0.6412676104371658
          },
          {
            "accuracy": 0.67236328125,
            "f1": 0.66135007733751,
            "f1_weighted": 0.661463776056001
          },
          {
            "accuracy": 0.671875,
            "f1": 0.6629382495623598,
            "f1_weighted": 0.6630279677814571
          },
          {
            "accuracy": 0.66259765625,
            "f1": 0.6530275032620626,
            "f1_weighted": 0.6530952276222671
          },
          {
            "accuracy": 0.67822265625,
            "f1": 0.665913133789097,
            "f1_weighted": 0.6660414948471616
          },
          {
            "accuracy": 0.6591796875,
            "f1": 0.6484716936319798,
            "f1_weighted": 0.6485803505158273
          },
          {
            "accuracy": 0.671875,
            "f1": 0.6620798628890334,
            "f1_weighted": 0.6621870228217818
          },
          {
            "accuracy": 0.6513671875,
            "f1": 0.6375053247142862,
            "f1_weighted": 0.6375899320360338
          }
        ]
      }
    ]
  },
  "task_name": "RuSciBenchGRNTIClassification"
}