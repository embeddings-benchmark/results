{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "evaluation_time": 20.388413429260254,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.515087890625,
        "ap": 0.5080279752180195,
        "ap_weighted": 0.5080279752180195,
        "f1": 0.5123686946331695,
        "f1_weighted": 0.5123686946331695,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.515087890625,
        "scores_per_experiment": [
          {
            "accuracy": 0.51904296875,
            "ap": 0.5099136046396515,
            "ap_weighted": 0.5099136046396515,
            "f1": 0.5183621332855821,
            "f1_weighted": 0.5183621332855821
          },
          {
            "accuracy": 0.51171875,
            "ap": 0.5059791577938672,
            "ap_weighted": 0.5059791577938672,
            "f1": 0.5090852782153716,
            "f1_weighted": 0.5090852782153716
          },
          {
            "accuracy": 0.52783203125,
            "ap": 0.5148184193686007,
            "ap_weighted": 0.5148184193686007,
            "f1": 0.5254532467155922,
            "f1_weighted": 0.5254532467155922
          },
          {
            "accuracy": 0.48828125,
            "ap": 0.4942989864864865,
            "ap_weighted": 0.4942989864864865,
            "f1": 0.48601468266740233,
            "f1_weighted": 0.48601468266740233
          },
          {
            "accuracy": 0.54638671875,
            "ap": 0.5253058897111698,
            "ap_weighted": 0.5253058897111698,
            "f1": 0.5463476732993271,
            "f1_weighted": 0.5463476732993271
          },
          {
            "accuracy": 0.52587890625,
            "ap": 0.5136218322450248,
            "ap_weighted": 0.5136218322450248,
            "f1": 0.5258380955582849,
            "f1_weighted": 0.5258380955582849
          },
          {
            "accuracy": 0.513671875,
            "ap": 0.507080078125,
            "ap_weighted": 0.507080078125,
            "f1": 0.5069001794665511,
            "f1_weighted": 0.5069001794665511
          },
          {
            "accuracy": 0.5009765625,
            "ap": 0.500489277742347,
            "ap_weighted": 0.500489277742347,
            "f1": 0.5007461177072242,
            "f1_weighted": 0.5007461177072242
          },
          {
            "accuracy": 0.51611328125,
            "ap": 0.5082829122340426,
            "ap_weighted": 0.5082829122340426,
            "f1": 0.5134684069506842,
            "f1_weighted": 0.5134684069506842
          },
          {
            "accuracy": 0.5009765625,
            "ap": 0.5004895938340054,
            "ap_weighted": 0.5004895938340054,
            "f1": 0.49147113246567464,
            "f1_weighted": 0.49147113246567464
          }
        ]
      }
    ]
  },
  "task_name": "InappropriatenessClassification"
}