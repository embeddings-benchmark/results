{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "2.1.7",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.821789,
            "f1": 0.791572,
            "f1_weighted": 0.809184,
            "precision": 0.775086,
            "precision_weighted": 0.842563,
            "recall": 0.857938,
            "recall_weighted": 0.821789,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.822461,
            "f1": 0.801761,
            "f1_weighted": 0.810855,
            "precision": 0.779224,
            "precision_weighted": 0.83993,
            "recall": 0.872703,
            "recall_weighted": 0.822461,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.803295,
            "f1": 0.777052,
            "f1_weighted": 0.790427,
            "precision": 0.753737,
            "precision_weighted": 0.825281,
            "recall": 0.861611,
            "recall_weighted": 0.803295,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.815736,
            "f1": 0.784965,
            "f1_weighted": 0.80473,
            "precision": 0.766504,
            "precision_weighted": 0.840019,
            "recall": 0.850734,
            "recall_weighted": 0.815736,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.803631,
            "f1": 0.771504,
            "f1_weighted": 0.787206,
            "precision": 0.759474,
            "precision_weighted": 0.847359,
            "recall": 0.849119,
            "recall_weighted": 0.803631,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.795898,
            "f1": 0.768474,
            "f1_weighted": 0.787157,
            "precision": 0.749075,
            "precision_weighted": 0.826336,
            "recall": 0.854349,
            "recall_weighted": 0.795898,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.798588,
            "f1": 0.777655,
            "f1_weighted": 0.784811,
            "precision": 0.770711,
            "precision_weighted": 0.843766,
            "recall": 0.850005,
            "recall_weighted": 0.798588,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.806994,
            "f1": 0.777374,
            "f1_weighted": 0.798828,
            "precision": 0.760285,
            "precision_weighted": 0.833082,
            "recall": 0.846949,
            "recall_weighted": 0.806994,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.792872,
            "f1": 0.763065,
            "f1_weighted": 0.783915,
            "precision": 0.748847,
            "precision_weighted": 0.825554,
            "recall": 0.838711,
            "recall_weighted": 0.792872,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.833557,
            "f1": 0.813002,
            "f1_weighted": 0.825399,
            "precision": 0.792306,
            "precision_weighted": 0.849426,
            "recall": 0.876985,
            "recall_weighted": 0.833557,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.809482,
        "f1": 0.782642,
        "f1_weighted": 0.798251,
        "precision": 0.765525,
        "precision_weighted": 0.837332,
        "recall": 0.855911,
        "recall_weighted": 0.809482,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.809482,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 40.02957081794739,
  "kg_co2_emissions": null
}