{
  "dataset_revision": "3d86128a09e091d6018b6d26cad27f2739fc2db7",
  "task_name": "ImdbClassification",
  "mteb_version": "2.1.7",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.95164,
            "f1": 0.951624,
            "f1_weighted": 0.951624,
            "precision": 0.952239,
            "precision_weighted": 0.952239,
            "recall": 0.95164,
            "recall_weighted": 0.95164,
            "ap": 0.937504,
            "ap_weighted": 0.937504
          },
          {
            "accuracy": 0.95232,
            "f1": 0.952312,
            "f1_weighted": 0.952312,
            "precision": 0.952628,
            "precision_weighted": 0.952628,
            "recall": 0.95232,
            "recall_weighted": 0.95232,
            "ap": 0.936232,
            "ap_weighted": 0.936232
          },
          {
            "accuracy": 0.95092,
            "f1": 0.950903,
            "f1_weighted": 0.950903,
            "precision": 0.951556,
            "precision_weighted": 0.951556,
            "recall": 0.95092,
            "recall_weighted": 0.95092,
            "ap": 0.921436,
            "ap_weighted": 0.921436
          },
          {
            "accuracy": 0.95448,
            "f1": 0.95448,
            "f1_weighted": 0.95448,
            "precision": 0.954481,
            "precision_weighted": 0.954481,
            "recall": 0.95448,
            "recall_weighted": 0.95448,
            "ap": 0.934057,
            "ap_weighted": 0.934057
          },
          {
            "accuracy": 0.95444,
            "f1": 0.95444,
            "f1_weighted": 0.95444,
            "precision": 0.954452,
            "precision_weighted": 0.954452,
            "recall": 0.95444,
            "recall_weighted": 0.95444,
            "ap": 0.932667,
            "ap_weighted": 0.932667
          },
          {
            "accuracy": 0.95496,
            "f1": 0.95496,
            "f1_weighted": 0.95496,
            "precision": 0.954965,
            "precision_weighted": 0.954965,
            "recall": 0.95496,
            "recall_weighted": 0.95496,
            "ap": 0.935166,
            "ap_weighted": 0.935166
          },
          {
            "accuracy": 0.95124,
            "f1": 0.951229,
            "f1_weighted": 0.951229,
            "precision": 0.95166,
            "precision_weighted": 0.95166,
            "recall": 0.95124,
            "recall_weighted": 0.95124,
            "ap": 0.923215,
            "ap_weighted": 0.923215
          },
          {
            "accuracy": 0.95316,
            "f1": 0.953154,
            "f1_weighted": 0.953154,
            "precision": 0.953376,
            "precision_weighted": 0.953376,
            "recall": 0.95316,
            "recall_weighted": 0.95316,
            "ap": 0.927545,
            "ap_weighted": 0.927545
          },
          {
            "accuracy": 0.9542,
            "f1": 0.954199,
            "f1_weighted": 0.954199,
            "precision": 0.95422,
            "precision_weighted": 0.95422,
            "recall": 0.9542,
            "recall_weighted": 0.9542,
            "ap": 0.932037,
            "ap_weighted": 0.932037
          },
          {
            "accuracy": 0.95244,
            "f1": 0.952431,
            "f1_weighted": 0.952431,
            "precision": 0.952769,
            "precision_weighted": 0.952769,
            "recall": 0.95244,
            "recall_weighted": 0.95244,
            "ap": 0.936594,
            "ap_weighted": 0.936594
          }
        ],
        "accuracy": 0.95298,
        "f1": 0.952973,
        "f1_weighted": 0.952973,
        "precision": 0.953235,
        "precision_weighted": 0.953235,
        "recall": 0.95298,
        "recall_weighted": 0.95298,
        "ap": 0.931645,
        "ap_weighted": 0.931645,
        "main_score": 0.95298,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 103.06555414199829,
  "kg_co2_emissions": null
}