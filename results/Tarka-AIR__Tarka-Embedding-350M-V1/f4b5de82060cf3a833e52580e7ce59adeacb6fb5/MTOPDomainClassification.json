{
  "dataset_revision": "a76d16fae880597b9c73047b50159220a441cb54",
  "task_name": "MTOPDomainClassification",
  "mteb_version": "2.1.7",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.971728,
            "f1": 0.968824,
            "f1_weighted": 0.971847,
            "precision": 0.968477,
            "precision_weighted": 0.972951,
            "recall": 0.970181,
            "recall_weighted": 0.971728,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.974464,
            "f1": 0.971533,
            "f1_weighted": 0.974611,
            "precision": 0.970421,
            "precision_weighted": 0.975272,
            "recall": 0.973177,
            "recall_weighted": 0.974464,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.970816,
            "f1": 0.968346,
            "f1_weighted": 0.971035,
            "precision": 0.968066,
            "precision_weighted": 0.97227,
            "recall": 0.969671,
            "recall_weighted": 0.970816,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.967624,
            "f1": 0.965889,
            "f1_weighted": 0.96789,
            "precision": 0.965664,
            "precision_weighted": 0.969549,
            "recall": 0.967546,
            "recall_weighted": 0.967624,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.97264,
            "f1": 0.968832,
            "f1_weighted": 0.972795,
            "precision": 0.967468,
            "precision_weighted": 0.973443,
            "recall": 0.970747,
            "recall_weighted": 0.97264,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.971272,
            "f1": 0.970308,
            "f1_weighted": 0.971514,
            "precision": 0.969161,
            "precision_weighted": 0.972814,
            "recall": 0.972485,
            "recall_weighted": 0.971272,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.978112,
            "f1": 0.974466,
            "f1_weighted": 0.978059,
            "precision": 0.97536,
            "precision_weighted": 0.978171,
            "recall": 0.973825,
            "recall_weighted": 0.978112,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.978796,
            "f1": 0.976195,
            "f1_weighted": 0.978893,
            "precision": 0.974299,
            "precision_weighted": 0.979357,
            "recall": 0.978536,
            "recall_weighted": 0.978796,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.965116,
            "f1": 0.963644,
            "f1_weighted": 0.965389,
            "precision": 0.963518,
            "precision_weighted": 0.967842,
            "recall": 0.965877,
            "recall_weighted": 0.965116,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.975148,
            "f1": 0.972332,
            "f1_weighted": 0.97523,
            "precision": 0.97236,
            "precision_weighted": 0.975571,
            "recall": 0.972595,
            "recall_weighted": 0.975148,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.972572,
        "f1": 0.970037,
        "f1_weighted": 0.972726,
        "precision": 0.969479,
        "precision_weighted": 0.973724,
        "recall": 0.971464,
        "recall_weighted": 0.972572,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.972572,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 41.512102365493774,
  "kg_co2_emissions": null
}