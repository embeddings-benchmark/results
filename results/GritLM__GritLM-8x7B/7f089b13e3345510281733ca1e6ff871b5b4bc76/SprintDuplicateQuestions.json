{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 588.708475112915,
  "kg_co2_emissions": 0.13312137076446118,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.9968316831683168,
        "cosine_accuracy_threshold": 0.814541220664978,
        "cosine_ap": 0.9126270445676203,
        "cosine_f1": 0.8388671875,
        "cosine_f1_threshold": 0.7964311838150024,
        "cosine_precision": 0.8196564885496184,
        "cosine_recall": 0.859,
        "dot_accuracy": 0.9968316831683168,
        "dot_accuracy_threshold": 0.8145414590835571,
        "dot_ap": 0.9126267541362274,
        "dot_f1": 0.8388671875,
        "dot_f1_threshold": 0.7964315414428711,
        "dot_precision": 0.8196564885496184,
        "dot_recall": 0.859,
        "euclidean_accuracy": 0.9968316831683168,
        "euclidean_accuracy_threshold": 0.60903000831604,
        "euclidean_ap": 0.9126270445676203,
        "euclidean_f1": 0.8388671875,
        "euclidean_f1_threshold": 0.6380733251571655,
        "euclidean_precision": 0.8196564885496184,
        "euclidean_recall": 0.859,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.9197328651896981,
        "manhattan_accuracy": 0.997049504950495,
        "manhattan_accuracy_threshold": 30.739938735961914,
        "manhattan_ap": 0.9197328651896981,
        "manhattan_f1": 0.8468653648509764,
        "manhattan_f1_threshold": 30.74636459350586,
        "manhattan_precision": 0.8710359408033826,
        "manhattan_recall": 0.824,
        "max_accuracy": 0.997049504950495,
        "max_ap": 0.9197328651896981,
        "max_f1": 0.8468653648509764,
        "max_precision": 0.8710359408033826,
        "max_recall": 0.859,
        "similarity_accuracy": 0.9968316831683168,
        "similarity_accuracy_threshold": 0.814541220664978,
        "similarity_ap": 0.9126270445676203,
        "similarity_f1": 0.8388671875,
        "similarity_f1_threshold": 0.7964311838150024,
        "similarity_precision": 0.8196564885496184,
        "similarity_recall": 0.859
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.9970990099009901,
        "cosine_accuracy_threshold": 0.7993952631950378,
        "cosine_ap": 0.9192975231132041,
        "cosine_f1": 0.8524590163934427,
        "cosine_f1_threshold": 0.7832316756248474,
        "cosine_precision": 0.8469891411648569,
        "cosine_recall": 0.858,
        "dot_accuracy": 0.9970990099009901,
        "dot_accuracy_threshold": 0.7993953227996826,
        "dot_ap": 0.9192975010057642,
        "dot_f1": 0.8524590163934427,
        "dot_f1_threshold": 0.7832310199737549,
        "dot_precision": 0.8469891411648569,
        "dot_recall": 0.858,
        "euclidean_accuracy": 0.9970990099009901,
        "euclidean_accuracy_threshold": 0.6334108710289001,
        "euclidean_ap": 0.9192975231132041,
        "euclidean_f1": 0.8524590163934427,
        "euclidean_f1_threshold": 0.6584349870681763,
        "euclidean_precision": 0.8469891411648569,
        "euclidean_recall": 0.858,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.9279092360560923,
        "manhattan_accuracy": 0.9973465346534653,
        "manhattan_accuracy_threshold": 31.933364868164062,
        "manhattan_ap": 0.9279092360560923,
        "manhattan_f1": 0.8629856850715746,
        "manhattan_f1_threshold": 31.933364868164062,
        "manhattan_precision": 0.8828451882845189,
        "manhattan_recall": 0.844,
        "max_accuracy": 0.9973465346534653,
        "max_ap": 0.9279092360560923,
        "max_f1": 0.8629856850715746,
        "max_precision": 0.8828451882845189,
        "max_recall": 0.858,
        "similarity_accuracy": 0.9970990099009901,
        "similarity_accuracy_threshold": 0.7993952631950378,
        "similarity_ap": 0.9192975231132041,
        "similarity_f1": 0.8524590163934427,
        "similarity_f1_threshold": 0.7832316756248474,
        "similarity_precision": 0.8469891411648569,
        "similarity_recall": 0.858
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}