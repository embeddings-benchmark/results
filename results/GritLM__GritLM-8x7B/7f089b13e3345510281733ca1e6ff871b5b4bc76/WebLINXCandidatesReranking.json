{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 83480.85112810135,
  "kg_co2_emissions": 19.69290853254514,
  "mteb_version": "1.18.0",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07948223203138934,
        "map": 0.09574026460015038,
        "mrr": 0.07948223203138934,
        "nAUC_map_diff1": 0.19998172435985265,
        "nAUC_map_max": 0.0029688097274941966,
        "nAUC_map_std": 0.34179127891228334,
        "nAUC_mrr_diff1": 0.19701823723146594,
        "nAUC_mrr_max": -0.0079518851732456,
        "nAUC_mrr_std": 0.30868262059765755
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.11200049724256396,
        "map": 0.13062779680378708,
        "mrr": 0.11200049724256396,
        "nAUC_map_diff1": 0.14135413223955928,
        "nAUC_map_max": -0.001529597849567976,
        "nAUC_map_std": 0.11269402710992193,
        "nAUC_mrr_diff1": 0.14191877977899922,
        "nAUC_mrr_max": -0.003383126537906556,
        "nAUC_mrr_std": 0.10411483318357088
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.11073277479744796,
        "map": 0.12927074450677842,
        "mrr": 0.11073277479744796,
        "nAUC_map_diff1": 0.10607210222008098,
        "nAUC_map_max": -0.02663656956923792,
        "nAUC_map_std": 0.11564781302949277,
        "nAUC_mrr_diff1": 0.1037641235391426,
        "nAUC_mrr_max": -0.01967016955932168,
        "nAUC_mrr_std": 0.10512308532610022
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.10963503748045084,
        "map": 0.12663931490778382,
        "mrr": 0.10963503748045084,
        "nAUC_map_diff1": 0.131163785404845,
        "nAUC_map_max": -0.013676317140538683,
        "nAUC_map_std": 0.1145057677669671,
        "nAUC_mrr_diff1": 0.1371305839971916,
        "nAUC_mrr_max": -0.011288673686222578,
        "nAUC_mrr_std": 0.09857382562182185
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.0828740306555192,
        "map": 0.09962853267936492,
        "mrr": 0.0828740306555192,
        "nAUC_map_diff1": 0.253470871249337,
        "nAUC_map_max": 0.013222594967749869,
        "nAUC_map_std": 0.19963625870903456,
        "nAUC_mrr_diff1": 0.25939531774754326,
        "nAUC_mrr_max": 0.01616428993798628,
        "nAUC_mrr_std": 0.18434105508381887
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.14502915949879824,
        "map": 0.1617911530498067,
        "mrr": 0.14502915949879824,
        "nAUC_map_diff1": 0.2649177416113137,
        "nAUC_map_max": -0.06141004293198862,
        "nAUC_map_std": -0.04543073148237211,
        "nAUC_mrr_diff1": 0.25952341803822776,
        "nAUC_mrr_max": -0.06010140111221158,
        "nAUC_mrr_std": -0.04501626066500974
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}