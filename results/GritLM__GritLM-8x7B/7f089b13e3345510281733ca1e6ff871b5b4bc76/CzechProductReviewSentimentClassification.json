{
  "dataset_revision": "2e6fedf42c9c104e83dfd95c3a453721e683e244",
  "evaluation_time": 110.24956583976746,
  "kg_co2_emissions": 0.0252109979742682,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.591650390625,
        "f1": 0.574635020040477,
        "f1_weighted": 0.5745804912118484,
        "hf_subset": "default",
        "languages": [
          "ces-Latn"
        ],
        "main_score": 0.591650390625,
        "scores_per_experiment": [
          {
            "accuracy": 0.63134765625,
            "f1": 0.6141992447366835,
            "f1_weighted": 0.614160511369926
          },
          {
            "accuracy": 0.5947265625,
            "f1": 0.5894664380034319,
            "f1_weighted": 0.5894188030118499
          },
          {
            "accuracy": 0.57080078125,
            "f1": 0.539569027249474,
            "f1_weighted": 0.5394870248792721
          },
          {
            "accuracy": 0.5634765625,
            "f1": 0.5484242736521401,
            "f1_weighted": 0.5483769957157619
          },
          {
            "accuracy": 0.59130859375,
            "f1": 0.5720254662971004,
            "f1_weighted": 0.5719880339017057
          },
          {
            "accuracy": 0.55712890625,
            "f1": 0.5330809161184629,
            "f1_weighted": 0.5330215645285195
          },
          {
            "accuracy": 0.6162109375,
            "f1": 0.6130046310308876,
            "f1_weighted": 0.6129565818292556
          },
          {
            "accuracy": 0.59716796875,
            "f1": 0.5764965116144974,
            "f1_weighted": 0.5764389198504203
          },
          {
            "accuracy": 0.599609375,
            "f1": 0.5818830420464818,
            "f1_weighted": 0.5818182433622608
          },
          {
            "accuracy": 0.5947265625,
            "f1": 0.5782006496556101,
            "f1_weighted": 0.5781382336695114
          }
        ]
      }
    ]
  },
  "task_name": "CzechProductReviewSentimentClassification"
}