{
  "dataset_revision": "ef5c383d1b87eb8feccde3dfb7f95e42b1b050dd",
  "evaluation_time": 498.7579207420349,
  "kg_co2_emissions": 0.11309249566024719,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "cosine_pearson": 0.8230371727050488,
        "cosine_spearman": 0.8114177283047166,
        "euclidean_pearson": 0.7953800486875298,
        "euclidean_spearman": 0.8114177283047166,
        "hf_subset": "afr",
        "languages": [
          "afr-Latn"
        ],
        "main_score": 0.8114177283047166,
        "manhattan_pearson": 0.7878506276416668,
        "manhattan_spearman": 0.8064658047301717,
        "pearson": 0.8230371727050488,
        "spearman": 0.8114177283047166
      },
      {
        "cosine_pearson": 0.758615420748772,
        "cosine_spearman": 0.7537907890995492,
        "euclidean_pearson": 0.7559449174287869,
        "euclidean_spearman": 0.7537907890995492,
        "hf_subset": "amh",
        "languages": [
          "amh-Ethi"
        ],
        "main_score": 0.7537907890995492,
        "manhattan_pearson": 0.7525862757030629,
        "manhattan_spearman": 0.7498595938247883,
        "pearson": 0.758615420748772,
        "spearman": 0.7537907890995492
      },
      {
        "cosine_pearson": 0.6722637556210862,
        "cosine_spearman": 0.6786939610694714,
        "euclidean_pearson": 0.6615271652915296,
        "euclidean_spearman": 0.6786939610694714,
        "hf_subset": "arb",
        "languages": [
          "arb-Arab"
        ],
        "main_score": 0.6786939610694714,
        "manhattan_pearson": 0.6461448385459803,
        "manhattan_spearman": 0.6612394102946605,
        "pearson": 0.6722637556210862,
        "spearman": 0.6786939610694714
      },
      {
        "cosine_pearson": 0.5822987644303538,
        "cosine_spearman": 0.5342671469828101,
        "euclidean_pearson": 0.5827120476270122,
        "euclidean_spearman": 0.5342671469828101,
        "hf_subset": "arq",
        "languages": [
          "arq-Arab"
        ],
        "main_score": 0.5342671469828101,
        "manhattan_pearson": 0.5783783183423007,
        "manhattan_spearman": 0.5317924696251989,
        "pearson": 0.5822987644303538,
        "spearman": 0.5342671469828101
      },
      {
        "cosine_pearson": 0.4324244384874914,
        "cosine_spearman": 0.4187593554825097,
        "euclidean_pearson": 0.43032682061068284,
        "euclidean_spearman": 0.4187593554825097,
        "hf_subset": "ary",
        "languages": [
          "ary-Arab"
        ],
        "main_score": 0.4187593554825097,
        "manhattan_pearson": 0.42821695284411476,
        "manhattan_spearman": 0.41666993182711104,
        "pearson": 0.4324244384874914,
        "spearman": 0.4187593554825097
      },
      {
        "cosine_pearson": 0.840362102247787,
        "cosine_spearman": 0.8268532399684739,
        "euclidean_pearson": 0.8353179953011497,
        "euclidean_spearman": 0.8268532398273404,
        "hf_subset": "eng",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8268532399684739,
        "manhattan_pearson": 0.83149494972364,
        "manhattan_spearman": 0.8213366535224803,
        "pearson": 0.840362102247787,
        "spearman": 0.8268532399684739
      },
      {
        "cosine_pearson": 0.5328327761844741,
        "cosine_spearman": 0.5315838664858155,
        "euclidean_pearson": 0.5364321408043538,
        "euclidean_spearman": 0.5315838664858155,
        "hf_subset": "hau",
        "languages": [
          "hau-Latn"
        ],
        "main_score": 0.5315838664858155,
        "manhattan_pearson": 0.5435315759911749,
        "manhattan_spearman": 0.5392471682376042,
        "pearson": 0.5328327761844741,
        "spearman": 0.5315838664858155
      },
      {
        "cosine_pearson": 0.789245627178786,
        "cosine_spearman": 0.7960838874007573,
        "euclidean_pearson": 0.7663213711465129,
        "euclidean_spearman": 0.7960838874007573,
        "hf_subset": "hin",
        "languages": [
          "hin-Deva"
        ],
        "main_score": 0.7960838874007573,
        "manhattan_pearson": 0.7593789860532643,
        "manhattan_spearman": 0.7919680086410265,
        "pearson": 0.789245627178786,
        "spearman": 0.7960838874007573
      },
      {
        "cosine_pearson": 0.5527465672555365,
        "cosine_spearman": 0.5626679046008927,
        "euclidean_pearson": 0.5686516995996357,
        "euclidean_spearman": 0.5626679046008927,
        "hf_subset": "ind",
        "languages": [
          "ind-Latn"
        ],
        "main_score": 0.5626679046008927,
        "manhattan_pearson": 0.5653716260149367,
        "manhattan_spearman": 0.5565358926542335,
        "pearson": 0.5527465672555365,
        "spearman": 0.5626679046008927
      },
      {
        "cosine_pearson": 0.5056156321770399,
        "cosine_spearman": 0.4746830472520019,
        "euclidean_pearson": 0.5164933420152974,
        "euclidean_spearman": 0.4746830472520019,
        "hf_subset": "kin",
        "languages": [
          "kin-Latn"
        ],
        "main_score": 0.4746830472520019,
        "manhattan_pearson": 0.5246059913369918,
        "manhattan_spearman": 0.48382659605524275,
        "pearson": 0.5056156321770399,
        "spearman": 0.4746830472520019
      },
      {
        "cosine_pearson": 0.8126955010225706,
        "cosine_spearman": 0.7925272283019439,
        "euclidean_pearson": 0.7998300106129169,
        "euclidean_spearman": 0.7925272283019439,
        "hf_subset": "mar",
        "languages": [
          "mar-Deva"
        ],
        "main_score": 0.7925272283019439,
        "manhattan_pearson": 0.7999094545212976,
        "manhattan_spearman": 0.7944875533396458,
        "pearson": 0.8126955010225706,
        "spearman": 0.7925272283019439
      },
      {
        "cosine_pearson": 0.8107164122609131,
        "cosine_spearman": 0.7811319839635525,
        "euclidean_pearson": 0.7954859247543727,
        "euclidean_spearman": 0.7811319839635525,
        "hf_subset": "tel",
        "languages": [
          "tel-Telu"
        ],
        "main_score": 0.7811319839635525,
        "manhattan_pearson": 0.7925853701913445,
        "manhattan_spearman": 0.7766727015948683,
        "pearson": 0.8107164122609131,
        "spearman": 0.7811319839635525
      }
    ]
  },
  "task_name": "SemRel24STS"
}