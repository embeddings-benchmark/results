{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 192.51037192344666,
  "kg_co2_emissions": 0.043785526202287116,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.412451171875,
        "f1": 0.3923043290863272,
        "f1_weighted": 0.3922908476574459,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.412451171875,
        "scores_per_experiment": [
          {
            "accuracy": 0.4091796875,
            "f1": 0.3885152683158913,
            "f1_weighted": 0.38852767127640275
          },
          {
            "accuracy": 0.427734375,
            "f1": 0.4014203338556113,
            "f1_weighted": 0.4013998011276089
          },
          {
            "accuracy": 0.39453125,
            "f1": 0.36327782792867264,
            "f1_weighted": 0.3633072445127453
          },
          {
            "accuracy": 0.40771484375,
            "f1": 0.38870619042627275,
            "f1_weighted": 0.38866254584282695
          },
          {
            "accuracy": 0.4287109375,
            "f1": 0.40571830141175375,
            "f1_weighted": 0.4056786146796153
          },
          {
            "accuracy": 0.4140625,
            "f1": 0.37931466972813493,
            "f1_weighted": 0.3792992389551346
          },
          {
            "accuracy": 0.42138671875,
            "f1": 0.4059414666861412,
            "f1_weighted": 0.40590537789450165
          },
          {
            "accuracy": 0.41845703125,
            "f1": 0.4092596637740111,
            "f1_weighted": 0.40925230489990805
          },
          {
            "accuracy": 0.42724609375,
            "f1": 0.41596313589208866,
            "f1_weighted": 0.41593894516547436
          },
          {
            "accuracy": 0.37548828125,
            "f1": 0.3649264328446945,
            "f1_weighted": 0.3649367322202405
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.409521484375,
        "f1": 0.38849545935289664,
        "f1_weighted": 0.3884831941600966,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.409521484375,
        "scores_per_experiment": [
          {
            "accuracy": 0.42626953125,
            "f1": 0.4061301568031503,
            "f1_weighted": 0.40613292633364184
          },
          {
            "accuracy": 0.41064453125,
            "f1": 0.38013111956168266,
            "f1_weighted": 0.3801433429801371
          },
          {
            "accuracy": 0.4072265625,
            "f1": 0.3759422965857121,
            "f1_weighted": 0.37597341982425925
          },
          {
            "accuracy": 0.4140625,
            "f1": 0.39164614771319883,
            "f1_weighted": 0.3916009512059646
          },
          {
            "accuracy": 0.41650390625,
            "f1": 0.3953411158756247,
            "f1_weighted": 0.3953074806785142
          },
          {
            "accuracy": 0.4072265625,
            "f1": 0.371149363408395,
            "f1_weighted": 0.37113663797457885
          },
          {
            "accuracy": 0.4111328125,
            "f1": 0.39595945503203944,
            "f1_weighted": 0.39591497163216144
          },
          {
            "accuracy": 0.41943359375,
            "f1": 0.4106891656889432,
            "f1_weighted": 0.4106777017501253
          },
          {
            "accuracy": 0.39990234375,
            "f1": 0.38750509608320777,
            "f1_weighted": 0.3874703124986378
          },
          {
            "accuracy": 0.3828125,
            "f1": 0.37046067677701283,
            "f1_weighted": 0.37047419672294507
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}