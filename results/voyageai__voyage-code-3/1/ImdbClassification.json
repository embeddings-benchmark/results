{
  "dataset_revision": "3d86128a09e091d6018b6d26cad27f2739fc2db7",
  "task_name": "ImdbClassification",
  "mteb_version": "1.36.10",
  "scores": {
    "test": [
      {
        "accuracy": 0.89634,
        "f1": 0.896126,
        "f1_weighted": 0.896126,
        "ap": 0.857766,
        "ap_weighted": 0.857766,
        "scores_per_experiment": [
          {
            "accuracy": 0.89508,
            "f1": 0.894879,
            "f1_weighted": 0.894879,
            "ap": 0.868584,
            "ap_weighted": 0.868584
          },
          {
            "accuracy": 0.85508,
            "f1": 0.853874,
            "f1_weighted": 0.853874,
            "ap": 0.831614,
            "ap_weighted": 0.831614
          },
          {
            "accuracy": 0.90568,
            "f1": 0.905482,
            "f1_weighted": 0.905482,
            "ap": 0.853617,
            "ap_weighted": 0.853617
          },
          {
            "accuracy": 0.92264,
            "f1": 0.922636,
            "f1_weighted": 0.922636,
            "ap": 0.887298,
            "ap_weighted": 0.887298
          },
          {
            "accuracy": 0.91532,
            "f1": 0.915302,
            "f1_weighted": 0.915302,
            "ap": 0.875257,
            "ap_weighted": 0.875257
          },
          {
            "accuracy": 0.87432,
            "f1": 0.873907,
            "f1_weighted": 0.873907,
            "ap": 0.845375,
            "ap_weighted": 0.845375
          },
          {
            "accuracy": 0.9218,
            "f1": 0.921781,
            "f1_weighted": 0.921781,
            "ap": 0.883392,
            "ap_weighted": 0.883392
          },
          {
            "accuracy": 0.90692,
            "f1": 0.906893,
            "f1_weighted": 0.906893,
            "ap": 0.86355,
            "ap_weighted": 0.86355
          },
          {
            "accuracy": 0.87936,
            "f1": 0.87931,
            "f1_weighted": 0.87931,
            "ap": 0.827952,
            "ap_weighted": 0.827952
          },
          {
            "accuracy": 0.8872,
            "f1": 0.887192,
            "f1_weighted": 0.887192,
            "ap": 0.841024,
            "ap_weighted": 0.841024
          }
        ],
        "main_score": 0.89634,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 765.9047341346741,
  "kg_co2_emissions": null
}