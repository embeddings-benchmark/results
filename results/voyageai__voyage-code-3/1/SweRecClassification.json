{
  "dataset_revision": "b07c6ce548f6a7ac8d546e1bbe197a0086409190",
  "task_name": "SweRecClassification",
  "mteb_version": "2.3.2",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.771973,
            "f1": 0.662823,
            "f1_weighted": 0.771535,
            "precision": 0.662569,
            "precision_weighted": 0.771778,
            "recall": 0.663609,
            "recall_weighted": 0.771973,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.756836,
            "f1": 0.679622,
            "f1_weighted": 0.771932,
            "precision": 0.686063,
            "precision_weighted": 0.799076,
            "recall": 0.692215,
            "recall_weighted": 0.756836,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.739746,
            "f1": 0.668959,
            "f1_weighted": 0.75989,
            "precision": 0.682279,
            "precision_weighted": 0.809346,
            "recall": 0.693609,
            "recall_weighted": 0.739746,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.777344,
            "f1": 0.687675,
            "f1_weighted": 0.790992,
            "precision": 0.693771,
            "precision_weighted": 0.813604,
            "recall": 0.694877,
            "recall_weighted": 0.777344,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.759766,
            "f1": 0.62532,
            "f1_weighted": 0.757582,
            "precision": 0.624617,
            "precision_weighted": 0.756204,
            "recall": 0.62672,
            "recall_weighted": 0.759766,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.774414,
            "f1": 0.665624,
            "f1_weighted": 0.77491,
            "precision": 0.665501,
            "precision_weighted": 0.775417,
            "recall": 0.665765,
            "recall_weighted": 0.774414,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.791016,
            "f1": 0.687492,
            "f1_weighted": 0.792613,
            "precision": 0.686701,
            "precision_weighted": 0.794286,
            "recall": 0.688443,
            "recall_weighted": 0.791016,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.739258,
            "f1": 0.686113,
            "f1_weighted": 0.773605,
            "precision": 0.710716,
            "precision_weighted": 0.842462,
            "recall": 0.723513,
            "recall_weighted": 0.739258,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.663086,
            "f1": 0.608726,
            "f1_weighted": 0.700359,
            "precision": 0.658931,
            "precision_weighted": 0.802528,
            "recall": 0.639081,
            "recall_weighted": 0.663086,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.77002,
            "f1": 0.668532,
            "f1_weighted": 0.77682,
            "precision": 0.668112,
            "precision_weighted": 0.784928,
            "recall": 0.671505,
            "recall_weighted": 0.77002,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.754346,
        "f1": 0.664089,
        "f1_weighted": 0.767024,
        "precision": 0.673926,
        "precision_weighted": 0.794963,
        "recall": 0.675934,
        "recall_weighted": 0.754346,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.754346,
        "hf_subset": "default",
        "languages": [
          "swe-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 57.424750089645386,
  "kg_co2_emissions": null
}