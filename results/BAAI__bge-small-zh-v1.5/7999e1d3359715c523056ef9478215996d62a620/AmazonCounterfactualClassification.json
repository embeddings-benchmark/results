{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "task_name": "AmazonCounterfactualClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.575107,
        "f1": 0.562742,
        "f1_weighted": 0.5904,
        "ap": 0.748662,
        "ap_weighted": 0.748662,
        "scores_per_experiment": [
          {
            "accuracy": 0.650215,
            "f1": 0.621185,
            "f1_weighted": 0.662591,
            "ap": 0.765598,
            "ap_weighted": 0.765598
          },
          {
            "accuracy": 0.530043,
            "f1": 0.506878,
            "f1_weighted": 0.549079,
            "ap": 0.70898,
            "ap_weighted": 0.70898
          },
          {
            "accuracy": 0.56867,
            "f1": 0.559001,
            "f1_weighted": 0.584784,
            "ap": 0.747686,
            "ap_weighted": 0.747686
          },
          {
            "accuracy": 0.583691,
            "f1": 0.57408,
            "f1_weighted": 0.599343,
            "ap": 0.756881,
            "ap_weighted": 0.756881
          },
          {
            "accuracy": 0.572961,
            "f1": 0.562218,
            "f1_weighted": 0.589297,
            "ap": 0.748146,
            "ap_weighted": 0.748146
          },
          {
            "accuracy": 0.545064,
            "f1": 0.540972,
            "f1_weighted": 0.558085,
            "ap": 0.74693,
            "ap_weighted": 0.74693
          },
          {
            "accuracy": 0.585837,
            "f1": 0.573582,
            "f1_weighted": 0.602125,
            "ap": 0.752973,
            "ap_weighted": 0.752973
          },
          {
            "accuracy": 0.536481,
            "f1": 0.532685,
            "f1_weighted": 0.549315,
            "ap": 0.742523,
            "ap_weighted": 0.742523
          },
          {
            "accuracy": 0.587983,
            "f1": 0.574818,
            "f1_weighted": 0.604359,
            "ap": 0.752622,
            "ap_weighted": 0.752622
          },
          {
            "accuracy": 0.590129,
            "f1": 0.581996,
            "f1_weighted": 0.605018,
            "ap": 0.764283,
            "ap_weighted": 0.764283
          }
        ],
        "main_score": 0.575107,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.560707,
        "f1": 0.54602,
        "f1_weighted": 0.576552,
        "ap": 0.734537,
        "ap_weighted": 0.734537,
        "scores_per_experiment": [
          {
            "accuracy": 0.656317,
            "f1": 0.614688,
            "f1_weighted": 0.664317,
            "ap": 0.754253,
            "ap_weighted": 0.754253
          },
          {
            "accuracy": 0.519272,
            "f1": 0.503786,
            "f1_weighted": 0.538137,
            "ap": 0.709881,
            "ap_weighted": 0.709881
          },
          {
            "accuracy": 0.519272,
            "f1": 0.509322,
            "f1_weighted": 0.536702,
            "ap": 0.71736,
            "ap_weighted": 0.71736
          },
          {
            "accuracy": 0.5803,
            "f1": 0.570081,
            "f1_weighted": 0.596054,
            "ap": 0.751587,
            "ap_weighted": 0.751587
          },
          {
            "accuracy": 0.549251,
            "f1": 0.533993,
            "f1_weighted": 0.567035,
            "ap": 0.725744,
            "ap_weighted": 0.725744
          },
          {
            "accuracy": 0.555675,
            "f1": 0.549539,
            "f1_weighted": 0.57014,
            "ap": 0.745713,
            "ap_weighted": 0.745713
          },
          {
            "accuracy": 0.567452,
            "f1": 0.553343,
            "f1_weighted": 0.584451,
            "ap": 0.737371,
            "ap_weighted": 0.737371
          },
          {
            "accuracy": 0.538544,
            "f1": 0.532637,
            "f1_weighted": 0.553226,
            "ap": 0.735947,
            "ap_weighted": 0.735947
          },
          {
            "accuracy": 0.544968,
            "f1": 0.52994,
            "f1_weighted": 0.562875,
            "ap": 0.723759,
            "ap_weighted": 0.723759
          },
          {
            "accuracy": 0.576017,
            "f1": 0.562866,
            "f1_weighted": 0.592577,
            "ap": 0.743759,
            "ap_weighted": 0.743759
          }
        ],
        "main_score": 0.560707,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 24.274449348449707,
  "kg_co2_emissions": null
}