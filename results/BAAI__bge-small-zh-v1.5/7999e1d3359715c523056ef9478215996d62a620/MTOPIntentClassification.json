{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "task_name": "MTOPIntentClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.436639,
        "f1": 0.265773,
        "f1_weighted": 0.487309,
        "scores_per_experiment": [
          {
            "accuracy": 0.429201,
            "f1": 0.272565,
            "f1_weighted": 0.484418
          },
          {
            "accuracy": 0.440771,
            "f1": 0.283424,
            "f1_weighted": 0.4838
          },
          {
            "accuracy": 0.438017,
            "f1": 0.27075,
            "f1_weighted": 0.495414
          },
          {
            "accuracy": 0.399449,
            "f1": 0.251963,
            "f1_weighted": 0.447034
          },
          {
            "accuracy": 0.429201,
            "f1": 0.255056,
            "f1_weighted": 0.478733
          },
          {
            "accuracy": 0.403306,
            "f1": 0.247506,
            "f1_weighted": 0.457472
          },
          {
            "accuracy": 0.459504,
            "f1": 0.271422,
            "f1_weighted": 0.503691
          },
          {
            "accuracy": 0.438017,
            "f1": 0.266413,
            "f1_weighted": 0.490929
          },
          {
            "accuracy": 0.460606,
            "f1": 0.267224,
            "f1_weighted": 0.508944
          },
          {
            "accuracy": 0.46832,
            "f1": 0.271411,
            "f1_weighted": 0.522652
          }
        ],
        "main_score": 0.436639,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.451367,
        "f1": 0.267123,
        "f1_weighted": 0.502279,
        "scores_per_experiment": [
          {
            "accuracy": 0.437025,
            "f1": 0.259788,
            "f1_weighted": 0.488723
          },
          {
            "accuracy": 0.440969,
            "f1": 0.278423,
            "f1_weighted": 0.484373
          },
          {
            "accuracy": 0.451958,
            "f1": 0.254223,
            "f1_weighted": 0.514125
          },
          {
            "accuracy": 0.40293,
            "f1": 0.258147,
            "f1_weighted": 0.452367
          },
          {
            "accuracy": 0.445196,
            "f1": 0.254967,
            "f1_weighted": 0.497801
          },
          {
            "accuracy": 0.430262,
            "f1": 0.264109,
            "f1_weighted": 0.483037
          },
          {
            "accuracy": 0.481826,
            "f1": 0.281546,
            "f1_weighted": 0.526372
          },
          {
            "accuracy": 0.449986,
            "f1": 0.271354,
            "f1_weighted": 0.502248
          },
          {
            "accuracy": 0.484362,
            "f1": 0.272637,
            "f1_weighted": 0.533226
          },
          {
            "accuracy": 0.489152,
            "f1": 0.276037,
            "f1_weighted": 0.540519
          }
        ],
        "main_score": 0.451367,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 22.86111354827881,
  "kg_co2_emissions": null
}