{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "task_name": "GreekLegalCodeClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.0229,
        "f1": 0.011952,
        "f1_weighted": 0.013988,
        "scores_per_experiment": [
          {
            "accuracy": 0.020996,
            "f1": 0.010949,
            "f1_weighted": 0.010719
          },
          {
            "accuracy": 0.023438,
            "f1": 0.012596,
            "f1_weighted": 0.018057
          },
          {
            "accuracy": 0.022461,
            "f1": 0.012743,
            "f1_weighted": 0.01494
          },
          {
            "accuracy": 0.022461,
            "f1": 0.0158,
            "f1_weighted": 0.01467
          },
          {
            "accuracy": 0.02002,
            "f1": 0.01015,
            "f1_weighted": 0.011897
          },
          {
            "accuracy": 0.023926,
            "f1": 0.01244,
            "f1_weighted": 0.013169
          },
          {
            "accuracy": 0.025879,
            "f1": 0.011174,
            "f1_weighted": 0.014823
          },
          {
            "accuracy": 0.021484,
            "f1": 0.010298,
            "f1_weighted": 0.0127
          },
          {
            "accuracy": 0.022461,
            "f1": 0.01142,
            "f1_weighted": 0.014193
          },
          {
            "accuracy": 0.025879,
            "f1": 0.011947,
            "f1_weighted": 0.014707
          }
        ],
        "main_score": 0.0229,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.023779,
        "f1": 0.011771,
        "f1_weighted": 0.013097,
        "scores_per_experiment": [
          {
            "accuracy": 0.024902,
            "f1": 0.014224,
            "f1_weighted": 0.012919
          },
          {
            "accuracy": 0.02832,
            "f1": 0.016303,
            "f1_weighted": 0.021549
          },
          {
            "accuracy": 0.020508,
            "f1": 0.009262,
            "f1_weighted": 0.011218
          },
          {
            "accuracy": 0.01709,
            "f1": 0.00709,
            "f1_weighted": 0.007674
          },
          {
            "accuracy": 0.024902,
            "f1": 0.01063,
            "f1_weighted": 0.013376
          },
          {
            "accuracy": 0.025391,
            "f1": 0.011869,
            "f1_weighted": 0.012257
          },
          {
            "accuracy": 0.024902,
            "f1": 0.012064,
            "f1_weighted": 0.014474
          },
          {
            "accuracy": 0.024902,
            "f1": 0.011697,
            "f1_weighted": 0.012477
          },
          {
            "accuracy": 0.021484,
            "f1": 0.011982,
            "f1_weighted": 0.012114
          },
          {
            "accuracy": 0.025391,
            "f1": 0.01259,
            "f1_weighted": 0.012908
          }
        ],
        "main_score": 0.023779,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ]
  },
  "evaluation_time": 283.96065306663513,
  "kg_co2_emissions": 0.013622862605289634
}