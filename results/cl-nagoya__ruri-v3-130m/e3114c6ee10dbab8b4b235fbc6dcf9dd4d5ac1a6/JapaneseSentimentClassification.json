{
  "dataset_revision": "113a72dd629207b956dd4db3c2d11445853f3b1f",
  "task_name": "JapaneseSentimentClassification",
  "mteb_version": "2.2.0",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.953762,
            "f1": 0.951094,
            "f1_weighted": 0.954156,
            "precision": 0.944707,
            "precision_weighted": 0.956899,
            "recall": 0.960141,
            "recall_weighted": 0.953762,
            "ap": 0.967522,
            "ap_weighted": 0.967522
          },
          {
            "accuracy": 0.951803,
            "f1": 0.947796,
            "f1_weighted": 0.951672,
            "precision": 0.950315,
            "precision_weighted": 0.951709,
            "recall": 0.94547,
            "recall_weighted": 0.951803,
            "ap": 0.94558,
            "ap_weighted": 0.94558
          },
          {
            "accuracy": 0.952586,
            "f1": 0.949622,
            "f1_weighted": 0.952897,
            "precision": 0.944439,
            "precision_weighted": 0.954498,
            "recall": 0.956272,
            "recall_weighted": 0.952586,
            "ap": 0.96197,
            "ap_weighted": 0.96197
          },
          {
            "accuracy": 0.869122,
            "f1": 0.864568,
            "f1_weighted": 0.871224,
            "precision": 0.859393,
            "precision_weighted": 0.8858,
            "recall": 0.883434,
            "recall_weighted": 0.869122,
            "ap": 0.902865,
            "ap_weighted": 0.902865
          },
          {
            "accuracy": 0.923197,
            "f1": 0.919456,
            "f1_weighted": 0.924109,
            "precision": 0.912308,
            "precision_weighted": 0.930213,
            "recall": 0.932416,
            "recall_weighted": 0.923197,
            "ap": 0.943918,
            "ap_weighted": 0.943918
          },
          {
            "accuracy": 0.941223,
            "f1": 0.936927,
            "f1_weighted": 0.941339,
            "precision": 0.935257,
            "precision_weighted": 0.941554,
            "recall": 0.93871,
            "recall_weighted": 0.941223,
            "ap": 0.941891,
            "ap_weighted": 0.941891
          },
          {
            "accuracy": 0.934953,
            "f1": 0.931857,
            "f1_weighted": 0.93575,
            "precision": 0.924373,
            "precision_weighted": 0.942286,
            "recall": 0.94576,
            "recall_weighted": 0.934953,
            "ap": 0.957428,
            "ap_weighted": 0.957428
          },
          {
            "accuracy": 0.87931,
            "f1": 0.869732,
            "f1_weighted": 0.8792,
            "precision": 0.870421,
            "precision_weighted": 0.879107,
            "recall": 0.869064,
            "recall_weighted": 0.87931,
            "ap": 0.877908,
            "ap_weighted": 0.877908
          },
          {
            "accuracy": 0.942398,
            "f1": 0.939226,
            "f1_weighted": 0.942948,
            "precision": 0.932516,
            "precision_weighted": 0.94654,
            "recall": 0.949368,
            "recall_weighted": 0.942398,
            "ap": 0.957952,
            "ap_weighted": 0.957952
          },
          {
            "accuracy": 0.958072,
            "f1": 0.954998,
            "f1_weighted": 0.95815,
            "precision": 0.953349,
            "precision_weighted": 0.958319,
            "recall": 0.956751,
            "recall_weighted": 0.958072,
            "ap": 0.958945,
            "ap_weighted": 0.958945
          }
        ],
        "accuracy": 0.930643,
        "f1": 0.926528,
        "f1_weighted": 0.931144,
        "precision": 0.922708,
        "precision_weighted": 0.934692,
        "recall": 0.933738,
        "recall_weighted": 0.930643,
        "ap": 0.941598,
        "ap_weighted": 0.941598,
        "main_score": 0.930643,
        "hf_subset": "default",
        "languages": [
          "jpn-Jpan"
        ]
      }
    ]
  },
  "evaluation_time": 3.815600633621216,
  "kg_co2_emissions": null
}