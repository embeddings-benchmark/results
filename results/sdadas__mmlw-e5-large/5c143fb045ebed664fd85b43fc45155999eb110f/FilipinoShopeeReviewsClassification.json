{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.290479,
        "f1": 0.287588,
        "f1_weighted": 0.28758,
        "scores_per_experiment": [
          {
            "accuracy": 0.290039,
            "f1": 0.28438,
            "f1_weighted": 0.284378
          },
          {
            "accuracy": 0.27832,
            "f1": 0.275205,
            "f1_weighted": 0.275192
          },
          {
            "accuracy": 0.293457,
            "f1": 0.291574,
            "f1_weighted": 0.291565
          },
          {
            "accuracy": 0.316406,
            "f1": 0.314566,
            "f1_weighted": 0.31458
          },
          {
            "accuracy": 0.27832,
            "f1": 0.272966,
            "f1_weighted": 0.27293
          },
          {
            "accuracy": 0.291016,
            "f1": 0.285486,
            "f1_weighted": 0.285478
          },
          {
            "accuracy": 0.229492,
            "f1": 0.229954,
            "f1_weighted": 0.229929
          },
          {
            "accuracy": 0.331055,
            "f1": 0.329821,
            "f1_weighted": 0.329827
          },
          {
            "accuracy": 0.31543,
            "f1": 0.314689,
            "f1_weighted": 0.314677
          },
          {
            "accuracy": 0.28125,
            "f1": 0.277244,
            "f1_weighted": 0.277241
          }
        ],
        "main_score": 0.290479,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.29209,
        "f1": 0.288291,
        "f1_weighted": 0.288289,
        "scores_per_experiment": [
          {
            "accuracy": 0.289062,
            "f1": 0.280817,
            "f1_weighted": 0.280808
          },
          {
            "accuracy": 0.276855,
            "f1": 0.274399,
            "f1_weighted": 0.274385
          },
          {
            "accuracy": 0.306152,
            "f1": 0.303751,
            "f1_weighted": 0.303749
          },
          {
            "accuracy": 0.298828,
            "f1": 0.298304,
            "f1_weighted": 0.298317
          },
          {
            "accuracy": 0.286621,
            "f1": 0.280914,
            "f1_weighted": 0.28088
          },
          {
            "accuracy": 0.273438,
            "f1": 0.264263,
            "f1_weighted": 0.264253
          },
          {
            "accuracy": 0.239258,
            "f1": 0.239627,
            "f1_weighted": 0.239615
          },
          {
            "accuracy": 0.338867,
            "f1": 0.335758,
            "f1_weighted": 0.335792
          },
          {
            "accuracy": 0.319336,
            "f1": 0.317026,
            "f1_weighted": 0.317018
          },
          {
            "accuracy": 0.29248,
            "f1": 0.288047,
            "f1_weighted": 0.28807
          }
        ],
        "main_score": 0.29209,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 20.973738193511963,
  "kg_co2_emissions": 0.000950221367680929
}