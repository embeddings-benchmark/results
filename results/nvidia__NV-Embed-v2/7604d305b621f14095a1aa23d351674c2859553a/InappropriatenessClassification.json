{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "evaluation_time": 75.39574694633484,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.609912109375,
        "ap": 0.5676846020508237,
        "ap_weighted": 0.5676846020508237,
        "f1": 0.6061637318348655,
        "f1_weighted": 0.6061637318348655,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.609912109375,
        "scores_per_experiment": [
          {
            "accuracy": 0.63671875,
            "ap": 0.5882560745841996,
            "ap_weighted": 0.5882560745841996,
            "f1": 0.6363855042167208,
            "f1_weighted": 0.6363855042167208
          },
          {
            "accuracy": 0.6171875,
            "ap": 0.5691195733532934,
            "ap_weighted": 0.5691195733532934,
            "f1": 0.6080918534718425,
            "f1_weighted": 0.6080918534718425
          },
          {
            "accuracy": 0.6318359375,
            "ap": 0.5856058576465708,
            "ap_weighted": 0.5856058576465708,
            "f1": 0.6305675919829737,
            "f1_weighted": 0.6305675919829737
          },
          {
            "accuracy": 0.56494140625,
            "ap": 0.5363026494565217,
            "ap_weighted": 0.5363026494565217,
            "f1": 0.5638381861010422,
            "f1_weighted": 0.5638381861010422
          },
          {
            "accuracy": 0.61474609375,
            "ap": 0.5710056717644085,
            "ap_weighted": 0.5710056717644085,
            "f1": 0.6146335425590599,
            "f1_weighted": 0.6146335425590599
          },
          {
            "accuracy": 0.541015625,
            "ap": 0.5217579258889695,
            "ap_weighted": 0.5217579258889695,
            "f1": 0.5268799023246075,
            "f1_weighted": 0.5268799023246075
          },
          {
            "accuracy": 0.623046875,
            "ap": 0.5730078125,
            "ap_weighted": 0.5730078125,
            "f1": 0.6132472673866226,
            "f1_weighted": 0.6132472673866226
          },
          {
            "accuracy": 0.66748046875,
            "ap": 0.6088696310422135,
            "ap_weighted": 0.6088696310422135,
            "f1": 0.6663539979373911,
            "f1_weighted": 0.6663539979373911
          },
          {
            "accuracy": 0.58935546875,
            "ap": 0.5529113953927491,
            "ap_weighted": 0.5529113953927491,
            "f1": 0.5892613602083112,
            "f1_weighted": 0.5892613602083112
          },
          {
            "accuracy": 0.61279296875,
            "ap": 0.5700094288793103,
            "ap_weighted": 0.5700094288793103,
            "f1": 0.6123781121600834,
            "f1_weighted": 0.6123781121600834
          }
        ]
      }
    ]
  },
  "task_name": "InappropriatenessClassification"
}