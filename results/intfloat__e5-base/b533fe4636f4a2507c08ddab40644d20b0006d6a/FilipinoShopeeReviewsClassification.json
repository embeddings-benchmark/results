{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.28501,
        "f1": 0.279357,
        "f1_weighted": 0.27934,
        "scores_per_experiment": [
          {
            "accuracy": 0.288574,
            "f1": 0.28819,
            "f1_weighted": 0.288169
          },
          {
            "accuracy": 0.256348,
            "f1": 0.247937,
            "f1_weighted": 0.247929
          },
          {
            "accuracy": 0.279785,
            "f1": 0.271005,
            "f1_weighted": 0.270968
          },
          {
            "accuracy": 0.311035,
            "f1": 0.311281,
            "f1_weighted": 0.311276
          },
          {
            "accuracy": 0.258301,
            "f1": 0.255865,
            "f1_weighted": 0.255848
          },
          {
            "accuracy": 0.289062,
            "f1": 0.280259,
            "f1_weighted": 0.280233
          },
          {
            "accuracy": 0.228516,
            "f1": 0.228826,
            "f1_weighted": 0.228803
          },
          {
            "accuracy": 0.322754,
            "f1": 0.312064,
            "f1_weighted": 0.312061
          },
          {
            "accuracy": 0.321777,
            "f1": 0.31906,
            "f1_weighted": 0.319039
          },
          {
            "accuracy": 0.293945,
            "f1": 0.279085,
            "f1_weighted": 0.279077
          }
        ],
        "main_score": 0.28501,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.290332,
        "f1": 0.28517,
        "f1_weighted": 0.285167,
        "scores_per_experiment": [
          {
            "accuracy": 0.270996,
            "f1": 0.270456,
            "f1_weighted": 0.270443
          },
          {
            "accuracy": 0.269531,
            "f1": 0.262819,
            "f1_weighted": 0.262814
          },
          {
            "accuracy": 0.300781,
            "f1": 0.292201,
            "f1_weighted": 0.292182
          },
          {
            "accuracy": 0.307617,
            "f1": 0.308409,
            "f1_weighted": 0.308408
          },
          {
            "accuracy": 0.287598,
            "f1": 0.287822,
            "f1_weighted": 0.28781
          },
          {
            "accuracy": 0.277344,
            "f1": 0.266432,
            "f1_weighted": 0.266423
          },
          {
            "accuracy": 0.257812,
            "f1": 0.256867,
            "f1_weighted": 0.256864
          },
          {
            "accuracy": 0.33252,
            "f1": 0.323616,
            "f1_weighted": 0.323625
          },
          {
            "accuracy": 0.299316,
            "f1": 0.294013,
            "f1_weighted": 0.294014
          },
          {
            "accuracy": 0.299805,
            "f1": 0.289063,
            "f1_weighted": 0.289084
          }
        ],
        "main_score": 0.290332,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 16.425312519073486,
  "kg_co2_emissions": 0.0005792877925993116
}