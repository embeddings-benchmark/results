{
  "dataset_revision": "f8650438298df086750ff4973661bb58a201a5ee",
  "evaluation_time": 632.388346195221,
  "kg_co2_emissions": 0.03503920900722772,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.03162055335968379,
        "f1": 0.027548302359266443,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.027548302359266443,
        "precision": 0.02699006550856639,
        "recall": 0.03162055335968379
      },
      {
        "accuracy": 0.08596837944664032,
        "f1": 0.04095750354358873,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ],
        "main_score": 0.04095750354358873,
        "precision": 0.03343476267557765,
        "recall": 0.08596837944664032
      },
      {
        "accuracy": 0.05434782608695652,
        "f1": 0.04351342055440858,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ],
        "main_score": 0.04351342055440858,
        "precision": 0.04090523329405426,
        "recall": 0.05434782608695652
      },
      {
        "accuracy": 0.11462450592885376,
        "f1": 0.062351817331563766,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ],
        "main_score": 0.062351817331563766,
        "precision": 0.05253879957989072,
        "recall": 0.11462450592885376
      },
      {
        "accuracy": 0.03557312252964427,
        "f1": 0.0285049329931793,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.0285049329931793,
        "precision": 0.02708219653246587,
        "recall": 0.03557312252964427
      },
      {
        "accuracy": 0.12845849802371542,
        "f1": 0.08346230247900725,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ],
        "main_score": 0.08346230247900725,
        "precision": 0.07464634813074249,
        "recall": 0.12845849802371542
      },
      {
        "accuracy": 0.06818181818181818,
        "f1": 0.05654349477177244,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ],
        "main_score": 0.05654349477177244,
        "precision": 0.05329723754730485,
        "recall": 0.06818181818181818
      },
      {
        "accuracy": 0.1422924901185771,
        "f1": 0.08323311847378287,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ],
        "main_score": 0.08323311847378287,
        "precision": 0.07121019685890799,
        "recall": 0.1422924901185771
      },
      {
        "accuracy": 0.06225296442687747,
        "f1": 0.04936821063760347,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.04936821063760347,
        "precision": 0.04593951815964138,
        "recall": 0.06225296442687747
      },
      {
        "accuracy": 0.12450592885375494,
        "f1": 0.07236359434800128,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ],
        "main_score": 0.07236359434800128,
        "precision": 0.06143147257338539,
        "recall": 0.12450592885375494
      },
      {
        "accuracy": 0.03260869565217391,
        "f1": 0.028814935064935064,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.028814935064935064,
        "precision": 0.02787502352719744,
        "recall": 0.03260869565217391
      },
      {
        "accuracy": 0.10375494071146245,
        "f1": 0.05759762467781684,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ],
        "main_score": 0.05759762467781684,
        "precision": 0.049400663493826484,
        "recall": 0.10375494071146245
      },
      {
        "accuracy": 0.04841897233201581,
        "f1": 0.04257427179204375,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.04257427179204375,
        "precision": 0.04137668428420359,
        "recall": 0.04841897233201581
      },
      {
        "accuracy": 0.12944664031620554,
        "f1": 0.07742091210825376,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ],
        "main_score": 0.07742091210825376,
        "precision": 0.06644588623375654,
        "recall": 0.12944664031620554
      },
      {
        "accuracy": 0.10869565217391304,
        "f1": 0.0929427331788651,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.0929427331788651,
        "precision": 0.08846907743230431,
        "recall": 0.10869565217391304
      },
      {
        "accuracy": 0.17490118577075098,
        "f1": 0.09803736848085469,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ],
        "main_score": 0.09803736848085469,
        "precision": 0.0822878920051239,
        "recall": 0.17490118577075098
      },
      {
        "accuracy": 0.023715415019762844,
        "f1": 0.01936344792896621,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.01936344792896621,
        "precision": 0.01860720595280872,
        "recall": 0.023715415019762844
      },
      {
        "accuracy": 0.11363636363636363,
        "f1": 0.06666765695809096,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ],
        "main_score": 0.06666765695809096,
        "precision": 0.056293631150522,
        "recall": 0.11363636363636363
      },
      {
        "accuracy": 0.02766798418972332,
        "f1": 0.02364691132807075,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ],
        "main_score": 0.02364691132807075,
        "precision": 0.022871660760528826,
        "recall": 0.02766798418972332
      },
      {
        "accuracy": 0.06521739130434782,
        "f1": 0.029102404237652743,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ],
        "main_score": 0.029102404237652743,
        "precision": 0.024072783984877295,
        "recall": 0.06521739130434782
      },
      {
        "accuracy": 0.042490118577075096,
        "f1": 0.03567573730617209,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ],
        "main_score": 0.03567573730617209,
        "precision": 0.034388538352518594,
        "recall": 0.042490118577075096
      },
      {
        "accuracy": 0.1383399209486166,
        "f1": 0.08755020619150639,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ],
        "main_score": 0.08755020619150639,
        "precision": 0.07703375964719485,
        "recall": 0.1383399209486166
      },
      {
        "accuracy": 0.06126482213438735,
        "f1": 0.05094421098548046,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ],
        "main_score": 0.05094421098548046,
        "precision": 0.04953523600860294,
        "recall": 0.06126482213438735
      },
      {
        "accuracy": 0.12055335968379446,
        "f1": 0.07160880499083692,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ],
        "main_score": 0.07160880499083692,
        "precision": 0.0626128859314685,
        "recall": 0.12055335968379446
      },
      {
        "accuracy": 0.06324110671936758,
        "f1": 0.05570628646715603,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ],
        "main_score": 0.05570628646715603,
        "precision": 0.05361347192043592,
        "recall": 0.06324110671936758
      },
      {
        "accuracy": 0.12154150197628459,
        "f1": 0.06128997963366469,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ],
        "main_score": 0.06128997963366469,
        "precision": 0.05031888867289964,
        "recall": 0.12154150197628459
      },
      {
        "accuracy": 0.06818181818181818,
        "f1": 0.05693376164695127,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ],
        "main_score": 0.05693376164695127,
        "precision": 0.05416120237525511,
        "recall": 0.06818181818181818
      },
      {
        "accuracy": 0.12944664031620554,
        "f1": 0.0756068991006839,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ],
        "main_score": 0.0756068991006839,
        "precision": 0.0653056075951914,
        "recall": 0.12944664031620554
      },
      {
        "accuracy": 0.09387351778656126,
        "f1": 0.08149687891339538,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ],
        "main_score": 0.08149687891339538,
        "precision": 0.07856783827854384,
        "recall": 0.09387351778656126
      },
      {
        "accuracy": 0.17984189723320157,
        "f1": 0.11196178526173557,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ],
        "main_score": 0.11196178526173557,
        "precision": 0.09779074688154236,
        "recall": 0.17984189723320157
      },
      {
        "accuracy": 0.05928853754940711,
        "f1": 0.0502589509305628,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ],
        "main_score": 0.0502589509305628,
        "precision": 0.04882492075612135,
        "recall": 0.05928853754940711
      },
      {
        "accuracy": 0.10869565217391304,
        "f1": 0.05748371123885926,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ],
        "main_score": 0.05748371123885926,
        "precision": 0.049353581670828525,
        "recall": 0.10869565217391304
      },
      {
        "accuracy": 0.030632411067193676,
        "f1": 0.02398314057923063,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.02398314057923063,
        "precision": 0.02325245583469102,
        "recall": 0.030632411067193676
      },
      {
        "accuracy": 0.09881422924901186,
        "f1": 0.05672519780862592,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ],
        "main_score": 0.05672519780862592,
        "precision": 0.0480895982263893,
        "recall": 0.09881422924901186
      },
      {
        "accuracy": 0.025691699604743084,
        "f1": 0.020031302627785286,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ],
        "main_score": 0.020031302627785286,
        "precision": 0.018987220691766147,
        "recall": 0.025691699604743084
      },
      {
        "accuracy": 0.11264822134387352,
        "f1": 0.06907737818396599,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ],
        "main_score": 0.06907737818396599,
        "precision": 0.06082229665849807,
        "recall": 0.11264822134387352
      },
      {
        "accuracy": 0.023715415019762844,
        "f1": 0.020790996802990805,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ],
        "main_score": 0.020790996802990805,
        "precision": 0.02027722915794654,
        "recall": 0.023715415019762844
      },
      {
        "accuracy": 0.0958498023715415,
        "f1": 0.04905532994892782,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ],
        "main_score": 0.04905532994892782,
        "precision": 0.04138080360910079,
        "recall": 0.0958498023715415
      },
      {
        "accuracy": 0.03557312252964427,
        "f1": 0.028270573896278923,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ],
        "main_score": 0.028270573896278923,
        "precision": 0.027436302713949215,
        "recall": 0.03557312252964427
      },
      {
        "accuracy": 0.12648221343873517,
        "f1": 0.07332511179116377,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ],
        "main_score": 0.07332511179116377,
        "precision": 0.06324967074711665,
        "recall": 0.12648221343873517
      },
      {
        "accuracy": 0.03458498023715415,
        "f1": 0.0275278110461588,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ],
        "main_score": 0.0275278110461588,
        "precision": 0.026421040331171004,
        "recall": 0.03458498023715415
      },
      {
        "accuracy": 0.11363636363636363,
        "f1": 0.06922137902587064,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ],
        "main_score": 0.06922137902587064,
        "precision": 0.06059128041859197,
        "recall": 0.11363636363636363
      },
      {
        "accuracy": 0.06719367588932806,
        "f1": 0.056016369349355104,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ],
        "main_score": 0.056016369349355104,
        "precision": 0.054071663742483525,
        "recall": 0.06719367588932806
      },
      {
        "accuracy": 0.16699604743083005,
        "f1": 0.10457742533955328,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ],
        "main_score": 0.10457742533955328,
        "precision": 0.09198420674124105,
        "recall": 0.16699604743083005
      },
      {
        "accuracy": 0.022727272727272728,
        "f1": 0.017452458891275564,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ],
        "main_score": 0.017452458891275564,
        "precision": 0.01630640645586298,
        "recall": 0.022727272727272728
      },
      {
        "accuracy": 0.09486166007905138,
        "f1": 0.060525565614387114,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ],
        "main_score": 0.060525565614387114,
        "precision": 0.05306473582718719,
        "recall": 0.09486166007905138
      },
      {
        "accuracy": 0.042490118577075096,
        "f1": 0.03649319950761475,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ],
        "main_score": 0.03649319950761475,
        "precision": 0.03507218194500222,
        "recall": 0.042490118577075096
      },
      {
        "accuracy": 0.13438735177865613,
        "f1": 0.0824303375266853,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ],
        "main_score": 0.0824303375266853,
        "precision": 0.07245294492627051,
        "recall": 0.13438735177865613
      },
      {
        "accuracy": 0.04743083003952569,
        "f1": 0.040608443188799664,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ],
        "main_score": 0.040608443188799664,
        "precision": 0.03868197020370934,
        "recall": 0.04743083003952569
      },
      {
        "accuracy": 0.12845849802371542,
        "f1": 0.07652038744312331,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ],
        "main_score": 0.07652038744312331,
        "precision": 0.06588906021346717,
        "recall": 0.12845849802371542
      },
      {
        "accuracy": 0.043478260869565216,
        "f1": 0.03396618591315893,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ],
        "main_score": 0.03396618591315893,
        "precision": 0.03214734588447403,
        "recall": 0.043478260869565216
      },
      {
        "accuracy": 0.1007905138339921,
        "f1": 0.04981935437156758,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ],
        "main_score": 0.04981935437156758,
        "precision": 0.040292668241472124,
        "recall": 0.1007905138339921
      },
      {
        "accuracy": 0.02766798418972332,
        "f1": 0.02372090469916557,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ],
        "main_score": 0.02372090469916557,
        "precision": 0.022826720381068207,
        "recall": 0.02766798418972332
      },
      {
        "accuracy": 0.11264822134387352,
        "f1": 0.07094035141788191,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ],
        "main_score": 0.07094035141788191,
        "precision": 0.06183371019248057,
        "recall": 0.11264822134387352
      },
      {
        "accuracy": 0.040513833992094864,
        "f1": 0.03290686366773324,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ],
        "main_score": 0.03290686366773324,
        "precision": 0.03155837475202385,
        "recall": 0.040513833992094864
      },
      {
        "accuracy": 0.12450592885375494,
        "f1": 0.07508680668666488,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ],
        "main_score": 0.07508680668666488,
        "precision": 0.06449616265453101,
        "recall": 0.12450592885375494
      },
      {
        "accuracy": 0.012845849802371542,
        "f1": 0.008776061984713571,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ],
        "main_score": 0.008776061984713571,
        "precision": 0.008258551750469488,
        "recall": 0.012845849802371542
      },
      {
        "accuracy": 0.0266798418972332,
        "f1": 0.007354748082732209,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ],
        "main_score": 0.007354748082732209,
        "precision": 0.006259830676222964,
        "recall": 0.0266798418972332
      }
    ],
    "validation": [
      {
        "accuracy": 0.0320962888665998,
        "f1": 0.02832740646180967,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.02832740646180967,
        "precision": 0.027655074065734617,
        "recall": 0.0320962888665998
      },
      {
        "accuracy": 0.07923771313941826,
        "f1": 0.03046646885925955,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ],
        "main_score": 0.03046646885925955,
        "precision": 0.02378820357914266,
        "recall": 0.07923771313941826
      },
      {
        "accuracy": 0.05917753259779338,
        "f1": 0.0475897878104498,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ],
        "main_score": 0.0475897878104498,
        "precision": 0.04420055699745284,
        "recall": 0.05917753259779338
      },
      {
        "accuracy": 0.11835506519558676,
        "f1": 0.06408874411986347,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ],
        "main_score": 0.06408874411986347,
        "precision": 0.054236884617649375,
        "recall": 0.11835506519558676
      },
      {
        "accuracy": 0.0160481444332999,
        "f1": 0.01404421816021579,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01404421816021579,
        "precision": 0.013742270653296003,
        "recall": 0.0160481444332999
      },
      {
        "accuracy": 0.12537612838515547,
        "f1": 0.07292351126249771,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ],
        "main_score": 0.07292351126249771,
        "precision": 0.06266012488892586,
        "recall": 0.12537612838515547
      },
      {
        "accuracy": 0.07422266800401203,
        "f1": 0.06252600600794436,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ],
        "main_score": 0.06252600600794436,
        "precision": 0.059132725124995414,
        "recall": 0.07422266800401203
      },
      {
        "accuracy": 0.14142427281845538,
        "f1": 0.07929029330775954,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ],
        "main_score": 0.07929029330775954,
        "precision": 0.06606495299765514,
        "recall": 0.14142427281845538
      },
      {
        "accuracy": 0.08224674022066199,
        "f1": 0.06701003886398735,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.06701003886398735,
        "precision": 0.06282001152238606,
        "recall": 0.08224674022066199
      },
      {
        "accuracy": 0.14944834503510532,
        "f1": 0.0945103320081307,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ],
        "main_score": 0.0945103320081307,
        "precision": 0.0837823102177302,
        "recall": 0.14944834503510532
      },
      {
        "accuracy": 0.01805416248746239,
        "f1": 0.01655179212852232,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01655179212852232,
        "precision": 0.016383553512587676,
        "recall": 0.01805416248746239
      },
      {
        "accuracy": 0.11334002006018054,
        "f1": 0.06627208555259669,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ],
        "main_score": 0.06627208555259669,
        "precision": 0.05766184331413408,
        "recall": 0.11334002006018054
      },
      {
        "accuracy": 0.04914744232698094,
        "f1": 0.04450633953141475,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.04450633953141475,
        "precision": 0.04328771145889166,
        "recall": 0.04914744232698094
      },
      {
        "accuracy": 0.14142427281845538,
        "f1": 0.0870744131241315,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ],
        "main_score": 0.0870744131241315,
        "precision": 0.0765119435128693,
        "recall": 0.14142427281845538
      },
      {
        "accuracy": 0.10531594784353059,
        "f1": 0.09403487470722416,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.09403487470722416,
        "precision": 0.09053250829696671,
        "recall": 0.10531594784353059
      },
      {
        "accuracy": 0.1534603811434303,
        "f1": 0.08541743102068816,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ],
        "main_score": 0.08541743102068816,
        "precision": 0.07247738482773589,
        "recall": 0.1534603811434303
      },
      {
        "accuracy": 0.03510531594784353,
        "f1": 0.03097051839278519,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.03097051839278519,
        "precision": 0.02994571998417429,
        "recall": 0.03510531594784353
      },
      {
        "accuracy": 0.1213640922768305,
        "f1": 0.07334742885590484,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ],
        "main_score": 0.07334742885590484,
        "precision": 0.06380091144071874,
        "recall": 0.1213640922768305
      },
      {
        "accuracy": 0.03610832497492478,
        "f1": 0.0307117445972966,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ],
        "main_score": 0.0307117445972966,
        "precision": 0.030177034370198415,
        "recall": 0.03610832497492478
      },
      {
        "accuracy": 0.06920762286860582,
        "f1": 0.028690282720862093,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ],
        "main_score": 0.028690282720862093,
        "precision": 0.02220544402563199,
        "recall": 0.06920762286860582
      },
      {
        "accuracy": 0.020060180541624874,
        "f1": 0.017638364829939555,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ],
        "main_score": 0.017638364829939555,
        "precision": 0.01719550297381248,
        "recall": 0.020060180541624874
      },
      {
        "accuracy": 0.1374122367101304,
        "f1": 0.08536110326953644,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ],
        "main_score": 0.08536110326953644,
        "precision": 0.07553563772545215,
        "recall": 0.1374122367101304
      },
      {
        "accuracy": 0.033099297893681046,
        "f1": 0.030246854290413537,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ],
        "main_score": 0.030246854290413537,
        "precision": 0.029673488873734333,
        "recall": 0.033099297893681046
      },
      {
        "accuracy": 0.12537612838515547,
        "f1": 0.06873770334281483,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ],
        "main_score": 0.06873770334281483,
        "precision": 0.058807492988606505,
        "recall": 0.12537612838515547
      },
      {
        "accuracy": 0.06920762286860582,
        "f1": 0.05951702310779542,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ],
        "main_score": 0.05951702310779542,
        "precision": 0.05664135689635059,
        "recall": 0.06920762286860582
      },
      {
        "accuracy": 0.12637913741223672,
        "f1": 0.07063995425775191,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ],
        "main_score": 0.07063995425775191,
        "precision": 0.060684822211573175,
        "recall": 0.12637913741223672
      },
      {
        "accuracy": 0.06820461384152457,
        "f1": 0.059608834848840574,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ],
        "main_score": 0.059608834848840574,
        "precision": 0.057462366159716456,
        "recall": 0.06820461384152457
      },
      {
        "accuracy": 0.1334002006018054,
        "f1": 0.07846791008179572,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ],
        "main_score": 0.07846791008179572,
        "precision": 0.06791852481868237,
        "recall": 0.1334002006018054
      },
      {
        "accuracy": 0.11133400200601805,
        "f1": 0.10362639483416448,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ],
        "main_score": 0.10362639483416448,
        "precision": 0.10134515715378475,
        "recall": 0.11133400200601805
      },
      {
        "accuracy": 0.20060180541624875,
        "f1": 0.11732467001889821,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ],
        "main_score": 0.11732467001889821,
        "precision": 0.09700801087621667,
        "recall": 0.20060180541624875
      },
      {
        "accuracy": 0.041123370110330994,
        "f1": 0.03708250326258196,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ],
        "main_score": 0.03708250326258196,
        "precision": 0.036276681545426175,
        "recall": 0.041123370110330994
      },
      {
        "accuracy": 0.12036108324974924,
        "f1": 0.06110563580664422,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ],
        "main_score": 0.06110563580664422,
        "precision": 0.050816344512141495,
        "recall": 0.12036108324974924
      },
      {
        "accuracy": 0.010030090270812437,
        "f1": 0.007846281318873006,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.007846281318873006,
        "precision": 0.0076072712184171555,
        "recall": 0.010030090270812437
      },
      {
        "accuracy": 0.11735205616850551,
        "f1": 0.06819248642852929,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ],
        "main_score": 0.06819248642852929,
        "precision": 0.05815036258572877,
        "recall": 0.11735205616850551
      },
      {
        "accuracy": 0.014042126379137413,
        "f1": 0.012192567479005573,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ],
        "main_score": 0.012192567479005573,
        "precision": 0.01212076860050825,
        "recall": 0.014042126379137413
      },
      {
        "accuracy": 0.12337011033099297,
        "f1": 0.08116305369963403,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ],
        "main_score": 0.08116305369963403,
        "precision": 0.07277835864119833,
        "recall": 0.12337011033099297
      },
      {
        "accuracy": 0.031093279839518557,
        "f1": 0.026507455910872352,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ],
        "main_score": 0.026507455910872352,
        "precision": 0.025558517643083666,
        "recall": 0.031093279839518557
      },
      {
        "accuracy": 0.10230692076228685,
        "f1": 0.0543012304486034,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ],
        "main_score": 0.0543012304486034,
        "precision": 0.046064507702649124,
        "recall": 0.10230692076228685
      },
      {
        "accuracy": 0.012036108324974924,
        "f1": 0.009155039119933804,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ],
        "main_score": 0.009155039119933804,
        "precision": 0.008593736536068687,
        "recall": 0.012036108324974924
      },
      {
        "accuracy": 0.119358074222668,
        "f1": 0.07324090249843776,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ],
        "main_score": 0.07324090249843776,
        "precision": 0.06442759108118368,
        "recall": 0.119358074222668
      },
      {
        "accuracy": 0.0160481444332999,
        "f1": 0.014226612183272862,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ],
        "main_score": 0.014226612183272862,
        "precision": 0.01414348866705938,
        "recall": 0.0160481444332999
      },
      {
        "accuracy": 0.10030090270812438,
        "f1": 0.06101213758333535,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ],
        "main_score": 0.06101213758333535,
        "precision": 0.05347103076185503,
        "recall": 0.10030090270812438
      },
      {
        "accuracy": 0.02708124373119358,
        "f1": 0.02536393829676663,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ],
        "main_score": 0.02536393829676663,
        "precision": 0.025243464295412025,
        "recall": 0.02708124373119358
      },
      {
        "accuracy": 0.160481444332999,
        "f1": 0.09643692604690336,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ],
        "main_score": 0.09643692604690336,
        "precision": 0.08271762958413094,
        "recall": 0.160481444332999
      },
      {
        "accuracy": 0.006018054162487462,
        "f1": 0.004181438152616722,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ],
        "main_score": 0.004181438152616722,
        "precision": 0.0041043369288975675,
        "recall": 0.006018054162487462
      },
      {
        "accuracy": 0.11133400200601805,
        "f1": 0.0726074105953225,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ],
        "main_score": 0.0726074105953225,
        "precision": 0.06453644835795325,
        "recall": 0.11133400200601805
      },
      {
        "accuracy": 0.01905717151454363,
        "f1": 0.01653899669729337,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01653899669729337,
        "precision": 0.01613283315438238,
        "recall": 0.01905717151454363
      },
      {
        "accuracy": 0.13039117352056168,
        "f1": 0.07954515698454358,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ],
        "main_score": 0.07954515698454358,
        "precision": 0.07044010261561484,
        "recall": 0.13039117352056168
      },
      {
        "accuracy": 0.020060180541624874,
        "f1": 0.01787413522619139,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01787413522619139,
        "precision": 0.017134737545971244,
        "recall": 0.020060180541624874
      },
      {
        "accuracy": 0.12637913741223672,
        "f1": 0.07513997603095951,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ],
        "main_score": 0.07513997603095951,
        "precision": 0.0653004229449098,
        "recall": 0.12637913741223672
      },
      {
        "accuracy": 0.044132397191574725,
        "f1": 0.038551966901531724,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ],
        "main_score": 0.038551966901531724,
        "precision": 0.0368300981375499,
        "recall": 0.044132397191574725
      },
      {
        "accuracy": 0.10732196589769308,
        "f1": 0.04915892257437767,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ],
        "main_score": 0.04915892257437767,
        "precision": 0.03881850800426294,
        "recall": 0.10732196589769308
      },
      {
        "accuracy": 0.011033099297893681,
        "f1": 0.00889546499406703,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ],
        "main_score": 0.00889546499406703,
        "precision": 0.0086380824408862,
        "recall": 0.011033099297893681
      },
      {
        "accuracy": 0.11534603811434303,
        "f1": 0.06886828085362989,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ],
        "main_score": 0.06886828085362989,
        "precision": 0.05928331749283495,
        "recall": 0.11534603811434303
      },
      {
        "accuracy": 0.015045135406218655,
        "f1": 0.012861244782572674,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ],
        "main_score": 0.012861244782572674,
        "precision": 0.012622275428435845,
        "recall": 0.015045135406218655
      },
      {
        "accuracy": 0.119358074222668,
        "f1": 0.0736449014815284,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ],
        "main_score": 0.0736449014815284,
        "precision": 0.06573339523584544,
        "recall": 0.119358074222668
      },
      {
        "accuracy": 0.006018054162487462,
        "f1": 0.0047048776852646286,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ],
        "main_score": 0.0047048776852646286,
        "precision": 0.0045257724392690265,
        "recall": 0.006018054162487462
      },
      {
        "accuracy": 0.01905717151454363,
        "f1": 0.005272533422025246,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ],
        "main_score": 0.005272533422025246,
        "precision": 0.004199261608810139,
        "recall": 0.01905717151454363
      }
    ]
  },
  "task_name": "IndicGenBenchFloresBitextMining"
}