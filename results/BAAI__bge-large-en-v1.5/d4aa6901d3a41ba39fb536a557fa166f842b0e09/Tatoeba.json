{
  "dataset_revision": "69e8f12da6e31d59addadda9a9c8a2e601a0e282",
  "evaluation_time": 383.3008403778076,
  "kg_co2_emissions": 0.0171068074070479,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.169,
        "f1": 0.1488789039565635,
        "hf_subset": "nld-eng",
        "languages": [
          "nld-Latn",
          "eng-Latn"
        ],
        "main_score": 0.1488789039565635,
        "precision": 0.14238820926724396,
        "recall": 0.169
      },
      {
        "accuracy": 0.06,
        "f1": 0.04574660514923672,
        "hf_subset": "isl-eng",
        "languages": [
          "isl-Latn",
          "eng-Latn"
        ],
        "main_score": 0.04574660514923672,
        "precision": 0.04194126984126984,
        "recall": 0.06
      },
      {
        "accuracy": 0.069,
        "f1": 0.054087563217907544,
        "hf_subset": "ces-eng",
        "languages": [
          "ces-Latn",
          "eng-Latn"
        ],
        "main_score": 0.054087563217907544,
        "precision": 0.05067500178115603,
        "recall": 0.069
      },
      {
        "accuracy": 0.438,
        "f1": 0.38549485281292006,
        "hf_subset": "ina-eng",
        "languages": [
          "ina-Latn",
          "eng-Latn"
        ],
        "main_score": 0.38549485281292006,
        "precision": 0.36811881853526585,
        "recall": 0.438
      },
      {
        "accuracy": 0.005390835579514825,
        "f1": 0.002998860880784192,
        "hf_subset": "hye-eng",
        "languages": [
          "hye-Armn",
          "eng-Latn"
        ],
        "main_score": 0.002998860880784192,
        "precision": 0.002865860416147175,
        "recall": 0.005390835579514825
      },
      {
        "accuracy": 0.011,
        "f1": 0.006983341129822685,
        "hf_subset": "heb-eng",
        "languages": [
          "heb-Hebr",
          "eng-Latn"
        ],
        "main_score": 0.006983341129822685,
        "precision": 0.006671024853282918,
        "recall": 0.011
      },
      {
        "accuracy": 0.282,
        "f1": 0.25084239839751754,
        "hf_subset": "spa-eng",
        "languages": [
          "spa-Latn",
          "eng-Latn"
        ],
        "main_score": 0.25084239839751754,
        "precision": 0.24160338376755255,
        "recall": 0.282
      },
      {
        "accuracy": 0.07776427703523693,
        "f1": 0.057710549676427365,
        "hf_subset": "slv-eng",
        "languages": [
          "slv-Latn",
          "eng-Latn"
        ],
        "main_score": 0.057710549676427365,
        "precision": 0.054200208227135185,
        "recall": 0.07776427703523693
      },
      {
        "accuracy": 0.013,
        "f1": 0.00899985724081834,
        "hf_subset": "tat-eng",
        "languages": [
          "tat-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.00899985724081834,
        "precision": 0.008690619012899784,
        "recall": 0.013
      },
      {
        "accuracy": 0.09268292682926829,
        "f1": 0.07310284330963016,
        "hf_subset": "kur-eng",
        "languages": [
          "kur-Latn",
          "eng-Latn"
        ],
        "main_score": 0.07310284330963016,
        "precision": 0.068978120585291,
        "recall": 0.09268292682926829
      },
      {
        "accuracy": 0.227,
        "f1": 0.18321762960818688,
        "hf_subset": "cbk-eng",
        "languages": [
          "cbk-Latn",
          "eng-Latn"
        ],
        "main_score": 0.18321762960818688,
        "precision": 0.17120082293837513,
        "recall": 0.227
      },
      {
        "accuracy": 0.13,
        "f1": 0.11342877941276563,
        "hf_subset": "nob-eng",
        "languages": [
          "nob-Latn",
          "eng-Latn"
        ],
        "main_score": 0.11342877941276563,
        "precision": 0.10939026474732996,
        "recall": 0.13
      },
      {
        "accuracy": 0.002,
        "f1": 5.606060606060606e-05,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 5.606060606060606e-05,
        "precision": 2.84434054159361e-05,
        "recall": 0.002
      },
      {
        "accuracy": 0.042,
        "f1": 0.03306380465624408,
        "hf_subset": "srp-eng",
        "languages": [
          "srp-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.03306380465624408,
        "precision": 0.031189983164983167,
        "recall": 0.042
      },
      {
        "accuracy": 0.02564102564102564,
        "f1": 0.007819810443077682,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.007819810443077682,
        "precision": 0.006467123858428205,
        "recall": 0.02564102564102564
      },
      {
        "accuracy": 0.025,
        "f1": 0.015501447228706415,
        "hf_subset": "yue-eng",
        "languages": [
          "yue-Hant",
          "eng-Latn"
        ],
        "main_score": 0.015501447228706415,
        "precision": 0.01426387903284514,
        "recall": 0.025
      },
      {
        "accuracy": 0.068,
        "f1": 0.051484622355963815,
        "hf_subset": "hun-eng",
        "languages": [
          "hun-Latn",
          "eng-Latn"
        ],
        "main_score": 0.051484622355963815,
        "precision": 0.0479734606813016,
        "recall": 0.068
      },
      {
        "accuracy": 0.129,
        "f1": 0.1060421950995864,
        "hf_subset": "swe-eng",
        "languages": [
          "swe-Latn",
          "eng-Latn"
        ],
        "main_score": 0.1060421950995864,
        "precision": 0.09988199426111907,
        "recall": 0.129
      },
      {
        "accuracy": 0.055,
        "f1": 0.04201769260549748,
        "hf_subset": "bre-eng",
        "languages": [
          "bre-Latn",
          "eng-Latn"
        ],
        "main_score": 0.04201769260549748,
        "precision": 0.03914887858305377,
        "recall": 0.055
      },
      {
        "accuracy": 0.08695652173913043,
        "f1": 0.06171012022991585,
        "hf_subset": "csb-eng",
        "languages": [
          "csb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.06171012022991585,
        "precision": 0.0554457619675011,
        "recall": 0.08695652173913043
      },
      {
        "accuracy": 0.152,
        "f1": 0.11926837553178618,
        "hf_subset": "epo-eng",
        "languages": [
          "epo-Latn",
          "eng-Latn"
        ],
        "main_score": 0.11926837553178618,
        "precision": 0.11126564241952366,
        "recall": 0.152
      },
      {
        "accuracy": 0.21,
        "f1": 0.17768824464654914,
        "hf_subset": "cat-eng",
        "languages": [
          "cat-Latn",
          "eng-Latn"
        ],
        "main_score": 0.17768824464654914,
        "precision": 0.16725446995464852,
        "recall": 0.21
      },
      {
        "accuracy": 0.06829268292682927,
        "f1": 0.054618494848050085,
        "hf_subset": "jav-eng",
        "languages": [
          "jav-Latn",
          "eng-Latn"
        ],
        "main_score": 0.054618494848050085,
        "precision": 0.05276887340301974,
        "recall": 0.06829268292682927
      },
      {
        "accuracy": 0.2677165354330709,
        "f1": 0.21064866891638545,
        "hf_subset": "ast-eng",
        "languages": [
          "ast-Latn",
          "eng-Latn"
        ],
        "main_score": 0.21064866891638545,
        "precision": 0.18766404199475065,
        "recall": 0.2677165354330709
      },
      {
        "accuracy": 0.088,
        "f1": 0.07670257224057028,
        "hf_subset": "ind-eng",
        "languages": [
          "ind-Latn",
          "eng-Latn"
        ],
        "main_score": 0.07670257224057028,
        "precision": 0.07403255534339777,
        "recall": 0.088
      },
      {
        "accuracy": 0.008695652173913044,
        "f1": 0.00608695652173913,
        "hf_subset": "kaz-eng",
        "languages": [
          "kaz-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.00608695652173913,
        "precision": 0.005743707093821509,
        "recall": 0.008695652173913044
      },
      {
        "accuracy": 0.2138728323699422,
        "f1": 0.1796636889122438,
        "hf_subset": "fry-eng",
        "languages": [
          "fry-Latn",
          "eng-Latn"
        ],
        "main_score": 0.1796636889122438,
        "precision": 0.16979416325955168,
        "recall": 0.2138728323699422
      },
      {
        "accuracy": 0.058,
        "f1": 0.05062579465342724,
        "hf_subset": "ber-eng",
        "languages": [
          "ber-Tfng",
          "eng-Latn"
        ],
        "main_score": 0.05062579465342724,
        "precision": 0.049964137033112366,
        "recall": 0.058
      },
      {
        "accuracy": 0.181,
        "f1": 0.15163910533910535,
        "hf_subset": "nds-eng",
        "languages": [
          "nds-Latn",
          "eng-Latn"
        ],
        "main_score": 0.15163910533910535,
        "precision": 0.14330667191245122,
        "recall": 0.181
      },
      {
        "accuracy": 0.042,
        "f1": 0.0276271487643649,
        "hf_subset": "est-eng",
        "languages": [
          "est-Latn",
          "eng-Latn"
        ],
        "main_score": 0.0276271487643649,
        "precision": 0.025289308756247623,
        "recall": 0.042
      },
      {
        "accuracy": 0.093,
        "f1": 0.08011596862567157,
        "hf_subset": "zsm-eng",
        "languages": [
          "zsm-Latn",
          "eng-Latn"
        ],
        "main_score": 0.08011596862567157,
        "precision": 0.07700278918114034,
        "recall": 0.093
      },
      {
        "accuracy": 0.004,
        "f1": 0.0008539568499155388,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.0008539568499155388,
        "precision": 0.0005437199554936211,
        "recall": 0.004
      },
      {
        "accuracy": 0.073,
        "f1": 0.05947222222222222,
        "hf_subset": "tgl-eng",
        "languages": [
          "tgl-Latn",
          "eng-Latn"
        ],
        "main_score": 0.05947222222222222,
        "precision": 0.05574948107448107,
        "recall": 0.073
      },
      {
        "accuracy": 0.044,
        "f1": 0.02950359831022165,
        "hf_subset": "cmn-eng",
        "languages": [
          "cmn-Hans",
          "eng-Latn"
        ],
        "main_score": 0.02950359831022165,
        "precision": 0.026617175483348927,
        "recall": 0.044
      },
      {
        "accuracy": 0.11016949152542373,
        "f1": 0.08901361517216483,
        "hf_subset": "bos-eng",
        "languages": [
          "bos-Latn",
          "eng-Latn"
        ],
        "main_score": 0.08901361517216483,
        "precision": 0.08290139669771958,
        "recall": 0.11016949152542373
      },
      {
        "accuracy": 0.069,
        "f1": 0.055842606361007845,
        "hf_subset": "sqi-eng",
        "languages": [
          "sqi-Latn",
          "eng-Latn"
        ],
        "main_score": 0.055842606361007845,
        "precision": 0.052258608229138956,
        "recall": 0.069
      },
      {
        "accuracy": 0.17518248175182483,
        "f1": 0.144566098945661,
        "hf_subset": "cha-eng",
        "languages": [
          "cha-Latn",
          "eng-Latn"
        ],
        "main_score": 0.144566098945661,
        "precision": 0.13601842196732708,
        "recall": 0.17518248175182483
      },
      {
        "accuracy": 0.289,
        "f1": 0.2537473349631735,
        "hf_subset": "ita-eng",
        "languages": [
          "ita-Latn",
          "eng-Latn"
        ],
        "main_score": 0.2537473349631735,
        "precision": 0.24367490590560123,
        "recall": 0.289
      },
      {
        "accuracy": 0.013,
        "f1": 0.00974178266178266,
        "hf_subset": "kab-eng",
        "languages": [
          "kab-Latn",
          "eng-Latn"
        ],
        "main_score": 0.00974178266178266,
        "precision": 0.009416666666666667,
        "recall": 0.013
      },
      {
        "accuracy": 0.18803418803418803,
        "f1": 0.15410123898495992,
        "hf_subset": "gsw-eng",
        "languages": [
          "gsw-Latn",
          "eng-Latn"
        ],
        "main_score": 0.15410123898495992,
        "precision": 0.146011396011396,
        "recall": 0.18803418803418803
      },
      {
        "accuracy": 0.071,
        "f1": 0.05944671309618859,
        "hf_subset": "slk-eng",
        "languages": [
          "slk-Latn",
          "eng-Latn"
        ],
        "main_score": 0.05944671309618859,
        "precision": 0.05659774834574457,
        "recall": 0.071
      },
      {
        "accuracy": 0.058,
        "f1": 0.0467012390397691,
        "hf_subset": "lvs-eng",
        "languages": [
          "lvs-Latn",
          "eng-Latn"
        ],
        "main_score": 0.0467012390397691,
        "precision": 0.04482715832778332,
        "recall": 0.058
      },
      {
        "accuracy": 0.081,
        "f1": 0.06707433025911286,
        "hf_subset": "nno-eng",
        "languages": [
          "nno-Latn",
          "eng-Latn"
        ],
        "main_score": 0.06707433025911286,
        "precision": 0.06279171058410188,
        "recall": 0.081
      },
      {
        "accuracy": 0.149,
        "f1": 0.11538756586264098,
        "hf_subset": "lat-eng",
        "languages": [
          "lat-Latn",
          "eng-Latn"
        ],
        "main_score": 0.11538756586264098,
        "precision": 0.10727214149232746,
        "recall": 0.149
      },
      {
        "accuracy": 0.36186770428015563,
        "f1": 0.3036594404298685,
        "hf_subset": "nov-eng",
        "languages": [
          "nov-Latn",
          "eng-Latn"
        ],
        "main_score": 0.3036594404298685,
        "precision": 0.28699210242942264,
        "recall": 0.36186770428015563
      },
      {
        "accuracy": 0.092,
        "f1": 0.07219333015125648,
        "hf_subset": "eus-eng",
        "languages": [
          "eus-Latn",
          "eng-Latn"
        ],
        "main_score": 0.07219333015125648,
        "precision": 0.06777762741254235,
        "recall": 0.092
      },
      {
        "accuracy": 0.039,
        "f1": 0.02776980667219435,
        "hf_subset": "lit-eng",
        "languages": [
          "lit-Latn",
          "eng-Latn"
        ],
        "main_score": 0.02776980667219435,
        "precision": 0.025507564026414586,
        "recall": 0.039
      },
      {
        "accuracy": 0.09,
        "f1": 0.07316406416406415,
        "hf_subset": "hrv-eng",
        "languages": [
          "hrv-Latn",
          "eng-Latn"
        ],
        "main_score": 0.07316406416406415,
        "precision": 0.06886234749083046,
        "recall": 0.09
      },
      {
        "accuracy": 0.091,
        "f1": 0.06979396122710253,
        "hf_subset": "pol-eng",
        "languages": [
          "pol-Latn",
          "eng-Latn"
        ],
        "main_score": 0.06979396122710253,
        "precision": 0.06444571303225964,
        "recall": 0.091
      },
      {
        "accuracy": 0.08,
        "f1": 0.058267623832285814,
        "hf_subset": "ceb-eng",
        "languages": [
          "ceb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.058267623832285814,
        "precision": 0.054245563660330426,
        "recall": 0.08
      },
      {
        "accuracy": 0.316,
        "f1": 0.27217047830171487,
        "hf_subset": "ile-eng",
        "languages": [
          "ile-Latn",
          "eng-Latn"
        ],
        "main_score": 0.27217047830171487,
        "precision": 0.2599027904535767,
        "recall": 0.316
      },
      {
        "accuracy": 0.194,
        "f1": 0.15995002121931867,
        "hf_subset": "lfn-eng",
        "languages": [
          "lfn-Latn",
          "eng-Latn"
        ],
        "main_score": 0.15995002121931867,
        "precision": 0.15098908436712058,
        "recall": 0.194
      },
      {
        "accuracy": 0.07389162561576355,
        "f1": 0.06188334404226458,
        "hf_subset": "tuk-eng",
        "languages": [
          "tuk-Latn",
          "eng-Latn"
        ],
        "main_score": 0.06188334404226458,
        "precision": 0.05973308863775396,
        "recall": 0.07389162561576355
      },
      {
        "accuracy": 0.006,
        "f1": 0.004044676333739892,
        "hf_subset": "ell-eng",
        "languages": [
          "ell-Grek",
          "eng-Latn"
        ],
        "main_score": 0.004044676333739892,
        "precision": 0.004022801829690894,
        "recall": 0.006
      },
      {
        "accuracy": 0.127,
        "f1": 0.09777860565863283,
        "hf_subset": "afr-eng",
        "languages": [
          "afr-Latn",
          "eng-Latn"
        ],
        "main_score": 0.09777860565863283,
        "precision": 0.0914327170861708,
        "recall": 0.127
      },
      {
        "accuracy": 0.23134328358208955,
        "f1": 0.16002687196717044,
        "hf_subset": "ang-eng",
        "languages": [
          "ang-Latn",
          "eng-Latn"
        ],
        "main_score": 0.16002687196717044,
        "precision": 0.14402419719583898,
        "recall": 0.23134328358208955
      },
      {
        "accuracy": 0.013029315960912053,
        "f1": 0.0037672868640760684,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.0037672868640760684,
        "precision": 0.003519776767229813,
        "recall": 0.013029315960912053
      },
      {
        "accuracy": 0.08,
        "f1": 0.06955932329845373,
        "hf_subset": "cym-eng",
        "languages": [
          "cym-Latn",
          "eng-Latn"
        ],
        "main_score": 0.06955932329845373,
        "precision": 0.06704030298192395,
        "recall": 0.08
      },
      {
        "accuracy": 0.009,
        "f1": 0.004043882052488159,
        "hf_subset": "rus-eng",
        "languages": [
          "rus-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.004043882052488159,
        "precision": 0.003637515848616688,
        "recall": 0.009
      },
      {
        "accuracy": 0.012773722627737226,
        "f1": 0.00904548614402629,
        "hf_subset": "tha-eng",
        "languages": [
          "tha-Thai",
          "eng-Latn"
        ],
        "main_score": 0.00904548614402629,
        "precision": 0.008519971844880812,
        "recall": 0.012773722627737226
      },
      {
        "accuracy": 0.061,
        "f1": 0.049619979507863975,
        "hf_subset": "vie-eng",
        "languages": [
          "vie-Latn",
          "eng-Latn"
        ],
        "main_score": 0.049619979507863975,
        "precision": 0.04704698413917164,
        "recall": 0.061
      },
      {
        "accuracy": 0.07306889352818371,
        "f1": 0.05919195391259366,
        "hf_subset": "dsb-eng",
        "languages": [
          "dsb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.05919195391259366,
        "precision": 0.05639809581241787,
        "recall": 0.07306889352818371
      },
      {
        "accuracy": 0.07,
        "f1": 0.053258705448293545,
        "hf_subset": "pam-eng",
        "languages": [
          "pam-Latn",
          "eng-Latn"
        ],
        "main_score": 0.053258705448293545,
        "precision": 0.04928330531440467,
        "recall": 0.07
      },
      {
        "accuracy": 0.03271028037383177,
        "f1": 0.021507009345794394,
        "hf_subset": "uzb-eng",
        "languages": [
          "uzb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.021507009345794394,
        "precision": 0.019204309449636553,
        "recall": 0.03271028037383177
      },
      {
        "accuracy": 0.0011792452830188679,
        "f1": 2.8245396000451925e-06,
        "hf_subset": "yid-eng",
        "languages": [
          "yid-Hebr",
          "eng-Latn"
        ],
        "main_score": 2.8245396000451925e-06,
        "precision": 1.4139631690873715e-06,
        "recall": 0.0011792452830188679
      },
      {
        "accuracy": 0.15178571428571427,
        "f1": 0.0977384222919937,
        "hf_subset": "swg-eng",
        "languages": [
          "swg-Latn",
          "eng-Latn"
        ],
        "main_score": 0.0977384222919937,
        "precision": 0.08589089481946624,
        "recall": 0.15178571428571427
      },
      {
        "accuracy": 0.008658008658008658,
        "f1": 0.0019522960699431286,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.0019522960699431286,
        "precision": 0.0011363636363636363,
        "recall": 0.008658008658008658
      },
      {
        "accuracy": 0.044,
        "f1": 0.03447877690914603,
        "hf_subset": "dtp-eng",
        "languages": [
          "dtp-Latn",
          "eng-Latn"
        ],
        "main_score": 0.03447877690914603,
        "precision": 0.03273487030987031,
        "recall": 0.044
      },
      {
        "accuracy": 0.022727272727272728,
        "f1": 0.013422706325932132,
        "hf_subset": "mon-eng",
        "languages": [
          "mon-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.013422706325932132,
        "precision": 0.012784457538462114,
        "recall": 0.022727272727272728
      },
      {
        "accuracy": 0.043,
        "f1": 0.03368968253968254,
        "hf_subset": "cor-eng",
        "languages": [
          "cor-Latn",
          "eng-Latn"
        ],
        "main_score": 0.03368968253968254,
        "precision": 0.031503968253968256,
        "recall": 0.043
      },
      {
        "accuracy": 0.022,
        "f1": 0.014201117847669573,
        "hf_subset": "bel-eng",
        "languages": [
          "bel-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.014201117847669573,
        "precision": 0.013408838238374526,
        "recall": 0.022
      },
      {
        "accuracy": 0.013,
        "f1": 0.010349041665083165,
        "hf_subset": "ukr-eng",
        "languages": [
          "ukr-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.010349041665083165,
        "precision": 0.010007890704800817,
        "recall": 0.013
      },
      {
        "accuracy": 0.15492957746478872,
        "f1": 0.11792801328146309,
        "hf_subset": "max-eng",
        "languages": [
          "max-Deva",
          "eng-Latn"
        ],
        "main_score": 0.11792801328146309,
        "precision": 0.10926664107560631,
        "recall": 0.15492957746478872
      },
      {
        "accuracy": 0.277,
        "f1": 0.23028243931310435,
        "hf_subset": "por-eng",
        "languages": [
          "por-Latn",
          "eng-Latn"
        ],
        "main_score": 0.23028243931310435,
        "precision": 0.2170504512436275,
        "recall": 0.277
      },
      {
        "accuracy": 0.008,
        "f1": 0.00530583571679462,
        "hf_subset": "uig-eng",
        "languages": [
          "uig-Arab",
          "eng-Latn"
        ],
        "main_score": 0.00530583571679462,
        "precision": 0.005164843566219712,
        "recall": 0.008
      },
      {
        "accuracy": 0.24,
        "f1": 0.1999987715987716,
        "hf_subset": "ido-eng",
        "languages": [
          "ido-Latn",
          "eng-Latn"
        ],
        "main_score": 0.1999987715987716,
        "precision": 0.18969225194763115,
        "recall": 0.24
      },
      {
        "accuracy": 0.06418219461697723,
        "f1": 0.051614140019937124,
        "hf_subset": "hsb-eng",
        "languages": [
          "hsb-Latn",
          "eng-Latn"
        ],
        "main_score": 0.051614140019937124,
        "precision": 0.049085276332881345,
        "recall": 0.06418219461697723
      },
      {
        "accuracy": 0.00804289544235925,
        "f1": 0.005923656325801098,
        "hf_subset": "kat-eng",
        "languages": [
          "kat-Geor",
          "eng-Latn"
        ],
        "main_score": 0.005923656325801098,
        "precision": 0.005709940193854403,
        "recall": 0.00804289544235925
      },
      {
        "accuracy": 0.006925207756232687,
        "f1": 0.00421733891854154,
        "hf_subset": "khm-eng",
        "languages": [
          "khm-Khmr",
          "eng-Latn"
        ],
        "main_score": 0.00421733891854154,
        "precision": 0.004186849390913843,
        "recall": 0.006925207756232687
      },
      {
        "accuracy": 0.004790419161676647,
        "f1": 0.0024213861810181143,
        "hf_subset": "orv-eng",
        "languages": [
          "orv-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.0024213861810181143,
        "precision": 0.002408408162289621,
        "recall": 0.004790419161676647
      },
      {
        "accuracy": 0.004366812227074236,
        "f1": 0.0015796903325059912,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.0015796903325059912,
        "precision": 0.0015197663988204197,
        "recall": 0.004366812227074236
      },
      {
        "accuracy": 0.10512820512820513,
        "f1": 0.07792343808067077,
        "hf_subset": "swh-eng",
        "languages": [
          "swh-Latn",
          "eng-Latn"
        ],
        "main_score": 0.07792343808067077,
        "precision": 0.07291753683302903,
        "recall": 0.10512820512820513
      },
      {
        "accuracy": 0.022919179734620022,
        "f1": 0.01930400620993517,
        "hf_subset": "gla-eng",
        "languages": [
          "gla-Latn",
          "eng-Latn"
        ],
        "main_score": 0.01930400620993517,
        "precision": 0.01809591418492652,
        "recall": 0.022919179734620022
      },
      {
        "accuracy": 0.043,
        "f1": 0.031399278499278496,
        "hf_subset": "gle-eng",
        "languages": [
          "gle-Latn",
          "eng-Latn"
        ],
        "main_score": 0.031399278499278496,
        "precision": 0.027809498834498833,
        "recall": 0.043
      },
      {
        "accuracy": 0.005,
        "f1": 0.003023273322422259,
        "hf_subset": "pes-eng",
        "languages": [
          "pes-Arab",
          "eng-Latn"
        ],
        "main_score": 0.003023273322422259,
        "precision": 0.0030117403543633054,
        "recall": 0.005
      },
      {
        "accuracy": 0.043,
        "f1": 0.02440094196440104,
        "hf_subset": "wuu-eng",
        "languages": [
          "wuu-Hans",
          "eng-Latn"
        ],
        "main_score": 0.02440094196440104,
        "precision": 0.021426464711087254,
        "recall": 0.043
      },
      {
        "accuracy": 0.147,
        "f1": 0.1275182030205495,
        "hf_subset": "dan-eng",
        "languages": [
          "dan-Latn",
          "eng-Latn"
        ],
        "main_score": 0.1275182030205495,
        "precision": 0.1219521340388007,
        "recall": 0.147
      },
      {
        "accuracy": 0.22115384615384615,
        "f1": 0.18682983682983684,
        "hf_subset": "tzl-eng",
        "languages": [
          "tzl-Latn",
          "eng-Latn"
        ],
        "main_score": 0.18682983682983684,
        "precision": 0.17900641025641026,
        "recall": 0.22115384615384615
      },
      {
        "accuracy": 0.044,
        "f1": 0.03733834586466165,
        "hf_subset": "fin-eng",
        "languages": [
          "fin-Latn",
          "eng-Latn"
        ],
        "main_score": 0.03733834586466165,
        "precision": 0.0350358458961474,
        "recall": 0.044
      },
      {
        "accuracy": 0.082,
        "f1": 0.06911322184955421,
        "hf_subset": "war-eng",
        "languages": [
          "war-Latn",
          "eng-Latn"
        ],
        "main_score": 0.06911322184955421,
        "precision": 0.06675940291120741,
        "recall": 0.082
      },
      {
        "accuracy": 0.151,
        "f1": 0.11347062195504276,
        "hf_subset": "ron-eng",
        "languages": [
          "ron-Latn",
          "eng-Latn"
        ],
        "main_score": 0.11347062195504276,
        "precision": 0.10487730592496922,
        "recall": 0.151
      },
      {
        "accuracy": 0.002,
        "f1": 0.0006696252465483234,
        "hf_subset": "mhr-eng",
        "languages": [
          "mhr-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.0006696252465483234,
        "precision": 0.0005014814814814815,
        "recall": 0.002
      },
      {
        "accuracy": 0.068,
        "f1": 0.04660528433616201,
        "hf_subset": "tur-eng",
        "languages": [
          "tur-Latn",
          "eng-Latn"
        ],
        "main_score": 0.04660528433616201,
        "precision": 0.04221241161616162,
        "recall": 0.068
      },
      {
        "accuracy": 0.05,
        "f1": 0.03561470115052696,
        "hf_subset": "kzj-eng",
        "languages": [
          "kzj-Latn",
          "eng-Latn"
        ],
        "main_score": 0.03561470115052696,
        "precision": 0.03375432584688781,
        "recall": 0.05
      },
      {
        "accuracy": 0.003,
        "f1": 4.1337931668735914e-05,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 4.1337931668735914e-05,
        "precision": 2.0920436480549274e-05,
        "recall": 0.003
      },
      {
        "accuracy": 0.16,
        "f1": 0.1349577406720264,
        "hf_subset": "pms-eng",
        "languages": [
          "pms-Latn",
          "eng-Latn"
        ],
        "main_score": 0.1349577406720264,
        "precision": 0.12681183478175959,
        "recall": 0.16
      },
      {
        "accuracy": 0.019,
        "f1": 0.009348552148334755,
        "hf_subset": "bul-eng",
        "languages": [
          "bul-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.009348552148334755,
        "precision": 0.008601164139442782,
        "recall": 0.019
      },
      {
        "accuracy": 0.0,
        "f1": 0.0,
        "hf_subset": "arz-eng",
        "languages": [
          "arz-Arab",
          "eng-Latn"
        ],
        "main_score": 0.0,
        "precision": 0.0,
        "recall": 0.0
      },
      {
        "accuracy": 0.05,
        "f1": 0.03793085644811777,
        "hf_subset": "aze-eng",
        "languages": [
          "aze-Latn",
          "eng-Latn"
        ],
        "main_score": 0.03793085644811777,
        "precision": 0.035433545498619025,
        "recall": 0.05
      },
      {
        "accuracy": 0.018,
        "f1": 0.013926322086623595,
        "hf_subset": "kor-eng",
        "languages": [
          "kor-Hang",
          "eng-Latn"
        ],
        "main_score": 0.013926322086623595,
        "precision": 0.013096639387739637,
        "recall": 0.018
      },
      {
        "accuracy": 0.01,
        "f1": 0.005814195023497475,
        "hf_subset": "ara-eng",
        "languages": [
          "ara-Arab",
          "eng-Latn"
        ],
        "main_score": 0.005814195023497475,
        "precision": 0.005577526508167922,
        "recall": 0.01
      },
      {
        "accuracy": 0.271,
        "f1": 0.2389561063964898,
        "hf_subset": "deu-eng",
        "languages": [
          "deu-Latn",
          "eng-Latn"
        ],
        "main_score": 0.2389561063964898,
        "precision": 0.22941510683760685,
        "recall": 0.271
      },
      {
        "accuracy": 0.388,
        "f1": 0.3493948491896722,
        "hf_subset": "fra-eng",
        "languages": [
          "fra-Latn",
          "eng-Latn"
        ],
        "main_score": 0.3493948491896722,
        "precision": 0.33855180171207544,
        "recall": 0.388
      },
      {
        "accuracy": 0.017857142857142856,
        "f1": 0.004509379509379509,
        "hf_subset": "amh-eng",
        "languages": [
          "amh-Ethi",
          "eng-Latn"
        ],
        "main_score": 0.004509379509379509,
        "precision": 0.002808699237270666,
        "recall": 0.017857142857142856
      },
      {
        "accuracy": 0.01,
        "f1": 0.0021426681985333013,
        "hf_subset": "mkd-eng",
        "languages": [
          "mkd-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.0021426681985333013,
        "precision": 0.0016537741906449263,
        "recall": 0.01
      },
      {
        "accuracy": 0.266,
        "f1": 0.22823655628983286,
        "hf_subset": "glg-eng",
        "languages": [
          "glg-Latn",
          "eng-Latn"
        ],
        "main_score": 0.22823655628983286,
        "precision": 0.21752530102566692,
        "recall": 0.266
      },
      {
        "accuracy": 0.003,
        "f1": 0.0007137601177336277,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.0007137601177336277,
        "precision": 0.0005240535326742222,
        "recall": 0.003
      },
      {
        "accuracy": 0.02,
        "f1": 0.00920499333999334,
        "hf_subset": "jpn-eng",
        "languages": [
          "jpn-Jpan",
          "eng-Latn"
        ],
        "main_score": 0.00920499333999334,
        "precision": 0.007901954933868164,
        "recall": 0.02
      },
      {
        "accuracy": 0.06338028169014084,
        "f1": 0.04220657276995305,
        "hf_subset": "xho-eng",
        "languages": [
          "xho-Latn",
          "eng-Latn"
        ],
        "main_score": 0.04220657276995305,
        "precision": 0.039565319847009985,
        "recall": 0.06338028169014084
      },
      {
        "accuracy": 0.1183206106870229,
        "f1": 0.09223633473745733,
        "hf_subset": "fao-eng",
        "languages": [
          "fao-Latn",
          "eng-Latn"
        ],
        "main_score": 0.09223633473745733,
        "precision": 0.08884740691308995,
        "recall": 0.1183206106870229
      },
      {
        "accuracy": 0.165,
        "f1": 0.13591411663639927,
        "hf_subset": "oci-eng",
        "languages": [
          "oci-Latn",
          "eng-Latn"
        ],
        "main_score": 0.13591411663639927,
        "precision": 0.12990468244842177,
        "recall": 0.165
      },
      {
        "accuracy": 0.009879253567508232,
        "f1": 0.0037222742114677793,
        "hf_subset": "arq-eng",
        "languages": [
          "arq-Arab",
          "eng-Latn"
        ],
        "main_score": 0.0037222742114677793,
        "precision": 0.003519780924107178,
        "recall": 0.009879253567508232
      }
    ]
  },
  "task_name": "Tatoeba"
}