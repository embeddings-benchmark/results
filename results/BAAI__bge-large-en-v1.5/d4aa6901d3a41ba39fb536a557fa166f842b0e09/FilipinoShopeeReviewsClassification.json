{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 25.50888752937317,
  "kg_co2_emissions": 0.001082854150908791,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.295361328125,
        "f1": 0.28683893061658605,
        "f1_weighted": 0.28683148684894244,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.295361328125,
        "scores_per_experiment": [
          {
            "accuracy": 0.2890625,
            "f1": 0.28246222830226203,
            "f1_weighted": 0.2824374748539989
          },
          {
            "accuracy": 0.28466796875,
            "f1": 0.27802033051786745,
            "f1_weighted": 0.27804544311159873
          },
          {
            "accuracy": 0.32275390625,
            "f1": 0.3048656077201028,
            "f1_weighted": 0.3048597264922422
          },
          {
            "accuracy": 0.298828125,
            "f1": 0.2962707969726194,
            "f1_weighted": 0.29625841360047483
          },
          {
            "accuracy": 0.27197265625,
            "f1": 0.26601579413461296,
            "f1_weighted": 0.26595857659678795
          },
          {
            "accuracy": 0.3095703125,
            "f1": 0.2964872461107046,
            "f1_weighted": 0.29646114790518807
          },
          {
            "accuracy": 0.259765625,
            "f1": 0.2589875424557572,
            "f1_weighted": 0.25897293957996337
          },
          {
            "accuracy": 0.314453125,
            "f1": 0.3078796328617546,
            "f1_weighted": 0.3079097340891052
          },
          {
            "accuracy": 0.3193359375,
            "f1": 0.30775213696851605,
            "f1_weighted": 0.30774894211905385
          },
          {
            "accuracy": 0.283203125,
            "f1": 0.2696479901216636,
            "f1_weighted": 0.26966247014101064
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.293310546875,
        "f1": 0.285300050291328,
        "f1_weighted": 0.28528608215186624,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.293310546875,
        "scores_per_experiment": [
          {
            "accuracy": 0.31884765625,
            "f1": 0.31297858198175543,
            "f1_weighted": 0.31294518457935827
          },
          {
            "accuracy": 0.29736328125,
            "f1": 0.2894505659990652,
            "f1_weighted": 0.289468393281814
          },
          {
            "accuracy": 0.306640625,
            "f1": 0.28731638468629195,
            "f1_weighted": 0.2872973851151292
          },
          {
            "accuracy": 0.31494140625,
            "f1": 0.31246505793708074,
            "f1_weighted": 0.31245056180378283
          },
          {
            "accuracy": 0.251953125,
            "f1": 0.24808501358849577,
            "f1_weighted": 0.24804207640991874
          },
          {
            "accuracy": 0.29931640625,
            "f1": 0.2916213438448967,
            "f1_weighted": 0.2916010054282646
          },
          {
            "accuracy": 0.2236328125,
            "f1": 0.2226899368167961,
            "f1_weighted": 0.2226604502272332
          },
          {
            "accuracy": 0.31201171875,
            "f1": 0.30576371165385563,
            "f1_weighted": 0.305785201934475
          },
          {
            "accuracy": 0.3125,
            "f1": 0.3010058892984623,
            "f1_weighted": 0.30099402958877985
          },
          {
            "accuracy": 0.2958984375,
            "f1": 0.2816240171065803,
            "f1_weighted": 0.2816165331499064
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}