{
  "dataset_revision": "ef5c383d1b87eb8feccde3dfb7f95e42b1b050dd",
  "evaluation_time": 37.07868218421936,
  "kg_co2_emissions": 0.0019328169697810254,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "cosine_pearson": 0.7450169603421538,
        "cosine_spearman": 0.7484679201507705,
        "euclidean_pearson": 0.7328414496136115,
        "euclidean_spearman": 0.7484679201507705,
        "hf_subset": "afr",
        "languages": [
          "afr-Latn"
        ],
        "main_score": 0.7484679201507705,
        "manhattan_pearson": 0.7348954856967658,
        "manhattan_spearman": 0.7504745857129784,
        "pearson": 0.7450169603421538,
        "spearman": 0.7484679201507705
      },
      {
        "cosine_pearson": 0.19944412145139587,
        "cosine_spearman": 0.1769587794084566,
        "euclidean_pearson": 0.1741484001891153,
        "euclidean_spearman": 0.17584919276058433,
        "hf_subset": "amh",
        "languages": [
          "amh-Ethi"
        ],
        "main_score": 0.1769587794084566,
        "manhattan_pearson": 0.17374243620336527,
        "manhattan_spearman": 0.17162606554107326,
        "pearson": 0.19944412145139587,
        "spearman": 0.1769587794084566
      },
      {
        "cosine_pearson": 0.11361314818282592,
        "cosine_spearman": 0.10103931976978332,
        "euclidean_pearson": 0.11792549423614372,
        "euclidean_spearman": 0.10103931976978332,
        "hf_subset": "arb",
        "languages": [
          "arb-Arab"
        ],
        "main_score": 0.10103931976978332,
        "manhattan_pearson": 0.11663735906637598,
        "manhattan_spearman": 0.10134904769922552,
        "pearson": 0.11361314818282592,
        "spearman": 0.10103931976978332
      },
      {
        "cosine_pearson": 0.30942954767856345,
        "cosine_spearman": 0.2789899900095492,
        "euclidean_pearson": 0.3152522022367837,
        "euclidean_spearman": 0.2789899900095492,
        "hf_subset": "arq",
        "languages": [
          "arq-Arab"
        ],
        "main_score": 0.2789899900095492,
        "manhattan_pearson": 0.31621739870211607,
        "manhattan_spearman": 0.2804703573234083,
        "pearson": 0.30942954767856345,
        "spearman": 0.2789899900095492
      },
      {
        "cosine_pearson": -0.025981071593059894,
        "cosine_spearman": -0.03850001269585105,
        "euclidean_pearson": -0.02420487513851667,
        "euclidean_spearman": -0.03850001269585105,
        "hf_subset": "ary",
        "languages": [
          "ary-Arab"
        ],
        "main_score": -0.03850001269585105,
        "manhattan_pearson": -0.023778414863411933,
        "manhattan_spearman": -0.03746147386531269,
        "pearson": -0.025981071593059894,
        "spearman": -0.03850001269585105
      },
      {
        "cosine_pearson": 0.8149137488676803,
        "cosine_spearman": 0.7957630322030566,
        "euclidean_pearson": 0.8116286530847296,
        "euclidean_spearman": 0.7957630322030566,
        "hf_subset": "eng",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7957630322030566,
        "manhattan_pearson": 0.8105238066543532,
        "manhattan_spearman": 0.7935634031801546,
        "pearson": 0.8149137488676803,
        "spearman": 0.7957630322030566
      },
      {
        "cosine_pearson": 0.2874202308287371,
        "cosine_spearman": 0.27235095523182823,
        "euclidean_pearson": 0.3080034584061853,
        "euclidean_spearman": 0.27235095523182823,
        "hf_subset": "hau",
        "languages": [
          "hau-Latn"
        ],
        "main_score": 0.27235095523182823,
        "manhattan_pearson": 0.30770561759692966,
        "manhattan_spearman": 0.2702995764626908,
        "pearson": 0.2874202308287371,
        "spearman": 0.27235095523182823
      },
      {
        "cosine_pearson": 0.41578208359099106,
        "cosine_spearman": 0.3900048928227525,
        "euclidean_pearson": 0.43481133085283646,
        "euclidean_spearman": 0.3900048928227525,
        "hf_subset": "hin",
        "languages": [
          "hin-Deva"
        ],
        "main_score": 0.3900048928227525,
        "manhattan_pearson": 0.4331482622383433,
        "manhattan_spearman": 0.38803541165405475,
        "pearson": 0.41578208359099106,
        "spearman": 0.3900048928227525
      },
      {
        "cosine_pearson": 0.41240734788228656,
        "cosine_spearman": 0.43790819550255494,
        "euclidean_pearson": 0.44909239681830443,
        "euclidean_spearman": 0.43790819550255494,
        "hf_subset": "ind",
        "languages": [
          "ind-Latn"
        ],
        "main_score": 0.43790819550255494,
        "manhattan_pearson": 0.44874849801901673,
        "manhattan_spearman": 0.4357499593657328,
        "pearson": 0.41240734788228656,
        "spearman": 0.43790819550255494
      },
      {
        "cosine_pearson": 0.4390688838445408,
        "cosine_spearman": 0.41958412424038305,
        "euclidean_pearson": 0.4338896174948727,
        "euclidean_spearman": 0.41958412424038305,
        "hf_subset": "kin",
        "languages": [
          "kin-Latn"
        ],
        "main_score": 0.41958412424038305,
        "manhattan_pearson": 0.4377635057440497,
        "manhattan_spearman": 0.4237626837859011,
        "pearson": 0.4390688838445408,
        "spearman": 0.41958412424038305
      },
      {
        "cosine_pearson": 0.46765777009461645,
        "cosine_spearman": 0.4530530488093528,
        "euclidean_pearson": 0.4863623789967971,
        "euclidean_spearman": 0.4530530488093528,
        "hf_subset": "mar",
        "languages": [
          "mar-Deva"
        ],
        "main_score": 0.4530530488093528,
        "manhattan_pearson": 0.4855780064636309,
        "manhattan_spearman": 0.45641619813933587,
        "pearson": 0.46765777009461645,
        "spearman": 0.4530530488093528
      },
      {
        "cosine_pearson": 0.1929147509629563,
        "cosine_spearman": 0.2617247322536357,
        "euclidean_pearson": 0.24937834868616252,
        "euclidean_spearman": 0.2612149055896373,
        "hf_subset": "tel",
        "languages": [
          "tel-Telu"
        ],
        "main_score": 0.2617247322536357,
        "manhattan_pearson": 0.2496139450097057,
        "manhattan_spearman": 0.26181285970800433,
        "pearson": 0.1929147509629563,
        "spearman": 0.2617247322536357
      }
    ]
  },
  "task_name": "SemRel24STS"
}