{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.778093,
        "f1": 0.659398,
        "f1_weighted": 0.808741,
        "ap": 0.283084,
        "ap_weighted": 0.283084,
        "scores_per_experiment": [
          {
            "accuracy": 0.658935,
            "f1": 0.549851,
            "f1_weighted": 0.715856,
            "ap": 0.186917,
            "ap_weighted": 0.186917
          },
          {
            "accuracy": 0.812715,
            "f1": 0.676072,
            "f1_weighted": 0.833681,
            "ap": 0.27975,
            "ap_weighted": 0.27975
          },
          {
            "accuracy": 0.843643,
            "f1": 0.727455,
            "f1_weighted": 0.860765,
            "ap": 0.357163,
            "ap_weighted": 0.357163
          },
          {
            "accuracy": 0.845361,
            "f1": 0.72832,
            "f1_weighted": 0.861906,
            "ap": 0.357214,
            "ap_weighted": 0.357214
          },
          {
            "accuracy": 0.793814,
            "f1": 0.6406,
            "f1_weighted": 0.816393,
            "ap": 0.23349,
            "ap_weighted": 0.23349
          },
          {
            "accuracy": 0.781787,
            "f1": 0.662505,
            "f1_weighted": 0.812814,
            "ap": 0.281409,
            "ap_weighted": 0.281409
          },
          {
            "accuracy": 0.82732,
            "f1": 0.692337,
            "f1_weighted": 0.845002,
            "ap": 0.298515,
            "ap_weighted": 0.298515
          },
          {
            "accuracy": 0.661512,
            "f1": 0.578726,
            "f1_weighted": 0.718628,
            "ap": 0.236388,
            "ap_weighted": 0.236388
          },
          {
            "accuracy": 0.786942,
            "f1": 0.672541,
            "f1_weighted": 0.817537,
            "ap": 0.29723,
            "ap_weighted": 0.29723
          },
          {
            "accuracy": 0.7689,
            "f1": 0.665577,
            "f1_weighted": 0.804832,
            "ap": 0.302768,
            "ap_weighted": 0.302768
          }
        ],
        "main_score": 0.778093,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 9.259387016296387,
  "kg_co2_emissions": 0.0003899670778187387
}