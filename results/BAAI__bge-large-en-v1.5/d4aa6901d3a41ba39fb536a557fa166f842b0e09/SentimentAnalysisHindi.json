{
  "dataset_revision": "1beac1b941da76a9c51e3e5b39d230fde9a80983",
  "evaluation_time": 12.492912292480469,
  "kg_co2_emissions": 0.0005193090844603111,
  "mteb_version": "1.18.0",
  "scores": {
    "train": [
      {
        "accuracy": 0.39580078125,
        "f1": 0.36129668399806836,
        "f1_weighted": 0.3982934831715134,
        "hf_subset": "default",
        "languages": [
          "hin-Deva"
        ],
        "main_score": 0.36129668399806836,
        "scores_per_experiment": [
          {
            "accuracy": 0.4296875,
            "f1": 0.38349865485652956,
            "f1_weighted": 0.4227306016403356
          },
          {
            "accuracy": 0.3330078125,
            "f1": 0.31426451255693516,
            "f1_weighted": 0.3352002504246201
          },
          {
            "accuracy": 0.42919921875,
            "f1": 0.3848306849018668,
            "f1_weighted": 0.42603332909702246
          },
          {
            "accuracy": 0.4130859375,
            "f1": 0.3966342258841873,
            "f1_weighted": 0.4291593621609011
          },
          {
            "accuracy": 0.32470703125,
            "f1": 0.29168404071589954,
            "f1_weighted": 0.30734533445784096
          },
          {
            "accuracy": 0.30322265625,
            "f1": 0.29956620038230836,
            "f1_weighted": 0.32430863535601034
          },
          {
            "accuracy": 0.42431640625,
            "f1": 0.3946253859527113,
            "f1_weighted": 0.432586377202792
          },
          {
            "accuracy": 0.47021484375,
            "f1": 0.41216021047619283,
            "f1_weighted": 0.47481337564592396
          },
          {
            "accuracy": 0.3828125,
            "f1": 0.33582253671325163,
            "f1_weighted": 0.37669113337053794
          },
          {
            "accuracy": 0.44775390625,
            "f1": 0.3998803875408013,
            "f1_weighted": 0.45406643235915056
          }
        ]
      }
    ]
  },
  "task_name": "SentimentAnalysisHindi"
}