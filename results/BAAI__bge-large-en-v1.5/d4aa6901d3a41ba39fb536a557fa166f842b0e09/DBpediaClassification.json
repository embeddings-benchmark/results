{
  "dataset_revision": "9abd46cf7fc8b4c64290f26993c540b92aa145ac",
  "evaluation_time": 16.268483638763428,
  "kg_co2_emissions": 0.0007497243038692673,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.8166015625,
        "f1": 0.8084728047688327,
        "f1_weighted": 0.8085250901146486,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8166015625,
        "scores_per_experiment": [
          {
            "accuracy": 0.80517578125,
            "f1": 0.7938686542129878,
            "f1_weighted": 0.7939676636186357
          },
          {
            "accuracy": 0.818359375,
            "f1": 0.8093380518491134,
            "f1_weighted": 0.8094040462158544
          },
          {
            "accuracy": 0.8232421875,
            "f1": 0.8157780463147357,
            "f1_weighted": 0.8158150890012259
          },
          {
            "accuracy": 0.8095703125,
            "f1": 0.7993791119947357,
            "f1_weighted": 0.7994312302538075
          },
          {
            "accuracy": 0.80078125,
            "f1": 0.7934070903684575,
            "f1_weighted": 0.7934879359039423
          },
          {
            "accuracy": 0.8349609375,
            "f1": 0.8283095686932647,
            "f1_weighted": 0.8283338868948854
          },
          {
            "accuracy": 0.82373046875,
            "f1": 0.8176985509561633,
            "f1_weighted": 0.8177404848514048
          },
          {
            "accuracy": 0.80615234375,
            "f1": 0.7998010506144643,
            "f1_weighted": 0.7998344744098433
          },
          {
            "accuracy": 0.8359375,
            "f1": 0.8291512135743463,
            "f1_weighted": 0.829152741758492
          },
          {
            "accuracy": 0.80810546875,
            "f1": 0.7979967091100588,
            "f1_weighted": 0.7980833482383953
          }
        ]
      }
    ]
  },
  "task_name": "DBpediaClassification"
}