{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.768471,
        "f1": 0.649757,
        "f1_weighted": 0.801219,
        "ap": 0.272756,
        "ap_weighted": 0.272756,
        "scores_per_experiment": [
          {
            "accuracy": 0.705326,
            "f1": 0.584877,
            "f1_weighted": 0.752392,
            "ap": 0.206918,
            "ap_weighted": 0.206918
          },
          {
            "accuracy": 0.809278,
            "f1": 0.671396,
            "f1_weighted": 0.830857,
            "ap": 0.273918,
            "ap_weighted": 0.273918
          },
          {
            "accuracy": 0.832474,
            "f1": 0.718322,
            "f1_weighted": 0.852655,
            "ap": 0.348841,
            "ap_weighted": 0.348841
          },
          {
            "accuracy": 0.809278,
            "f1": 0.69319,
            "f1_weighted": 0.834571,
            "ap": 0.31795,
            "ap_weighted": 0.31795
          },
          {
            "accuracy": 0.756873,
            "f1": 0.608219,
            "f1_weighted": 0.789009,
            "ap": 0.207989,
            "ap_weighted": 0.207989
          },
          {
            "accuracy": 0.776632,
            "f1": 0.657763,
            "f1_weighted": 0.808862,
            "ap": 0.276898,
            "ap_weighted": 0.276898
          },
          {
            "accuracy": 0.843643,
            "f1": 0.706289,
            "f1_weighted": 0.856757,
            "ap": 0.312378,
            "ap_weighted": 0.312378
          },
          {
            "accuracy": 0.633162,
            "f1": 0.558376,
            "f1_weighted": 0.69452,
            "ap": 0.226069,
            "ap_weighted": 0.226069
          },
          {
            "accuracy": 0.75945,
            "f1": 0.648482,
            "f1_weighted": 0.796439,
            "ap": 0.275484,
            "ap_weighted": 0.275484
          },
          {
            "accuracy": 0.758591,
            "f1": 0.650658,
            "f1_weighted": 0.796126,
            "ap": 0.281117,
            "ap_weighted": 0.281117
          }
        ],
        "main_score": 0.768471,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 7.21021580696106,
  "kg_co2_emissions": 0.00021216923711370194
}