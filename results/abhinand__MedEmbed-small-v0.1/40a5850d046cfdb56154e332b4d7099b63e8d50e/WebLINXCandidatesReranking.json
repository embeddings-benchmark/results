{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "task_name": "WebLINXCandidatesReranking",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "map": 0.130737,
        "mrr": 0.114031,
        "nAUC_map_max": 0.110812,
        "nAUC_map_std": 0.041683,
        "nAUC_map_diff1": 0.301379,
        "nAUC_mrr_max": 0.120071,
        "nAUC_mrr_std": 0.050668,
        "nAUC_mrr_diff1": 0.306246,
        "main_score": 0.114031,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_iid": [
      {
        "map": 0.11093,
        "mrr": 0.095791,
        "nAUC_map_max": 0.08759,
        "nAUC_map_std": 0.134252,
        "nAUC_map_diff1": 0.182836,
        "nAUC_mrr_max": 0.104461,
        "nAUC_mrr_std": 0.136849,
        "nAUC_mrr_diff1": 0.189736,
        "main_score": 0.095791,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_cat": [
      {
        "map": 0.076603,
        "mrr": 0.06007,
        "nAUC_map_max": 0.042456,
        "nAUC_map_std": 0.098228,
        "nAUC_map_diff1": 0.176567,
        "nAUC_mrr_max": 0.026017,
        "nAUC_mrr_std": 0.073842,
        "nAUC_mrr_diff1": 0.162172,
        "main_score": 0.06007,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_geo": [
      {
        "map": 0.097053,
        "mrr": 0.078026,
        "nAUC_map_max": 0.052454,
        "nAUC_map_std": 0.032713,
        "nAUC_map_diff1": 0.098851,
        "nAUC_mrr_max": 0.04551,
        "nAUC_mrr_std": 0.026323,
        "nAUC_mrr_diff1": 0.094458,
        "main_score": 0.078026,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_vis": [
      {
        "map": 0.096739,
        "mrr": 0.078096,
        "nAUC_map_max": 0.008751,
        "nAUC_map_std": 0.138667,
        "nAUC_map_diff1": 0.023414,
        "nAUC_mrr_max": 0.009871,
        "nAUC_mrr_std": 0.127401,
        "nAUC_mrr_diff1": 0.021777,
        "main_score": 0.078096,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_web": [
      {
        "map": 0.075058,
        "mrr": 0.057542,
        "nAUC_map_max": 0.143694,
        "nAUC_map_std": 0.301751,
        "nAUC_map_diff1": 0.088187,
        "nAUC_mrr_max": 0.168813,
        "nAUC_mrr_std": 0.298057,
        "nAUC_mrr_diff1": 0.087359,
        "main_score": 0.057542,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 1308.5082612037659,
  "kg_co2_emissions": 0.0812723456365425
}