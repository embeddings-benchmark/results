{
  "dataset_revision": "23a20c659d868740ef9c54854de631fe19cd5c17",
  "evaluation_time": 107.10527682304382,
  "kg_co2_emissions": null,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.405322265625,
        "f1": 0.393188612537221,
        "f1_weighted": 0.3956490060717732,
        "hf_subset": "default",
        "languages": [
          "slk-Latn"
        ],
        "main_score": 0.405322265625,
        "scores_per_experiment": [
          {
            "accuracy": 0.4033203125,
            "f1": 0.39109853807509243,
            "f1_weighted": 0.39376502207768316
          },
          {
            "accuracy": 0.42529296875,
            "f1": 0.4077495175347036,
            "f1_weighted": 0.41079545004169626
          },
          {
            "accuracy": 0.40380859375,
            "f1": 0.3842873322174824,
            "f1_weighted": 0.3871224824051847
          },
          {
            "accuracy": 0.3935546875,
            "f1": 0.3900163797252565,
            "f1_weighted": 0.39226016788361884
          },
          {
            "accuracy": 0.41796875,
            "f1": 0.40711639457796595,
            "f1_weighted": 0.4093022513222727
          },
          {
            "accuracy": 0.388671875,
            "f1": 0.36929116097341774,
            "f1_weighted": 0.3712740187895572
          },
          {
            "accuracy": 0.400390625,
            "f1": 0.38754935918680083,
            "f1_weighted": 0.38984306074069347
          },
          {
            "accuracy": 0.4130859375,
            "f1": 0.40955267994830885,
            "f1_weighted": 0.4117431646028888
          },
          {
            "accuracy": 0.40966796875,
            "f1": 0.3959931162825203,
            "f1_weighted": 0.3992202806239596
          },
          {
            "accuracy": 0.3974609375,
            "f1": 0.3892316468506612,
            "f1_weighted": 0.39116416223017747
          }
        ]
      }
    ]
  },
  "task_name": "CSFDSKMovieReviewSentimentClassification"
}