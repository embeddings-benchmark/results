{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 150.78369116783142,
  "kg_co2_emissions": null,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.41572265625,
        "f1": 0.397035269117952,
        "f1_weighted": 0.39701405142640483,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.41572265625,
        "scores_per_experiment": [
          {
            "accuracy": 0.41259765625,
            "f1": 0.3967056553672724,
            "f1_weighted": 0.3966857230347084
          },
          {
            "accuracy": 0.42578125,
            "f1": 0.40404594744034694,
            "f1_weighted": 0.4040449099585014
          },
          {
            "accuracy": 0.41650390625,
            "f1": 0.4016316117056443,
            "f1_weighted": 0.4016389450911235
          },
          {
            "accuracy": 0.39599609375,
            "f1": 0.3802162770284273,
            "f1_weighted": 0.38019164662326693
          },
          {
            "accuracy": 0.4306640625,
            "f1": 0.39163645028113514,
            "f1_weighted": 0.39156908369018084
          },
          {
            "accuracy": 0.41748046875,
            "f1": 0.37692225647634314,
            "f1_weighted": 0.3768570764086661
          },
          {
            "accuracy": 0.4306640625,
            "f1": 0.4242828846302622,
            "f1_weighted": 0.42425745205860366
          },
          {
            "accuracy": 0.41943359375,
            "f1": 0.40494291141219235,
            "f1_weighted": 0.40493825189607313
          },
          {
            "accuracy": 0.42724609375,
            "f1": 0.42089203361284,
            "f1_weighted": 0.42088286045844875
          },
          {
            "accuracy": 0.380859375,
            "f1": 0.3690766632250564,
            "f1_weighted": 0.3690745650444761
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.41220703125,
        "f1": 0.39349507519430876,
        "f1_weighted": 0.39347683075098916,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.41220703125,
        "scores_per_experiment": [
          {
            "accuracy": 0.40673828125,
            "f1": 0.3902099636848644,
            "f1_weighted": 0.39019324216880247
          },
          {
            "accuracy": 0.41748046875,
            "f1": 0.39195167656027174,
            "f1_weighted": 0.39194326041416294
          },
          {
            "accuracy": 0.4140625,
            "f1": 0.394497754969369,
            "f1_weighted": 0.3945069942943261
          },
          {
            "accuracy": 0.42041015625,
            "f1": 0.405158031530018,
            "f1_weighted": 0.4051277237851607
          },
          {
            "accuracy": 0.421875,
            "f1": 0.38905747364519294,
            "f1_weighted": 0.38900480922200875
          },
          {
            "accuracy": 0.40576171875,
            "f1": 0.3691773615916576,
            "f1_weighted": 0.369148788729316
          },
          {
            "accuracy": 0.4111328125,
            "f1": 0.4016912391945816,
            "f1_weighted": 0.40165593700401175
          },
          {
            "accuracy": 0.4296875,
            "f1": 0.4176941467126296,
            "f1_weighted": 0.4176853481836237
          },
          {
            "accuracy": 0.4091796875,
            "f1": 0.40292216861788754,
            "f1_weighted": 0.4028984815751818
          },
          {
            "accuracy": 0.3857421875,
            "f1": 0.37259093543661514,
            "f1_weighted": 0.37260372213329745
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}