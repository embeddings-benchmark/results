{
  "dataset_revision": "3d86128a09e091d6018b6d26cad27f2739fc2db7",
  "task_name": "ImdbClassification",
  "mteb_version": "2.7.26",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.89148,
            "f1": 0.8914646819330059,
            "f1_weighted": 0.8914646819330059,
            "precision": 0.8917011300158564,
            "precision_weighted": 0.8917011300158565,
            "recall": 0.89148,
            "recall_weighted": 0.89148,
            "ap": 0.8527265918216833,
            "ap_weighted": 0.8527265918216833
          },
          {
            "accuracy": 0.8896,
            "f1": 0.8893634438192954,
            "f1_weighted": 0.8893634438192956,
            "precision": 0.8929608171942793,
            "precision_weighted": 0.8929608171942792,
            "recall": 0.8896,
            "recall_weighted": 0.8896,
            "ap": 0.8620559943582511,
            "ap_weighted": 0.8620559943582511
          },
          {
            "accuracy": 0.89092,
            "f1": 0.8905644438781601,
            "f1_weighted": 0.8905644438781601,
            "precision": 0.8960672905074346,
            "precision_weighted": 0.8960672905074346,
            "recall": 0.8909199999999999,
            "recall_weighted": 0.89092,
            "ap": 0.8326399339317774,
            "ap_weighted": 0.8326399339317774
          },
          {
            "accuracy": 0.90052,
            "f1": 0.9005191046719421,
            "f1_weighted": 0.9005191046719421,
            "precision": 0.9005344192390926,
            "precision_weighted": 0.9005344192390927,
            "recall": 0.90052,
            "recall_weighted": 0.90052,
            "ap": 0.8616445778672033,
            "ap_weighted": 0.8616445778672033
          },
          {
            "accuracy": 0.89592,
            "f1": 0.8957867344103411,
            "f1_weighted": 0.8957867344103411,
            "precision": 0.8979555867605771,
            "precision_weighted": 0.8979555867605772,
            "recall": 0.89592,
            "recall_weighted": 0.89592,
            "ap": 0.8442499865611468,
            "ap_weighted": 0.8442499865611468
          },
          {
            "accuracy": 0.89004,
            "f1": 0.8900387288477054,
            "f1_weighted": 0.8900387288477055,
            "precision": 0.8900580362835977,
            "precision_weighted": 0.8900580362835978,
            "recall": 0.8900399999999999,
            "recall_weighted": 0.89004,
            "ap": 0.8461236964640445,
            "ap_weighted": 0.8461236964640445
          },
          {
            "accuracy": 0.8914,
            "f1": 0.8912206925407564,
            "f1_weighted": 0.8912206925407564,
            "precision": 0.893997800860103,
            "precision_weighted": 0.893997800860103,
            "recall": 0.8914,
            "recall_weighted": 0.8914,
            "ap": 0.8373888272290049,
            "ap_weighted": 0.8373888272290049
          },
          {
            "accuracy": 0.8954,
            "f1": 0.8953055235369285,
            "f1_weighted": 0.8953055235369285,
            "precision": 0.896832408802541,
            "precision_weighted": 0.896832408802541,
            "recall": 0.8954,
            "recall_weighted": 0.8954,
            "ap": 0.845180529771338,
            "ap_weighted": 0.845180529771338
          },
          {
            "accuracy": 0.8972,
            "f1": 0.897126552638119,
            "f1_weighted": 0.897126552638119,
            "precision": 0.8983375858618472,
            "precision_weighted": 0.8983375858618471,
            "recall": 0.8972,
            "recall_weighted": 0.8972,
            "ap": 0.8483644289185905,
            "ap_weighted": 0.8483644289185905
          },
          {
            "accuracy": 0.88428,
            "f1": 0.8840451915128135,
            "f1_weighted": 0.8840451915128134,
            "precision": 0.8874180865006553,
            "precision_weighted": 0.8874180865006553,
            "recall": 0.88428,
            "recall_weighted": 0.88428,
            "ap": 0.8544159542857143,
            "ap_weighted": 0.8544159542857143
          }
        ],
        "accuracy": 0.892676,
        "f1": 0.8925435097789066,
        "f1_weighted": 0.8925435097789067,
        "precision": 0.8945863162025984,
        "precision_weighted": 0.8945863162025984,
        "recall": 0.892676,
        "recall_weighted": 0.892676,
        "ap": 0.8484790521208755,
        "ap_weighted": 0.8484790521208755,
        "main_score": 0.892676,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 223.87496709823608,
  "kg_co2_emissions": null
}
