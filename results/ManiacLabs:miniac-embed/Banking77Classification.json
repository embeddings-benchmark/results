{
  "dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300",
  "task_name": "Banking77Classification",
  "mteb_version": "2.7.26",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.812012987012987,
            "f1": 0.8006771765053957,
            "f1_weighted": 0.8006771765053959,
            "precision": 0.8141347385626312,
            "precision_weighted": 0.8141347385626314,
            "recall": 0.8120129870129869,
            "recall_weighted": 0.812012987012987,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.8123376623376624,
            "f1": 0.8049759320829918,
            "f1_weighted": 0.8049759320829919,
            "precision": 0.8291142737614948,
            "precision_weighted": 0.8291142737614948,
            "recall": 0.8123376623376624,
            "recall_weighted": 0.8123376623376624,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.8094155844155844,
            "f1": 0.8003849276426195,
            "f1_weighted": 0.8003849276426196,
            "precision": 0.8295606364361711,
            "precision_weighted": 0.8295606364361713,
            "recall": 0.8094155844155844,
            "recall_weighted": 0.8094155844155844,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.8012987012987013,
            "f1": 0.7950017648523704,
            "f1_weighted": 0.7950017648523705,
            "precision": 0.8211906689003848,
            "precision_weighted": 0.8211906689003848,
            "recall": 0.8012987012987013,
            "recall_weighted": 0.8012987012987013,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.7925324675324675,
            "f1": 0.7826480061240442,
            "f1_weighted": 0.7826480061240443,
            "precision": 0.8177409083368676,
            "precision_weighted": 0.8177409083368677,
            "recall": 0.7925324675324675,
            "recall_weighted": 0.7925324675324675,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.8071428571428572,
            "f1": 0.8021086024577118,
            "f1_weighted": 0.8021086024577119,
            "precision": 0.8306337903132461,
            "precision_weighted": 0.8306337903132461,
            "recall": 0.8071428571428572,
            "recall_weighted": 0.8071428571428572,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.7928571428571428,
            "f1": 0.7830102771090268,
            "f1_weighted": 0.783010277109027,
            "precision": 0.8094669210529942,
            "precision_weighted": 0.8094669210529943,
            "recall": 0.7928571428571428,
            "recall_weighted": 0.7928571428571428,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.7954545454545454,
            "f1": 0.7865829811574785,
            "f1_weighted": 0.7865829811574785,
            "precision": 0.8220552313610358,
            "precision_weighted": 0.8220552313610361,
            "recall": 0.7954545454545454,
            "recall_weighted": 0.7954545454545454,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.8032467532467532,
            "f1": 0.7931988116344154,
            "f1_weighted": 0.7931988116344157,
            "precision": 0.8310658356911393,
            "precision_weighted": 0.8310658356911393,
            "recall": 0.8032467532467533,
            "recall_weighted": 0.8032467532467532,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.7996753246753247,
            "f1": 0.7908538233709179,
            "f1_weighted": 0.790853823370918,
            "precision": 0.8247680377159693,
            "precision_weighted": 0.8247680377159692,
            "recall": 0.7996753246753247,
            "recall_weighted": 0.7996753246753247,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.8025974025974026,
        "f1": 0.7939442302936971,
        "f1_weighted": 0.7939442302936974,
        "precision": 0.8229731042131935,
        "precision_weighted": 0.8229731042131935,
        "recall": 0.8025974025974026,
        "recall_weighted": 0.8025974025974026,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.8025974025974026,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 27.14931631088257,
  "kg_co2_emissions": null
}
