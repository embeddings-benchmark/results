{
  "dataset_revision": "d604517c81ca91fe16a244d1248fc021f9ecee7a",
  "task_name": "TweetSentimentExtractionClassification",
  "mteb_version": "2.7.26",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.6428975664968873,
            "f1": 0.6418901851488455,
            "f1_weighted": 0.630248492847563,
            "precision": 0.6451368214731795,
            "precision_weighted": 0.6478804271182563,
            "recall": 0.6663237638758944,
            "recall_weighted": 0.6428975664968873,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.6335597057159027,
            "f1": 0.6364435890719794,
            "f1_weighted": 0.6257255196372921,
            "precision": 0.63272869197347,
            "precision_weighted": 0.632128286615983,
            "recall": 0.6534950694515517,
            "recall_weighted": 0.6335597057159027,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.6584606677985286,
            "f1": 0.6613571551548061,
            "f1_weighted": 0.6524313572775712,
            "precision": 0.657838085196241,
            "precision_weighted": 0.6573435899800648,
            "recall": 0.6750122346677198,
            "recall_weighted": 0.6584606677985286,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.6346915676287493,
            "f1": 0.633700783363257,
            "f1_weighted": 0.6207206800386827,
            "precision": 0.6391963614129695,
            "precision_weighted": 0.6431984450977785,
            "recall": 0.6622511969746784,
            "recall_weighted": 0.6346915676287493,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.6542161856253537,
            "f1": 0.6560416070932781,
            "f1_weighted": 0.6470382713775776,
            "precision": 0.6538473437263117,
            "precision_weighted": 0.655576500619787,
            "recall": 0.6728144324699175,
            "recall_weighted": 0.6542161856253537,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.6502546689303905,
            "f1": 0.6500887508402685,
            "f1_weighted": 0.6375986145379945,
            "precision": 0.6531459229682537,
            "precision_weighted": 0.6564268771429016,
            "recall": 0.6764523780842909,
            "recall_weighted": 0.6502546689303905,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.5837577815506508,
            "f1": 0.5778865409263108,
            "f1_weighted": 0.5637107677039394,
            "precision": 0.5826025034837296,
            "precision_weighted": 0.5803531032467992,
            "recall": 0.607482937129356,
            "recall_weighted": 0.5837577815506508,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.6638370118845501,
            "f1": 0.6639571598320367,
            "f1_weighted": 0.6516479986505905,
            "precision": 0.6675466426182322,
            "precision_weighted": 0.6696149555925128,
            "recall": 0.6879297493078091,
            "recall_weighted": 0.6638370118845501,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.6157328805885682,
            "f1": 0.6146940062544047,
            "f1_weighted": 0.600044976889642,
            "precision": 0.6140655036921867,
            "precision_weighted": 0.6139239287498999,
            "recall": 0.6427803082381505,
            "recall_weighted": 0.6157328805885682,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.6253537068477646,
            "f1": 0.6275962355751714,
            "f1_weighted": 0.61535069657571,
            "precision": 0.6246545459712189,
            "precision_weighted": 0.6239649726073829,
            "recall": 0.6479114116466791,
            "recall_weighted": 0.6253537068477646,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.6362761743067347,
        "f1": 0.6363656013260359,
        "f1_weighted": 0.6244517375536562,
        "precision": 0.6370762422515793,
        "precision_weighted": 0.6380411086771367,
        "recall": 0.6592453481846048,
        "recall_weighted": 0.6362761743067347,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.6362761743067347,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 29.515071153640747,
  "kg_co2_emissions": null
}
