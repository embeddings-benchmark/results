{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "task_name": "MassiveScenarioClassification",
  "mteb_version": "2.7.26",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.7427706792199058,
            "f1": 0.7426818564884305,
            "f1_weighted": 0.7395522285171672,
            "precision": 0.7254949557064755,
            "precision_weighted": 0.7779966149597984,
            "recall": 0.8032954742637664,
            "recall_weighted": 0.7427706792199058,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.765635507733692,
            "f1": 0.7591907803542042,
            "f1_weighted": 0.7618763907526681,
            "precision": 0.737554801241571,
            "precision_weighted": 0.7951756137639034,
            "recall": 0.820448329818364,
            "recall_weighted": 0.765635507733692,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.746133154001345,
            "f1": 0.7406586699570382,
            "f1_weighted": 0.7435784551029191,
            "precision": 0.723690574030954,
            "precision_weighted": 0.7895335877099425,
            "recall": 0.8089074982217993,
            "recall_weighted": 0.746133154001345,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.7256220578345662,
            "f1": 0.7238289346630518,
            "f1_weighted": 0.7207028481557121,
            "precision": 0.7191984899174302,
            "precision_weighted": 0.7762782755575692,
            "recall": 0.7871773341830298,
            "recall_weighted": 0.7256220578345662,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.7488231338264963,
            "f1": 0.7419425312866029,
            "f1_weighted": 0.7393454297532224,
            "precision": 0.7366943014377502,
            "precision_weighted": 0.7896601059816407,
            "recall": 0.8088710885316279,
            "recall_weighted": 0.7488231338264963,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.7273032952252858,
            "f1": 0.719379682917403,
            "f1_weighted": 0.7212915444212039,
            "precision": 0.6987127876720305,
            "precision_weighted": 0.7723943693828949,
            "recall": 0.7955567367608499,
            "recall_weighted": 0.7273032952252858,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.7283120376597175,
            "f1": 0.7245012822024399,
            "f1_weighted": 0.7241316080514111,
            "precision": 0.7132978092292647,
            "precision_weighted": 0.784863302150922,
            "recall": 0.7969848282677194,
            "recall_weighted": 0.7283120376597175,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.7303295225285811,
            "f1": 0.7286745524806899,
            "f1_weighted": 0.7306912494136572,
            "precision": 0.7135145543972958,
            "precision_weighted": 0.7804611264218554,
            "recall": 0.7945973443965537,
            "recall_weighted": 0.7303295225285811,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.7568930733019502,
            "f1": 0.7547555540104479,
            "f1_weighted": 0.754017585766379,
            "precision": 0.7348078964704015,
            "precision_weighted": 0.7874099470833057,
            "recall": 0.8145383468707749,
            "recall_weighted": 0.7568930733019502,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.7410894418291862,
            "f1": 0.7410176743283146,
            "f1_weighted": 0.7402218443088914,
            "precision": 0.7302450489378246,
            "precision_weighted": 0.7871257779362124,
            "recall": 0.7994070546454933,
            "recall_weighted": 0.7410894418291862,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.7412911903160726,
        "f1": 0.7376631518688622,
        "f1_weighted": 0.7375409184243231,
        "precision": 0.7233211219040998,
        "precision_weighted": 0.7840898720948044,
        "recall": 0.8029784035959979,
        "recall_weighted": 0.7412911903160726,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.7412911903160726,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 19.27573275566101,
  "kg_co2_emissions": null
}
