{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.291211,
        "f1": 0.28725,
        "f1_weighted": 0.287234,
        "scores_per_experiment": [
          {
            "accuracy": 0.288574,
            "f1": 0.282753,
            "f1_weighted": 0.282731
          },
          {
            "accuracy": 0.283203,
            "f1": 0.275203,
            "f1_weighted": 0.275179
          },
          {
            "accuracy": 0.306152,
            "f1": 0.30216,
            "f1_weighted": 0.302134
          },
          {
            "accuracy": 0.3125,
            "f1": 0.314403,
            "f1_weighted": 0.314415
          },
          {
            "accuracy": 0.263184,
            "f1": 0.256544,
            "f1_weighted": 0.256498
          },
          {
            "accuracy": 0.272461,
            "f1": 0.264977,
            "f1_weighted": 0.264968
          },
          {
            "accuracy": 0.245117,
            "f1": 0.245414,
            "f1_weighted": 0.245371
          },
          {
            "accuracy": 0.33252,
            "f1": 0.332753,
            "f1_weighted": 0.332748
          },
          {
            "accuracy": 0.318359,
            "f1": 0.316832,
            "f1_weighted": 0.316829
          },
          {
            "accuracy": 0.290039,
            "f1": 0.281466,
            "f1_weighted": 0.281468
          }
        ],
        "main_score": 0.291211,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.295654,
        "f1": 0.292303,
        "f1_weighted": 0.292298,
        "scores_per_experiment": [
          {
            "accuracy": 0.274414,
            "f1": 0.26662,
            "f1_weighted": 0.2666
          },
          {
            "accuracy": 0.299805,
            "f1": 0.292029,
            "f1_weighted": 0.292011
          },
          {
            "accuracy": 0.308105,
            "f1": 0.30584,
            "f1_weighted": 0.305834
          },
          {
            "accuracy": 0.290039,
            "f1": 0.295052,
            "f1_weighted": 0.295057
          },
          {
            "accuracy": 0.285645,
            "f1": 0.279073,
            "f1_weighted": 0.279032
          },
          {
            "accuracy": 0.273438,
            "f1": 0.267261,
            "f1_weighted": 0.267262
          },
          {
            "accuracy": 0.258789,
            "f1": 0.259758,
            "f1_weighted": 0.259728
          },
          {
            "accuracy": 0.333496,
            "f1": 0.333136,
            "f1_weighted": 0.333171
          },
          {
            "accuracy": 0.330566,
            "f1": 0.330321,
            "f1_weighted": 0.330322
          },
          {
            "accuracy": 0.302246,
            "f1": 0.293944,
            "f1_weighted": 0.293963
          }
        ],
        "main_score": 0.295654,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 21.165554523468018,
  "kg_co2_emissions": 0.0010242418539117889
}