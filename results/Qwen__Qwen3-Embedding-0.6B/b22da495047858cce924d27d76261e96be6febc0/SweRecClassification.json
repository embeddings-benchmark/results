{
  "dataset_revision": "b07c6ce548f6a7ac8d546e1bbe197a0086409190",
  "task_name": "SweRecClassification",
  "mteb_version": "2.1.10",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.824707,
            "f1": 0.713268,
            "f1_weighted": 0.820891,
            "precision": 0.716423,
            "precision_weighted": 0.817975,
            "recall": 0.71156,
            "recall_weighted": 0.824707,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.794434,
            "f1": 0.730505,
            "f1_weighted": 0.812949,
            "precision": 0.731779,
            "precision_weighted": 0.848235,
            "recall": 0.759597,
            "recall_weighted": 0.794434,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.804199,
            "f1": 0.733425,
            "f1_weighted": 0.818455,
            "precision": 0.730943,
            "precision_weighted": 0.843214,
            "recall": 0.754766,
            "recall_weighted": 0.804199,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.80957,
            "f1": 0.737159,
            "f1_weighted": 0.823006,
            "precision": 0.73397,
            "precision_weighted": 0.844,
            "recall": 0.756024,
            "recall_weighted": 0.80957,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.80957,
            "f1": 0.718265,
            "f1_weighted": 0.816619,
            "precision": 0.718213,
            "precision_weighted": 0.827647,
            "recall": 0.724122,
            "recall_weighted": 0.80957,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.819336,
            "f1": 0.743983,
            "f1_weighted": 0.828649,
            "precision": 0.738757,
            "precision_weighted": 0.842055,
            "recall": 0.757881,
            "recall_weighted": 0.819336,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.800781,
            "f1": 0.735584,
            "f1_weighted": 0.818382,
            "precision": 0.735811,
            "precision_weighted": 0.849381,
            "recall": 0.762577,
            "recall_weighted": 0.800781,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.808105,
            "f1": 0.733445,
            "f1_weighted": 0.820444,
            "precision": 0.730146,
            "precision_weighted": 0.839081,
            "recall": 0.749931,
            "recall_weighted": 0.808105,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.787109,
            "f1": 0.720992,
            "f1_weighted": 0.805485,
            "precision": 0.723302,
            "precision_weighted": 0.841241,
            "recall": 0.748317,
            "recall_weighted": 0.787109,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.809082,
            "f1": 0.729062,
            "f1_weighted": 0.817025,
            "precision": 0.724508,
            "precision_weighted": 0.827724,
            "recall": 0.739432,
            "recall_weighted": 0.809082,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.806689,
        "f1": 0.729569,
        "f1_weighted": 0.818191,
        "precision": 0.728385,
        "precision_weighted": 0.838055,
        "recall": 0.746421,
        "recall_weighted": 0.806689,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.806689,
        "hf_subset": "default",
        "languages": [
          "swe-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 55.46390771865845,
  "kg_co2_emissions": null
}