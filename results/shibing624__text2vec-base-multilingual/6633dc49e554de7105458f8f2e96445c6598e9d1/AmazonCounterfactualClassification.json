{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "task_name": "AmazonCounterfactualClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.695279,
        "f1": 0.673793,
        "f1_weighted": 0.706517,
        "ap": 0.803739,
        "ap_weighted": 0.803739,
        "scores_per_experiment": [
          {
            "accuracy": 0.699571,
            "f1": 0.681843,
            "f1_weighted": 0.711497,
            "ap": 0.812194,
            "ap_weighted": 0.812194
          },
          {
            "accuracy": 0.714592,
            "f1": 0.678048,
            "f1_weighted": 0.720877,
            "ap": 0.791447,
            "ap_weighted": 0.791447
          },
          {
            "accuracy": 0.714592,
            "f1": 0.690102,
            "f1_weighted": 0.7245,
            "ap": 0.80844,
            "ap_weighted": 0.80844
          },
          {
            "accuracy": 0.693133,
            "f1": 0.680404,
            "f1_weighted": 0.705588,
            "ap": 0.819962,
            "ap_weighted": 0.819962
          },
          {
            "accuracy": 0.706009,
            "f1": 0.683993,
            "f1_weighted": 0.716927,
            "ap": 0.807738,
            "ap_weighted": 0.807738
          },
          {
            "accuracy": 0.716738,
            "f1": 0.70189,
            "f1_weighted": 0.72816,
            "ap": 0.829804,
            "ap_weighted": 0.829804
          },
          {
            "accuracy": 0.695279,
            "f1": 0.673637,
            "f1_weighted": 0.706821,
            "ap": 0.802038,
            "ap_weighted": 0.802038
          },
          {
            "accuracy": 0.60515,
            "f1": 0.581851,
            "f1_weighted": 0.620824,
            "ap": 0.747753,
            "ap_weighted": 0.747753
          },
          {
            "accuracy": 0.712446,
            "f1": 0.688179,
            "f1_weighted": 0.722527,
            "ap": 0.807575,
            "ap_weighted": 0.807575
          },
          {
            "accuracy": 0.695279,
            "f1": 0.677983,
            "f1_weighted": 0.70745,
            "ap": 0.810439,
            "ap_weighted": 0.810439
          }
        ],
        "main_score": 0.695279,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.687045,
        "f1": 0.665641,
        "f1_weighted": 0.698448,
        "ap": 0.796931,
        "ap_weighted": 0.796931,
        "scores_per_experiment": [
          {
            "accuracy": 0.684154,
            "f1": 0.664326,
            "f1_weighted": 0.696295,
            "ap": 0.796805,
            "ap_weighted": 0.796805
          },
          {
            "accuracy": 0.702355,
            "f1": 0.668622,
            "f1_weighted": 0.710053,
            "ap": 0.786676,
            "ap_weighted": 0.786676
          },
          {
            "accuracy": 0.708779,
            "f1": 0.682651,
            "f1_weighted": 0.718334,
            "ap": 0.800707,
            "ap_weighted": 0.800707
          },
          {
            "accuracy": 0.667024,
            "f1": 0.654031,
            "f1_weighted": 0.680303,
            "ap": 0.800195,
            "ap_weighted": 0.800195
          },
          {
            "accuracy": 0.701285,
            "f1": 0.681461,
            "f1_weighted": 0.7126,
            "ap": 0.807184,
            "ap_weighted": 0.807184
          },
          {
            "accuracy": 0.721627,
            "f1": 0.702642,
            "f1_weighted": 0.732085,
            "ap": 0.821279,
            "ap_weighted": 0.821279
          },
          {
            "accuracy": 0.638116,
            "f1": 0.612073,
            "f1_weighted": 0.65146,
            "ap": 0.76081,
            "ap_weighted": 0.76081
          },
          {
            "accuracy": 0.639186,
            "f1": 0.618195,
            "f1_weighted": 0.653276,
            "ap": 0.768243,
            "ap_weighted": 0.768243
          },
          {
            "accuracy": 0.721627,
            "f1": 0.698632,
            "f1_weighted": 0.731253,
            "ap": 0.813397,
            "ap_weighted": 0.813397
          },
          {
            "accuracy": 0.686296,
            "f1": 0.673772,
            "f1_weighted": 0.698819,
            "ap": 0.814014,
            "ap_weighted": 0.814014
          }
        ],
        "main_score": 0.687045,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 24.15538454055786,
  "kg_co2_emissions": null
}