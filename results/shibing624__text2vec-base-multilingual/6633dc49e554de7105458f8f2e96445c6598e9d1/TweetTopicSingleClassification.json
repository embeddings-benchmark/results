{
  "dataset_revision": "87b7a0d1c402dbb481db649569c556d9aa27ac05",
  "task_name": "TweetTopicSingleClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test_2021": [
      {
        "accuracy": 0.518901,
        "f1": 0.403232,
        "f1_weighted": 0.560917,
        "scores_per_experiment": [
          {
            "accuracy": 0.544595,
            "f1": 0.399886,
            "f1_weighted": 0.577901
          },
          {
            "accuracy": 0.512699,
            "f1": 0.423529,
            "f1_weighted": 0.550136
          },
          {
            "accuracy": 0.49557,
            "f1": 0.378642,
            "f1_weighted": 0.521755
          },
          {
            "accuracy": 0.464855,
            "f1": 0.38741,
            "f1_weighted": 0.518659
          },
          {
            "accuracy": 0.600118,
            "f1": 0.458236,
            "f1_weighted": 0.64299
          },
          {
            "accuracy": 0.538098,
            "f1": 0.428412,
            "f1_weighted": 0.573614
          },
          {
            "accuracy": 0.533373,
            "f1": 0.397049,
            "f1_weighted": 0.583527
          },
          {
            "accuracy": 0.538098,
            "f1": 0.410085,
            "f1_weighted": 0.583426
          },
          {
            "accuracy": 0.432959,
            "f1": 0.33286,
            "f1_weighted": 0.479555
          },
          {
            "accuracy": 0.528647,
            "f1": 0.416216,
            "f1_weighted": 0.57761
          }
        ],
        "main_score": 0.518901,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 7.597537517547607,
  "kg_co2_emissions": 0.0002309782971802674
}