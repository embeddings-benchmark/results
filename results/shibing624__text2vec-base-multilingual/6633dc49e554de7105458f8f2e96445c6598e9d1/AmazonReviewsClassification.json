{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "task_name": "AmazonReviewsClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.33596,
        "f1": 0.33313,
        "f1_weighted": 0.33313,
        "scores_per_experiment": [
          {
            "accuracy": 0.3858,
            "f1": 0.384151,
            "f1_weighted": 0.384151
          },
          {
            "accuracy": 0.32,
            "f1": 0.316271,
            "f1_weighted": 0.316271
          },
          {
            "accuracy": 0.3356,
            "f1": 0.328853,
            "f1_weighted": 0.328853
          },
          {
            "accuracy": 0.3396,
            "f1": 0.334589,
            "f1_weighted": 0.334589
          },
          {
            "accuracy": 0.3334,
            "f1": 0.331795,
            "f1_weighted": 0.331795
          },
          {
            "accuracy": 0.3446,
            "f1": 0.343729,
            "f1_weighted": 0.343729
          },
          {
            "accuracy": 0.2794,
            "f1": 0.281941,
            "f1_weighted": 0.281941
          },
          {
            "accuracy": 0.3552,
            "f1": 0.354249,
            "f1_weighted": 0.354249
          },
          {
            "accuracy": 0.328,
            "f1": 0.325996,
            "f1_weighted": 0.325996
          },
          {
            "accuracy": 0.338,
            "f1": 0.329723,
            "f1_weighted": 0.329723
          }
        ],
        "main_score": 0.33596,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.33394,
        "f1": 0.331978,
        "f1_weighted": 0.331978,
        "scores_per_experiment": [
          {
            "accuracy": 0.3748,
            "f1": 0.374319,
            "f1_weighted": 0.374319
          },
          {
            "accuracy": 0.325,
            "f1": 0.323455,
            "f1_weighted": 0.323455
          },
          {
            "accuracy": 0.3342,
            "f1": 0.329901,
            "f1_weighted": 0.329901
          },
          {
            "accuracy": 0.3386,
            "f1": 0.335603,
            "f1_weighted": 0.335603
          },
          {
            "accuracy": 0.3326,
            "f1": 0.332394,
            "f1_weighted": 0.332394
          },
          {
            "accuracy": 0.3364,
            "f1": 0.337795,
            "f1_weighted": 0.337795
          },
          {
            "accuracy": 0.2918,
            "f1": 0.294337,
            "f1_weighted": 0.294337
          },
          {
            "accuracy": 0.3466,
            "f1": 0.34317,
            "f1_weighted": 0.34317
          },
          {
            "accuracy": 0.3226,
            "f1": 0.319688,
            "f1_weighted": 0.319688
          },
          {
            "accuracy": 0.3368,
            "f1": 0.329121,
            "f1_weighted": 0.329121
          }
        ],
        "main_score": 0.33394,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 37.28035569190979,
  "kg_co2_emissions": null
}