{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "task_name": "MTOPIntentClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.546832,
        "f1": 0.356526,
        "f1_weighted": 0.593777,
        "scores_per_experiment": [
          {
            "accuracy": 0.555923,
            "f1": 0.369012,
            "f1_weighted": 0.599311
          },
          {
            "accuracy": 0.551515,
            "f1": 0.365264,
            "f1_weighted": 0.601706
          },
          {
            "accuracy": 0.567493,
            "f1": 0.376636,
            "f1_weighted": 0.610419
          },
          {
            "accuracy": 0.557576,
            "f1": 0.352619,
            "f1_weighted": 0.607518
          },
          {
            "accuracy": 0.536088,
            "f1": 0.35328,
            "f1_weighted": 0.58046
          },
          {
            "accuracy": 0.524518,
            "f1": 0.35448,
            "f1_weighted": 0.574988
          },
          {
            "accuracy": 0.543251,
            "f1": 0.34267,
            "f1_weighted": 0.593026
          },
          {
            "accuracy": 0.552066,
            "f1": 0.367481,
            "f1_weighted": 0.596148
          },
          {
            "accuracy": 0.537741,
            "f1": 0.350639,
            "f1_weighted": 0.587173
          },
          {
            "accuracy": 0.542149,
            "f1": 0.333179,
            "f1_weighted": 0.587019
          }
        ],
        "main_score": 0.546832,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.552578,
        "f1": 0.379394,
        "f1_weighted": 0.598045,
        "scores_per_experiment": [
          {
            "accuracy": 0.56523,
            "f1": 0.38411,
            "f1_weighted": 0.60955
          },
          {
            "accuracy": 0.54466,
            "f1": 0.379038,
            "f1_weighted": 0.593165
          },
          {
            "accuracy": 0.560158,
            "f1": 0.393551,
            "f1_weighted": 0.595091
          },
          {
            "accuracy": 0.55255,
            "f1": 0.36243,
            "f1_weighted": 0.605802
          },
          {
            "accuracy": 0.550859,
            "f1": 0.383485,
            "f1_weighted": 0.595259
          },
          {
            "accuracy": 0.522119,
            "f1": 0.360199,
            "f1_weighted": 0.568394
          },
          {
            "accuracy": 0.559031,
            "f1": 0.393272,
            "f1_weighted": 0.605839
          },
          {
            "accuracy": 0.551423,
            "f1": 0.381976,
            "f1_weighted": 0.59388
          },
          {
            "accuracy": 0.560158,
            "f1": 0.378374,
            "f1_weighted": 0.607985
          },
          {
            "accuracy": 0.559594,
            "f1": 0.377507,
            "f1_weighted": 0.605487
          }
        ],
        "main_score": 0.552578,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 26.755773067474365,
  "kg_co2_emissions": null
}