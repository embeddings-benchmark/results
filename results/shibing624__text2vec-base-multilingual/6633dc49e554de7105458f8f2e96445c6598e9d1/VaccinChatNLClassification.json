{
  "dataset_revision": "bd27d0058bea2ad52470d9072a3b5da6b97c1ac3",
  "task_name": "VaccinChatNLClassification",
  "mteb_version": "2.1.17",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.493162,
            "f1": 0.511197,
            "f1_weighted": 0.489319,
            "precision": 0.514262,
            "precision_weighted": 0.582326,
            "recall": 0.6287,
            "recall_weighted": 0.493162,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.497436,
            "f1": 0.515143,
            "f1_weighted": 0.482394,
            "precision": 0.505117,
            "precision_weighted": 0.587005,
            "recall": 0.644925,
            "recall_weighted": 0.497436,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.479487,
            "f1": 0.510549,
            "f1_weighted": 0.47048,
            "precision": 0.499783,
            "precision_weighted": 0.582767,
            "recall": 0.640269,
            "recall_weighted": 0.479487,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.501709,
            "f1": 0.501254,
            "f1_weighted": 0.497427,
            "precision": 0.493229,
            "precision_weighted": 0.607761,
            "recall": 0.62349,
            "recall_weighted": 0.501709,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.483761,
            "f1": 0.480376,
            "f1_weighted": 0.480268,
            "precision": 0.467908,
            "precision_weighted": 0.59071,
            "recall": 0.60488,
            "recall_weighted": 0.483761,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.507692,
            "f1": 0.530611,
            "f1_weighted": 0.506782,
            "precision": 0.523175,
            "precision_weighted": 0.61061,
            "recall": 0.643948,
            "recall_weighted": 0.507692,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.477778,
            "f1": 0.522821,
            "f1_weighted": 0.464332,
            "precision": 0.514284,
            "precision_weighted": 0.536618,
            "recall": 0.635722,
            "recall_weighted": 0.477778,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.476923,
            "f1": 0.508336,
            "f1_weighted": 0.468143,
            "precision": 0.490351,
            "precision_weighted": 0.569233,
            "recall": 0.638919,
            "recall_weighted": 0.476923,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.478632,
            "f1": 0.494351,
            "f1_weighted": 0.463727,
            "precision": 0.474456,
            "precision_weighted": 0.550333,
            "recall": 0.62114,
            "recall_weighted": 0.478632,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.487179,
            "f1": 0.509577,
            "f1_weighted": 0.477245,
            "precision": 0.503137,
            "precision_weighted": 0.585547,
            "recall": 0.630846,
            "recall_weighted": 0.487179,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.488376,
        "f1": 0.508421,
        "f1_weighted": 0.480012,
        "precision": 0.49857,
        "precision_weighted": 0.580291,
        "recall": 0.631284,
        "recall_weighted": 0.488376,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.508421,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 43.70789909362793,
  "kg_co2_emissions": null
}