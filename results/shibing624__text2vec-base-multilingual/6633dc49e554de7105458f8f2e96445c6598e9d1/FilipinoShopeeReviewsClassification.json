{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.264014,
        "f1": 0.259809,
        "f1_weighted": 0.259811,
        "scores_per_experiment": [
          {
            "accuracy": 0.270996,
            "f1": 0.262753,
            "f1_weighted": 0.262748
          },
          {
            "accuracy": 0.229492,
            "f1": 0.225603,
            "f1_weighted": 0.225597
          },
          {
            "accuracy": 0.28418,
            "f1": 0.275874,
            "f1_weighted": 0.275867
          },
          {
            "accuracy": 0.286621,
            "f1": 0.286965,
            "f1_weighted": 0.286946
          },
          {
            "accuracy": 0.252441,
            "f1": 0.246005,
            "f1_weighted": 0.24604
          },
          {
            "accuracy": 0.242676,
            "f1": 0.235182,
            "f1_weighted": 0.235199
          },
          {
            "accuracy": 0.240723,
            "f1": 0.242718,
            "f1_weighted": 0.242729
          },
          {
            "accuracy": 0.293457,
            "f1": 0.289695,
            "f1_weighted": 0.289716
          },
          {
            "accuracy": 0.288086,
            "f1": 0.283418,
            "f1_weighted": 0.28339
          },
          {
            "accuracy": 0.251465,
            "f1": 0.249876,
            "f1_weighted": 0.249882
          }
        ],
        "main_score": 0.264014,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.256445,
        "f1": 0.252755,
        "f1_weighted": 0.252757,
        "scores_per_experiment": [
          {
            "accuracy": 0.250977,
            "f1": 0.244099,
            "f1_weighted": 0.244073
          },
          {
            "accuracy": 0.230469,
            "f1": 0.227391,
            "f1_weighted": 0.227388
          },
          {
            "accuracy": 0.255371,
            "f1": 0.247089,
            "f1_weighted": 0.247086
          },
          {
            "accuracy": 0.270508,
            "f1": 0.270977,
            "f1_weighted": 0.270966
          },
          {
            "accuracy": 0.276855,
            "f1": 0.270289,
            "f1_weighted": 0.270311
          },
          {
            "accuracy": 0.237793,
            "f1": 0.233581,
            "f1_weighted": 0.233596
          },
          {
            "accuracy": 0.241211,
            "f1": 0.242784,
            "f1_weighted": 0.242787
          },
          {
            "accuracy": 0.282227,
            "f1": 0.278195,
            "f1_weighted": 0.278215
          },
          {
            "accuracy": 0.280273,
            "f1": 0.276032,
            "f1_weighted": 0.276016
          },
          {
            "accuracy": 0.23877,
            "f1": 0.237117,
            "f1_weighted": 0.237129
          }
        ],
        "main_score": 0.256445,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 14.119486570358276,
  "kg_co2_emissions": 0.0004277439523178541
}