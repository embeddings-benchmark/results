{
  "dataset_revision": "5a79d6472db143690c7ce6e974995d3610eee7f0",
  "task_name": "SanskritShlokasClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "train": [
      {
        "accuracy": 0.67154,
        "f1": 0.671093,
        "f1_weighted": 0.673184,
        "scores_per_experiment": [
          {
            "accuracy": 0.605744,
            "f1": 0.605307,
            "f1_weighted": 0.604202
          },
          {
            "accuracy": 0.744125,
            "f1": 0.740312,
            "f1_weighted": 0.742695
          },
          {
            "accuracy": 0.605744,
            "f1": 0.597266,
            "f1_weighted": 0.599092
          },
          {
            "accuracy": 0.590078,
            "f1": 0.590847,
            "f1_weighted": 0.594353
          },
          {
            "accuracy": 0.694517,
            "f1": 0.697794,
            "f1_weighted": 0.700407
          },
          {
            "accuracy": 0.694517,
            "f1": 0.693043,
            "f1_weighted": 0.696341
          },
          {
            "accuracy": 0.684073,
            "f1": 0.677614,
            "f1_weighted": 0.681642
          },
          {
            "accuracy": 0.689295,
            "f1": 0.693715,
            "f1_weighted": 0.69265
          },
          {
            "accuracy": 0.704961,
            "f1": 0.708843,
            "f1_weighted": 0.711876
          },
          {
            "accuracy": 0.70235,
            "f1": 0.706184,
            "f1_weighted": 0.708579
          }
        ],
        "main_score": 0.67154,
        "hf_subset": "default",
        "languages": [
          "san-Deva"
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.703125,
        "f1": 0.696401,
        "f1_weighted": 0.700632,
        "scores_per_experiment": [
          {
            "accuracy": 0.625,
            "f1": 0.627714,
            "f1_weighted": 0.621014
          },
          {
            "accuracy": 0.739583,
            "f1": 0.722897,
            "f1_weighted": 0.73274
          },
          {
            "accuracy": 0.614583,
            "f1": 0.614256,
            "f1_weighted": 0.610765
          },
          {
            "accuracy": 0.6875,
            "f1": 0.693441,
            "f1_weighted": 0.694577
          },
          {
            "accuracy": 0.729167,
            "f1": 0.71497,
            "f1_weighted": 0.72527
          },
          {
            "accuracy": 0.791667,
            "f1": 0.784897,
            "f1_weighted": 0.792839
          },
          {
            "accuracy": 0.6875,
            "f1": 0.660491,
            "f1_weighted": 0.674912
          },
          {
            "accuracy": 0.75,
            "f1": 0.756716,
            "f1_weighted": 0.748838
          },
          {
            "accuracy": 0.739583,
            "f1": 0.734959,
            "f1_weighted": 0.743648
          },
          {
            "accuracy": 0.666667,
            "f1": 0.65367,
            "f1_weighted": 0.66172
          }
        ],
        "main_score": 0.703125,
        "hf_subset": "default",
        "languages": [
          "san-Deva"
        ]
      }
    ]
  },
  "evaluation_time": 12.51048469543457,
  "kg_co2_emissions": 0.0003755978418382205
}