{
  "dataset_revision": "5b740b7c42c73d586420812a35745fc37118862f",
  "task_name": "NoRecClassification",
  "mteb_version": "2.1.5",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.460938,
            "f1": 0.445453,
            "f1_weighted": 0.479921,
            "precision": 0.474158,
            "precision_weighted": 0.549272,
            "recall": 0.472222,
            "recall_weighted": 0.460938,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.408691,
            "f1": 0.37083,
            "f1_weighted": 0.414367,
            "precision": 0.373688,
            "precision_weighted": 0.428904,
            "recall": 0.375406,
            "recall_weighted": 0.408691,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.480957,
            "f1": 0.457005,
            "f1_weighted": 0.49579,
            "precision": 0.503452,
            "precision_weighted": 0.57683,
            "recall": 0.504029,
            "recall_weighted": 0.480957,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.48291,
            "f1": 0.453465,
            "f1_weighted": 0.495662,
            "precision": 0.460346,
            "precision_weighted": 0.525092,
            "recall": 0.472049,
            "recall_weighted": 0.48291,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.443848,
            "f1": 0.423701,
            "f1_weighted": 0.460227,
            "precision": 0.438275,
            "precision_weighted": 0.50496,
            "recall": 0.44242,
            "recall_weighted": 0.443848,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.493652,
            "f1": 0.466934,
            "f1_weighted": 0.506284,
            "precision": 0.481301,
            "precision_weighted": 0.543342,
            "recall": 0.489841,
            "recall_weighted": 0.493652,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.426758,
            "f1": 0.412669,
            "f1_weighted": 0.454974,
            "precision": 0.487911,
            "precision_weighted": 0.569798,
            "recall": 0.449901,
            "recall_weighted": 0.426758,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.464844,
            "f1": 0.438901,
            "f1_weighted": 0.479834,
            "precision": 0.4534,
            "precision_weighted": 0.523774,
            "recall": 0.454667,
            "recall_weighted": 0.464844,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.478516,
            "f1": 0.466393,
            "f1_weighted": 0.492472,
            "precision": 0.487752,
            "precision_weighted": 0.556152,
            "recall": 0.502226,
            "recall_weighted": 0.478516,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.413086,
            "f1": 0.400834,
            "f1_weighted": 0.408005,
            "precision": 0.426649,
            "precision_weighted": 0.48065,
            "recall": 0.447819,
            "recall_weighted": 0.413086,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.45542,
        "f1": 0.433619,
        "f1_weighted": 0.468754,
        "precision": 0.458693,
        "precision_weighted": 0.525877,
        "recall": 0.461058,
        "recall_weighted": 0.45542,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.45542,
        "hf_subset": "default",
        "languages": [
          "nob-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 58.98996424674988,
  "kg_co2_emissions": null
}