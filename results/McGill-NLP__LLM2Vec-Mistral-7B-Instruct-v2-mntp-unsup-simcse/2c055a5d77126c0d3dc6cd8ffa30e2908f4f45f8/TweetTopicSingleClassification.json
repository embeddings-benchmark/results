{
  "dataset_revision": "87b7a0d1c402dbb481db649569c556d9aa27ac05",
  "task_name": "TweetTopicSingleClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test_2021": [
      {
        "accuracy": 0.637685,
        "f1": 0.487128,
        "f1_weighted": 0.676778,
        "scores_per_experiment": [
          {
            "accuracy": 0.640284,
            "f1": 0.496378,
            "f1_weighted": 0.678474
          },
          {
            "accuracy": 0.622563,
            "f1": 0.490347,
            "f1_weighted": 0.666572
          },
          {
            "accuracy": 0.639102,
            "f1": 0.489886,
            "f1_weighted": 0.670301
          },
          {
            "accuracy": 0.620791,
            "f1": 0.493938,
            "f1_weighted": 0.660858
          },
          {
            "accuracy": 0.665092,
            "f1": 0.529779,
            "f1_weighted": 0.70892
          },
          {
            "accuracy": 0.669817,
            "f1": 0.505056,
            "f1_weighted": 0.701009
          },
          {
            "accuracy": 0.624926,
            "f1": 0.446224,
            "f1_weighted": 0.664185
          },
          {
            "accuracy": 0.647372,
            "f1": 0.488441,
            "f1_weighted": 0.693355
          },
          {
            "accuracy": 0.607206,
            "f1": 0.462793,
            "f1_weighted": 0.647385
          },
          {
            "accuracy": 0.639693,
            "f1": 0.468439,
            "f1_weighted": 0.676725
          }
        ],
        "main_score": 0.637685,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 21.866787433624268,
  "kg_co2_emissions": 0.0011915231971983255
}