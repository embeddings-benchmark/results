{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.755155,
        "f1": 0.637115,
        "f1_weighted": 0.789696,
        "ap": 0.260892,
        "ap_weighted": 0.260892,
        "scores_per_experiment": [
          {
            "accuracy": 0.690722,
            "f1": 0.583789,
            "f1_weighted": 0.741832,
            "ap": 0.216724,
            "ap_weighted": 0.216724
          },
          {
            "accuracy": 0.856529,
            "f1": 0.686697,
            "f1_weighted": 0.859502,
            "ap": 0.27375,
            "ap_weighted": 0.27375
          },
          {
            "accuracy": 0.729381,
            "f1": 0.619754,
            "f1_weighted": 0.772706,
            "ap": 0.24783,
            "ap_weighted": 0.24783
          },
          {
            "accuracy": 0.792955,
            "f1": 0.668618,
            "f1_weighted": 0.820683,
            "ap": 0.282809,
            "ap_weighted": 0.282809
          },
          {
            "accuracy": 0.620275,
            "f1": 0.54555,
            "f1_weighted": 0.683601,
            "ap": 0.21405,
            "ap_weighted": 0.21405
          },
          {
            "accuracy": 0.820447,
            "f1": 0.687633,
            "f1_weighted": 0.84022,
            "ap": 0.295217,
            "ap_weighted": 0.295217
          },
          {
            "accuracy": 0.786942,
            "f1": 0.656086,
            "f1_weighted": 0.815008,
            "ap": 0.264145,
            "ap_weighted": 0.264145
          },
          {
            "accuracy": 0.634021,
            "f1": 0.551212,
            "f1_weighted": 0.69563,
            "ap": 0.210441,
            "ap_weighted": 0.210441
          },
          {
            "accuracy": 0.801546,
            "f1": 0.671133,
            "f1_weighted": 0.826277,
            "ap": 0.279846,
            "ap_weighted": 0.279846
          },
          {
            "accuracy": 0.818729,
            "f1": 0.700678,
            "f1_weighted": 0.841499,
            "ap": 0.324112,
            "ap_weighted": 0.324112
          }
        ],
        "main_score": 0.755155,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 14.279989242553711,
  "kg_co2_emissions": 0.0008363298959907501
}