{
  "dataset_revision": "d743dcd5abb03d5ab357757a0e83522fc6696fcd",
  "task_name": "DanishPoliticalCommentsClassification",
  "mteb_version": "2.1.5",
  "scores": {
    "train": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.332778,
            "f1": 0.309323,
            "f1_weighted": 0.353857,
            "precision": 0.322617,
            "precision_weighted": 0.459719,
            "recall": 0.394234,
            "recall_weighted": 0.332778,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.360949,
            "f1": 0.32067,
            "f1_weighted": 0.377969,
            "precision": 0.321729,
            "precision_weighted": 0.445161,
            "recall": 0.373226,
            "recall_weighted": 0.360949,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.339994,
            "f1": 0.306255,
            "f1_weighted": 0.363206,
            "precision": 0.314405,
            "precision_weighted": 0.435843,
            "recall": 0.361142,
            "recall_weighted": 0.339994,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.379267,
            "f1": 0.33758,
            "f1_weighted": 0.397671,
            "precision": 0.333741,
            "precision_weighted": 0.455853,
            "recall": 0.411163,
            "recall_weighted": 0.379267,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.376075,
            "f1": 0.336439,
            "f1_weighted": 0.397115,
            "precision": 0.337177,
            "precision_weighted": 0.46636,
            "recall": 0.406167,
            "recall_weighted": 0.376075,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.295587,
            "f1": 0.278632,
            "f1_weighted": 0.302635,
            "precision": 0.278773,
            "precision_weighted": 0.380377,
            "recall": 0.382071,
            "recall_weighted": 0.295587,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.349986,
            "f1": 0.330507,
            "f1_weighted": 0.363715,
            "precision": 0.331655,
            "precision_weighted": 0.450366,
            "recall": 0.417628,
            "recall_weighted": 0.349986,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.35526,
            "f1": 0.322398,
            "f1_weighted": 0.376524,
            "precision": 0.329325,
            "precision_weighted": 0.464533,
            "recall": 0.395912,
            "recall_weighted": 0.35526,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.389398,
            "f1": 0.341648,
            "f1_weighted": 0.398566,
            "precision": 0.331357,
            "precision_weighted": 0.442397,
            "recall": 0.413969,
            "recall_weighted": 0.389398,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.386206,
            "f1": 0.350386,
            "f1_weighted": 0.40367,
            "precision": 0.346758,
            "precision_weighted": 0.467232,
            "recall": 0.420572,
            "recall_weighted": 0.386206,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.35655,
        "f1": 0.323384,
        "f1_weighted": 0.373493,
        "precision": 0.324754,
        "precision_weighted": 0.446784,
        "recall": 0.397608,
        "recall_weighted": 0.35655,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.35655,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 30.736281156539917,
  "kg_co2_emissions": null
}