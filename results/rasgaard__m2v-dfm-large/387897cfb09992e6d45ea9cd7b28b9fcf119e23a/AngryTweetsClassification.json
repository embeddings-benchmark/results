{
  "dataset_revision": "20b0e6081892e78179356fada741b7afa381443d",
  "task_name": "AngryTweetsClassification",
  "mteb_version": "2.1.5",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.534862,
            "f1": 0.533859,
            "f1_weighted": 0.536779,
            "precision": 0.539056,
            "precision_weighted": 0.541357,
            "recall": 0.531286,
            "recall_weighted": 0.534862,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.528176,
            "f1": 0.525068,
            "f1_weighted": 0.526189,
            "precision": 0.524643,
            "precision_weighted": 0.53079,
            "recall": 0.532946,
            "recall_weighted": 0.528176,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.505253,
            "f1": 0.494888,
            "f1_weighted": 0.500078,
            "precision": 0.493763,
            "precision_weighted": 0.498569,
            "recall": 0.499445,
            "recall_weighted": 0.505253,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.469914,
            "f1": 0.465536,
            "f1_weighted": 0.469224,
            "precision": 0.468909,
            "precision_weighted": 0.469923,
            "recall": 0.463768,
            "recall_weighted": 0.469914,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.543457,
            "f1": 0.532813,
            "f1_weighted": 0.540825,
            "precision": 0.536584,
            "precision_weighted": 0.540656,
            "recall": 0.531771,
            "recall_weighted": 0.543457,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.561605,
            "f1": 0.546773,
            "f1_weighted": 0.554885,
            "precision": 0.549417,
            "precision_weighted": 0.554844,
            "recall": 0.550241,
            "recall_weighted": 0.561605,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.51767,
            "f1": 0.516809,
            "f1_weighted": 0.520694,
            "precision": 0.523383,
            "precision_weighted": 0.534544,
            "recall": 0.521034,
            "recall_weighted": 0.51767,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.510029,
            "f1": 0.50814,
            "f1_weighted": 0.514377,
            "precision": 0.515103,
            "precision_weighted": 0.528336,
            "recall": 0.511217,
            "recall_weighted": 0.510029,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.535817,
            "f1": 0.534968,
            "f1_weighted": 0.539247,
            "precision": 0.541907,
            "precision_weighted": 0.550312,
            "recall": 0.534945,
            "recall_weighted": 0.535817,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.545368,
            "f1": 0.531516,
            "f1_weighted": 0.538351,
            "precision": 0.533335,
            "precision_weighted": 0.539777,
            "recall": 0.537632,
            "recall_weighted": 0.545368,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.525215,
        "f1": 0.519037,
        "f1_weighted": 0.524065,
        "precision": 0.52261,
        "precision_weighted": 0.528911,
        "recall": 0.521429,
        "recall_weighted": 0.525215,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.525215,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 20.379902839660645,
  "kg_co2_emissions": null
}