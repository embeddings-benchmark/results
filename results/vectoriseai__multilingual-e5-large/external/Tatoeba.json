{
    "dataset_revision": "9080400076fbadbb4c4dcb136ff4eddc40b42553",
    "task_name": "Tatoeba",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "sqi-eng",
                "languages": [
                    "sqi-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.959,
                "f1": 0.9469999999999998,
                "precision": 0.9411666666666667,
                "recall": 0.959,
                "main_score": 0.9469999999999998
            },
            {
                "hf_subset": "fry-eng",
                "languages": [
                    "fry-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.6820809248554913,
                "f1": 0.6343104872006606,
                "precision": 0.6169143958161298,
                "recall": 0.6820809248554913,
                "main_score": 0.6343104872006606
            },
            {
                "hf_subset": "kur-eng",
                "languages": [
                    "kur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7121951219512195,
                "f1": 0.6682926829268293,
                "precision": 0.651260162601626,
                "recall": 0.7121951219512195,
                "main_score": 0.6682926829268293
            },
            {
                "hf_subset": "tur-eng",
                "languages": [
                    "tur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.972,
                "f1": 0.9626666666666667,
                "precision": 0.958,
                "recall": 0.972,
                "main_score": 0.9626666666666667
            },
            {
                "hf_subset": "deu-eng",
                "languages": [
                    "deu-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.993,
                "f1": 0.9906666666666666,
                "precision": 0.9895,
                "recall": 0.993,
                "main_score": 0.9906666666666666
            },
            {
                "hf_subset": "nld-eng",
                "languages": [
                    "nld-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9739999999999999,
                "f1": 0.9663333333333333,
                "precision": 0.9626666666666668,
                "recall": 0.9739999999999999,
                "main_score": 0.9663333333333333
            },
            {
                "hf_subset": "ron-eng",
                "languages": [
                    "ron-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.96,
                "f1": 0.9486666666666665,
                "precision": 0.9431666666666668,
                "recall": 0.96,
                "main_score": 0.9486666666666665
            },
            {
                "hf_subset": "ang-eng",
                "languages": [
                    "ang-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.4701492537313433,
                "f1": 0.4017886756692727,
                "precision": 0.38179295828549553,
                "recall": 0.4701492537313433,
                "main_score": 0.4017886756692727
            },
            {
                "hf_subset": "ido-eng",
                "languages": [
                    "ido-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.865,
                "f1": 0.8362537480063795,
                "precision": 0.8244555555555554,
                "recall": 0.865,
                "main_score": 0.8362537480063795
            },
            {
                "hf_subset": "jav-eng",
                "languages": [
                    "jav-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.804878048780488,
                "f1": 0.7545644599303138,
                "precision": 0.7337398373983739,
                "recall": 0.804878048780488,
                "main_score": 0.7545644599303138
            },
            {
                "hf_subset": "isl-eng",
                "languages": [
                    "isl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.937,
                "f1": 0.9195666666666666,
                "precision": 0.91125,
                "recall": 0.937,
                "main_score": 0.9195666666666666
            },
            {
                "hf_subset": "slv-eng",
                "languages": [
                    "slv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9173754556500607,
                "f1": 0.8965168084244632,
                "precision": 0.8873025516403402,
                "recall": 0.9173754556500607,
                "main_score": 0.8965168084244632
            },
            {
                "hf_subset": "cym-eng",
                "languages": [
                    "cym-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8104347826086956,
                "f1": 0.762128364389234,
                "precision": 0.742,
                "recall": 0.8104347826086956,
                "main_score": 0.762128364389234
            },
            {
                "hf_subset": "kaz-eng",
                "languages": [
                    "kaz-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.8365217391304348,
                "f1": 0.7943768115942029,
                "precision": 0.7765797101449274,
                "recall": 0.8365217391304348,
                "main_score": 0.7943768115942029
            },
            {
                "hf_subset": "est-eng",
                "languages": [
                    "est-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.875,
                "f1": 0.8502690476190476,
                "precision": 0.8396261904761905,
                "recall": 0.875,
                "main_score": 0.8502690476190476
            },
            {
                "hf_subset": "heb-eng",
                "languages": [
                    "heb-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.893,
                "f1": 0.8652333333333333,
                "precision": 0.8522833333333333,
                "recall": 0.893,
                "main_score": 0.8652333333333333
            },
            {
                "hf_subset": "gla-eng",
                "languages": [
                    "gla-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.6501809408926418,
                "f1": 0.5900594446432805,
                "precision": 0.5682721580791544,
                "recall": 0.6501809408926418,
                "main_score": 0.5900594446432805
            },
            {
                "hf_subset": "mar-eng",
                "languages": [
                    "mar-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.912,
                "f1": 0.8858,
                "precision": 0.8733333333333334,
                "recall": 0.912,
                "main_score": 0.8858
            },
            {
                "hf_subset": "lat-eng",
                "languages": [
                    "lat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.592,
                "f1": 0.5329916627628491,
                "precision": 0.513383908045977,
                "recall": 0.592,
                "main_score": 0.5329916627628491
            },
            {
                "hf_subset": "bel-eng",
                "languages": [
                    "bel-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.932,
                "f1": 0.912,
                "precision": 0.9025,
                "recall": 0.932,
                "main_score": 0.912
            },
            {
                "hf_subset": "pms-eng",
                "languages": [
                    "pms-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.6476190476190475,
                "f1": 0.5986711066711067,
                "precision": 0.5807390192653351,
                "recall": 0.6476190476190475,
                "main_score": 0.5986711066711067
            },
            {
                "hf_subset": "gle-eng",
                "languages": [
                    "gle-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.762,
                "f1": 0.7148147546897547,
                "precision": 0.6965409090909092,
                "recall": 0.762,
                "main_score": 0.7148147546897547
            },
            {
                "hf_subset": "pes-eng",
                "languages": [
                    "pes-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.938,
                "f1": 0.9214,
                "precision": 0.9135833333333333,
                "recall": 0.938,
                "main_score": 0.9214
            },
            {
                "hf_subset": "nob-eng",
                "languages": [
                    "nob-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9789999999999999,
                "f1": 0.972,
                "precision": 0.9685000000000001,
                "recall": 0.9789999999999999,
                "main_score": 0.972
            },
            {
                "hf_subset": "bul-eng",
                "languages": [
                    "bul-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.946,
                "f1": 0.9293333333333333,
                "precision": 0.9213333333333332,
                "recall": 0.946,
                "main_score": 0.9293333333333333
            },
            {
                "hf_subset": "cbk-eng",
                "languages": [
                    "cbk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.741,
                "f1": 0.6914817460317461,
                "precision": 0.6725158730158731,
                "recall": 0.741,
                "main_score": 0.6914817460317461
            },
            {
                "hf_subset": "hun-eng",
                "languages": [
                    "hun-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9519999999999998,
                "f1": 0.9401333333333335,
                "precision": 0.9346666666666666,
                "recall": 0.9519999999999998,
                "main_score": 0.9401333333333335
            },
            {
                "hf_subset": "uig-eng",
                "languages": [
                    "uig-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.769,
                "f1": 0.7207523809523809,
                "precision": 0.7019777777777779,
                "recall": 0.769,
                "main_score": 0.7207523809523809
            },
            {
                "hf_subset": "rus-eng",
                "languages": [
                    "rus-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.941,
                "f1": 0.9231666666666666,
                "precision": 0.9143333333333332,
                "recall": 0.941,
                "main_score": 0.9231666666666666
            },
            {
                "hf_subset": "spa-eng",
                "languages": [
                    "spa-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.978,
                "f1": 0.971,
                "precision": 0.9676666666666668,
                "recall": 0.978,
                "main_score": 0.971
            },
            {
                "hf_subset": "hye-eng",
                "languages": [
                    "hye-Armn",
                    "eng-Latn"
                ],
                "accuracy": 0.9285714285714286,
                "f1": 0.9092093441150045,
                "precision": 0.9000449236298294,
                "recall": 0.9285714285714286,
                "main_score": 0.9092093441150045
            },
            {
                "hf_subset": "tel-eng",
                "languages": [
                    "tel-Telu",
                    "eng-Latn"
                ],
                "accuracy": 0.9316239316239315,
                "f1": 0.9133903133903132,
                "precision": 0.9056267806267806,
                "recall": 0.9316239316239315,
                "main_score": 0.9133903133903132
            },
            {
                "hf_subset": "afr-eng",
                "languages": [
                    "afr-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.924,
                "f1": 0.9025666666666666,
                "precision": 0.8925833333333334,
                "recall": 0.924,
                "main_score": 0.9025666666666666
            },
            {
                "hf_subset": "mon-eng",
                "languages": [
                    "mon-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.9022727272727272,
                "f1": 0.8753030303030304,
                "precision": 0.8637121212121212,
                "recall": 0.9022727272727272,
                "main_score": 0.8753030303030304
            },
            {
                "hf_subset": "arz-eng",
                "languages": [
                    "arz-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.790356394129979,
                "f1": 0.747349505840072,
                "precision": 0.729035639412998,
                "recall": 0.790356394129979,
                "main_score": 0.747349505840072
            },
            {
                "hf_subset": "hrv-eng",
                "languages": [
                    "hrv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.97,
                "f1": 0.9615,
                "precision": 0.9576666666666668,
                "recall": 0.97,
                "main_score": 0.9615
            },
            {
                "hf_subset": "nov-eng",
                "languages": [
                    "nov-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7626459143968871,
                "f1": 0.7155642023346303,
                "precision": 0.6975449323698351,
                "recall": 0.7626459143968871,
                "main_score": 0.7155642023346303
            },
            {
                "hf_subset": "gsw-eng",
                "languages": [
                    "gsw-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.5811965811965812,
                "f1": 0.5165242165242165,
                "precision": 0.4941768108434775,
                "recall": 0.5811965811965812,
                "main_score": 0.5165242165242165
            },
            {
                "hf_subset": "nds-eng",
                "languages": [
                    "nds-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.743,
                "f1": 0.6952055555555554,
                "precision": 0.677574938949939,
                "recall": 0.743,
                "main_score": 0.6952055555555554
            },
            {
                "hf_subset": "ukr-eng",
                "languages": [
                    "ukr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.948,
                "f1": 0.9331666666666666,
                "precision": 0.926,
                "recall": 0.948,
                "main_score": 0.9331666666666666
            },
            {
                "hf_subset": "uzb-eng",
                "languages": [
                    "uzb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7663551401869158,
                "f1": 0.7235202492211837,
                "precision": 0.7060358255451713,
                "recall": 0.7663551401869158,
                "main_score": 0.7235202492211837
            },
            {
                "hf_subset": "lit-eng",
                "languages": [
                    "lit-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.904,
                "f1": 0.884811111111111,
                "precision": 0.877452380952381,
                "recall": 0.904,
                "main_score": 0.884811111111111
            },
            {
                "hf_subset": "ina-eng",
                "languages": [
                    "ina-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.95,
                "f1": 0.9360666666666667,
                "precision": 0.92975,
                "recall": 0.95,
                "main_score": 0.9360666666666667
            },
            {
                "hf_subset": "lfn-eng",
                "languages": [
                    "lfn-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.672,
                "f1": 0.6301595782872099,
                "precision": 0.615965873015873,
                "recall": 0.672,
                "main_score": 0.6301595782872099
            },
            {
                "hf_subset": "zsm-eng",
                "languages": [
                    "zsm-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9570000000000001,
                "f1": 0.9452999999999999,
                "precision": 0.94,
                "recall": 0.9570000000000001,
                "main_score": 0.9452999999999999
            },
            {
                "hf_subset": "ita-eng",
                "languages": [
                    "ita-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.946,
                "f1": 0.9329,
                "precision": 0.92675,
                "recall": 0.946,
                "main_score": 0.9329
            },
            {
                "hf_subset": "cmn-eng",
                "languages": [
                    "cmn-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.964,
                "f1": 0.9528333333333333,
                "precision": 0.9475,
                "recall": 0.964,
                "main_score": 0.9528333333333333
            },
            {
                "hf_subset": "lvs-eng",
                "languages": [
                    "lvs-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.919,
                "f1": 0.8983,
                "precision": 0.8892,
                "recall": 0.919,
                "main_score": 0.8983
            },
            {
                "hf_subset": "glg-eng",
                "languages": [
                    "glg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9469999999999998,
                "f1": 0.9334222222222224,
                "precision": 0.9275416666666668,
                "recall": 0.9469999999999998,
                "main_score": 0.9334222222222224
            },
            {
                "hf_subset": "ceb-eng",
                "languages": [
                    "ceb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.6033333333333334,
                "f1": 0.5531203703703703,
                "precision": 0.5339971108326371,
                "recall": 0.6033333333333334,
                "main_score": 0.5531203703703703
            },
            {
                "hf_subset": "bre-eng",
                "languages": [
                    "bre-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.129,
                "f1": 0.11099861903031458,
                "precision": 0.10589187932631877,
                "recall": 0.129,
                "main_score": 0.11099861903031458
            },
            {
                "hf_subset": "ben-eng",
                "languages": [
                    "ben-Beng",
                    "eng-Latn"
                ],
                "accuracy": 0.867,
                "f1": 0.8301523809523811,
                "precision": 0.8137833333333333,
                "recall": 0.867,
                "main_score": 0.8301523809523811
            },
            {
                "hf_subset": "swg-eng",
                "languages": [
                    "swg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.6339285714285714,
                "f1": 0.5683248299319728,
                "precision": 0.5456845238095237,
                "recall": 0.6339285714285714,
                "main_score": 0.5683248299319728
            },
            {
                "hf_subset": "arq-eng",
                "languages": [
                    "arq-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.4873765093304062,
                "f1": 0.41555736920720454,
                "precision": 0.39068745317373194,
                "recall": 0.4873765093304062,
                "main_score": 0.41555736920720454
            },
            {
                "hf_subset": "kab-eng",
                "languages": [
                    "kab-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.4109999999999999,
                "f1": 0.3654016594516595,
                "precision": 0.3505175685425686,
                "recall": 0.4109999999999999,
                "main_score": 0.3654016594516595
            },
            {
                "hf_subset": "fra-eng",
                "languages": [
                    "fra-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.949,
                "f1": 0.9342333333333334,
                "precision": 0.9275833333333332,
                "recall": 0.949,
                "main_score": 0.9342333333333334
            },
            {
                "hf_subset": "por-eng",
                "languages": [
                    "por-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.949,
                "f1": 0.9363333333333334,
                "precision": 0.9301666666666665,
                "recall": 0.949,
                "main_score": 0.9363333333333334
            },
            {
                "hf_subset": "tat-eng",
                "languages": [
                    "tat-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.779,
                "f1": 0.7364833333333334,
                "precision": 0.7190282106782105,
                "recall": 0.779,
                "main_score": 0.7364833333333334
            },
            {
                "hf_subset": "oci-eng",
                "languages": [
                    "oci-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.594,
                "f1": 0.5490521367521367,
                "precision": 0.5343284002547161,
                "recall": 0.594,
                "main_score": 0.5490521367521367
            },
            {
                "hf_subset": "pol-eng",
                "languages": [
                    "pol-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9739999999999999,
                "f1": 0.966,
                "precision": 0.9620000000000001,
                "recall": 0.9739999999999999,
                "main_score": 0.966
            },
            {
                "hf_subset": "war-eng",
                "languages": [
                    "war-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.672,
                "f1": 0.6225926129426129,
                "precision": 0.6040837662337663,
                "recall": 0.672,
                "main_score": 0.6225926129426129
            },
            {
                "hf_subset": "aze-eng",
                "languages": [
                    "aze-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.902,
                "f1": 0.8760666666666667,
                "precision": 0.8645277777777778,
                "recall": 0.902,
                "main_score": 0.8760666666666667
            },
            {
                "hf_subset": "vie-eng",
                "languages": [
                    "vie-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.977,
                "f1": 0.97,
                "precision": 0.9665,
                "recall": 0.977,
                "main_score": 0.97
            },
            {
                "hf_subset": "nno-eng",
                "languages": [
                    "nno-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.932,
                "f1": 0.9139746031746031,
                "precision": 0.906125,
                "recall": 0.932,
                "main_score": 0.9139746031746031
            },
            {
                "hf_subset": "cha-eng",
                "languages": [
                    "cha-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.32116788321167883,
                "f1": 0.27210415386260234,
                "precision": 0.2620408990846947,
                "recall": 0.32116788321167883,
                "main_score": 0.27210415386260234
            },
            {
                "hf_subset": "mhr-eng",
                "languages": [
                    "mhr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.085,
                "f1": 0.06787319277832475,
                "precision": 0.06345209443334443,
                "recall": 0.085,
                "main_score": 0.06787319277832475
            },
            {
                "hf_subset": "dan-eng",
                "languages": [
                    "dan-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.961,
                "f1": 0.9508,
                "precision": 0.9461666666666667,
                "recall": 0.961,
                "main_score": 0.9508
            },
            {
                "hf_subset": "ell-eng",
                "languages": [
                    "ell-Grek",
                    "eng-Latn"
                ],
                "accuracy": 0.953,
                "f1": 0.9388333333333333,
                "precision": 0.9318333333333332,
                "recall": 0.953,
                "main_score": 0.9388333333333333
            },
            {
                "hf_subset": "amh-eng",
                "languages": [
                    "amh-Ethi",
                    "eng-Latn"
                ],
                "accuracy": 0.8511904761904762,
                "f1": 0.8069444444444445,
                "precision": 0.7872023809523809,
                "recall": 0.8511904761904762,
                "main_score": 0.8069444444444445
            },
            {
                "hf_subset": "pam-eng",
                "languages": [
                    "pam-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.111,
                "f1": 0.09276381801735853,
                "precision": 0.08798174603174601,
                "recall": 0.111,
                "main_score": 0.09276381801735853
            },
            {
                "hf_subset": "hsb-eng",
                "languages": [
                    "hsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.6356107660455487,
                "f1": 0.5870433569191332,
                "precision": 0.5689692658146401,
                "recall": 0.6356107660455487,
                "main_score": 0.5870433569191332
            },
            {
                "hf_subset": "srp-eng",
                "languages": [
                    "srp-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.9469999999999998,
                "f1": 0.931,
                "precision": 0.9235,
                "recall": 0.9469999999999998,
                "main_score": 0.931
            },
            {
                "hf_subset": "epo-eng",
                "languages": [
                    "epo-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.968,
                "f1": 0.9601222222222222,
                "precision": 0.9567083333333332,
                "recall": 0.968,
                "main_score": 0.9601222222222222
            },
            {
                "hf_subset": "kzj-eng",
                "languages": [
                    "kzj-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.092,
                "f1": 0.0791155525030525,
                "precision": 0.07631246556216846,
                "recall": 0.092,
                "main_score": 0.0791155525030525
            },
            {
                "hf_subset": "awa-eng",
                "languages": [
                    "awa-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.7748917748917747,
                "f1": 0.7227375798804371,
                "precision": 0.7014430014430013,
                "recall": 0.7748917748917747,
                "main_score": 0.7227375798804371
            },
            {
                "hf_subset": "fao-eng",
                "languages": [
                    "fao-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7709923664122137,
                "f1": 0.7261541257724463,
                "precision": 0.708998380754106,
                "recall": 0.7709923664122137,
                "main_score": 0.7261541257724463
            },
            {
                "hf_subset": "mal-eng",
                "languages": [
                    "mal-Mlym",
                    "eng-Latn"
                ],
                "accuracy": 0.982532751091703,
                "f1": 0.9769529354682193,
                "precision": 0.9742843279961184,
                "recall": 0.982532751091703,
                "main_score": 0.9769529354682193
            },
            {
                "hf_subset": "ile-eng",
                "languages": [
                    "ile-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.828,
                "f1": 0.7914672619047619,
                "precision": 0.7759489247311828,
                "recall": 0.828,
                "main_score": 0.7914672619047619
            },
            {
                "hf_subset": "bos-eng",
                "languages": [
                    "bos-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9435028248587571,
                "f1": 0.9286252354048965,
                "precision": 0.9220809792843689,
                "recall": 0.9435028248587571,
                "main_score": 0.9286252354048965
            },
            {
                "hf_subset": "cor-eng",
                "languages": [
                    "cor-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.085,
                "f1": 0.06282429263935621,
                "precision": 0.05783274240739785,
                "recall": 0.085,
                "main_score": 0.06282429263935621
            },
            {
                "hf_subset": "cat-eng",
                "languages": [
                    "cat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.927,
                "f1": 0.91025,
                "precision": 0.9030428571428571,
                "recall": 0.927,
                "main_score": 0.91025
            },
            {
                "hf_subset": "eus-eng",
                "languages": [
                    "eus-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.81,
                "f1": 0.778232380952381,
                "precision": 0.7660194444444444,
                "recall": 0.81,
                "main_score": 0.778232380952381
            },
            {
                "hf_subset": "yue-eng",
                "languages": [
                    "yue-Hant",
                    "eng-Latn"
                ],
                "accuracy": 0.91,
                "f1": 0.8870857142857141,
                "precision": 0.877,
                "recall": 0.91,
                "main_score": 0.8870857142857141
            },
            {
                "hf_subset": "swe-eng",
                "languages": [
                    "swe-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.964,
                "f1": 0.953,
                "precision": 0.9476666666666667,
                "recall": 0.964,
                "main_score": 0.953
            },
            {
                "hf_subset": "dtp-eng",
                "languages": [
                    "dtp-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.081,
                "f1": 0.07001008218834306,
                "precision": 0.06708329562594269,
                "recall": 0.081,
                "main_score": 0.07001008218834306
            },
            {
                "hf_subset": "kat-eng",
                "languages": [
                    "kat-Geor",
                    "eng-Latn"
                ],
                "accuracy": 0.871313672922252,
                "f1": 0.8409070598748882,
                "precision": 0.8279171454104428,
                "recall": 0.871313672922252,
                "main_score": 0.8409070598748882
            },
            {
                "hf_subset": "jpn-eng",
                "languages": [
                    "jpn-Jpan",
                    "eng-Latn"
                ],
                "accuracy": 0.964,
                "f1": 0.9528333333333333,
                "precision": 0.9473333333333332,
                "recall": 0.964,
                "main_score": 0.9528333333333333
            },
            {
                "hf_subset": "csb-eng",
                "languages": [
                    "csb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.42292490118577075,
                "f1": 0.36981018542283367,
                "precision": 0.35415877813576024,
                "recall": 0.42292490118577075,
                "main_score": 0.36981018542283367
            },
            {
                "hf_subset": "xho-eng",
                "languages": [
                    "xho-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8380281690140845,
                "f1": 0.8086854460093896,
                "precision": 0.7960093896713615,
                "recall": 0.8380281690140845,
                "main_score": 0.8086854460093896
            },
            {
                "hf_subset": "orv-eng",
                "languages": [
                    "orv-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.45269461077844314,
                "f1": 0.3980235464678088,
                "precision": 0.3814342660001342,
                "recall": 0.45269461077844314,
                "main_score": 0.3980235464678088
            },
            {
                "hf_subset": "ind-eng",
                "languages": [
                    "ind-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.943,
                "f1": 0.929,
                "precision": 0.9226666666666667,
                "recall": 0.943,
                "main_score": 0.929
            },
            {
                "hf_subset": "tuk-eng",
                "languages": [
                    "tuk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.3793103448275862,
                "f1": 0.33151927437641715,
                "precision": 0.3157456528146183,
                "recall": 0.3793103448275862,
                "main_score": 0.33151927437641715
            },
            {
                "hf_subset": "max-eng",
                "languages": [
                    "max-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.6901408450704225,
                "f1": 0.6341549295774648,
                "precision": 0.6134277889559581,
                "recall": 0.6901408450704225,
                "main_score": 0.6341549295774648
            },
            {
                "hf_subset": "swh-eng",
                "languages": [
                    "swh-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7666666666666667,
                "f1": 0.7160705960705961,
                "precision": 0.6960683760683761,
                "recall": 0.7666666666666667,
                "main_score": 0.7160705960705961
            },
            {
                "hf_subset": "hin-eng",
                "languages": [
                    "hin-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.958,
                "f1": 0.9448333333333333,
                "precision": 0.9383333333333332,
                "recall": 0.958,
                "main_score": 0.9448333333333333
            },
            {
                "hf_subset": "dsb-eng",
                "languages": [
                    "dsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.5281837160751566,
                "f1": 0.4843597773138482,
                "precision": 0.4711291973845539,
                "recall": 0.5281837160751566,
                "main_score": 0.4843597773138482
            },
            {
                "hf_subset": "ber-eng",
                "languages": [
                    "ber-Tfng",
                    "eng-Latn"
                ],
                "accuracy": 0.449,
                "f1": 0.38889626216077827,
                "precision": 0.3695936507936508,
                "recall": 0.449,
                "main_score": 0.38889626216077827
            },
            {
                "hf_subset": "tam-eng",
                "languages": [
                    "tam-Taml",
                    "eng-Latn"
                ],
                "accuracy": 0.9055374592833876,
                "f1": 0.8822553125484721,
                "precision": 0.8726927252985884,
                "recall": 0.9055374592833876,
                "main_score": 0.8822553125484721
            },
            {
                "hf_subset": "slk-eng",
                "languages": [
                    "slk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.946,
                "f1": 0.9313333333333332,
                "precision": 0.9245333333333333,
                "recall": 0.946,
                "main_score": 0.9313333333333332
            },
            {
                "hf_subset": "tgl-eng",
                "languages": [
                    "tgl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.937,
                "f1": 0.9199666666666667,
                "precision": 0.9126666666666668,
                "recall": 0.937,
                "main_score": 0.9199666666666667
            },
            {
                "hf_subset": "ast-eng",
                "languages": [
                    "ast-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8503937007874016,
                "f1": 0.8175853018372703,
                "precision": 0.8034120734908137,
                "recall": 0.8503937007874016,
                "main_score": 0.8175853018372703
            },
            {
                "hf_subset": "mkd-eng",
                "languages": [
                    "mkd-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.883,
                "f1": 0.855,
                "precision": 0.8425833333333334,
                "recall": 0.883,
                "main_score": 0.855
            },
            {
                "hf_subset": "khm-eng",
                "languages": [
                    "khm-Khmr",
                    "eng-Latn"
                ],
                "accuracy": 0.6551246537396122,
                "f1": 0.6002297410192148,
                "precision": 0.5813346772728923,
                "recall": 0.6551246537396122,
                "main_score": 0.6002297410192148
            },
            {
                "hf_subset": "ces-eng",
                "languages": [
                    "ces-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.96,
                "f1": 0.9489,
                "precision": 0.9439166666666666,
                "recall": 0.96,
                "main_score": 0.9489
            },
            {
                "hf_subset": "tzl-eng",
                "languages": [
                    "tzl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.5769230769230769,
                "f1": 0.5316239316239316,
                "precision": 0.5170673076923077,
                "recall": 0.5769230769230769,
                "main_score": 0.5316239316239316
            },
            {
                "hf_subset": "urd-eng",
                "languages": [
                    "urd-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.916,
                "f1": 0.8921190476190475,
                "precision": 0.8808666666666667,
                "recall": 0.916,
                "main_score": 0.8921190476190475
            },
            {
                "hf_subset": "ara-eng",
                "languages": [
                    "ara-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.88,
                "f1": 0.8547,
                "precision": 0.8443266233766233,
                "recall": 0.88,
                "main_score": 0.8547
            },
            {
                "hf_subset": "kor-eng",
                "languages": [
                    "kor-Hang",
                    "eng-Latn"
                ],
                "accuracy": 0.927,
                "f1": 0.9064999999999999,
                "precision": 0.8968333333333333,
                "recall": 0.927,
                "main_score": 0.9064999999999999
            },
            {
                "hf_subset": "yid-eng",
                "languages": [
                    "yid-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.8030660377358491,
                "f1": 0.7633044137466307,
                "precision": 0.7478970125786164,
                "recall": 0.8030660377358491,
                "main_score": 0.7633044137466307
            },
            {
                "hf_subset": "fin-eng",
                "languages": [
                    "fin-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.964,
                "f1": 0.9544,
                "precision": 0.9499166666666666,
                "recall": 0.964,
                "main_score": 0.9544
            },
            {
                "hf_subset": "tha-eng",
                "languages": [
                    "tha-Thai",
                    "eng-Latn"
                ],
                "accuracy": 0.9653284671532848,
                "f1": 0.953771289537713,
                "precision": 0.947992700729927,
                "recall": 0.9653284671532848,
                "main_score": 0.953771289537713
            },
            {
                "hf_subset": "wuu-eng",
                "languages": [
                    "wuu-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.89,
                "f1": 0.8623190476190475,
                "precision": 0.8503499999999999,
                "recall": 0.89,
                "main_score": 0.8623190476190475
            }
        ]
    }
}