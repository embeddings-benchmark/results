{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "task_name": "MassiveScenarioClassification",
  "mteb_version": "1.34.14",
  "scores": {
    "validation": [
      {
        "accuracy": 0.787555,
        "f1": 0.777712,
        "f1_weighted": 0.789382,
        "scores_per_experiment": [
          {
            "accuracy": 0.798819,
            "f1": 0.78773,
            "f1_weighted": 0.799537
          },
          {
            "accuracy": 0.78062,
            "f1": 0.770543,
            "f1_weighted": 0.784636
          },
          {
            "accuracy": 0.783079,
            "f1": 0.774036,
            "f1_weighted": 0.785975
          },
          {
            "accuracy": 0.772258,
            "f1": 0.764711,
            "f1_weighted": 0.774404
          },
          {
            "accuracy": 0.807182,
            "f1": 0.798513,
            "f1_weighted": 0.803036
          },
          {
            "accuracy": 0.785539,
            "f1": 0.772223,
            "f1_weighted": 0.783169
          },
          {
            "accuracy": 0.783079,
            "f1": 0.773304,
            "f1_weighted": 0.787149
          },
          {
            "accuracy": 0.772258,
            "f1": 0.764159,
            "f1_weighted": 0.777504
          },
          {
            "accuracy": 0.810625,
            "f1": 0.796755,
            "f1_weighted": 0.813029
          },
          {
            "accuracy": 0.782095,
            "f1": 0.775149,
            "f1_weighted": 0.785381
          }
        ],
        "main_score": 0.787555,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.785205,
        "f1": 0.77557,
        "f1_weighted": 0.787567,
        "scores_per_experiment": [
          {
            "accuracy": 0.796907,
            "f1": 0.78533,
            "f1_weighted": 0.799133
          },
          {
            "accuracy": 0.775387,
            "f1": 0.76627,
            "f1_weighted": 0.779082
          },
          {
            "accuracy": 0.784465,
            "f1": 0.779709,
            "f1_weighted": 0.786886
          },
          {
            "accuracy": 0.777068,
            "f1": 0.769127,
            "f1_weighted": 0.781153
          },
          {
            "accuracy": 0.79926,
            "f1": 0.787342,
            "f1_weighted": 0.796249
          },
          {
            "accuracy": 0.782784,
            "f1": 0.769395,
            "f1_weighted": 0.779415
          },
          {
            "accuracy": 0.777404,
            "f1": 0.762883,
            "f1_weighted": 0.784066
          },
          {
            "accuracy": 0.762609,
            "f1": 0.760936,
            "f1_weighted": 0.769758
          },
          {
            "accuracy": 0.805985,
            "f1": 0.794163,
            "f1_weighted": 0.807776
          },
          {
            "accuracy": 0.790182,
            "f1": 0.780549,
            "f1_weighted": 0.792153
          }
        ],
        "main_score": 0.785205,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 4.32363224029541,
  "kg_co2_emissions": null
}