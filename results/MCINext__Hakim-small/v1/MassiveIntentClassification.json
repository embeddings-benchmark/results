{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.34.14",
  "scores": {
    "validation": [
      {
        "accuracy": 0.679636,
        "f1": 0.633681,
        "f1_weighted": 0.682074,
        "scores_per_experiment": [
          {
            "accuracy": 0.684702,
            "f1": 0.647554,
            "f1_weighted": 0.687783
          },
          {
            "accuracy": 0.697491,
            "f1": 0.638073,
            "f1_weighted": 0.698482
          },
          {
            "accuracy": 0.654206,
            "f1": 0.610303,
            "f1_weighted": 0.654624
          },
          {
            "accuracy": 0.701918,
            "f1": 0.645869,
            "f1_weighted": 0.703641
          },
          {
            "accuracy": 0.701426,
            "f1": 0.659295,
            "f1_weighted": 0.701868
          },
          {
            "accuracy": 0.678308,
            "f1": 0.630696,
            "f1_weighted": 0.676369
          },
          {
            "accuracy": 0.689621,
            "f1": 0.651504,
            "f1_weighted": 0.696035
          },
          {
            "accuracy": 0.657649,
            "f1": 0.61077,
            "f1_weighted": 0.659478
          },
          {
            "accuracy": 0.6606,
            "f1": 0.618906,
            "f1_weighted": 0.669267
          },
          {
            "accuracy": 0.670438,
            "f1": 0.623841,
            "f1_weighted": 0.673189
          }
        ],
        "main_score": 0.679636,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.675151,
        "f1": 0.635283,
        "f1_weighted": 0.679077,
        "scores_per_experiment": [
          {
            "accuracy": 0.68729,
            "f1": 0.648632,
            "f1_weighted": 0.692593
          },
          {
            "accuracy": 0.695696,
            "f1": 0.641315,
            "f1_weighted": 0.70077
          },
          {
            "accuracy": 0.669132,
            "f1": 0.636193,
            "f1_weighted": 0.667285
          },
          {
            "accuracy": 0.698722,
            "f1": 0.649974,
            "f1_weighted": 0.702966
          },
          {
            "accuracy": 0.6846,
            "f1": 0.646216,
            "f1_weighted": 0.685285
          },
          {
            "accuracy": 0.66577,
            "f1": 0.623162,
            "f1_weighted": 0.663922
          },
          {
            "accuracy": 0.682246,
            "f1": 0.639577,
            "f1_weighted": 0.689127
          },
          {
            "accuracy": 0.658036,
            "f1": 0.617508,
            "f1_weighted": 0.662308
          },
          {
            "accuracy": 0.650975,
            "f1": 0.6204,
            "f1_weighted": 0.660322
          },
          {
            "accuracy": 0.659045,
            "f1": 0.629856,
            "f1_weighted": 0.666189
          }
        ],
        "main_score": 0.675151,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 11.327388525009155,
  "kg_co2_emissions": null
}