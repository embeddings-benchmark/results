{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.38.27",
  "scores": {
    "validation": [
      {
        "accuracy": 0.640039,
        "f1": 0.564109,
        "f1_weighted": 0.622877,
        "scores_per_experiment": [
          {
            "accuracy": 0.643384,
            "f1": 0.557497,
            "f1_weighted": 0.62083
          },
          {
            "accuracy": 0.677324,
            "f1": 0.584001,
            "f1_weighted": 0.666718
          },
          {
            "accuracy": 0.637482,
            "f1": 0.565261,
            "f1_weighted": 0.611172
          },
          {
            "accuracy": 0.651254,
            "f1": 0.567987,
            "f1_weighted": 0.637985
          },
          {
            "accuracy": 0.653714,
            "f1": 0.554475,
            "f1_weighted": 0.636846
          },
          {
            "accuracy": 0.627644,
            "f1": 0.57,
            "f1_weighted": 0.615284
          },
          {
            "accuracy": 0.653222,
            "f1": 0.588707,
            "f1_weighted": 0.636024
          },
          {
            "accuracy": 0.619774,
            "f1": 0.535186,
            "f1_weighted": 0.599803
          },
          {
            "accuracy": 0.613379,
            "f1": 0.547507,
            "f1_weighted": 0.588804
          },
          {
            "accuracy": 0.623217,
            "f1": 0.570469,
            "f1_weighted": 0.615308
          }
        ],
        "main_score": 0.640039,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.639038,
        "f1": 0.588124,
        "f1_weighted": 0.621806,
        "scores_per_experiment": [
          {
            "accuracy": 0.656355,
            "f1": 0.590467,
            "f1_weighted": 0.636625
          },
          {
            "accuracy": 0.676866,
            "f1": 0.606185,
            "f1_weighted": 0.665879
          },
          {
            "accuracy": 0.62542,
            "f1": 0.588414,
            "f1_weighted": 0.601406
          },
          {
            "accuracy": 0.65501,
            "f1": 0.593845,
            "f1_weighted": 0.635948
          },
          {
            "accuracy": 0.635171,
            "f1": 0.565259,
            "f1_weighted": 0.615794
          },
          {
            "accuracy": 0.629455,
            "f1": 0.594729,
            "f1_weighted": 0.622283
          },
          {
            "accuracy": 0.658373,
            "f1": 0.60579,
            "f1_weighted": 0.639071
          },
          {
            "accuracy": 0.628447,
            "f1": 0.576503,
            "f1_weighted": 0.609849
          },
          {
            "accuracy": 0.611298,
            "f1": 0.576872,
            "f1_weighted": 0.585291
          },
          {
            "accuracy": 0.613988,
            "f1": 0.583178,
            "f1_weighted": 0.605909
          }
        ],
        "main_score": 0.639038,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 8.38645076751709,
  "kg_co2_emissions": null
}