{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "task_name": "MTOPDomainClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.622314,
        "f1": 0.606493,
        "f1_weighted": 0.626374,
        "scores_per_experiment": [
          {
            "accuracy": 0.653444,
            "f1": 0.634748,
            "f1_weighted": 0.655518
          },
          {
            "accuracy": 0.607713,
            "f1": 0.598231,
            "f1_weighted": 0.611119
          },
          {
            "accuracy": 0.60551,
            "f1": 0.586799,
            "f1_weighted": 0.605704
          },
          {
            "accuracy": 0.624242,
            "f1": 0.608473,
            "f1_weighted": 0.627599
          },
          {
            "accuracy": 0.619284,
            "f1": 0.600935,
            "f1_weighted": 0.625395
          },
          {
            "accuracy": 0.606061,
            "f1": 0.581365,
            "f1_weighted": 0.607066
          },
          {
            "accuracy": 0.621488,
            "f1": 0.598906,
            "f1_weighted": 0.628283
          },
          {
            "accuracy": 0.620386,
            "f1": 0.614521,
            "f1_weighted": 0.630837
          },
          {
            "accuracy": 0.634711,
            "f1": 0.627142,
            "f1_weighted": 0.643679
          },
          {
            "accuracy": 0.630303,
            "f1": 0.61381,
            "f1_weighted": 0.628544
          }
        ],
        "main_score": 0.622314,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.638321,
        "f1": 0.615872,
        "f1_weighted": 0.642559,
        "scores_per_experiment": [
          {
            "accuracy": 0.669766,
            "f1": 0.647226,
            "f1_weighted": 0.673376
          },
          {
            "accuracy": 0.607213,
            "f1": 0.588516,
            "f1_weighted": 0.610094
          },
          {
            "accuracy": 0.626092,
            "f1": 0.599541,
            "f1_weighted": 0.628404
          },
          {
            "accuracy": 0.633981,
            "f1": 0.610967,
            "f1_weighted": 0.639619
          },
          {
            "accuracy": 0.648915,
            "f1": 0.625087,
            "f1_weighted": 0.653897
          },
          {
            "accuracy": 0.615103,
            "f1": 0.588328,
            "f1_weighted": 0.615494
          },
          {
            "accuracy": 0.652296,
            "f1": 0.627394,
            "f1_weighted": 0.659211
          },
          {
            "accuracy": 0.634263,
            "f1": 0.618711,
            "f1_weighted": 0.643216
          },
          {
            "accuracy": 0.652296,
            "f1": 0.634461,
            "f1_weighted": 0.659254
          },
          {
            "accuracy": 0.64328,
            "f1": 0.61849,
            "f1_weighted": 0.643022
          }
        ],
        "main_score": 0.638321,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 25.143383502960205,
  "kg_co2_emissions": null
}