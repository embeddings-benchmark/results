{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "task_name": "InappropriatenessClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.514648,
        "f1": 0.507866,
        "f1_weighted": 0.507866,
        "ap": 0.508635,
        "ap_weighted": 0.508635,
        "scores_per_experiment": [
          {
            "accuracy": 0.54541,
            "f1": 0.531328,
            "f1_weighted": 0.531328,
            "ap": 0.525861,
            "ap_weighted": 0.525861
          },
          {
            "accuracy": 0.557129,
            "f1": 0.555728,
            "f1_weighted": 0.555728,
            "ap": 0.531499,
            "ap_weighted": 0.531499
          },
          {
            "accuracy": 0.475586,
            "f1": 0.475578,
            "f1_weighted": 0.475578,
            "ap": 0.488394,
            "ap_weighted": 0.488394
          },
          {
            "accuracy": 0.480469,
            "f1": 0.459649,
            "f1_weighted": 0.459649,
            "ap": 0.490862,
            "ap_weighted": 0.490862
          },
          {
            "accuracy": 0.518066,
            "f1": 0.499052,
            "f1_weighted": 0.499052,
            "ap": 0.509268,
            "ap_weighted": 0.509268
          },
          {
            "accuracy": 0.522461,
            "f1": 0.51512,
            "f1_weighted": 0.51512,
            "ap": 0.511635,
            "ap_weighted": 0.511635
          },
          {
            "accuracy": 0.557129,
            "f1": 0.5564,
            "f1_weighted": 0.5564,
            "ap": 0.531583,
            "ap_weighted": 0.531583
          },
          {
            "accuracy": 0.48291,
            "f1": 0.482866,
            "f1_weighted": 0.482866,
            "ap": 0.491742,
            "ap_weighted": 0.491742
          },
          {
            "accuracy": 0.532715,
            "f1": 0.532701,
            "f1_weighted": 0.532701,
            "ap": 0.517416,
            "ap_weighted": 0.517416
          },
          {
            "accuracy": 0.474609,
            "f1": 0.47024,
            "f1_weighted": 0.47024,
            "ap": 0.488092,
            "ap_weighted": 0.488092
          }
        ],
        "main_score": 0.514648,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 9.475225687026978,
  "kg_co2_emissions": 0.0002806237787970288
}