{
  "dataset_revision": "55caf0e52693a1ea63b15a4980a73fc137fb862b",
  "task_name": "IsiZuluNewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "train": [
      {
        "accuracy": 0.24375,
        "f1": 0.1839,
        "f1_weighted": 0.246911,
        "scores_per_experiment": [
          {
            "accuracy": 0.232713,
            "f1": 0.178377,
            "f1_weighted": 0.23052
          },
          {
            "accuracy": 0.261968,
            "f1": 0.19422,
            "f1_weighted": 0.26471
          },
          {
            "accuracy": 0.243351,
            "f1": 0.182374,
            "f1_weighted": 0.247113
          },
          {
            "accuracy": 0.238032,
            "f1": 0.1844,
            "f1_weighted": 0.230701
          },
          {
            "accuracy": 0.25,
            "f1": 0.1812,
            "f1_weighted": 0.266203
          },
          {
            "accuracy": 0.253989,
            "f1": 0.187408,
            "f1_weighted": 0.260408
          },
          {
            "accuracy": 0.240691,
            "f1": 0.181641,
            "f1_weighted": 0.247489
          },
          {
            "accuracy": 0.214096,
            "f1": 0.166665,
            "f1_weighted": 0.211989
          },
          {
            "accuracy": 0.24867,
            "f1": 0.190703,
            "f1_weighted": 0.251066
          },
          {
            "accuracy": 0.253989,
            "f1": 0.192012,
            "f1_weighted": 0.258912
          }
        ],
        "main_score": 0.24375,
        "hf_subset": "default",
        "languages": [
          "zul-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 7.161127328872681,
  "kg_co2_emissions": 0.00020413396796631697
}