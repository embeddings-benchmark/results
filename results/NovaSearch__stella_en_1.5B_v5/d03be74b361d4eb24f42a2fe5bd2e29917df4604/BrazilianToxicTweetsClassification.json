{
  "dataset_revision": "f333c1fcfa3ab43f008a327c8bd0140441354d34",
  "evaluation_time": 19.393840789794922,
  "kg_co2_emissions": 0.001272087124450529,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.240869140625,
        "f1": 0.1550613714555423,
        "hf_subset": "default",
        "languages": [
          "por-Latn"
        ],
        "lrap": 0.7869750976562508,
        "main_score": 0.240869140625,
        "scores_per_experiment": [
          {
            "accuracy": 0.166015625,
            "f1": 0.15063733764050669,
            "lrap": 0.8094075520833344
          },
          {
            "accuracy": 0.19384765625,
            "f1": 0.13528412118810404,
            "lrap": 0.7598470052083346
          },
          {
            "accuracy": 0.18017578125,
            "f1": 0.15953502806741146,
            "lrap": 0.8256429036458337
          },
          {
            "accuracy": 0.28759765625,
            "f1": 0.1505465686733122,
            "lrap": 0.8014187282986125
          },
          {
            "accuracy": 0.2802734375,
            "f1": 0.14816783033608302,
            "lrap": 0.7669406467013897
          },
          {
            "accuracy": 0.23388671875,
            "f1": 0.2078455406728061,
            "lrap": 0.8423800998263895
          },
          {
            "accuracy": 0.2080078125,
            "f1": 0.1438132496070688,
            "lrap": 0.7707926432291681
          },
          {
            "accuracy": 0.22216796875,
            "f1": 0.16728243777862484,
            "lrap": 0.7822129991319449
          },
          {
            "accuracy": 0.2919921875,
            "f1": 0.1499205363319367,
            "lrap": 0.7429606119791674
          },
          {
            "accuracy": 0.3447265625,
            "f1": 0.13758106425956942,
            "lrap": 0.7681477864583341
          }
        ]
      }
    ]
  },
  "task_name": "BrazilianToxicTweetsClassification"
}