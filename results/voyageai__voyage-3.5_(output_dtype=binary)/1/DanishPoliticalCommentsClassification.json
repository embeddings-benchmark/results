{
  "dataset_revision": "d743dcd5abb03d5ab357757a0e83522fc6696fcd",
  "task_name": "DanishPoliticalCommentsClassification",
  "mteb_version": "2.1.11",
  "scores": {
    "train": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.382737,
            "f1": 0.348418,
            "f1_weighted": 0.39904,
            "precision": 0.345909,
            "precision_weighted": 0.469125,
            "recall": 0.427562,
            "recall_weighted": 0.382737,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.394394,
            "f1": 0.359624,
            "f1_weighted": 0.410635,
            "precision": 0.351959,
            "precision_weighted": 0.469446,
            "recall": 0.422743,
            "recall_weighted": 0.394394,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.384124,
            "f1": 0.350539,
            "f1_weighted": 0.405951,
            "precision": 0.350315,
            "precision_weighted": 0.482722,
            "recall": 0.424719,
            "recall_weighted": 0.384124,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.41285,
            "f1": 0.365645,
            "f1_weighted": 0.42997,
            "precision": 0.355176,
            "precision_weighted": 0.476882,
            "recall": 0.425375,
            "recall_weighted": 0.41285,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.394532,
            "f1": 0.353582,
            "f1_weighted": 0.413368,
            "precision": 0.349106,
            "precision_weighted": 0.473076,
            "recall": 0.4268,
            "recall_weighted": 0.394532,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.324036,
            "f1": 0.304635,
            "f1_weighted": 0.33505,
            "precision": 0.298832,
            "precision_weighted": 0.402097,
            "recall": 0.390506,
            "recall_weighted": 0.324036,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.357064,
            "f1": 0.340227,
            "f1_weighted": 0.371024,
            "precision": 0.349238,
            "precision_weighted": 0.474469,
            "recall": 0.415912,
            "recall_weighted": 0.357064,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.391202,
            "f1": 0.352778,
            "f1_weighted": 0.409813,
            "precision": 0.349644,
            "precision_weighted": 0.472396,
            "recall": 0.417468,
            "recall_weighted": 0.391202,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.411601,
            "f1": 0.36194,
            "f1_weighted": 0.417053,
            "precision": 0.347868,
            "precision_weighted": 0.450511,
            "recall": 0.426987,
            "recall_weighted": 0.411601,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.41285,
            "f1": 0.375782,
            "f1_weighted": 0.427696,
            "precision": 0.365487,
            "precision_weighted": 0.47545,
            "recall": 0.430902,
            "recall_weighted": 0.41285,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.386539,
        "f1": 0.351317,
        "f1_weighted": 0.40196,
        "precision": 0.346353,
        "precision_weighted": 0.464618,
        "recall": 0.420897,
        "recall_weighted": 0.386539,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.386539,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 255.0472731590271,
  "kg_co2_emissions": null
}