{
  "dataset_revision": "de7ba3406ee55ea2cc52a0a41408fa6aede6d3c6",
  "task_name": "LccSentimentClassification",
  "mteb_version": "2.1.11",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.633333,
            "f1": 0.62924,
            "f1_weighted": 0.635269,
            "precision": 0.619766,
            "precision_weighted": 0.678614,
            "recall": 0.684128,
            "recall_weighted": 0.633333,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.52,
            "f1": 0.500615,
            "f1_weighted": 0.529425,
            "precision": 0.497641,
            "precision_weighted": 0.561288,
            "recall": 0.526733,
            "recall_weighted": 0.52,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.566667,
            "f1": 0.563795,
            "f1_weighted": 0.575221,
            "precision": 0.57762,
            "precision_weighted": 0.644583,
            "recall": 0.619129,
            "recall_weighted": 0.566667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.546667,
            "f1": 0.546432,
            "f1_weighted": 0.552077,
            "precision": 0.576763,
            "precision_weighted": 0.66746,
            "recall": 0.617315,
            "recall_weighted": 0.546667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.58,
            "f1": 0.571226,
            "f1_weighted": 0.583319,
            "precision": 0.559598,
            "precision_weighted": 0.603888,
            "recall": 0.600137,
            "recall_weighted": 0.58,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.6,
            "f1": 0.549481,
            "f1_weighted": 0.600125,
            "precision": 0.549024,
            "precision_weighted": 0.600345,
            "recall": 0.550087,
            "recall_weighted": 0.6,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.566667,
            "f1": 0.541553,
            "f1_weighted": 0.578218,
            "precision": 0.539375,
            "precision_weighted": 0.612262,
            "recall": 0.567265,
            "recall_weighted": 0.566667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.64,
            "f1": 0.614499,
            "f1_weighted": 0.644566,
            "precision": 0.606419,
            "precision_weighted": 0.653739,
            "recall": 0.62722,
            "recall_weighted": 0.64,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.546667,
            "f1": 0.534255,
            "f1_weighted": 0.559877,
            "precision": 0.550128,
            "precision_weighted": 0.623812,
            "recall": 0.57771,
            "recall_weighted": 0.546667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.606667,
            "f1": 0.592602,
            "f1_weighted": 0.615699,
            "precision": 0.593262,
            "precision_weighted": 0.652812,
            "recall": 0.627448,
            "recall_weighted": 0.606667,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.580667,
        "f1": 0.56437,
        "f1_weighted": 0.58738,
        "precision": 0.56696,
        "precision_weighted": 0.62988,
        "recall": 0.599717,
        "recall_weighted": 0.580667,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.580667,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 31.44915509223938,
  "kg_co2_emissions": null
}