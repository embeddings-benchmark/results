{
  "dataset_revision": "f6d2c31f4dc6b88f468552750bfec05b4b41b05a",
  "task_name": "RuReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.336963,
        "f1": 0.330362,
        "f1_weighted": 0.330372,
        "scores_per_experiment": [
          {
            "accuracy": 0.359863,
            "f1": 0.355513,
            "f1_weighted": 0.355514
          },
          {
            "accuracy": 0.353516,
            "f1": 0.342156,
            "f1_weighted": 0.342136
          },
          {
            "accuracy": 0.331543,
            "f1": 0.325293,
            "f1_weighted": 0.32531
          },
          {
            "accuracy": 0.350098,
            "f1": 0.337012,
            "f1_weighted": 0.337063
          },
          {
            "accuracy": 0.315918,
            "f1": 0.31493,
            "f1_weighted": 0.314938
          },
          {
            "accuracy": 0.338379,
            "f1": 0.332931,
            "f1_weighted": 0.332957
          },
          {
            "accuracy": 0.345215,
            "f1": 0.326613,
            "f1_weighted": 0.326618
          },
          {
            "accuracy": 0.325684,
            "f1": 0.324008,
            "f1_weighted": 0.323996
          },
          {
            "accuracy": 0.337891,
            "f1": 0.334819,
            "f1_weighted": 0.334837
          },
          {
            "accuracy": 0.311523,
            "f1": 0.310345,
            "f1_weighted": 0.310353
          }
        ],
        "main_score": 0.336963,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 9.790231227874756,
  "kg_co2_emissions": 0.00038661464242215965
}