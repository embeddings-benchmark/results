{
  "dataset_revision": "20b0e6081892e78179356fada741b7afa381443d",
  "task_name": "AngryTweetsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.367813,
        "f1": 0.359403,
        "f1_weighted": 0.366808,
        "scores_per_experiment": [
          {
            "accuracy": 0.372493,
            "f1": 0.350951,
            "f1_weighted": 0.362615
          },
          {
            "accuracy": 0.373448,
            "f1": 0.372357,
            "f1_weighted": 0.377002
          },
          {
            "accuracy": 0.337154,
            "f1": 0.333972,
            "f1_weighted": 0.337135
          },
          {
            "accuracy": 0.344795,
            "f1": 0.333893,
            "f1_weighted": 0.343223
          },
          {
            "accuracy": 0.357211,
            "f1": 0.35455,
            "f1_weighted": 0.357615
          },
          {
            "accuracy": 0.363897,
            "f1": 0.345683,
            "f1_weighted": 0.359045
          },
          {
            "accuracy": 0.377268,
            "f1": 0.371556,
            "f1_weighted": 0.377925
          },
          {
            "accuracy": 0.382999,
            "f1": 0.375466,
            "f1_weighted": 0.385117
          },
          {
            "accuracy": 0.365807,
            "f1": 0.361522,
            "f1_weighted": 0.367907
          },
          {
            "accuracy": 0.403056,
            "f1": 0.394075,
            "f1_weighted": 0.400496
          }
        ],
        "main_score": 0.367813,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 8.45203971862793,
  "kg_co2_emissions": 0.0002757036393777661
}