{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "task_name": "MTOPDomainClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.67708,
        "f1": 0.650038,
        "f1_weighted": 0.671396,
        "scores_per_experiment": [
          {
            "accuracy": 0.633609,
            "f1": 0.602275,
            "f1_weighted": 0.626172
          },
          {
            "accuracy": 0.68595,
            "f1": 0.664766,
            "f1_weighted": 0.681289
          },
          {
            "accuracy": 0.674931,
            "f1": 0.651794,
            "f1_weighted": 0.669549
          },
          {
            "accuracy": 0.674931,
            "f1": 0.655583,
            "f1_weighted": 0.670665
          },
          {
            "accuracy": 0.712397,
            "f1": 0.686262,
            "f1_weighted": 0.708339
          },
          {
            "accuracy": 0.656198,
            "f1": 0.616182,
            "f1_weighted": 0.642444
          },
          {
            "accuracy": 0.690358,
            "f1": 0.658118,
            "f1_weighted": 0.684977
          },
          {
            "accuracy": 0.659504,
            "f1": 0.635782,
            "f1_weighted": 0.661357
          },
          {
            "accuracy": 0.684298,
            "f1": 0.654098,
            "f1_weighted": 0.674632
          },
          {
            "accuracy": 0.698623,
            "f1": 0.675523,
            "f1_weighted": 0.694534
          }
        ],
        "main_score": 0.67708,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.685912,
        "f1": 0.656953,
        "f1_weighted": 0.68115,
        "scores_per_experiment": [
          {
            "accuracy": 0.64497,
            "f1": 0.615003,
            "f1_weighted": 0.637689
          },
          {
            "accuracy": 0.695125,
            "f1": 0.676418,
            "f1_weighted": 0.691895
          },
          {
            "accuracy": 0.690899,
            "f1": 0.667499,
            "f1_weighted": 0.684777
          },
          {
            "accuracy": 0.680192,
            "f1": 0.658953,
            "f1_weighted": 0.677145
          },
          {
            "accuracy": 0.712032,
            "f1": 0.682145,
            "f1_weighted": 0.708466
          },
          {
            "accuracy": 0.673429,
            "f1": 0.639369,
            "f1_weighted": 0.665939
          },
          {
            "accuracy": 0.702733,
            "f1": 0.658746,
            "f1_weighted": 0.696873
          },
          {
            "accuracy": 0.659059,
            "f1": 0.62854,
            "f1_weighted": 0.659222
          },
          {
            "accuracy": 0.68949,
            "f1": 0.656818,
            "f1_weighted": 0.681236
          },
          {
            "accuracy": 0.711186,
            "f1": 0.686036,
            "f1_weighted": 0.708258
          }
        ],
        "main_score": 0.685912,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 33.211376905441284,
  "kg_co2_emissions": null
}