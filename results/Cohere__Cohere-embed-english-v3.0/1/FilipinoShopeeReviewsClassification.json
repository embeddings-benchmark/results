{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.330518,
        "f1": 0.318767,
        "f1_weighted": 0.318752,
        "scores_per_experiment": [
          {
            "accuracy": 0.34082,
            "f1": 0.334555,
            "f1_weighted": 0.334538
          },
          {
            "accuracy": 0.322266,
            "f1": 0.305131,
            "f1_weighted": 0.30513
          },
          {
            "accuracy": 0.350586,
            "f1": 0.338521,
            "f1_weighted": 0.338528
          },
          {
            "accuracy": 0.341309,
            "f1": 0.33808,
            "f1_weighted": 0.338061
          },
          {
            "accuracy": 0.321289,
            "f1": 0.297709,
            "f1_weighted": 0.297662
          },
          {
            "accuracy": 0.338379,
            "f1": 0.321378,
            "f1_weighted": 0.321342
          },
          {
            "accuracy": 0.271484,
            "f1": 0.270355,
            "f1_weighted": 0.270345
          },
          {
            "accuracy": 0.348633,
            "f1": 0.332643,
            "f1_weighted": 0.332635
          },
          {
            "accuracy": 0.334961,
            "f1": 0.326741,
            "f1_weighted": 0.326718
          },
          {
            "accuracy": 0.335449,
            "f1": 0.322557,
            "f1_weighted": 0.322565
          }
        ],
        "main_score": 0.330518,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.331738,
        "f1": 0.320487,
        "f1_weighted": 0.320485,
        "scores_per_experiment": [
          {
            "accuracy": 0.324219,
            "f1": 0.318014,
            "f1_weighted": 0.31799
          },
          {
            "accuracy": 0.343262,
            "f1": 0.325147,
            "f1_weighted": 0.325139
          },
          {
            "accuracy": 0.346191,
            "f1": 0.332769,
            "f1_weighted": 0.332792
          },
          {
            "accuracy": 0.327637,
            "f1": 0.325457,
            "f1_weighted": 0.325447
          },
          {
            "accuracy": 0.321289,
            "f1": 0.30031,
            "f1_weighted": 0.300279
          },
          {
            "accuracy": 0.333496,
            "f1": 0.313342,
            "f1_weighted": 0.31332
          },
          {
            "accuracy": 0.303223,
            "f1": 0.303152,
            "f1_weighted": 0.30316
          },
          {
            "accuracy": 0.358887,
            "f1": 0.344639,
            "f1_weighted": 0.344649
          },
          {
            "accuracy": 0.342285,
            "f1": 0.334476,
            "f1_weighted": 0.334481
          },
          {
            "accuracy": 0.316895,
            "f1": 0.307563,
            "f1_weighted": 0.307588
          }
        ],
        "main_score": 0.331738,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 70.49219083786011,
  "kg_co2_emissions": null
}