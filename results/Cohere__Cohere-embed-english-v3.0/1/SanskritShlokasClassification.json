{
  "dataset_revision": "5a79d6472db143690c7ce6e974995d3610eee7f0",
  "task_name": "SanskritShlokasClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "train": [
      {
        "accuracy": 0.685117,
        "f1": 0.681495,
        "f1_weighted": 0.681043,
        "scores_per_experiment": [
          {
            "accuracy": 0.699739,
            "f1": 0.700934,
            "f1_weighted": 0.700595
          },
          {
            "accuracy": 0.684073,
            "f1": 0.68116,
            "f1_weighted": 0.678192
          },
          {
            "accuracy": 0.684073,
            "f1": 0.677967,
            "f1_weighted": 0.679903
          },
          {
            "accuracy": 0.673629,
            "f1": 0.656893,
            "f1_weighted": 0.651965
          },
          {
            "accuracy": 0.746736,
            "f1": 0.74214,
            "f1_weighted": 0.743094
          },
          {
            "accuracy": 0.694517,
            "f1": 0.693322,
            "f1_weighted": 0.693846
          },
          {
            "accuracy": 0.699739,
            "f1": 0.693086,
            "f1_weighted": 0.69447
          },
          {
            "accuracy": 0.671018,
            "f1": 0.669824,
            "f1_weighted": 0.669934
          },
          {
            "accuracy": 0.663185,
            "f1": 0.663208,
            "f1_weighted": 0.662905
          },
          {
            "accuracy": 0.634465,
            "f1": 0.636418,
            "f1_weighted": 0.63553
          }
        ],
        "main_score": 0.685117,
        "hf_subset": "default",
        "languages": [
          "san-Deva"
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.661458,
        "f1": 0.667505,
        "f1_weighted": 0.656173,
        "scores_per_experiment": [
          {
            "accuracy": 0.604167,
            "f1": 0.615066,
            "f1_weighted": 0.597611
          },
          {
            "accuracy": 0.677083,
            "f1": 0.691267,
            "f1_weighted": 0.675211
          },
          {
            "accuracy": 0.677083,
            "f1": 0.67637,
            "f1_weighted": 0.670164
          },
          {
            "accuracy": 0.635417,
            "f1": 0.634484,
            "f1_weighted": 0.610911
          },
          {
            "accuracy": 0.6875,
            "f1": 0.692834,
            "f1_weighted": 0.68443
          },
          {
            "accuracy": 0.645833,
            "f1": 0.64648,
            "f1_weighted": 0.640608
          },
          {
            "accuracy": 0.666667,
            "f1": 0.669719,
            "f1_weighted": 0.663819
          },
          {
            "accuracy": 0.635417,
            "f1": 0.641986,
            "f1_weighted": 0.631237
          },
          {
            "accuracy": 0.71875,
            "f1": 0.725895,
            "f1_weighted": 0.719282
          },
          {
            "accuracy": 0.666667,
            "f1": 0.680952,
            "f1_weighted": 0.668452
          }
        ],
        "main_score": 0.661458,
        "hf_subset": "default",
        "languages": [
          "san-Deva"
        ]
      }
    ]
  },
  "evaluation_time": 24.6646511554718,
  "kg_co2_emissions": null
}