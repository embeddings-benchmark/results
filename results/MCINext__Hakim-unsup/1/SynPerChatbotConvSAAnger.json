{
  "dataset_revision": "5cae68b7fc094cb2fa6890a464e4d836e8107f5e",
  "task_name": "SynPerChatbotConvSAAnger",
  "mteb_version": "1.38.27",
  "scores": {
    "test": [
      {
        "accuracy": 0.81878,
        "f1": 0.802586,
        "f1_weighted": 0.822104,
        "ap": 0.860576,
        "ap_weighted": 0.860576,
        "scores_per_experiment": [
          {
            "accuracy": 0.839024,
            "f1": 0.823387,
            "f1_weighted": 0.841588,
            "ap": 0.873235,
            "ap_weighted": 0.873235
          },
          {
            "accuracy": 0.853659,
            "f1": 0.838925,
            "f1_weighted": 0.855797,
            "ap": 0.883924,
            "ap_weighted": 0.883924
          },
          {
            "accuracy": 0.809756,
            "f1": 0.796674,
            "f1_weighted": 0.814536,
            "ap": 0.864944,
            "ap_weighted": 0.864944
          },
          {
            "accuracy": 0.792683,
            "f1": 0.773612,
            "f1_weighted": 0.796369,
            "ap": 0.837499,
            "ap_weighted": 0.837499
          },
          {
            "accuracy": 0.773171,
            "f1": 0.752304,
            "f1_weighted": 0.777204,
            "ap": 0.82205,
            "ap_weighted": 0.82205
          },
          {
            "accuracy": 0.812195,
            "f1": 0.796158,
            "f1_weighted": 0.81596,
            "ap": 0.856938,
            "ap_weighted": 0.856938
          },
          {
            "accuracy": 0.839024,
            "f1": 0.825017,
            "f1_weighted": 0.842164,
            "ap": 0.878862,
            "ap_weighted": 0.878862
          },
          {
            "accuracy": 0.821951,
            "f1": 0.801012,
            "f1_weighted": 0.823368,
            "ap": 0.849403,
            "ap_weighted": 0.849403
          },
          {
            "accuracy": 0.792683,
            "f1": 0.779847,
            "f1_weighted": 0.798258,
            "ap": 0.854984,
            "ap_weighted": 0.854984
          },
          {
            "accuracy": 0.853659,
            "f1": 0.838925,
            "f1_weighted": 0.855797,
            "ap": 0.883924,
            "ap_weighted": 0.883924
          }
        ],
        "main_score": 0.81878,
        "hf_subset": "default",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 30.92933201789856,
  "kg_co2_emissions": null
}