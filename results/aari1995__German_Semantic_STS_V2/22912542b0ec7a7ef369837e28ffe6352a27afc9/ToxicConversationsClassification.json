{
  "dataset_revision": "edfaf9da55d3dd50d43143d90c1ac476895ae6de",
  "task_name": "ToxicConversationsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.58291,
        "f1": 0.450812,
        "f1_weighted": 0.675732,
        "ap": 0.096804,
        "ap_weighted": 0.096804,
        "scores_per_experiment": [
          {
            "accuracy": 0.545898,
            "f1": 0.436614,
            "f1_weighted": 0.64549,
            "ap": 0.099538,
            "ap_weighted": 0.099538
          },
          {
            "accuracy": 0.608887,
            "f1": 0.46906,
            "f1_weighted": 0.698424,
            "ap": 0.102249,
            "ap_weighted": 0.102249
          },
          {
            "accuracy": 0.625,
            "f1": 0.476033,
            "f1_weighted": 0.711215,
            "ap": 0.101897,
            "ap_weighted": 0.101897
          },
          {
            "accuracy": 0.649414,
            "f1": 0.482123,
            "f1_weighted": 0.729898,
            "ap": 0.097473,
            "ap_weighted": 0.097473
          },
          {
            "accuracy": 0.503418,
            "f1": 0.402118,
            "f1_weighted": 0.609285,
            "ap": 0.085498,
            "ap_weighted": 0.085498
          },
          {
            "accuracy": 0.506836,
            "f1": 0.415921,
            "f1_weighted": 0.609903,
            "ap": 0.098678,
            "ap_weighted": 0.098678
          },
          {
            "accuracy": 0.666504,
            "f1": 0.486295,
            "f1_weighted": 0.74242,
            "ap": 0.09497,
            "ap_weighted": 0.09497
          },
          {
            "accuracy": 0.547852,
            "f1": 0.429116,
            "f1_weighted": 0.648281,
            "ap": 0.090496,
            "ap_weighted": 0.090496
          },
          {
            "accuracy": 0.59375,
            "f1": 0.452587,
            "f1_weighted": 0.686592,
            "ap": 0.092637,
            "ap_weighted": 0.092637
          },
          {
            "accuracy": 0.581543,
            "f1": 0.458254,
            "f1_weighted": 0.675808,
            "ap": 0.104603,
            "ap_weighted": 0.104603
          }
        ],
        "main_score": 0.58291,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 14.70969033241272,
  "kg_co2_emissions": 0.0007693648984551395
}