{
  "dataset_revision": "b07c6ce548f6a7ac8d546e1bbe197a0086409190",
  "task_name": "SweRecClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.499561,
        "f1": 0.45785,
        "f1_weighted": 0.533497,
        "scores_per_experiment": [
          {
            "accuracy": 0.44873,
            "f1": 0.41981,
            "f1_weighted": 0.487815
          },
          {
            "accuracy": 0.510742,
            "f1": 0.483259,
            "f1_weighted": 0.554644
          },
          {
            "accuracy": 0.476562,
            "f1": 0.462324,
            "f1_weighted": 0.524147
          },
          {
            "accuracy": 0.53418,
            "f1": 0.491232,
            "f1_weighted": 0.570312
          },
          {
            "accuracy": 0.463867,
            "f1": 0.434442,
            "f1_weighted": 0.501472
          },
          {
            "accuracy": 0.542969,
            "f1": 0.465267,
            "f1_weighted": 0.555146
          },
          {
            "accuracy": 0.504395,
            "f1": 0.465222,
            "f1_weighted": 0.541723
          },
          {
            "accuracy": 0.584473,
            "f1": 0.511472,
            "f1_weighted": 0.606381
          },
          {
            "accuracy": 0.473633,
            "f1": 0.439023,
            "f1_weighted": 0.505085
          },
          {
            "accuracy": 0.456055,
            "f1": 0.406445,
            "f1_weighted": 0.488245
          }
        ],
        "main_score": 0.499561,
        "hf_subset": "default",
        "languages": [
          "swe-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 13.727477788925171,
  "kg_co2_emissions": 0.0007681101194926269
}