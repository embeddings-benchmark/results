{
  "dataset_revision": "edfaf9da55d3dd50d43143d90c1ac476895ae6de",
  "task_name": "ToxicConversationsClassification",
  "mteb_version": "2.1.0",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.71142578125,
            "f1": 0.5297304542937902,
            "f1_weighted": 0.7757970243428912,
            "precision": 0.5506505391318187,
            "precision_weighted": 0.8878482488784666,
            "recall": 0.6458276056190513,
            "recall_weighted": 0.71142578125,
            "ap": 0.11941119415619399,
            "ap_weighted": 0.11941119415619399
          },
          {
            "accuracy": 0.677734375,
            "f1": 0.5075287245763452,
            "f1_weighted": 0.7512451210301498,
            "precision": 0.5418264255934875,
            "precision_weighted": 0.8843780407642882,
            "recall": 0.627534922692222,
            "recall_weighted": 0.677734375,
            "ap": 0.11078806518364649,
            "ap_weighted": 0.11078806518364649
          },
          {
            "accuracy": 0.736328125,
            "f1": 0.5358324645301663,
            "f1_weighted": 0.7926337413875164,
            "precision": 0.5481432360742705,
            "precision_weighted": 0.8832989680039788,
            "recall": 0.6283138918345705,
            "recall_weighted": 0.736328125,
            "ap": 0.11455078125,
            "ap_weighted": 0.11455078125
          },
          {
            "accuracy": 0.6435546875,
            "f1": 0.49529471974246203,
            "f1_weighted": 0.725565156151467,
            "precision": 0.5442115671399858,
            "precision_weighted": 0.8895090258106804,
            "recall": 0.6428328292944765,
            "recall_weighted": 0.6435546875,
            "ap": 0.11435824046232658,
            "ap_weighted": 0.11435824046232658
          },
          {
            "accuracy": 0.5751953125,
            "f1": 0.4581021897810219,
            "f1_weighted": 0.6701489792427007,
            "precision": 0.5384996866818649,
            "precision_weighted": 0.8899622361844484,
            "recall": 0.6311090164041737,
            "recall_weighted": 0.5751953125,
            "ap": 0.1083165603231488,
            "ap_weighted": 0.1083165603231488
          },
          {
            "accuracy": 0.482421875,
            "f1": 0.39976287802374755,
            "f1_weighted": 0.5872682869540886,
            "precision": 0.5245913008634407,
            "precision_weighted": 0.8809642919838909,
            "recall": 0.5835591689250226,
            "recall_weighted": 0.482421875,
            "ap": 0.09468281280836788,
            "ap_weighted": 0.09468281280836788
          },
          {
            "accuracy": 0.74365234375,
            "f1": 0.5365802883701565,
            "f1_weighted": 0.7973488957294335,
            "precision": 0.5467084508803869,
            "precision_weighted": 0.8815519137080131,
            "recall": 0.621005328410772,
            "recall_weighted": 0.74365234375,
            "ap": 0.1122945550548332,
            "ap_weighted": 0.1122945550548332
          },
          {
            "accuracy": 0.6318359375,
            "f1": 0.48633159958539074,
            "f1_weighted": 0.7164687358833599,
            "precision": 0.540108690416941,
            "precision_weighted": 0.8869020543888573,
            "recall": 0.6308275401594596,
            "recall_weighted": 0.6318359375,
            "ap": 0.10997805869625908,
            "ap_weighted": 0.10997805869625908
          },
          {
            "accuracy": 0.6005859375,
            "f1": 0.4639409077586085,
            "f1_weighted": 0.6917707252384526,
            "precision": 0.5307558842463925,
            "precision_weighted": 0.880718127877844,
            "recall": 0.6025751803411754,
            "recall_weighted": 0.6005859375,
            "ap": 0.10083210166347882,
            "ap_weighted": 0.10083210166347882
          },
          {
            "accuracy": 0.71142578125,
            "f1": 0.5352216030901054,
            "f1_weighted": 0.7761224779098186,
            "precision": 0.555912297962052,
            "precision_weighted": 0.8918837831605763,
            "recall": 0.6627554560569761,
            "recall_weighted": 0.71142578125,
            "ap": 0.12610432098765434,
            "ap_weighted": 0.12610432098765434
          }
        ],
        "accuracy": 0.651416015625,
        "f1": 0.4948325829751795,
        "f1_weighted": 0.7284369143869879,
        "precision": 0.542140807899064,
        "precision_weighted": 0.8857016690761045,
        "recall": 0.62763409397379,
        "recall_weighted": 0.651416015625,
        "ap": 0.11113166905859091,
        "ap_weighted": 0.11113166905859091,
        "main_score": 0.651416015625,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 6206.736694186926,
  "mteb_dataset_name": "ToxicConversationsClassification"
}