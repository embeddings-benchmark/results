{
  "dataset_revision": "d604517c81ca91fe16a244d1248fc021f9ecee7a",
  "task_name": "TweetSentimentExtractionClassification",
  "mteb_version": "2.1.0",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.5752688172043011,
            "f1": 0.5736880513342696,
            "f1_weighted": 0.5692370313377912,
            "precision": 0.5770057509194376,
            "precision_weighted": 0.5802437538519631,
            "recall": 0.5864420559796807,
            "recall_weighted": 0.5752688172043011,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.566213921901528,
            "f1": 0.5675431776427643,
            "f1_weighted": 0.5625276106737923,
            "precision": 0.5647591900510186,
            "precision_weighted": 0.5663808607618714,
            "recall": 0.5773189639010129,
            "recall_weighted": 0.566213921901528,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.5947934352009054,
            "f1": 0.5975593632700722,
            "f1_weighted": 0.5928294169384922,
            "precision": 0.5996390502618302,
            "precision_weighted": 0.6038907393029254,
            "recall": 0.6083943557198318,
            "recall_weighted": 0.5947934352009054,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.5639501980758348,
            "f1": 0.5666818228888181,
            "f1_weighted": 0.5628184988402628,
            "precision": 0.5689212636194338,
            "precision_weighted": 0.572422049345395,
            "recall": 0.5754517166121881,
            "recall_weighted": 0.5639501980758348,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.5690435766836446,
            "f1": 0.568989153666211,
            "f1_weighted": 0.567676839932252,
            "precision": 0.5668179571258566,
            "precision_weighted": 0.570270076371672,
            "recall": 0.5748329337631242,
            "recall_weighted": 0.5690435766836446,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.5696095076400679,
            "f1": 0.5710881782717346,
            "f1_weighted": 0.562229531726479,
            "precision": 0.5798921016516019,
            "precision_weighted": 0.5848375430524388,
            "recall": 0.5913139444417777,
            "recall_weighted": 0.5696095076400679,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.5749858517260894,
            "f1": 0.5760517902928846,
            "f1_weighted": 0.5696275417190433,
            "precision": 0.5742657136946093,
            "precision_weighted": 0.5761108492136268,
            "recall": 0.5887627029966106,
            "recall_weighted": 0.5749858517260894,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.5874363327674024,
            "f1": 0.5883017777255861,
            "f1_weighted": 0.5834051575290119,
            "precision": 0.5873632221547055,
            "precision_weighted": 0.5889911741575895,
            "recall": 0.5982116704691501,
            "recall_weighted": 0.5874363327674024,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.5577249575551783,
            "f1": 0.5601663232440641,
            "f1_weighted": 0.551265794073674,
            "precision": 0.5585595878136201,
            "precision_weighted": 0.5592183097680901,
            "recall": 0.57553652150207,
            "recall_weighted": 0.5577249575551783,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.5529145444255801,
            "f1": 0.5552433718893418,
            "f1_weighted": 0.552928500983863,
            "precision": 0.5544722979563892,
            "precision_weighted": 0.553141731585261,
            "recall": 0.5562243438036729,
            "recall_weighted": 0.5529145444255801,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.5711941143180532,
        "f1": 0.5725313010225745,
        "f1_weighted": 0.5674545923754662,
        "precision": 0.5731696135248503,
        "precision_weighted": 0.5755507087410833,
        "recall": 0.5832489209189119,
        "recall_weighted": 0.5711941143180532,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.5711941143180532,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 6206.736694186926,
  "mteb_dataset_name": "TweetSentimentExtractionClassification",
  "mteb_model_name": "Said-Research/SAID-LAM-v1"
}