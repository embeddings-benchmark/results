{
    "dataset_revision": "9080400076fbadbb4c4dcb136ff4eddc40b42553",
    "task_name": "Tatoeba",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "sqi-eng",
                "languages": [
                    "sqi-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.168,
                "f1": 0.13168299935527422,
                "precision": 0.12209559281760876,
                "recall": 0.168,
                "main_score": 0.13168299935527422
            },
            {
                "hf_subset": "fry-eng",
                "languages": [
                    "fry-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.3583815028901734,
                "f1": 0.290852500101055,
                "precision": 0.2696531791907515,
                "recall": 0.3583815028901734,
                "main_score": 0.290852500101055
            },
            {
                "hf_subset": "kur-eng",
                "languages": [
                    "kur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.15121951219512195,
                "f1": 0.11844149203614325,
                "precision": 0.11042929292929295,
                "recall": 0.15121951219512195,
                "main_score": 0.11844149203614325
            },
            {
                "hf_subset": "tur-eng",
                "languages": [
                    "tur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.099,
                "f1": 0.07139634818700721,
                "precision": 0.06501835713997978,
                "recall": 0.099,
                "main_score": 0.07139634818700721
            },
            {
                "hf_subset": "deu-eng",
                "languages": [
                    "deu-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7659999999999999,
                "f1": 0.7273241758241759,
                "precision": 0.7118867647058823,
                "recall": 0.7659999999999999,
                "main_score": 0.7273241758241759
            },
            {
                "hf_subset": "nld-eng",
                "languages": [
                    "nld-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.42,
                "f1": 0.36810031024531026,
                "precision": 0.3519870269535562,
                "recall": 0.42,
                "main_score": 0.36810031024531026
            },
            {
                "hf_subset": "ron-eng",
                "languages": [
                    "ron-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.353,
                "f1": 0.30353777056277054,
                "precision": 0.287739567785156,
                "recall": 0.353,
                "main_score": 0.30353777056277054
            },
            {
                "hf_subset": "ang-eng",
                "languages": [
                    "ang-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.3582089552238806,
                "f1": 0.2744136460554371,
                "precision": 0.24340796019900496,
                "recall": 0.3582089552238806,
                "main_score": 0.2744136460554371
            },
            {
                "hf_subset": "ido-eng",
                "languages": [
                    "ido-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.518,
                "f1": 0.4582491836793846,
                "precision": 0.4372930309462286,
                "recall": 0.518,
                "main_score": 0.4582491836793846
            },
            {
                "hf_subset": "jav-eng",
                "languages": [
                    "jav-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.25853658536585367,
                "f1": 0.19798693627961922,
                "precision": 0.18250680214094847,
                "recall": 0.25853658536585367,
                "main_score": 0.19798693627961922
            },
            {
                "hf_subset": "isl-eng",
                "languages": [
                    "isl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.09,
                "f1": 0.0692659076228166,
                "precision": 0.06507185696775364,
                "recall": 0.09,
                "main_score": 0.0692659076228166
            },
            {
                "hf_subset": "slv-eng",
                "languages": [
                    "slv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.1433778857837181,
                "f1": 0.10888963524130242,
                "precision": 0.10189272116928368,
                "recall": 0.1433778857837181,
                "main_score": 0.10888963524130242
            },
            {
                "hf_subset": "cym-eng",
                "languages": [
                    "cym-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.11304347826086956,
                "f1": 0.08459121175343064,
                "precision": 0.07721864466975997,
                "recall": 0.11304347826086956,
                "main_score": 0.08459121175343064
            },
            {
                "hf_subset": "kaz-eng",
                "languages": [
                    "kaz-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.08521739130434783,
                "f1": 0.06751744703151352,
                "precision": 0.06387004921960017,
                "recall": 0.08521739130434783,
                "main_score": 0.06751744703151352
            },
            {
                "hf_subset": "est-eng",
                "languages": [
                    "est-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.073,
                "f1": 0.056267660117660115,
                "precision": 0.051270385799923004,
                "recall": 0.073,
                "main_score": 0.056267660117660115
            },
            {
                "hf_subset": "heb-eng",
                "languages": [
                    "heb-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.032,
                "f1": 0.0191950282507703,
                "precision": 0.016684431360304505,
                "recall": 0.032,
                "main_score": 0.0191950282507703
            },
            {
                "hf_subset": "gla-eng",
                "languages": [
                    "gla-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.05790108564535586,
                "f1": 0.04128499324411468,
                "precision": 0.038151453928788916,
                "recall": 0.05790108564535586,
                "main_score": 0.04128499324411468
            },
            {
                "hf_subset": "mar-eng",
                "languages": [
                    "mar-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.703,
                "f1": 0.6518318181818181,
                "precision": 0.6312691197691198,
                "recall": 0.703,
                "main_score": 0.6518318181818181
            },
            {
                "hf_subset": "lat-eng",
                "languages": [
                    "lat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.45300000000000007,
                "f1": 0.38339152873270516,
                "precision": 0.36130903304212125,
                "recall": 0.45300000000000007,
                "main_score": 0.38339152873270516
            },
            {
                "hf_subset": "bel-eng",
                "languages": [
                    "bel-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.16,
                "f1": 0.12172850459161384,
                "precision": 0.1127855570316309,
                "recall": 0.16,
                "main_score": 0.12172850459161384
            },
            {
                "hf_subset": "pms-eng",
                "languages": [
                    "pms-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.37714285714285717,
                "f1": 0.32188793178089947,
                "precision": 0.30457500778089014,
                "recall": 0.37714285714285717,
                "main_score": 0.32188793178089947
            },
            {
                "hf_subset": "gle-eng",
                "languages": [
                    "gle-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.065,
                "f1": 0.045285441319281264,
                "precision": 0.04171387799947767,
                "recall": 0.065,
                "main_score": 0.045285441319281264
            },
            {
                "hf_subset": "pes-eng",
                "languages": [
                    "pes-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.21,
                "f1": 0.17006564035803165,
                "precision": 0.15844832112332113,
                "recall": 0.21,
                "main_score": 0.17006564035803165
            },
            {
                "hf_subset": "nob-eng",
                "languages": [
                    "nob-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.255,
                "f1": 0.22794308201649963,
                "precision": 0.21938476924594044,
                "recall": 0.255,
                "main_score": 0.22794308201649963
            },
            {
                "hf_subset": "bul-eng",
                "languages": [
                    "bul-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.337,
                "f1": 0.2689892216642216,
                "precision": 0.24939117884031678,
                "recall": 0.337,
                "main_score": 0.2689892216642216
            },
            {
                "hf_subset": "cbk-eng",
                "languages": [
                    "cbk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.69,
                "f1": 0.6368992285492286,
                "precision": 0.6172837301587302,
                "recall": 0.69,
                "main_score": 0.6368992285492286
            },
            {
                "hf_subset": "hun-eng",
                "languages": [
                    "hun-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.074,
                "f1": 0.05565568622365857,
                "precision": 0.05119921502146487,
                "recall": 0.074,
                "main_score": 0.05565568622365857
            },
            {
                "hf_subset": "uig-eng",
                "languages": [
                    "uig-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.015,
                "f1": 0.01001208686507139,
                "precision": 0.009683730903243098,
                "recall": 0.015,
                "main_score": 0.01001208686507139
            },
            {
                "hf_subset": "rus-eng",
                "languages": [
                    "rus-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.69,
                "f1": 0.6261056277056276,
                "precision": 0.5996357142857143,
                "recall": 0.69,
                "main_score": 0.6261056277056276
            },
            {
                "hf_subset": "spa-eng",
                "languages": [
                    "spa-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.983,
                "f1": 0.9776666666666668,
                "precision": 0.9751666666666668,
                "recall": 0.983,
                "main_score": 0.9776666666666668
            },
            {
                "hf_subset": "hye-eng",
                "languages": [
                    "hye-Armn",
                    "eng-Latn"
                ],
                "accuracy": 0.02021563342318059,
                "f1": 0.015634923413129036,
                "precision": 0.014895885785373653,
                "recall": 0.02021563342318059,
                "main_score": 0.015634923413129036
            },
            {
                "hf_subset": "tel-eng",
                "languages": [
                    "tel-Telu",
                    "eng-Latn"
                ],
                "accuracy": 0.8333333333333335,
                "f1": 0.7930199430199429,
                "precision": 0.7745726495726495,
                "recall": 0.8333333333333335,
                "main_score": 0.7930199430199429
            },
            {
                "hf_subset": "afr-eng",
                "languages": [
                    "afr-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.234,
                "f1": 0.18655079988631995,
                "precision": 0.17338269096494904,
                "recall": 0.234,
                "main_score": 0.18655079988631995
            },
            {
                "hf_subset": "mon-eng",
                "languages": [
                    "mon-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.06363636363636363,
                "f1": 0.0448376251469035,
                "precision": 0.04071778641679957,
                "recall": 0.06363636363636363,
                "main_score": 0.0448376251469035
            },
            {
                "hf_subset": "arz-eng",
                "languages": [
                    "arz-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.7756813417190775,
                "f1": 0.7316561844863732,
                "precision": 0.7134404845096669,
                "recall": 0.7756813417190775,
                "main_score": 0.7316561844863732
            },
            {
                "hf_subset": "hrv-eng",
                "languages": [
                    "hrv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.17299999999999996,
                "f1": 0.13693204564375855,
                "precision": 0.12830651358081277,
                "recall": 0.17299999999999996,
                "main_score": 0.13693204564375855
            },
            {
                "hf_subset": "nov-eng",
                "languages": [
                    "nov-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.5992217898832685,
                "f1": 0.5329591938541354,
                "precision": 0.5058736335000926,
                "recall": 0.5992217898832685,
                "main_score": 0.5329591938541354
            },
            {
                "hf_subset": "gsw-eng",
                "languages": [
                    "gsw-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.2564102564102564,
                "f1": 0.1931404777558624,
                "precision": 0.17413105413105417,
                "recall": 0.2564102564102564,
                "main_score": 0.1931404777558624
            },
            {
                "hf_subset": "nds-eng",
                "languages": [
                    "nds-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.297,
                "f1": 0.2444977050316952,
                "precision": 0.22798075396825396,
                "recall": 0.297,
                "main_score": 0.2444977050316952
            },
            {
                "hf_subset": "ukr-eng",
                "languages": [
                    "ukr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.322,
                "f1": 0.25423187804627434,
                "precision": 0.2340400330949244,
                "recall": 0.322,
                "main_score": 0.25423187804627434
            },
            {
                "hf_subset": "uzb-eng",
                "languages": [
                    "uzb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.0911214953271028,
                "f1": 0.05910063827286792,
                "precision": 0.05296401380795872,
                "recall": 0.0911214953271028,
                "main_score": 0.05910063827286792
            },
            {
                "hf_subset": "lit-eng",
                "languages": [
                    "lit-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.072,
                "f1": 0.058167267973961524,
                "precision": 0.05508698718788661,
                "recall": 0.072,
                "main_score": 0.058167267973961524
            },
            {
                "hf_subset": "ina-eng",
                "languages": [
                    "ina-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.872,
                "f1": 0.8388333333333332,
                "precision": 0.8242833333333333,
                "recall": 0.872,
                "main_score": 0.8388333333333332
            },
            {
                "hf_subset": "lfn-eng",
                "languages": [
                    "lfn-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.537,
                "f1": 0.4825312435500516,
                "precision": 0.4634107401656314,
                "recall": 0.537,
                "main_score": 0.4825312435500516
            },
            {
                "hf_subset": "zsm-eng",
                "languages": [
                    "zsm-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8809999999999999,
                "f1": 0.8521690476190475,
                "precision": 0.8396761904761906,
                "recall": 0.8809999999999999,
                "main_score": 0.8521690476190475
            },
            {
                "hf_subset": "ita-eng",
                "languages": [
                    "ita-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7810000000000001,
                "f1": 0.7338746031746033,
                "precision": 0.7147583333333334,
                "recall": 0.7810000000000001,
                "main_score": 0.7338746031746033
            },
            {
                "hf_subset": "cmn-eng",
                "languages": [
                    "cmn-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.961,
                "f1": 0.9508333333333333,
                "precision": 0.9458333333333334,
                "recall": 0.961,
                "main_score": 0.9508333333333333
            },
            {
                "hf_subset": "lvs-eng",
                "languages": [
                    "lvs-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.09,
                "f1": 0.06952605595133894,
                "precision": 0.06457724621713984,
                "recall": 0.09,
                "main_score": 0.06952605595133894
            },
            {
                "hf_subset": "glg-eng",
                "languages": [
                    "glg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.847,
                "f1": 0.8097880952380954,
                "precision": 0.7936428571428571,
                "recall": 0.847,
                "main_score": 0.8097880952380954
            },
            {
                "hf_subset": "ceb-eng",
                "languages": [
                    "ceb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.105,
                "f1": 0.08146458694813959,
                "precision": 0.07618942433110826,
                "recall": 0.105,
                "main_score": 0.08146458694813959
            },
            {
                "hf_subset": "bre-eng",
                "languages": [
                    "bre-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.084,
                "f1": 0.06144921607886653,
                "precision": 0.055261043562899584,
                "recall": 0.084,
                "main_score": 0.06144921607886653
            },
            {
                "hf_subset": "ben-eng",
                "languages": [
                    "ben-Beng",
                    "eng-Latn"
                ],
                "accuracy": 0.8439999999999999,
                "f1": 0.8065333333333333,
                "precision": 0.7897833333333333,
                "recall": 0.8439999999999999,
                "main_score": 0.8065333333333333
            },
            {
                "hf_subset": "swg-eng",
                "languages": [
                    "swg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.2857142857142857,
                "f1": 0.22767379679144387,
                "precision": 0.212016369047619,
                "recall": 0.2857142857142857,
                "main_score": 0.22767379679144387
            },
            {
                "hf_subset": "arq-eng",
                "languages": [
                    "arq-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.34248079034028545,
                "f1": 0.2924157273030522,
                "precision": 0.276428310072657,
                "recall": 0.34248079034028545,
                "main_score": 0.2924157273030522
            },
            {
                "hf_subset": "kab-eng",
                "languages": [
                    "kab-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.029000000000000005,
                "f1": 0.01915673469669371,
                "precision": 0.017528460881307183,
                "recall": 0.029000000000000005,
                "main_score": 0.01915673469669371
            },
            {
                "hf_subset": "fra-eng",
                "languages": [
                    "fra-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.949,
                "f1": 0.9353333333333331,
                "precision": 0.9290666666666667,
                "recall": 0.949,
                "main_score": 0.9353333333333331
            },
            {
                "hf_subset": "por-eng",
                "languages": [
                    "por-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.95,
                "f1": 0.9361666666666666,
                "precision": 0.9293333333333332,
                "recall": 0.95,
                "main_score": 0.9361666666666666
            },
            {
                "hf_subset": "tat-eng",
                "languages": [
                    "tat-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.063,
                "f1": 0.04920070356472795,
                "precision": 0.04565811270125224,
                "recall": 0.063,
                "main_score": 0.04920070356472795
            },
            {
                "hf_subset": "oci-eng",
                "languages": [
                    "oci-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.474,
                "f1": 0.41083928571428574,
                "precision": 0.38999704968944093,
                "recall": 0.474,
                "main_score": 0.41083928571428574
            },
            {
                "hf_subset": "pol-eng",
                "languages": [
                    "pol-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.182,
                "f1": 0.14826165036734296,
                "precision": 0.1398855933045449,
                "recall": 0.182,
                "main_score": 0.14826165036734296
            },
            {
                "hf_subset": "war-eng",
                "languages": [
                    "war-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.133,
                "f1": 0.10734512257894609,
                "precision": 0.10065245080300249,
                "recall": 0.133,
                "main_score": 0.10734512257894609
            },
            {
                "hf_subset": "aze-eng",
                "languages": [
                    "aze-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.09300000000000001,
                "f1": 0.07613044370901514,
                "precision": 0.07184100384035204,
                "recall": 0.09300000000000001,
                "main_score": 0.07613044370901514
            },
            {
                "hf_subset": "vie-eng",
                "languages": [
                    "vie-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.97,
                "f1": 0.9605,
                "precision": 0.9558333333333334,
                "recall": 0.97,
                "main_score": 0.9605
            },
            {
                "hf_subset": "nno-eng",
                "languages": [
                    "nno-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.198,
                "f1": 0.16070523504273504,
                "precision": 0.14848185626325228,
                "recall": 0.198,
                "main_score": 0.16070523504273504
            },
            {
                "hf_subset": "cha-eng",
                "languages": [
                    "cha-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.291970802919708,
                "f1": 0.22579707397225646,
                "precision": 0.20792945550165476,
                "recall": 0.291970802919708,
                "main_score": 0.22579707397225646
            },
            {
                "hf_subset": "mhr-eng",
                "languages": [
                    "mhr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.043,
                "f1": 0.028844954964520178,
                "precision": 0.026280916815877506,
                "recall": 0.043,
                "main_score": 0.028844954964520178
            },
            {
                "hf_subset": "dan-eng",
                "languages": [
                    "dan-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.287,
                "f1": 0.249056519214062,
                "precision": 0.23800155414494334,
                "recall": 0.287,
                "main_score": 0.249056519214062
            },
            {
                "hf_subset": "ell-eng",
                "languages": [
                    "ell-Grek",
                    "eng-Latn"
                ],
                "accuracy": 0.095,
                "f1": 0.06723431537130878,
                "precision": 0.06078266616597544,
                "recall": 0.095,
                "main_score": 0.06723431537130878
            },
            {
                "hf_subset": "amh-eng",
                "languages": [
                    "amh-Ethi",
                    "eng-Latn"
                ],
                "accuracy": 0.017857142857142856,
                "f1": 0.004579590594653929,
                "precision": 0.0032939943654229364,
                "recall": 0.017857142857142856,
                "main_score": 0.004579590594653929
            },
            {
                "hf_subset": "pam-eng",
                "languages": [
                    "pam-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.091,
                "f1": 0.07179418261477084,
                "precision": 0.0681138018671376,
                "recall": 0.091,
                "main_score": 0.07179418261477084
            },
            {
                "hf_subset": "hsb-eng",
                "languages": [
                    "hsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.15113871635610765,
                "f1": 0.12353104530336957,
                "precision": 0.1166106754766342,
                "recall": 0.15113871635610765,
                "main_score": 0.12353104530336957
            },
            {
                "hf_subset": "srp-eng",
                "languages": [
                    "srp-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.184,
                "f1": 0.15091645001025805,
                "precision": 0.14200823959052217,
                "recall": 0.184,
                "main_score": 0.15091645001025805
            },
            {
                "hf_subset": "epo-eng",
                "languages": [
                    "epo-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.332,
                "f1": 0.28066634199134194,
                "precision": 0.2654372717117398,
                "recall": 0.332,
                "main_score": 0.28066634199134194
            },
            {
                "hf_subset": "kzj-eng",
                "languages": [
                    "kzj-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.076,
                "f1": 0.05992580343865051,
                "precision": 0.05740912573883906,
                "recall": 0.076,
                "main_score": 0.05992580343865051
            },
            {
                "hf_subset": "awa-eng",
                "languages": [
                    "awa-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.5281385281385281,
                "f1": 0.4686834810211434,
                "precision": 0.4513687899402185,
                "recall": 0.5281385281385281,
                "main_score": 0.4686834810211434
            },
            {
                "hf_subset": "fao-eng",
                "languages": [
                    "fao-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.16030534351145037,
                "f1": 0.12902313597194603,
                "precision": 0.12197579773915651,
                "recall": 0.16030534351145037,
                "main_score": 0.12902313597194603
            },
            {
                "hf_subset": "mal-eng",
                "languages": [
                    "mal-Mlym",
                    "eng-Latn"
                ],
                "accuracy": 0.947598253275109,
                "f1": 0.9311984473556527,
                "precision": 0.923216885007278,
                "recall": 0.947598253275109,
                "main_score": 0.9311984473556527
            },
            {
                "hf_subset": "ile-eng",
                "languages": [
                    "ile-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7019999999999998,
                "f1": 0.6441237595737596,
                "precision": 0.6207428571428572,
                "recall": 0.7019999999999998,
                "main_score": 0.6441237595737596
            },
            {
                "hf_subset": "bos-eng",
                "languages": [
                    "bos-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.192090395480226,
                "f1": 0.14986259497894083,
                "precision": 0.1408083152750014,
                "recall": 0.192090395480226,
                "main_score": 0.14986259497894083
            },
            {
                "hf_subset": "cor-eng",
                "languages": [
                    "cor-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.05800000000000001,
                "f1": 0.04004811414639001,
                "precision": 0.03611296721493974,
                "recall": 0.05800000000000001,
                "main_score": 0.04004811414639001
            },
            {
                "hf_subset": "cat-eng",
                "languages": [
                    "cat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.931,
                "f1": 0.9117333333333335,
                "precision": 0.9027833333333334,
                "recall": 0.931,
                "main_score": 0.9117333333333335
            },
            {
                "hf_subset": "eus-eng",
                "languages": [
                    "eus-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.682,
                "f1": 0.6380587027914614,
                "precision": 0.6206492402945891,
                "recall": 0.682,
                "main_score": 0.6380587027914614
            },
            {
                "hf_subset": "yue-eng",
                "languages": [
                    "yue-Hant",
                    "eng-Latn"
                ],
                "accuracy": 0.889,
                "f1": 0.8638250000000001,
                "precision": 0.85345,
                "recall": 0.889,
                "main_score": 0.8638250000000001
            },
            {
                "hf_subset": "swe-eng",
                "languages": [
                    "swe-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.263,
                "f1": 0.21726019075408248,
                "precision": 0.203161132602622,
                "recall": 0.263,
                "main_score": 0.21726019075408248
            },
            {
                "hf_subset": "dtp-eng",
                "languages": [
                    "dtp-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.066,
                "f1": 0.05410791944650359,
                "precision": 0.05143205186348676,
                "recall": 0.066,
                "main_score": 0.05410791944650359
            },
            {
                "hf_subset": "kat-eng",
                "languages": [
                    "kat-Geor",
                    "eng-Latn"
                ],
                "accuracy": 0.012064343163538873,
                "f1": 0.007118331023204635,
                "precision": 0.006930197065411955,
                "recall": 0.012064343163538873,
                "main_score": 0.007118331023204635
            },
            {
                "hf_subset": "jpn-eng",
                "languages": [
                    "jpn-Jpan",
                    "eng-Latn"
                ],
                "accuracy": 0.78,
                "f1": 0.7395134920634919,
                "precision": 0.723770634920635,
                "recall": 0.78,
                "main_score": 0.7395134920634919
            },
            {
                "hf_subset": "csb-eng",
                "languages": [
                    "csb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.12648221343873517,
                "f1": 0.10259994816302727,
                "precision": 0.09677206851119895,
                "recall": 0.12648221343873517,
                "main_score": 0.10259994816302727
            },
            {
                "hf_subset": "xho-eng",
                "languages": [
                    "xho-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.1056338028169014,
                "f1": 0.0779264475743349,
                "precision": 0.07299087316692951,
                "recall": 0.1056338028169014,
                "main_score": 0.0779264475743349
            },
            {
                "hf_subset": "orv-eng",
                "languages": [
                    "orv-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.081437125748503,
                "f1": 0.05611330340509872,
                "precision": 0.051560759802239294,
                "recall": 0.081437125748503,
                "main_score": 0.05611330340509872
            },
            {
                "hf_subset": "ind-eng",
                "languages": [
                    "ind-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.925,
                "f1": 0.9053999999999999,
                "precision": 0.8964500000000001,
                "recall": 0.925,
                "main_score": 0.9053999999999999
            },
            {
                "hf_subset": "tuk-eng",
                "languages": [
                    "tuk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.08374384236453201,
                "f1": 0.05831645092728836,
                "precision": 0.05241568776051535,
                "recall": 0.08374384236453201,
                "main_score": 0.05831645092728836
            },
            {
                "hf_subset": "max-eng",
                "languages": [
                    "max-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.45422535211267606,
                "f1": 0.40878561970111266,
                "precision": 0.3952681669728516,
                "recall": 0.45422535211267606,
                "main_score": 0.40878561970111266
            },
            {
                "hf_subset": "swh-eng",
                "languages": [
                    "swh-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.3205128205128205,
                "f1": 0.2543301042069852,
                "precision": 0.2354568530884321,
                "recall": 0.3205128205128205,
                "main_score": 0.2543301042069852
            },
            {
                "hf_subset": "hin-eng",
                "languages": [
                    "hin-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.946,
                "f1": 0.9286666666666666,
                "precision": 0.9201666666666667,
                "recall": 0.946,
                "main_score": 0.9286666666666666
            },
            {
                "hf_subset": "dsb-eng",
                "languages": [
                    "dsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.14822546972860126,
                "f1": 0.12439321820122155,
                "precision": 0.11940341857811414,
                "recall": 0.14822546972860126,
                "main_score": 0.12439321820122155
            },
            {
                "hf_subset": "ber-eng",
                "languages": [
                    "ber-Tfng",
                    "eng-Latn"
                ],
                "accuracy": 0.067,
                "f1": 0.055344432986074575,
                "precision": 0.052991072733918114,
                "recall": 0.067,
                "main_score": 0.055344432986074575
            },
            {
                "hf_subset": "tam-eng",
                "languages": [
                    "tam-Taml",
                    "eng-Latn"
                ],
                "accuracy": 0.8794788273615635,
                "f1": 0.8465798045602605,
                "precision": 0.8320846905537459,
                "recall": 0.8794788273615635,
                "main_score": 0.8465798045602605
            },
            {
                "hf_subset": "slk-eng",
                "languages": [
                    "slk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.138,
                "f1": 0.11356912127897373,
                "precision": 0.10778191051205624,
                "recall": 0.138,
                "main_score": 0.11356912127897373
            },
            {
                "hf_subset": "tgl-eng",
                "languages": [
                    "tgl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.137,
                "f1": 0.1074774895608627,
                "precision": 0.09966243757837462,
                "recall": 0.137,
                "main_score": 0.1074774895608627
            },
            {
                "hf_subset": "ast-eng",
                "languages": [
                    "ast-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7637795275590551,
                "f1": 0.7124671916010499,
                "precision": 0.6920697412823397,
                "recall": 0.7637795275590551,
                "main_score": 0.7124671916010499
            },
            {
                "hf_subset": "mkd-eng",
                "languages": [
                    "mkd-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.18099999999999997,
                "f1": 0.13934122253809159,
                "precision": 0.12815974391105972,
                "recall": 0.18099999999999997,
                "main_score": 0.13934122253809159
            },
            {
                "hf_subset": "khm-eng",
                "languages": [
                    "khm-Khmr",
                    "eng-Latn"
                ],
                "accuracy": 0.006925207756232687,
                "f1": 0.0008966600365830146,
                "precision": 0.0005066184676394412,
                "recall": 0.006925207756232687,
                "main_score": 0.0008966600365830146
            },
            {
                "hf_subset": "ces-eng",
                "languages": [
                    "ces-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.111,
                "f1": 0.08286460432380521,
                "precision": 0.07686198801198801,
                "recall": 0.111,
                "main_score": 0.08286460432380521
            },
            {
                "hf_subset": "tzl-eng",
                "languages": [
                    "tzl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.3846153846153847,
                "f1": 0.3164089994972347,
                "precision": 0.29298878205128204,
                "recall": 0.3846153846153847,
                "main_score": 0.3164089994972347
            },
            {
                "hf_subset": "urd-eng",
                "languages": [
                    "urd-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.812,
                "f1": 0.7677103174603175,
                "precision": 0.7496511904761906,
                "recall": 0.812,
                "main_score": 0.7677103174603175
            },
            {
                "hf_subset": "ara-eng",
                "languages": [
                    "ara-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.9060000000000001,
                "f1": 0.8820666666666664,
                "precision": 0.8714833333333334,
                "recall": 0.9060000000000001,
                "main_score": 0.8820666666666664
            },
            {
                "hf_subset": "kor-eng",
                "languages": [
                    "kor-Hang",
                    "eng-Latn"
                ],
                "accuracy": 0.357,
                "f1": 0.2915912762074527,
                "precision": 0.27109529030910606,
                "recall": 0.357,
                "main_score": 0.2915912762074527
            },
            {
                "hf_subset": "yid-eng",
                "languages": [
                    "yid-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.009433962264150943,
                "f1": 0.0028088681664921334,
                "precision": 0.0022694150916099465,
                "recall": 0.009433962264150943,
                "main_score": 0.0028088681664921334
            },
            {
                "hf_subset": "fin-eng",
                "languages": [
                    "fin-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.075,
                "f1": 0.05825362182391272,
                "precision": 0.05526187577939453,
                "recall": 0.075,
                "main_score": 0.05825362182391272
            },
            {
                "hf_subset": "tha-eng",
                "languages": [
                    "tha-Thai",
                    "eng-Latn"
                ],
                "accuracy": 0.041970802919708027,
                "f1": 0.03079215618580677,
                "precision": 0.028501768792419,
                "recall": 0.041970802919708027,
                "main_score": 0.03079215618580677
            },
            {
                "hf_subset": "wuu-eng",
                "languages": [
                    "wuu-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.879,
                "f1": 0.8460499999999999,
                "precision": 0.8311428571428572,
                "recall": 0.879,
                "main_score": 0.8460499999999999
            }
        ]
    }
}