{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "task_name": "WebLINXCandidatesReranking",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "map": 0.125282,
        "mrr": 0.106134,
        "nAUC_map_max": -0.005121,
        "nAUC_map_std": -0.008887,
        "nAUC_map_diff1": 0.05828,
        "nAUC_mrr_max": -0.000349,
        "nAUC_mrr_std": -0.012587,
        "nAUC_mrr_diff1": 0.056647,
        "main_score": 0.106134,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_iid": [
      {
        "map": 0.090264,
        "mrr": 0.072578,
        "nAUC_map_max": -0.061289,
        "nAUC_map_std": 0.070965,
        "nAUC_map_diff1": 0.051274,
        "nAUC_mrr_max": -0.046934,
        "nAUC_mrr_std": 0.050673,
        "nAUC_mrr_diff1": 0.052931,
        "main_score": 0.072578,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_cat": [
      {
        "map": 0.078554,
        "mrr": 0.060707,
        "nAUC_map_max": -0.042588,
        "nAUC_map_std": 0.048301,
        "nAUC_map_diff1": 0.076181,
        "nAUC_mrr_max": -0.034076,
        "nAUC_mrr_std": 0.040329,
        "nAUC_mrr_diff1": 0.086001,
        "main_score": 0.060707,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_geo": [
      {
        "map": 0.088105,
        "mrr": 0.067694,
        "nAUC_map_max": -0.037489,
        "nAUC_map_std": -0.005706,
        "nAUC_map_diff1": 0.124408,
        "nAUC_mrr_max": -0.032739,
        "nAUC_mrr_std": -0.000381,
        "nAUC_mrr_diff1": 0.130695,
        "main_score": 0.067694,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_vis": [
      {
        "map": 0.092512,
        "mrr": 0.074343,
        "nAUC_map_max": -0.036816,
        "nAUC_map_std": 0.099081,
        "nAUC_map_diff1": 0.076772,
        "nAUC_mrr_max": -0.023466,
        "nAUC_mrr_std": 0.084519,
        "nAUC_mrr_diff1": 0.076887,
        "main_score": 0.074343,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_web": [
      {
        "map": 0.077747,
        "mrr": 0.06041,
        "nAUC_map_max": -0.034661,
        "nAUC_map_std": 0.070544,
        "nAUC_map_diff1": 0.101643,
        "nAUC_mrr_max": -0.026489,
        "nAUC_mrr_std": 0.066007,
        "nAUC_mrr_diff1": 0.106983,
        "main_score": 0.06041,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 2380.6596920490265,
  "kg_co2_emissions": 0.20433180867632533
}