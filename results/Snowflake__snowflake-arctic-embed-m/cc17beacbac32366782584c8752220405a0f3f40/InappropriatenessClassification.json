{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "task_name": "InappropriatenessClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.522119,
        "f1": 0.512066,
        "f1_weighted": 0.512066,
        "ap": 0.513301,
        "ap_weighted": 0.513301,
        "scores_per_experiment": [
          {
            "accuracy": 0.529297,
            "f1": 0.529243,
            "f1_weighted": 0.529243,
            "ap": 0.515526,
            "ap_weighted": 0.515526
          },
          {
            "accuracy": 0.575195,
            "f1": 0.570853,
            "f1_weighted": 0.570853,
            "ap": 0.544676,
            "ap_weighted": 0.544676
          },
          {
            "accuracy": 0.491211,
            "f1": 0.472066,
            "f1_weighted": 0.472066,
            "ap": 0.495661,
            "ap_weighted": 0.495661
          },
          {
            "accuracy": 0.506348,
            "f1": 0.458462,
            "f1_weighted": 0.458462,
            "ap": 0.503199,
            "ap_weighted": 0.503199
          },
          {
            "accuracy": 0.566406,
            "f1": 0.564912,
            "f1_weighted": 0.564912,
            "ap": 0.53715,
            "ap_weighted": 0.53715
          },
          {
            "accuracy": 0.506836,
            "f1": 0.491648,
            "f1_weighted": 0.491648,
            "ap": 0.503453,
            "ap_weighted": 0.503453
          },
          {
            "accuracy": 0.564453,
            "f1": 0.564453,
            "f1_weighted": 0.564453,
            "ap": 0.536373,
            "ap_weighted": 0.536373
          },
          {
            "accuracy": 0.436523,
            "f1": 0.432887,
            "f1_weighted": 0.432887,
            "ap": 0.473059,
            "ap_weighted": 0.473059
          },
          {
            "accuracy": 0.541016,
            "f1": 0.537601,
            "f1_weighted": 0.537601,
            "ap": 0.521943,
            "ap_weighted": 0.521943
          },
          {
            "accuracy": 0.503906,
            "f1": 0.498533,
            "f1_weighted": 0.498533,
            "ap": 0.501972,
            "ap_weighted": 0.501972
          }
        ],
        "main_score": 0.522119,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 8.810211181640625,
  "kg_co2_emissions": 0.000310093263565196
}