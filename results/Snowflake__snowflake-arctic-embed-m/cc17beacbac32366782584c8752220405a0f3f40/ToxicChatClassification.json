{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.785567,
        "f1": 0.65263,
        "f1_weighted": 0.812231,
        "ap": 0.26485,
        "ap_weighted": 0.26485,
        "scores_per_experiment": [
          {
            "accuracy": 0.761168,
            "f1": 0.617096,
            "f1_weighted": 0.79305,
            "ap": 0.218183,
            "ap_weighted": 0.218183
          },
          {
            "accuracy": 0.85567,
            "f1": 0.690776,
            "f1_weighted": 0.859938,
            "ap": 0.279977,
            "ap_weighted": 0.279977
          },
          {
            "accuracy": 0.823024,
            "f1": 0.684042,
            "f1_weighted": 0.841027,
            "ap": 0.286345,
            "ap_weighted": 0.286345
          },
          {
            "accuracy": 0.686426,
            "f1": 0.582379,
            "f1_weighted": 0.738539,
            "ap": 0.218004,
            "ap_weighted": 0.218004
          },
          {
            "accuracy": 0.695876,
            "f1": 0.565274,
            "f1_weighted": 0.743777,
            "ap": 0.184856,
            "ap_weighted": 0.184856
          },
          {
            "accuracy": 0.815292,
            "f1": 0.664051,
            "f1_weighted": 0.832914,
            "ap": 0.257183,
            "ap_weighted": 0.257183
          },
          {
            "accuracy": 0.886598,
            "f1": 0.755723,
            "f1_weighted": 0.88967,
            "ap": 0.382324,
            "ap_weighted": 0.382324
          },
          {
            "accuracy": 0.704467,
            "f1": 0.606856,
            "f1_weighted": 0.75361,
            "ap": 0.24752,
            "ap_weighted": 0.24752
          },
          {
            "accuracy": 0.804983,
            "f1": 0.665913,
            "f1_weighted": 0.82739,
            "ap": 0.267404,
            "ap_weighted": 0.267404
          },
          {
            "accuracy": 0.822165,
            "f1": 0.694185,
            "f1_weighted": 0.84239,
            "ap": 0.306707,
            "ap_weighted": 0.306707
          }
        ],
        "main_score": 0.785567,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 9.028916835784912,
  "kg_co2_emissions": 0.000293503162133883
}