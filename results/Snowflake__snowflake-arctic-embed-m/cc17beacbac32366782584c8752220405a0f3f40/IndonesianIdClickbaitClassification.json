{
  "dataset_revision": "9fa4d0824015fe537ae2c8166781f5c79873da2c",
  "task_name": "IndonesianIdClickbaitClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "train": [
      {
        "accuracy": 0.565674,
        "f1": 0.559054,
        "f1_weighted": 0.565472,
        "ap": 0.458358,
        "ap_weighted": 0.458358,
        "scores_per_experiment": [
          {
            "accuracy": 0.614258,
            "f1": 0.606254,
            "f1_weighted": 0.615299,
            "ap": 0.486361,
            "ap_weighted": 0.486361
          },
          {
            "accuracy": 0.500977,
            "f1": 0.500929,
            "f1_weighted": 0.500144,
            "ap": 0.427443,
            "ap_weighted": 0.427443
          },
          {
            "accuracy": 0.562012,
            "f1": 0.540103,
            "f1_weighted": 0.556277,
            "ap": 0.441955,
            "ap_weighted": 0.441955
          },
          {
            "accuracy": 0.534668,
            "f1": 0.532697,
            "f1_weighted": 0.537587,
            "ap": 0.439625,
            "ap_weighted": 0.439625
          },
          {
            "accuracy": 0.586426,
            "f1": 0.586421,
            "f1_weighted": 0.586193,
            "ap": 0.479611,
            "ap_weighted": 0.479611
          },
          {
            "accuracy": 0.501465,
            "f1": 0.495909,
            "f1_weighted": 0.504436,
            "ap": 0.418207,
            "ap_weighted": 0.418207
          },
          {
            "accuracy": 0.602539,
            "f1": 0.601837,
            "f1_weighted": 0.604531,
            "ap": 0.486901,
            "ap_weighted": 0.486901
          },
          {
            "accuracy": 0.595215,
            "f1": 0.59511,
            "f1_weighted": 0.596161,
            "ap": 0.484009,
            "ap_weighted": 0.484009
          },
          {
            "accuracy": 0.562988,
            "f1": 0.537922,
            "f1_weighted": 0.555263,
            "ap": 0.441187,
            "ap_weighted": 0.441187
          },
          {
            "accuracy": 0.596191,
            "f1": 0.593356,
            "f1_weighted": 0.598827,
            "ap": 0.47828,
            "ap_weighted": 0.47828
          }
        ],
        "main_score": 0.559054,
        "hf_subset": "default",
        "languages": [
          "ind-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 7.6020827293396,
  "kg_co2_emissions": 0.00023738383331911372
}