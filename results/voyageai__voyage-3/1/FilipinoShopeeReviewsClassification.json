{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.332861,
        "f1": 0.323961,
        "f1_weighted": 0.323951,
        "scores_per_experiment": [
          {
            "accuracy": 0.345215,
            "f1": 0.335792,
            "f1_weighted": 0.33578
          },
          {
            "accuracy": 0.342285,
            "f1": 0.33411,
            "f1_weighted": 0.334125
          },
          {
            "accuracy": 0.327148,
            "f1": 0.319747,
            "f1_weighted": 0.319761
          },
          {
            "accuracy": 0.338867,
            "f1": 0.331624,
            "f1_weighted": 0.331596
          },
          {
            "accuracy": 0.312012,
            "f1": 0.304115,
            "f1_weighted": 0.304097
          },
          {
            "accuracy": 0.325195,
            "f1": 0.311231,
            "f1_weighted": 0.311219
          },
          {
            "accuracy": 0.316406,
            "f1": 0.313166,
            "f1_weighted": 0.313155
          },
          {
            "accuracy": 0.37793,
            "f1": 0.377253,
            "f1_weighted": 0.377226
          },
          {
            "accuracy": 0.334473,
            "f1": 0.320749,
            "f1_weighted": 0.320706
          },
          {
            "accuracy": 0.309082,
            "f1": 0.291825,
            "f1_weighted": 0.291843
          }
        ],
        "main_score": 0.332861,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.334961,
        "f1": 0.325616,
        "f1_weighted": 0.325616,
        "scores_per_experiment": [
          {
            "accuracy": 0.335449,
            "f1": 0.327684,
            "f1_weighted": 0.32769
          },
          {
            "accuracy": 0.349121,
            "f1": 0.339311,
            "f1_weighted": 0.339313
          },
          {
            "accuracy": 0.32959,
            "f1": 0.319883,
            "f1_weighted": 0.319913
          },
          {
            "accuracy": 0.364258,
            "f1": 0.356239,
            "f1_weighted": 0.356223
          },
          {
            "accuracy": 0.322754,
            "f1": 0.31704,
            "f1_weighted": 0.317041
          },
          {
            "accuracy": 0.329102,
            "f1": 0.313156,
            "f1_weighted": 0.313131
          },
          {
            "accuracy": 0.32666,
            "f1": 0.320572,
            "f1_weighted": 0.320567
          },
          {
            "accuracy": 0.361328,
            "f1": 0.361121,
            "f1_weighted": 0.361118
          },
          {
            "accuracy": 0.335938,
            "f1": 0.324858,
            "f1_weighted": 0.324842
          },
          {
            "accuracy": 0.29541,
            "f1": 0.276291,
            "f1_weighted": 0.276326
          }
        ],
        "main_score": 0.334961,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 640.0576689243317,
  "kg_co2_emissions": null
}