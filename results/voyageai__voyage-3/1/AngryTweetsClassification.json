{
  "dataset_revision": "20b0e6081892e78179356fada741b7afa381443d",
  "task_name": "AngryTweetsClassification",
  "mteb_version": "2.1.11",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.551098,
            "f1": 0.547964,
            "f1_weighted": 0.554153,
            "precision": 0.550822,
            "precision_weighted": 0.562568,
            "recall": 0.550839,
            "recall_weighted": 0.551098,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.536772,
            "f1": 0.525803,
            "f1_weighted": 0.530027,
            "precision": 0.531128,
            "precision_weighted": 0.541625,
            "recall": 0.540473,
            "recall_weighted": 0.536772,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.547278,
            "f1": 0.526216,
            "f1_weighted": 0.534294,
            "precision": 0.544374,
            "precision_weighted": 0.55558,
            "recall": 0.541914,
            "recall_weighted": 0.547278,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.526266,
            "f1": 0.523902,
            "f1_weighted": 0.523667,
            "precision": 0.522255,
            "precision_weighted": 0.525374,
            "recall": 0.530318,
            "recall_weighted": 0.526266,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.548233,
            "f1": 0.539128,
            "f1_weighted": 0.547238,
            "precision": 0.541773,
            "precision_weighted": 0.552233,
            "recall": 0.542696,
            "recall_weighted": 0.548233,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.553009,
            "f1": 0.53505,
            "f1_weighted": 0.5465,
            "precision": 0.536248,
            "precision_weighted": 0.545797,
            "recall": 0.539161,
            "recall_weighted": 0.553009,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.527221,
            "f1": 0.52271,
            "f1_weighted": 0.53115,
            "precision": 0.527091,
            "precision_weighted": 0.540251,
            "recall": 0.523334,
            "recall_weighted": 0.527221,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.505253,
            "f1": 0.494515,
            "f1_weighted": 0.505308,
            "precision": 0.495206,
            "precision_weighted": 0.50594,
            "recall": 0.49438,
            "recall_weighted": 0.505253,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.543457,
            "f1": 0.540165,
            "f1_weighted": 0.545191,
            "precision": 0.541428,
            "precision_weighted": 0.551481,
            "recall": 0.543935,
            "recall_weighted": 0.543457,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.580707,
            "f1": 0.566369,
            "f1_weighted": 0.575192,
            "precision": 0.572699,
            "precision_weighted": 0.581656,
            "recall": 0.571416,
            "recall_weighted": 0.580707,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.541929,
        "f1": 0.532182,
        "f1_weighted": 0.539272,
        "precision": 0.536303,
        "precision_weighted": 0.54625,
        "recall": 0.537847,
        "recall_weighted": 0.541929,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.541929,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 93.25021624565125,
  "kg_co2_emissions": null
}