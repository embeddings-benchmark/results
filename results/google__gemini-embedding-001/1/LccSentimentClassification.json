{
  "dataset_revision": "de7ba3406ee55ea2cc52a0a41408fa6aede6d3c6",
  "task_name": "LccSentimentClassification",
  "mteb_version": "2.3.7",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.713333,
            "f1": 0.697623,
            "f1_weighted": 0.718272,
            "precision": 0.710493,
            "precision_weighted": 0.759846,
            "recall": 0.739037,
            "recall_weighted": 0.713333,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.706667,
            "f1": 0.69774,
            "f1_weighted": 0.709799,
            "precision": 0.689025,
            "precision_weighted": 0.738381,
            "recall": 0.740725,
            "recall_weighted": 0.706667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.673333,
            "f1": 0.675022,
            "f1_weighted": 0.681924,
            "precision": 0.694665,
            "precision_weighted": 0.768867,
            "recall": 0.745484,
            "recall_weighted": 0.673333,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.673333,
            "f1": 0.677764,
            "f1_weighted": 0.671691,
            "precision": 0.669935,
            "precision_weighted": 0.725163,
            "recall": 0.746657,
            "recall_weighted": 0.673333,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.74,
            "f1": 0.736357,
            "f1_weighted": 0.741142,
            "precision": 0.720151,
            "precision_weighted": 0.766266,
            "recall": 0.782287,
            "recall_weighted": 0.74,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.726667,
            "f1": 0.711564,
            "f1_weighted": 0.725408,
            "precision": 0.712885,
            "precision_weighted": 0.72558,
            "recall": 0.712156,
            "recall_weighted": 0.726667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.72,
            "f1": 0.694534,
            "f1_weighted": 0.721924,
            "precision": 0.691071,
            "precision_weighted": 0.728131,
            "recall": 0.704782,
            "recall_weighted": 0.72,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.653333,
            "f1": 0.650529,
            "f1_weighted": 0.662228,
            "precision": 0.676167,
            "precision_weighted": 0.713644,
            "recall": 0.668141,
            "recall_weighted": 0.653333,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.726667,
            "f1": 0.720524,
            "f1_weighted": 0.731384,
            "precision": 0.720072,
            "precision_weighted": 0.774916,
            "recall": 0.773081,
            "recall_weighted": 0.726667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.66,
            "f1": 0.64903,
            "f1_weighted": 0.661233,
            "precision": 0.64,
            "precision_weighted": 0.6664,
            "recall": 0.661922,
            "recall_weighted": 0.66,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.699333,
        "f1": 0.691069,
        "f1_weighted": 0.7025,
        "precision": 0.692446,
        "precision_weighted": 0.736719,
        "recall": 0.727427,
        "recall_weighted": 0.699333,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.699333,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 76.166987657547,
  "kg_co2_emissions": null
}