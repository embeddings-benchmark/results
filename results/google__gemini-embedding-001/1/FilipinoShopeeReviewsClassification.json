{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.34.7",
  "scores": {
    "validation": [
      {
        "accuracy": 0.481543,
        "f1": 0.450836,
        "f1_weighted": 0.450797,
        "scores_per_experiment": [
          {
            "accuracy": 0.489258,
            "f1": 0.469459,
            "f1_weighted": 0.469419
          },
          {
            "accuracy": 0.456543,
            "f1": 0.415106,
            "f1_weighted": 0.415071
          },
          {
            "accuracy": 0.494629,
            "f1": 0.470282,
            "f1_weighted": 0.47029
          },
          {
            "accuracy": 0.478516,
            "f1": 0.442405,
            "f1_weighted": 0.442357
          },
          {
            "accuracy": 0.458984,
            "f1": 0.401818,
            "f1_weighted": 0.40176
          },
          {
            "accuracy": 0.476562,
            "f1": 0.433439,
            "f1_weighted": 0.433425
          },
          {
            "accuracy": 0.472168,
            "f1": 0.438892,
            "f1_weighted": 0.438828
          },
          {
            "accuracy": 0.495605,
            "f1": 0.49048,
            "f1_weighted": 0.490496
          },
          {
            "accuracy": 0.492676,
            "f1": 0.469286,
            "f1_weighted": 0.469211
          },
          {
            "accuracy": 0.500488,
            "f1": 0.477192,
            "f1_weighted": 0.477112
          }
        ],
        "main_score": 0.481543,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.4875,
        "f1": 0.457166,
        "f1_weighted": 0.457129,
        "scores_per_experiment": [
          {
            "accuracy": 0.506348,
            "f1": 0.486746,
            "f1_weighted": 0.486706
          },
          {
            "accuracy": 0.478027,
            "f1": 0.436977,
            "f1_weighted": 0.436931
          },
          {
            "accuracy": 0.504395,
            "f1": 0.478204,
            "f1_weighted": 0.478227
          },
          {
            "accuracy": 0.467285,
            "f1": 0.42848,
            "f1_weighted": 0.428423
          },
          {
            "accuracy": 0.467773,
            "f1": 0.408398,
            "f1_weighted": 0.408331
          },
          {
            "accuracy": 0.484375,
            "f1": 0.442199,
            "f1_weighted": 0.44219
          },
          {
            "accuracy": 0.479004,
            "f1": 0.451784,
            "f1_weighted": 0.451737
          },
          {
            "accuracy": 0.493652,
            "f1": 0.489763,
            "f1_weighted": 0.489786
          },
          {
            "accuracy": 0.496582,
            "f1": 0.474318,
            "f1_weighted": 0.474256
          },
          {
            "accuracy": 0.497559,
            "f1": 0.474786,
            "f1_weighted": 0.474704
          }
        ],
        "main_score": 0.4875,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 29.027270317077637,
  "kg_co2_emissions": null
}