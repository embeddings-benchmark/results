{
  "dataset_revision": "e42d330f33d65b7b72dfd408883daf1661f06f18",
  "task_name": "CyrillicTurkicLangClassification",
  "mteb_version": "1.34.7",
  "scores": {
    "test": [
      {
        "accuracy": 0.952979,
        "f1": 0.952726,
        "f1_weighted": 0.952759,
        "scores_per_experiment": [
          {
            "accuracy": 0.95459,
            "f1": 0.95442,
            "f1_weighted": 0.954453
          },
          {
            "accuracy": 0.959473,
            "f1": 0.959396,
            "f1_weighted": 0.959416
          },
          {
            "accuracy": 0.955078,
            "f1": 0.954898,
            "f1_weighted": 0.954929
          },
          {
            "accuracy": 0.944336,
            "f1": 0.943809,
            "f1_weighted": 0.943857
          },
          {
            "accuracy": 0.949219,
            "f1": 0.94884,
            "f1_weighted": 0.948885
          },
          {
            "accuracy": 0.953125,
            "f1": 0.952902,
            "f1_weighted": 0.95294
          },
          {
            "accuracy": 0.956543,
            "f1": 0.95633,
            "f1_weighted": 0.956354
          },
          {
            "accuracy": 0.95459,
            "f1": 0.954456,
            "f1_weighted": 0.954485
          },
          {
            "accuracy": 0.953613,
            "f1": 0.953419,
            "f1_weighted": 0.95345
          },
          {
            "accuracy": 0.949219,
            "f1": 0.948784,
            "f1_weighted": 0.948817
          }
        ],
        "main_score": 0.952979,
        "hf_subset": "default",
        "languages": [
          "bak-Cyrl",
          "chv-Cyrl",
          "tat-Cyrl",
          "kir-Cyrl",
          "rus-Cyrl",
          "kaz-Cyrl",
          "tyv-Cyrl",
          "krc-Cyrl",
          "sah-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 15.54067611694336,
  "kg_co2_emissions": null
}