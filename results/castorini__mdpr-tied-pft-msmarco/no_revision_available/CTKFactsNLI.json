{
  "dataset_revision": "387ae4582c8054cb52ef57ef0941f19bd8012abf",
  "evaluation_time": 5.582190990447998,
  "kg_co2_emissions": 0.00017672375805368053,
  "mteb_version": "1.16.5",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.7093333333333334,
        "cosine_accuracy_threshold": 0.5779852867126465,
        "cosine_ap": 0.7581248669986413,
        "cosine_f1": 0.8256,
        "cosine_f1_threshold": 0.5779852867126465,
        "cosine_precision": 0.7068493150684931,
        "cosine_recall": 0.9923076923076923,
        "dot_accuracy": 0.7066666666666667,
        "dot_accuracy_threshold": 56.42096710205078,
        "dot_ap": 0.7184471151286586,
        "dot_f1": 0.8248407643312101,
        "dot_f1_threshold": 52.04518127441406,
        "dot_precision": 0.7038043478260869,
        "dot_recall": 0.9961538461538462,
        "euclidean_accuracy": 0.7093333333333334,
        "euclidean_accuracy_threshold": 8.959742546081543,
        "euclidean_ap": 0.7658559922542355,
        "euclidean_f1": 0.8256,
        "euclidean_f1_threshold": 8.959742546081543,
        "euclidean_precision": 0.7068493150684931,
        "euclidean_recall": 0.9923076923076923,
        "hf_subset": "default",
        "languages": [
          "ces-Latn"
        ],
        "main_score": 0.7658559922542355,
        "manhattan_accuracy": 0.704,
        "manhattan_accuracy_threshold": 199.57261657714844,
        "manhattan_ap": 0.7631046244913242,
        "manhattan_f1": 0.8235294117647061,
        "manhattan_f1_threshold": 199.57261657714844,
        "manhattan_precision": 0.7018970189701897,
        "manhattan_recall": 0.9961538461538462,
        "max_accuracy": 0.7093333333333334,
        "max_ap": 0.7658559922542355,
        "max_f1": 0.8256,
        "max_precision": 0.7068493150684931,
        "max_recall": 0.9961538461538462,
        "similarity_accuracy": 0.7093333333333334,
        "similarity_accuracy_threshold": 0.5779852867126465,
        "similarity_ap": 0.7581248669986413,
        "similarity_f1": 0.8256,
        "similarity_f1_threshold": 0.5779852867126465,
        "similarity_precision": 0.7068493150684931,
        "similarity_recall": 0.9923076923076923
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.6459016393442623,
        "cosine_accuracy_threshold": 0.6802624464035034,
        "cosine_ap": 0.6988404049540149,
        "cosine_f1": 0.7717171717171717,
        "cosine_f1_threshold": 0.547226071357727,
        "cosine_precision": 0.6282894736842105,
        "cosine_recall": 1.0,
        "dot_accuracy": 0.6295081967213115,
        "dot_accuracy_threshold": 51.901824951171875,
        "dot_ap": 0.7054034890923918,
        "dot_f1": 0.7717171717171717,
        "dot_f1_threshold": 51.901824951171875,
        "dot_precision": 0.6282894736842105,
        "dot_recall": 1.0,
        "euclidean_accuracy": 0.6491803278688525,
        "euclidean_accuracy_threshold": 8.032059669494629,
        "euclidean_ap": 0.6877655629992765,
        "euclidean_f1": 0.7732793522267206,
        "euclidean_f1_threshold": 9.283866882324219,
        "euclidean_precision": 0.6303630363036303,
        "euclidean_recall": 1.0,
        "hf_subset": "default",
        "languages": [
          "ces-Latn"
        ],
        "main_score": 0.7054034890923918,
        "manhattan_accuracy": 0.6524590163934426,
        "manhattan_accuracy_threshold": 172.28628540039062,
        "manhattan_ap": 0.6911270821935526,
        "manhattan_f1": 0.7748478701825557,
        "manhattan_f1_threshold": 203.76580810546875,
        "manhattan_precision": 0.6324503311258278,
        "manhattan_recall": 1.0,
        "max_accuracy": 0.6524590163934426,
        "max_ap": 0.7054034890923918,
        "max_f1": 0.7748478701825557,
        "max_precision": 0.6324503311258278,
        "max_recall": 1.0,
        "similarity_accuracy": 0.6459016393442623,
        "similarity_accuracy_threshold": 0.6802624464035034,
        "similarity_ap": 0.6988404049540149,
        "similarity_f1": 0.7717171717171717,
        "similarity_f1_threshold": 0.547226071357727,
        "similarity_precision": 0.6282894736842105,
        "similarity_recall": 1.0
      }
    ]
  },
  "task_name": "CTKFactsNLI"
}