{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 32.12063980102539,
  "kg_co2_emissions": 0.0010331409005146814,
  "mteb_version": "1.16.5",
  "scores": {
    "test": [
      {
        "accuracy": 0.258984375,
        "f1": 0.25463214431170433,
        "f1_weighted": 0.25463464358897875,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.258984375,
        "scores_per_experiment": [
          {
            "accuracy": 0.263671875,
            "f1": 0.25769048981535275,
            "f1_weighted": 0.2576955088284457
          },
          {
            "accuracy": 0.2626953125,
            "f1": 0.25396161980765053,
            "f1_weighted": 0.25393101537596524
          },
          {
            "accuracy": 0.23388671875,
            "f1": 0.22977462303508905,
            "f1_weighted": 0.22972781943368561
          },
          {
            "accuracy": 0.25634765625,
            "f1": 0.2568949259016802,
            "f1_weighted": 0.25688309973209655
          },
          {
            "accuracy": 0.26513671875,
            "f1": 0.26445815963963126,
            "f1_weighted": 0.26446099798295336
          },
          {
            "accuracy": 0.26025390625,
            "f1": 0.25212342778610675,
            "f1_weighted": 0.25214222665731645
          },
          {
            "accuracy": 0.25439453125,
            "f1": 0.254359844408842,
            "f1_weighted": 0.2543722647732517
          },
          {
            "accuracy": 0.2529296875,
            "f1": 0.24623231253661287,
            "f1_weighted": 0.2462434345763676
          },
          {
            "accuracy": 0.27099609375,
            "f1": 0.26961049647037105,
            "f1_weighted": 0.26961880450297676
          },
          {
            "accuracy": 0.26953125,
            "f1": 0.2612155437157069,
            "f1_weighted": 0.2612712640267287
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.257861328125,
        "f1": 0.25377212899220114,
        "f1_weighted": 0.2537767863417006,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.257861328125,
        "scores_per_experiment": [
          {
            "accuracy": 0.27685546875,
            "f1": 0.2701923455116736,
            "f1_weighted": 0.27021247500374435
          },
          {
            "accuracy": 0.244140625,
            "f1": 0.23676986603443434,
            "f1_weighted": 0.23673599695157355
          },
          {
            "accuracy": 0.23095703125,
            "f1": 0.22675099604637122,
            "f1_weighted": 0.2267247489178946
          },
          {
            "accuracy": 0.26513671875,
            "f1": 0.2652780307125815,
            "f1_weighted": 0.2652780453002538
          },
          {
            "accuracy": 0.23779296875,
            "f1": 0.23659995746139914,
            "f1_weighted": 0.23659973431178483
          },
          {
            "accuracy": 0.2734375,
            "f1": 0.2659104789726455,
            "f1_weighted": 0.26593761219035195
          },
          {
            "accuracy": 0.2451171875,
            "f1": 0.24534809688195644,
            "f1_weighted": 0.24535213617881618
          },
          {
            "accuracy": 0.287109375,
            "f1": 0.28189254075236153,
            "f1_weighted": 0.281895962169859
          },
          {
            "accuracy": 0.26904296875,
            "f1": 0.26802225742095415,
            "f1_weighted": 0.2680279611098715
          },
          {
            "accuracy": 0.2490234375,
            "f1": 0.2409567201276337,
            "f1_weighted": 0.24100319128285624
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}