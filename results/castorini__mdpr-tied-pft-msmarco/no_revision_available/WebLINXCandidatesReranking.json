{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 4309.162761211395,
  "kg_co2_emissions": 0.25283097648117947,
  "mteb_version": "1.16.5",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.04849774835027644,
        "map": 0.06362978362985319,
        "mrr": 0.04849774835027644,
        "nAUC_map_diff1": 0.09598573072220351,
        "nAUC_map_max": -0.33792270141133496,
        "nAUC_map_std": -0.17166826628933854,
        "nAUC_mrr_diff1": 0.09378387195069238,
        "nAUC_mrr_max": -0.3423538603250585,
        "nAUC_mrr_std": -0.16969388207689917
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.05490905950120759,
        "map": 0.07444088483949991,
        "mrr": 0.05490905950120759,
        "nAUC_map_diff1": 0.15318031476568186,
        "nAUC_map_max": -0.14529482019133733,
        "nAUC_map_std": 0.0487638959936668,
        "nAUC_mrr_diff1": 0.146118810744622,
        "nAUC_mrr_max": -0.13681361705659062,
        "nAUC_mrr_std": 0.029294607893595868
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.05572388899927148,
        "map": 0.07372630356241877,
        "mrr": 0.05572388899927148,
        "nAUC_map_diff1": 0.21931771980954254,
        "nAUC_map_max": -0.08870576482969149,
        "nAUC_map_std": -0.04771624292731945,
        "nAUC_mrr_diff1": 0.25442025918641087,
        "nAUC_mrr_max": -0.10098148055901253,
        "nAUC_mrr_std": -0.013516923527177033
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.06538795712068644,
        "map": 0.08191318648863682,
        "mrr": 0.06538795712068644,
        "nAUC_map_diff1": 0.054443170370228444,
        "nAUC_map_max": -0.23431788176591836,
        "nAUC_map_std": -0.017180732939186257,
        "nAUC_mrr_diff1": 0.054285235100440525,
        "nAUC_mrr_max": -0.2261310781168299,
        "nAUC_mrr_std": -0.01633649695609807
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.044135213457732546,
        "map": 0.06088664837379767,
        "mrr": 0.044135213457732546,
        "nAUC_map_diff1": 0.0028544396468412796,
        "nAUC_map_max": -0.12211564004426911,
        "nAUC_map_std": 0.06073984023431681,
        "nAUC_mrr_diff1": 0.006378704264906907,
        "nAUC_mrr_max": -0.1405058800285038,
        "nAUC_mrr_std": 0.029219175833448973
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.06868892061051937,
        "map": 0.08604833237150769,
        "mrr": 0.06868892061051937,
        "nAUC_map_diff1": 0.08609196220350208,
        "nAUC_map_max": 0.011783376425754435,
        "nAUC_map_std": -0.01869002191127045,
        "nAUC_mrr_diff1": 0.08942355641802578,
        "nAUC_mrr_max": 0.01683496686689991,
        "nAUC_mrr_std": -0.02206797742298645
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}