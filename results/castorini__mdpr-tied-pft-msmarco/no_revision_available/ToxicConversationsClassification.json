{
  "dataset_revision": "edfaf9da55d3dd50d43143d90c1ac476895ae6de",
  "evaluation_time": 23.50910711288452,
  "kg_co2_emissions": 0.0007477752000133927,
  "mteb_version": "1.16.5",
  "scores": {
    "test": [
      {
        "accuracy": 0.609814453125,
        "ap": 0.09673646099887712,
        "ap_weighted": 0.09673646099887712,
        "f1": 0.4632273658497006,
        "f1_weighted": 0.6975983947811567,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.609814453125,
        "scores_per_experiment": [
          {
            "accuracy": 0.5869140625,
            "ap": 0.08982103497611096,
            "ap_weighted": 0.08982103497611096,
            "f1": 0.44632375498517235,
            "f1_weighted": 0.68118585784909
          },
          {
            "accuracy": 0.68017578125,
            "ap": 0.10310334068933943,
            "ap_weighted": 0.10310334068933943,
            "f1": 0.5010962241169306,
            "f1_weighted": 0.7527124885809988
          },
          {
            "accuracy": 0.6708984375,
            "ap": 0.09723651655023288,
            "ap_weighted": 0.09723651655023288,
            "f1": 0.4907915802831657,
            "f1_weighted": 0.7457211548659426
          },
          {
            "accuracy": 0.640625,
            "ap": 0.09986943801440329,
            "ap_weighted": 0.09986943801440329,
            "f1": 0.4809133386229393,
            "f1_weighted": 0.7232926557127745
          },
          {
            "accuracy": 0.5546875,
            "ap": 0.09011856713623866,
            "ap_weighted": 0.09011856713623866,
            "f1": 0.43194160583941604,
            "f1_weighted": 0.6542251368613139
          },
          {
            "accuracy": 0.51904296875,
            "ap": 0.09500671224679157,
            "ap_weighted": 0.09500671224679157,
            "f1": 0.4192375823484247,
            "f1_weighted": 0.6219048216373716
          },
          {
            "accuracy": 0.6494140625,
            "ap": 0.09902846263030132,
            "ap_weighted": 0.09902846263030132,
            "f1": 0.48381299004740397,
            "f1_weighted": 0.7299304460028139
          },
          {
            "accuracy": 0.6416015625,
            "ap": 0.10164561170212767,
            "ap_weighted": 0.10164561170212767,
            "f1": 0.48313200483573493,
            "f1_weighted": 0.7240504505263813
          },
          {
            "accuracy": 0.623046875,
            "ap": 0.09477556065012663,
            "ap_weighted": 0.09477556065012663,
            "f1": 0.4678492717097017,
            "f1_weighted": 0.7097666931133855
          },
          {
            "accuracy": 0.53173828125,
            "ap": 0.09675936539309876,
            "ap_weighted": 0.09675936539309876,
            "f1": 0.42717530570811674,
            "f1_weighted": 0.6331942426614959
          }
        ]
      }
    ]
  },
  "task_name": "ToxicConversationsClassification"
}