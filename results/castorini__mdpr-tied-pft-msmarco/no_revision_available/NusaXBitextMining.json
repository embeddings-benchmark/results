{
  "dataset_revision": "fba4f2cfe2592641056f7a274c9aa8453b27a4a8",
  "evaluation_time": 16.175588369369507,
  "kg_co2_emissions": 0.0006986940569209714,
  "mteb_version": "1.16.5",
  "scores": {
    "train": [
      {
        "accuracy": 0.222,
        "f1": 0.16255431940034204,
        "hf_subset": "eng-ace",
        "languages": [
          "eng-Latn",
          "ace-Latn"
        ],
        "main_score": 0.16255431940034204,
        "precision": 0.1472862951862952,
        "recall": 0.222
      },
      {
        "accuracy": 0.292,
        "f1": 0.22757808768725551,
        "hf_subset": "eng-ban",
        "languages": [
          "eng-Latn",
          "ban-Latn"
        ],
        "main_score": 0.22757808768725551,
        "precision": 0.20987126216077828,
        "recall": 0.292
      },
      {
        "accuracy": 0.214,
        "f1": 0.14414862321141392,
        "hf_subset": "eng-bbc",
        "languages": [
          "eng-Latn",
          "bbc-Latn"
        ],
        "main_score": 0.14414862321141392,
        "precision": 0.12747446705042614,
        "recall": 0.214
      },
      {
        "accuracy": 0.236,
        "f1": 0.17996624978536196,
        "hf_subset": "eng-bjn",
        "languages": [
          "eng-Latn",
          "bjn-Latn"
        ],
        "main_score": 0.17996624978536196,
        "precision": 0.16681847874347874,
        "recall": 0.236
      },
      {
        "accuracy": 0.152,
        "f1": 0.10440217283452874,
        "hf_subset": "eng-bug",
        "languages": [
          "eng-Latn",
          "bug-Latn"
        ],
        "main_score": 0.10440217283452874,
        "precision": 0.09149566545974283,
        "recall": 0.152
      },
      {
        "accuracy": 0.644,
        "f1": 0.5885657731157732,
        "hf_subset": "eng-ind",
        "languages": [
          "eng-Latn",
          "ind-Latn"
        ],
        "main_score": 0.5885657731157732,
        "precision": 0.5713588023088023,
        "recall": 0.644
      },
      {
        "accuracy": 0.33,
        "f1": 0.26508200688200684,
        "hf_subset": "eng-jav",
        "languages": [
          "eng-Latn",
          "jav-Latn"
        ],
        "main_score": 0.26508200688200684,
        "precision": 0.24696699586962745,
        "recall": 0.33
      },
      {
        "accuracy": 0.196,
        "f1": 0.14868841285003348,
        "hf_subset": "eng-mad",
        "languages": [
          "eng-Latn",
          "mad-Latn"
        ],
        "main_score": 0.14868841285003348,
        "precision": 0.13610537763600422,
        "recall": 0.196
      },
      {
        "accuracy": 0.396,
        "f1": 0.3292023532023532,
        "hf_subset": "eng-min",
        "languages": [
          "eng-Latn",
          "min-Latn"
        ],
        "main_score": 0.3292023532023532,
        "precision": 0.30768700918964076,
        "recall": 0.396
      },
      {
        "accuracy": 0.204,
        "f1": 0.14086456445555826,
        "hf_subset": "eng-nij",
        "languages": [
          "eng-Latn",
          "nij-Latn"
        ],
        "main_score": 0.14086456445555826,
        "precision": 0.1253673742923743,
        "recall": 0.204
      },
      {
        "accuracy": 0.318,
        "f1": 0.2686201887206495,
        "hf_subset": "eng-sun",
        "languages": [
          "eng-Latn",
          "sun-Latn"
        ],
        "main_score": 0.2686201887206495,
        "precision": 0.2527628028404344,
        "recall": 0.318
      }
    ]
  },
  "task_name": "NusaXBitextMining"
}