{
    "dataset_revision": "ad9991cb51e31e31e430383c75ffb2885547b5f0",
    "task_name": "CQADupstackEnglishRetrieval",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "default",
                "languages": [
                    "eng-Latn"
                ],
                "map_at_1": 0.24313,
                "map_at_10": 0.33225,
                "map_at_100": 0.34293,
                "map_at_1000": 0.34421,
                "map_at_20": 0.33818,
                "map_at_3": 0.30545,
                "map_at_5": 0.32144,
                "mrr_at_1": 0.31083,
                "mrr_at_10": 0.39199,
                "mrr_at_100": 0.39835,
                "mrr_at_1000": 0.39892000000000005,
                "mrr_at_20": 0.3957,
                "mrr_at_3": 0.36878999999999995,
                "mrr_at_5": 0.38245000000000007,
                "ndcg_at_1": 0.31083,
                "ndcg_at_10": 0.38553,
                "ndcg_at_100": 0.42685,
                "ndcg_at_1000": 0.45144,
                "ndcg_at_20": 0.40116,
                "ndcg_at_3": 0.34607999999999994,
                "ndcg_at_5": 0.36551,
                "precision_at_1": 0.31083,
                "precision_at_10": 0.0728,
                "precision_at_100": 0.01183,
                "precision_at_1000": 0.00168,
                "precision_at_20": 0.04322,
                "precision_at_3": 0.16858,
                "precision_at_5": 0.12127,
                "recall_at_1": 0.24313,
                "recall_at_10": 0.48117,
                "recall_at_100": 0.65768,
                "recall_at_1000": 0.81935,
                "recall_at_20": 0.53689,
                "recall_at_3": 0.36335,
                "recall_at_5": 0.41803000000000007,
                "main_score": 0.38553
            }
        ]
    }
}