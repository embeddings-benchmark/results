{
  "dataset_revision": "8f95949846bb9e33c6aaf730ccfdb8fe6bcfb7a9",
  "task_name": "MultiHateClassification",
  "mteb_version": "2.1.17",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.595,
            "f1": 0.534311,
            "f1_weighted": 0.602229,
            "precision": 0.534225,
            "precision_weighted": 0.611436,
            "recall": 0.536765,
            "recall_weighted": 0.595,
            "ap": 0.31571,
            "ap_weighted": 0.31571
          },
          {
            "accuracy": 0.445,
            "f1": 0.444533,
            "f1_weighted": 0.438025,
            "precision": 0.548724,
            "precision_weighted": 0.639426,
            "recall": 0.545799,
            "recall_weighted": 0.445,
            "ap": 0.318848,
            "ap_weighted": 0.318848
          },
          {
            "accuracy": 0.608,
            "f1": 0.551109,
            "f1_weighted": 0.615671,
            "precision": 0.550582,
            "precision_weighted": 0.625896,
            "recall": 0.554714,
            "recall_weighted": 0.608,
            "ap": 0.325974,
            "ap_weighted": 0.325974
          },
          {
            "accuracy": 0.542,
            "f1": 0.503639,
            "f1_weighted": 0.559387,
            "precision": 0.511563,
            "precision_weighted": 0.592461,
            "recall": 0.513499,
            "recall_weighted": 0.542,
            "ap": 0.3039,
            "ap_weighted": 0.3039
          },
          {
            "accuracy": 0.558,
            "f1": 0.508275,
            "f1_weighted": 0.571448,
            "precision": 0.511769,
            "precision_weighted": 0.592274,
            "recall": 0.513308,
            "recall_weighted": 0.558,
            "ap": 0.303839,
            "ap_weighted": 0.303839
          },
          {
            "accuracy": 0.509,
            "f1": 0.487897,
            "f1_weighted": 0.529895,
            "precision": 0.508596,
            "precision_weighted": 0.590197,
            "recall": 0.510273,
            "recall_weighted": 0.509,
            "ap": 0.302422,
            "ap_weighted": 0.302422
          },
          {
            "accuracy": 0.457,
            "f1": 0.453048,
            "f1_weighted": 0.471831,
            "precision": 0.504515,
            "precision_weighted": 0.58655,
            "recall": 0.5051,
            "recall_weighted": 0.457,
            "ap": 0.300159,
            "ap_weighted": 0.300159
          },
          {
            "accuracy": 0.543,
            "f1": 0.507351,
            "f1_weighted": 0.560891,
            "precision": 0.516223,
            "precision_weighted": 0.596953,
            "recall": 0.51904,
            "recall_weighted": 0.543,
            "ap": 0.306458,
            "ap_weighted": 0.306458
          },
          {
            "accuracy": 0.532,
            "f1": 0.50489,
            "f1_weighted": 0.551695,
            "precision": 0.51915,
            "precision_weighted": 0.600263,
            "recall": 0.522792,
            "recall_weighted": 0.532,
            "ap": 0.308188,
            "ap_weighted": 0.308188
          },
          {
            "accuracy": 0.551,
            "f1": 0.518178,
            "f1_weighted": 0.568983,
            "precision": 0.527548,
            "precision_weighted": 0.607842,
            "recall": 0.532462,
            "recall_weighted": 0.551,
            "ap": 0.312986,
            "ap_weighted": 0.312986
          }
        ],
        "accuracy": 0.534,
        "f1": 0.501323,
        "f1_weighted": 0.547005,
        "precision": 0.52329,
        "precision_weighted": 0.60433,
        "recall": 0.525375,
        "recall_weighted": 0.534,
        "ap": 0.309848,
        "ap_weighted": 0.309848,
        "main_score": 0.534,
        "hf_subset": "nld",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 10.877569437026978,
  "kg_co2_emissions": null
}