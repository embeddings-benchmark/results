{
    "dataset_revision": "9080400076fbadbb4c4dcb136ff4eddc40b42553",
    "task_name": "Tatoeba",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "sqi-eng",
                "languages": [
                    "sqi-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.204,
                "f1": 0.15828455908556527,
                "precision": 0.148183395857142,
                "recall": 0.204,
                "main_score": 0.15828455908556527
            },
            {
                "hf_subset": "fry-eng",
                "languages": [
                    "fry-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.2658959537572254,
                "f1": 0.19027433709514635,
                "precision": 0.17053635189473337,
                "recall": 0.2658959537572254,
                "main_score": 0.19027433709514635
            },
            {
                "hf_subset": "kur-eng",
                "languages": [
                    "kur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.08780487804878048,
                "f1": 0.06111094140071713,
                "precision": 0.05623318968152088,
                "recall": 0.08780487804878048,
                "main_score": 0.06111094140071713
            },
            {
                "hf_subset": "tur-eng",
                "languages": [
                    "tur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.124,
                "f1": 0.09377654588051434,
                "precision": 0.08787308104062777,
                "recall": 0.124,
                "main_score": 0.09377654588051434
            },
            {
                "hf_subset": "deu-eng",
                "languages": [
                    "deu-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.23,
                "f1": 0.20202147869674186,
                "precision": 0.19391492475731603,
                "recall": 0.23,
                "main_score": 0.20202147869674186
            },
            {
                "hf_subset": "nld-eng",
                "languages": [
                    "nld-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.341,
                "f1": 0.2941077591689356,
                "precision": 0.28070429087454624,
                "recall": 0.341,
                "main_score": 0.2941077591689356
            },
            {
                "hf_subset": "ron-eng",
                "languages": [
                    "ron-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.206,
                "f1": 0.17356323598639312,
                "precision": 0.16518293570846235,
                "recall": 0.206,
                "main_score": 0.17356323598639312
            },
            {
                "hf_subset": "ang-eng",
                "languages": [
                    "ang-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.208955223880597,
                "f1": 0.13264176317293086,
                "precision": 0.1176782203505206,
                "recall": 0.208955223880597,
                "main_score": 0.13264176317293086
            },
            {
                "hf_subset": "ido-eng",
                "languages": [
                    "ido-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.156,
                "f1": 0.117763376390295,
                "precision": 0.10914347870755636,
                "recall": 0.156,
                "main_score": 0.117763376390295
            },
            {
                "hf_subset": "jav-eng",
                "languages": [
                    "jav-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.10243902439024391,
                "f1": 0.06743976890318354,
                "precision": 0.0610895202358617,
                "recall": 0.10243902439024391,
                "main_score": 0.06743976890318354
            },
            {
                "hf_subset": "isl-eng",
                "languages": [
                    "isl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.09700000000000002,
                "f1": 0.07491822942738051,
                "precision": 0.07074516864427854,
                "recall": 0.09700000000000002,
                "main_score": 0.07491822942738051
            },
            {
                "hf_subset": "slv-eng",
                "languages": [
                    "slv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.14094775212636695,
                "f1": 0.10166440808088711,
                "precision": 0.09417657228214014,
                "recall": 0.14094775212636695,
                "main_score": 0.10166440808088711
            },
            {
                "hf_subset": "cym-eng",
                "languages": [
                    "cym-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.08521739130434783,
                "f1": 0.056376204265661964,
                "precision": 0.051815790476192634,
                "recall": 0.08521739130434783,
                "main_score": 0.056376204265661964
            },
            {
                "hf_subset": "kaz-eng",
                "languages": [
                    "kaz-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.21913043478260869,
                "f1": 0.16974584031105772,
                "precision": 0.15775659428291006,
                "recall": 0.21913043478260869,
                "main_score": 0.16974584031105772
            },
            {
                "hf_subset": "est-eng",
                "languages": [
                    "est-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.078,
                "f1": 0.054504196974716486,
                "precision": 0.050623622153006434,
                "recall": 0.078,
                "main_score": 0.054504196974716486
            },
            {
                "hf_subset": "heb-eng",
                "languages": [
                    "heb-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.344,
                "f1": 0.3026098706848707,
                "precision": 0.28893481007908994,
                "recall": 0.344,
                "main_score": 0.3026098706848707
            },
            {
                "hf_subset": "gla-eng",
                "languages": [
                    "gla-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.04221954161640531,
                "f1": 0.02684516189451713,
                "precision": 0.02463954323534627,
                "recall": 0.04221954161640531,
                "main_score": 0.02684516189451713
            },
            {
                "hf_subset": "mar-eng",
                "languages": [
                    "mar-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.333,
                "f1": 0.2855938229523907,
                "precision": 0.2710483987488783,
                "recall": 0.333,
                "main_score": 0.2855938229523907
            },
            {
                "hf_subset": "lat-eng",
                "languages": [
                    "lat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.11200000000000002,
                "f1": 0.07868244347487681,
                "precision": 0.07121914265161029,
                "recall": 0.11200000000000002,
                "main_score": 0.07868244347487681
            },
            {
                "hf_subset": "bel-eng",
                "languages": [
                    "bel-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.298,
                "f1": 0.2434143569640063,
                "precision": 0.22947270794132874,
                "recall": 0.298,
                "main_score": 0.2434143569640063
            },
            {
                "hf_subset": "pms-eng",
                "languages": [
                    "pms-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.1295238095238095,
                "f1": 0.09269868736708248,
                "precision": 0.08408018250035058,
                "recall": 0.1295238095238095,
                "main_score": 0.09269868736708248
            },
            {
                "hf_subset": "gle-eng",
                "languages": [
                    "gle-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.06,
                "f1": 0.03647551182853867,
                "precision": 0.03268027565498654,
                "recall": 0.06,
                "main_score": 0.03647551182853867
            },
            {
                "hf_subset": "pes-eng",
                "languages": [
                    "pes-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.571,
                "f1": 0.5106459207459207,
                "precision": 0.48901510822510813,
                "recall": 0.571,
                "main_score": 0.5106459207459207
            },
            {
                "hf_subset": "nob-eng",
                "languages": [
                    "nob-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.324,
                "f1": 0.2789405452374492,
                "precision": 0.2651932166043579,
                "recall": 0.324,
                "main_score": 0.2789405452374492
            },
            {
                "hf_subset": "bul-eng",
                "languages": [
                    "bul-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.474,
                "f1": 0.4217928673178673,
                "precision": 0.4043667375987138,
                "recall": 0.474,
                "main_score": 0.4217928673178673
            },
            {
                "hf_subset": "cbk-eng",
                "languages": [
                    "cbk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.14,
                "f1": 0.10949295388694702,
                "precision": 0.10259331172194963,
                "recall": 0.14,
                "main_score": 0.10949295388694702
            },
            {
                "hf_subset": "hun-eng",
                "languages": [
                    "hun-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.182,
                "f1": 0.14773692094643706,
                "precision": 0.13903416806571642,
                "recall": 0.182,
                "main_score": 0.14773692094643706
            },
            {
                "hf_subset": "uig-eng",
                "languages": [
                    "uig-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.045,
                "f1": 0.026456893501074597,
                "precision": 0.024019067331153898,
                "recall": 0.045,
                "main_score": 0.026456893501074597
            },
            {
                "hf_subset": "rus-eng",
                "languages": [
                    "rus-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.6579999999999999,
                "f1": 0.6067913129553374,
                "precision": 0.5881261904761904,
                "recall": 0.6579999999999999,
                "main_score": 0.6067913129553374
            },
            {
                "hf_subset": "spa-eng",
                "languages": [
                    "spa-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.32899999999999996,
                "f1": 0.28186653736628603,
                "precision": 0.268010262685625,
                "recall": 0.32899999999999996,
                "main_score": 0.28186653736628603
            },
            {
                "hf_subset": "hye-eng",
                "languages": [
                    "hye-Armn",
                    "eng-Latn"
                ],
                "accuracy": 0.2560646900269542,
                "f1": 0.23126119921228783,
                "precision": 0.22426345061728545,
                "recall": 0.2560646900269542,
                "main_score": 0.23126119921228783
            },
            {
                "hf_subset": "tel-eng",
                "languages": [
                    "tel-Telu",
                    "eng-Latn"
                ],
                "accuracy": 0.2948717948717949,
                "f1": 0.23686952473184358,
                "precision": 0.22098688348688347,
                "recall": 0.2948717948717949,
                "main_score": 0.23686952473184358
            },
            {
                "hf_subset": "afr-eng",
                "languages": [
                    "afr-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.16400000000000003,
                "f1": 0.13221122174926522,
                "precision": 0.12334699312773809,
                "recall": 0.16400000000000003,
                "main_score": 0.13221122174926522
            },
            {
                "hf_subset": "mon-eng",
                "languages": [
                    "mon-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.29772727272727273,
                "f1": 0.24851015943121205,
                "precision": 0.23526050931889744,
                "recall": 0.29772727272727273,
                "main_score": 0.24851015943121205
            },
            {
                "hf_subset": "arz-eng",
                "languages": [
                    "arz-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.13626834381551362,
                "f1": 0.11419087551163022,
                "precision": 0.10700808625336927,
                "recall": 0.13626834381551362,
                "main_score": 0.11419087551163022
            },
            {
                "hf_subset": "hrv-eng",
                "languages": [
                    "hrv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.134,
                "f1": 0.10050831111434282,
                "precision": 0.09371874912594967,
                "recall": 0.134,
                "main_score": 0.10050831111434282
            },
            {
                "hf_subset": "nov-eng",
                "languages": [
                    "nov-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.311284046692607,
                "f1": 0.2522000332941052,
                "precision": 0.23700114736106953,
                "recall": 0.311284046692607,
                "main_score": 0.2522000332941052
            },
            {
                "hf_subset": "gsw-eng",
                "languages": [
                    "gsw-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.1794871794871795,
                "f1": 0.1168781835448502,
                "precision": 0.1060333295111417,
                "recall": 0.1794871794871795,
                "main_score": 0.1168781835448502
            },
            {
                "hf_subset": "nds-eng",
                "languages": [
                    "nds-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.18899999999999997,
                "f1": 0.14988123337504142,
                "precision": 0.14116226418627315,
                "recall": 0.18899999999999997,
                "main_score": 0.14988123337504142
            },
            {
                "hf_subset": "ukr-eng",
                "languages": [
                    "ukr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.44800000000000006,
                "f1": 0.39799992433716597,
                "precision": 0.38316922517054086,
                "recall": 0.44800000000000006,
                "main_score": 0.39799992433716597
            },
            {
                "hf_subset": "uzb-eng",
                "languages": [
                    "uzb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.056074766355140186,
                "f1": 0.035310597299663245,
                "precision": 0.032762993077737716,
                "recall": 0.056074766355140186,
                "main_score": 0.035310597299663245
            },
            {
                "hf_subset": "lit-eng",
                "languages": [
                    "lit-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.11,
                "f1": 0.08130418841674222,
                "precision": 0.0748629585563434,
                "recall": 0.11,
                "main_score": 0.08130418841674222
            },
            {
                "hf_subset": "ina-eng",
                "languages": [
                    "ina-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.277,
                "f1": 0.2321816159401837,
                "precision": 0.22008571800809704,
                "recall": 0.277,
                "main_score": 0.2321816159401837
            },
            {
                "hf_subset": "lfn-eng",
                "languages": [
                    "lfn-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.121,
                "f1": 0.08762797265443173,
                "precision": 0.08084865893503229,
                "recall": 0.121,
                "main_score": 0.08762797265443173
            },
            {
                "hf_subset": "zsm-eng",
                "languages": [
                    "zsm-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.3509999999999999,
                "f1": 0.30896978216485954,
                "precision": 0.29761690118074224,
                "recall": 0.3509999999999999,
                "main_score": 0.30896978216485954
            },
            {
                "hf_subset": "ita-eng",
                "languages": [
                    "ita-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.205,
                "f1": 0.15977011148466255,
                "precision": 0.1496707597911526,
                "recall": 0.205,
                "main_score": 0.15977011148466255
            },
            {
                "hf_subset": "cmn-eng",
                "languages": [
                    "cmn-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.893,
                "f1": 0.8626333333333335,
                "precision": 0.8488666666666665,
                "recall": 0.893,
                "main_score": 0.8626333333333335
            },
            {
                "hf_subset": "lvs-eng",
                "languages": [
                    "lvs-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.103,
                "f1": 0.07404212810418141,
                "precision": 0.06846457701679114,
                "recall": 0.103,
                "main_score": 0.07404212810418141
            },
            {
                "hf_subset": "glg-eng",
                "languages": [
                    "glg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.236,
                "f1": 0.1932375026259617,
                "precision": 0.1823710400667444,
                "recall": 0.236,
                "main_score": 0.1932375026259617
            },
            {
                "hf_subset": "ceb-eng",
                "languages": [
                    "ceb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.065,
                "f1": 0.047965805571051616,
                "precision": 0.04547019472479999,
                "recall": 0.065,
                "main_score": 0.047965805571051616
            },
            {
                "hf_subset": "bre-eng",
                "languages": [
                    "bre-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.045,
                "f1": 0.032163409836594194,
                "precision": 0.029845718982896056,
                "recall": 0.045,
                "main_score": 0.032163409836594194
            },
            {
                "hf_subset": "ben-eng",
                "languages": [
                    "ben-Beng",
                    "eng-Latn"
                ],
                "accuracy": 0.284,
                "f1": 0.2344851541733121,
                "precision": 0.2201784835391281,
                "recall": 0.284,
                "main_score": 0.2344851541733121
            },
            {
                "hf_subset": "swg-eng",
                "languages": [
                    "swg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.16071428571428573,
                "f1": 0.10953798185941042,
                "precision": 0.09992677626606197,
                "recall": 0.16071428571428573,
                "main_score": 0.10953798185941042
            },
            {
                "hf_subset": "arq-eng",
                "languages": [
                    "arq-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.0526893523600439,
                "f1": 0.039908791491106925,
                "precision": 0.03777904722005294,
                "recall": 0.0526893523600439,
                "main_score": 0.039908791491106925
            },
            {
                "hf_subset": "kab-eng",
                "languages": [
                    "kab-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.013000000000000001,
                "f1": 0.00603027485224566,
                "precision": 0.005195844597160387,
                "recall": 0.013000000000000001,
                "main_score": 0.00603027485224566
            },
            {
                "hf_subset": "fra-eng",
                "languages": [
                    "fra-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.24,
                "f1": 0.1996240217555452,
                "precision": 0.18802741591297084,
                "recall": 0.24,
                "main_score": 0.1996240217555452
            },
            {
                "hf_subset": "por-eng",
                "languages": [
                    "por-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.336,
                "f1": 0.28597393691065415,
                "precision": 0.27188430429408494,
                "recall": 0.336,
                "main_score": 0.28597393691065415
            },
            {
                "hf_subset": "tat-eng",
                "languages": [
                    "tat-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.045,
                "f1": 0.025102180790224263,
                "precision": 0.02236767959648205,
                "recall": 0.045,
                "main_score": 0.025102180790224263
            },
            {
                "hf_subset": "oci-eng",
                "languages": [
                    "oci-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.087,
                "f1": 0.06688281884758027,
                "precision": 0.06198557972329553,
                "recall": 0.087,
                "main_score": 0.06688281884758027
            },
            {
                "hf_subset": "pol-eng",
                "languages": [
                    "pol-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.174,
                "f1": 0.1451350686990167,
                "precision": 0.13789660772909998,
                "recall": 0.174,
                "main_score": 0.1451350686990167
            },
            {
                "hf_subset": "war-eng",
                "languages": [
                    "war-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.066,
                "f1": 0.04842727926144423,
                "precision": 0.045424212613482305,
                "recall": 0.066,
                "main_score": 0.04842727926144423
            },
            {
                "hf_subset": "aze-eng",
                "languages": [
                    "aze-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.118,
                "f1": 0.085483313166162,
                "precision": 0.07820128737629438,
                "recall": 0.118,
                "main_score": 0.085483313166162
            },
            {
                "hf_subset": "vie-eng",
                "languages": [
                    "vie-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.555,
                "f1": 0.49560057165057164,
                "precision": 0.4750350140056023,
                "recall": 0.555,
                "main_score": 0.49560057165057164
            },
            {
                "hf_subset": "nno-eng",
                "languages": [
                    "nno-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.169,
                "f1": 0.1380254156510093,
                "precision": 0.1302623654922266,
                "recall": 0.169,
                "main_score": 0.1380254156510093
            },
            {
                "hf_subset": "cha-eng",
                "languages": [
                    "cha-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.145985401459854,
                "f1": 0.0910786699107867,
                "precision": 0.07985485722712,
                "recall": 0.145985401459854,
                "main_score": 0.0910786699107867
            },
            {
                "hf_subset": "mhr-eng",
                "languages": [
                    "mhr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.057,
                "f1": 0.03518897834647557,
                "precision": 0.03158298308588556,
                "recall": 0.057,
                "main_score": 0.03518897834647557
            },
            {
                "hf_subset": "dan-eng",
                "languages": [
                    "dan-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.305,
                "f1": 0.25997416905156034,
                "precision": 0.24596070762988057,
                "recall": 0.305,
                "main_score": 0.25997416905156034
            },
            {
                "hf_subset": "ell-eng",
                "languages": [
                    "ell-Grek",
                    "eng-Latn"
                ],
                "accuracy": 0.117,
                "f1": 0.09498482945625802,
                "precision": 0.08987285930402825,
                "recall": 0.117,
                "main_score": 0.09498482945625802
            },
            {
                "hf_subset": "amh-eng",
                "languages": [
                    "amh-Ethi",
                    "eng-Latn"
                ],
                "accuracy": 0.08333333333333331,
                "f1": 0.05745982981006023,
                "precision": 0.054011331864592727,
                "recall": 0.08333333333333331,
                "main_score": 0.05745982981006023
            },
            {
                "hf_subset": "pam-eng",
                "languages": [
                    "pam-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.05,
                "f1": 0.03085855110083055,
                "precision": 0.02816700840061992,
                "recall": 0.05,
                "main_score": 0.03085855110083055
            },
            {
                "hf_subset": "hsb-eng",
                "languages": [
                    "hsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.08902691511387163,
                "f1": 0.06435632935092832,
                "precision": 0.05983137847614094,
                "recall": 0.08902691511387163,
                "main_score": 0.06435632935092832
            },
            {
                "hf_subset": "srp-eng",
                "languages": [
                    "srp-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.155,
                "f1": 0.1291642697840956,
                "precision": 0.1227655802325753,
                "recall": 0.155,
                "main_score": 0.1291642697840956
            },
            {
                "hf_subset": "epo-eng",
                "languages": [
                    "epo-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.106,
                "f1": 0.08254440661517909,
                "precision": 0.07838679485283888,
                "recall": 0.106,
                "main_score": 0.08254440661517909
            },
            {
                "hf_subset": "kzj-eng",
                "languages": [
                    "kzj-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.035,
                "f1": 0.02339236097978308,
                "precision": 0.02223062696511718,
                "recall": 0.035,
                "main_score": 0.02339236097978308
            },
            {
                "hf_subset": "awa-eng",
                "languages": [
                    "awa-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.22510822510822512,
                "f1": 0.16748326511504508,
                "precision": 0.15469328651146835,
                "recall": 0.22510822510822512,
                "main_score": 0.16748326511504508
            },
            {
                "hf_subset": "fao-eng",
                "languages": [
                    "fao-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.09160305343511449,
                "f1": 0.06340001193595815,
                "precision": 0.05839561202156622,
                "recall": 0.09160305343511449,
                "main_score": 0.06340001193595815
            },
            {
                "hf_subset": "mal-eng",
                "languages": [
                    "mal-Mlym",
                    "eng-Latn"
                ],
                "accuracy": 0.42066957787481807,
                "f1": 0.3661324818401623,
                "precision": 0.3471464471808596,
                "recall": 0.42066957787481807,
                "main_score": 0.3661324818401623
            },
            {
                "hf_subset": "ile-eng",
                "languages": [
                    "ile-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.215,
                "f1": 0.17235331398320522,
                "precision": 0.16204611769215213,
                "recall": 0.215,
                "main_score": 0.17235331398320522
            },
            {
                "hf_subset": "bos-eng",
                "languages": [
                    "bos-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.22598870056497178,
                "f1": 0.15808069297141553,
                "precision": 0.14216539784336393,
                "recall": 0.22598870056497178,
                "main_score": 0.15808069297141553
            },
            {
                "hf_subset": "cor-eng",
                "languages": [
                    "cor-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.039,
                "f1": 0.019378227948724843,
                "precision": 0.017177968874340983,
                "recall": 0.039,
                "main_score": 0.019378227948724843
            },
            {
                "hf_subset": "cat-eng",
                "languages": [
                    "cat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.20200000000000004,
                "f1": 0.15962850129692935,
                "precision": 0.1483149256651459,
                "recall": 0.20200000000000004,
                "main_score": 0.15962850129692935
            },
            {
                "hf_subset": "eus-eng",
                "languages": [
                    "eus-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.09700000000000002,
                "f1": 0.0737708973261353,
                "precision": 0.06927006519505818,
                "recall": 0.09700000000000002,
                "main_score": 0.0737708973261353
            },
            {
                "hf_subset": "yue-eng",
                "languages": [
                    "yue-Hant",
                    "eng-Latn"
                ],
                "accuracy": 0.635,
                "f1": 0.5835388888888889,
                "precision": 0.5641114468864469,
                "recall": 0.635,
                "main_score": 0.5835388888888889
            },
            {
                "hf_subset": "swe-eng",
                "languages": [
                    "swe-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.26,
                "f1": 0.2175037437399202,
                "precision": 0.2057120606116242,
                "recall": 0.26,
                "main_score": 0.2175037437399202
            },
            {
                "hf_subset": "dtp-eng",
                "languages": [
                    "dtp-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.024,
                "f1": 0.017611348003539112,
                "precision": 0.016490144671379943,
                "recall": 0.024,
                "main_score": 0.017611348003539112
            },
            {
                "hf_subset": "kat-eng",
                "languages": [
                    "kat-Geor",
                    "eng-Latn"
                ],
                "accuracy": 0.38605898123324395,
                "f1": 0.3300341324278513,
                "precision": 0.31243423531164033,
                "recall": 0.38605898123324395,
                "main_score": 0.3300341324278513
            },
            {
                "hf_subset": "jpn-eng",
                "languages": [
                    "jpn-Jpan",
                    "eng-Latn"
                ],
                "accuracy": 0.574,
                "f1": 0.5234129067990833,
                "precision": 0.5074419584500466,
                "recall": 0.574,
                "main_score": 0.5234129067990833
            },
            {
                "hf_subset": "csb-eng",
                "languages": [
                    "csb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.11462450592885375,
                "f1": 0.07168350568600204,
                "precision": 0.06267734051287926,
                "recall": 0.11462450592885375,
                "main_score": 0.07168350568600204
            },
            {
                "hf_subset": "xho-eng",
                "languages": [
                    "xho-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.11267605633802819,
                "f1": 0.08034434678244909,
                "precision": 0.074930465143804,
                "recall": 0.11267605633802819,
                "main_score": 0.08034434678244909
            },
            {
                "hf_subset": "orv-eng",
                "languages": [
                    "orv-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.05029940119760479,
                "f1": 0.03109491516992305,
                "precision": 0.02708633372048006,
                "recall": 0.05029940119760479,
                "main_score": 0.03109491516992305
            },
            {
                "hf_subset": "ind-eng",
                "languages": [
                    "ind-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.36700000000000005,
                "f1": 0.3212717818306083,
                "precision": 0.30816954398121377,
                "recall": 0.36700000000000005,
                "main_score": 0.3212717818306083
            },
            {
                "hf_subset": "tuk-eng",
                "languages": [
                    "tuk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.06896551724137931,
                "f1": 0.040229885057471264,
                "precision": 0.03391354561953473,
                "recall": 0.06896551724137931,
                "main_score": 0.040229885057471264
            },
            {
                "hf_subset": "max-eng",
                "languages": [
                    "max-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.1936619718309859,
                "f1": 0.13728751930776578,
                "precision": 0.1240776989741364,
                "recall": 0.1936619718309859,
                "main_score": 0.13728751930776578
            },
            {
                "hf_subset": "swh-eng",
                "languages": [
                    "swh-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.09743589743589745,
                "f1": 0.06368220492881126,
                "precision": 0.057559264653925905,
                "recall": 0.09743589743589745,
                "main_score": 0.06368220492881126
            },
            {
                "hf_subset": "hin-eng",
                "languages": [
                    "hin-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.552,
                "f1": 0.49453612906006517,
                "precision": 0.47434083591084736,
                "recall": 0.552,
                "main_score": 0.49453612906006517
            },
            {
                "hf_subset": "dsb-eng",
                "languages": [
                    "dsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.06680584551148225,
                "f1": 0.04493424007014965,
                "precision": 0.04131033519879768,
                "recall": 0.06680584551148225,
                "main_score": 0.04493424007014965
            },
            {
                "hf_subset": "ber-eng",
                "languages": [
                    "ber-Tfng",
                    "eng-Latn"
                ],
                "accuracy": 0.045,
                "f1": 0.024882233602841367,
                "precision": 0.021928034718812547,
                "recall": 0.045,
                "main_score": 0.024882233602841367
            },
            {
                "hf_subset": "tam-eng",
                "languages": [
                    "tam-Taml",
                    "eng-Latn"
                ],
                "accuracy": 0.31921824104234525,
                "f1": 0.26717853265084535,
                "precision": 0.2508341519742171,
                "recall": 0.31921824104234525,
                "main_score": 0.26717853265084535
            },
            {
                "hf_subset": "slk-eng",
                "languages": [
                    "slk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.174,
                "f1": 0.140498488819329,
                "precision": 0.13225483025465493,
                "recall": 0.174,
                "main_score": 0.140498488819329
            },
            {
                "hf_subset": "tgl-eng",
                "languages": [
                    "tgl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.079,
                "f1": 0.05764422668778745,
                "precision": 0.053187045968603346,
                "recall": 0.079,
                "main_score": 0.05764422668778745
            },
            {
                "hf_subset": "ast-eng",
                "languages": [
                    "ast-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.23622047244094488,
                "f1": 0.17735908011498563,
                "precision": 0.1631534545023977,
                "recall": 0.23622047244094488,
                "main_score": 0.17735908011498563
            },
            {
                "hf_subset": "mkd-eng",
                "languages": [
                    "mkd-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.19400000000000003,
                "f1": 0.16688139723374676,
                "precision": 0.160446811984312,
                "recall": 0.19400000000000003,
                "main_score": 0.16688139723374676
            },
            {
                "hf_subset": "khm-eng",
                "languages": [
                    "khm-Khmr",
                    "eng-Latn"
                ],
                "accuracy": 0.19390581717451524,
                "f1": 0.15330085364864166,
                "precision": 0.1423910323480727,
                "recall": 0.19390581717451524,
                "main_score": 0.15330085364864166
            },
            {
                "hf_subset": "ces-eng",
                "languages": [
                    "ces-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.149,
                "f1": 0.11890413421217721,
                "precision": 0.11273006536745667,
                "recall": 0.149,
                "main_score": 0.11890413421217721
            },
            {
                "hf_subset": "tzl-eng",
                "languages": [
                    "tzl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.2403846153846154,
                "f1": 0.20606022267206478,
                "precision": 0.19935897435897437,
                "recall": 0.2403846153846154,
                "main_score": 0.20606022267206478
            },
            {
                "hf_subset": "urd-eng",
                "languages": [
                    "urd-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.296,
                "f1": 0.24793469753676278,
                "precision": 0.2343004941257573,
                "recall": 0.296,
                "main_score": 0.24793469753676278
            },
            {
                "hf_subset": "ara-eng",
                "languages": [
                    "ara-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.24600000000000002,
                "f1": 0.19561599234099236,
                "precision": 0.18231733884473017,
                "recall": 0.24600000000000002,
                "main_score": 0.19561599234099236
            },
            {
                "hf_subset": "kor-eng",
                "languages": [
                    "kor-Hang",
                    "eng-Latn"
                ],
                "accuracy": 0.538,
                "f1": 0.481892730115302,
                "precision": 0.46161646825396824,
                "recall": 0.538,
                "main_score": 0.481892730115302
            },
            {
                "hf_subset": "yid-eng",
                "languages": [
                    "yid-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.036556603773584904,
                "f1": 0.022631181434426762,
                "precision": 0.020826082346870792,
                "recall": 0.036556603773584904,
                "main_score": 0.022631181434426762
            },
            {
                "hf_subset": "fin-eng",
                "languages": [
                    "fin-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.16,
                "f1": 0.13533088016975683,
                "precision": 0.12965331925224502,
                "recall": 0.16,
                "main_score": 0.13533088016975683
            },
            {
                "hf_subset": "tha-eng",
                "languages": [
                    "tha-Thai",
                    "eng-Latn"
                ],
                "accuracy": 0.6569343065693432,
                "f1": 0.6020814629766968,
                "precision": 0.5818983631939836,
                "recall": 0.6569343065693432,
                "main_score": 0.6020814629766968
            },
            {
                "hf_subset": "wuu-eng",
                "languages": [
                    "wuu-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.782,
                "f1": 0.7325492063492063,
                "precision": 0.7114833333333334,
                "recall": 0.782,
                "main_score": 0.7325492063492063
            }
        ]
    }
}