{
  "dataset_revision": "79125f20d858a08f71ec4923169a6545221725c4",
  "task_name": "NepaliNewsClassification",
  "mteb_version": "2.3.3",
  "scores": {
    "train": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.349121,
            "f1": 0.345058,
            "f1_weighted": 0.345866,
            "precision": 0.350093,
            "precision_weighted": 0.35003,
            "recall": 0.347462,
            "recall_weighted": 0.349121,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.327148,
            "f1": 0.326933,
            "f1_weighted": 0.327282,
            "precision": 0.327514,
            "precision_weighted": 0.328645,
            "recall": 0.327639,
            "recall_weighted": 0.327148,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.355957,
            "f1": 0.34514,
            "f1_weighted": 0.346727,
            "precision": 0.354493,
            "precision_weighted": 0.355855,
            "recall": 0.353727,
            "recall_weighted": 0.355957,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.340332,
            "f1": 0.33962,
            "f1_weighted": 0.340169,
            "precision": 0.341441,
            "precision_weighted": 0.342673,
            "recall": 0.340496,
            "recall_weighted": 0.340332,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.333496,
            "f1": 0.332626,
            "f1_weighted": 0.333046,
            "precision": 0.333457,
            "precision_weighted": 0.334585,
            "recall": 0.333832,
            "recall_weighted": 0.333496,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.330566,
            "f1": 0.324248,
            "f1_weighted": 0.323766,
            "precision": 0.33107,
            "precision_weighted": 0.332485,
            "recall": 0.332274,
            "recall_weighted": 0.330566,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.344238,
            "f1": 0.336529,
            "f1_weighted": 0.338513,
            "precision": 0.343724,
            "precision_weighted": 0.345826,
            "recall": 0.341988,
            "recall_weighted": 0.344238,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.346191,
            "f1": 0.345856,
            "f1_weighted": 0.344632,
            "precision": 0.347874,
            "precision_weighted": 0.348484,
            "recall": 0.3495,
            "recall_weighted": 0.346191,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.351074,
            "f1": 0.350163,
            "f1_weighted": 0.35095,
            "precision": 0.350513,
            "precision_weighted": 0.351237,
            "recall": 0.350216,
            "recall_weighted": 0.351074,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.342773,
            "f1": 0.34143,
            "f1_weighted": 0.342855,
            "precision": 0.341839,
            "precision_weighted": 0.343631,
            "recall": 0.341687,
            "recall_weighted": 0.342773,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.34209,
        "f1": 0.33876,
        "f1_weighted": 0.33938,
        "precision": 0.342202,
        "precision_weighted": 0.343345,
        "recall": 0.341882,
        "recall_weighted": 0.34209,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.34209,
        "hf_subset": "default",
        "languages": [
          "nep-Deva"
        ]
      }
    ]
  },
  "evaluation_time": 50.43342185020447,
  "kg_co2_emissions": null
}