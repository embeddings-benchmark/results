{
  "dataset_revision": "20b0e6081892e78179356fada741b7afa381443d",
  "task_name": "AngryTweetsClassification",
  "mteb_version": "2.3.3",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.332378,
            "f1": 0.32427,
            "f1_weighted": 0.329599,
            "precision": 0.32465,
            "precision_weighted": 0.329433,
            "recall": 0.326307,
            "recall_weighted": 0.332378,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.293219,
            "f1": 0.284335,
            "f1_weighted": 0.291376,
            "precision": 0.28449,
            "precision_weighted": 0.292589,
            "recall": 0.28697,
            "recall_weighted": 0.293219,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.315186,
            "f1": 0.314763,
            "f1_weighted": 0.313892,
            "precision": 0.316916,
            "precision_weighted": 0.321078,
            "recall": 0.322056,
            "recall_weighted": 0.315186,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.30086,
            "f1": 0.298636,
            "f1_weighted": 0.303604,
            "precision": 0.301908,
            "precision_weighted": 0.309922,
            "recall": 0.299307,
            "recall_weighted": 0.30086,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.333333,
            "f1": 0.327831,
            "f1_weighted": 0.334448,
            "precision": 0.33529,
            "precision_weighted": 0.344922,
            "recall": 0.330311,
            "recall_weighted": 0.333333,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.357211,
            "f1": 0.350641,
            "f1_weighted": 0.358018,
            "precision": 0.351873,
            "precision_weighted": 0.360541,
            "recall": 0.351237,
            "recall_weighted": 0.357211,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.30277,
            "f1": 0.302375,
            "f1_weighted": 0.304046,
            "precision": 0.309671,
            "precision_weighted": 0.316559,
            "recall": 0.307484,
            "recall_weighted": 0.30277,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.335244,
            "f1": 0.327183,
            "f1_weighted": 0.331656,
            "precision": 0.333361,
            "precision_weighted": 0.340342,
            "recall": 0.333634,
            "recall_weighted": 0.335244,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.30659,
            "f1": 0.301587,
            "f1_weighted": 0.30783,
            "precision": 0.302843,
            "precision_weighted": 0.310772,
            "recall": 0.301915,
            "recall_weighted": 0.30659,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.338109,
            "f1": 0.336177,
            "f1_weighted": 0.340204,
            "precision": 0.338421,
            "precision_weighted": 0.345905,
            "recall": 0.337832,
            "recall_weighted": 0.338109,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.32149,
        "f1": 0.31678,
        "f1_weighted": 0.321467,
        "precision": 0.319942,
        "precision_weighted": 0.327206,
        "recall": 0.319705,
        "recall_weighted": 0.32149,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.32149,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 42.400633573532104,
  "kg_co2_emissions": null
}