{
  "dataset_revision": "fafcfc4fee815f7017848e54b26c47ece8ff1626",
  "task_name": "ItaCaseholdClassification",
  "mteb_version": "2.3.3",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.004525,
            "f1": 0.000893,
            "f1_weighted": 0.00724,
            "precision": 0.002232,
            "precision_weighted": 0.0181,
            "recall": 0.000558,
            "recall_weighted": 0.004525,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.013575,
            "f1": 0.005104,
            "f1_weighted": 0.015371,
            "precision": 0.006399,
            "precision_weighted": 0.028356,
            "recall": 0.005394,
            "recall_weighted": 0.013575,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.0181,
            "f1": 0.002993,
            "f1_weighted": 0.026318,
            "precision": 0.005554,
            "precision_weighted": 0.048645,
            "recall": 0.002056,
            "recall_weighted": 0.0181,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.036199,
            "f1": 0.008143,
            "f1_weighted": 0.050351,
            "precision": 0.01472,
            "precision_weighted": 0.117853,
            "recall": 0.007142,
            "recall_weighted": 0.036199,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.027149,
            "f1": 0.008202,
            "f1_weighted": 0.035493,
            "precision": 0.011203,
            "precision_weighted": 0.063428,
            "recall": 0.007647,
            "recall_weighted": 0.027149,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.040724,
            "f1": 0.014024,
            "f1_weighted": 0.047387,
            "precision": 0.016744,
            "precision_weighted": 0.079184,
            "recall": 0.014572,
            "recall_weighted": 0.040724,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.013575,
            "f1": 0.002448,
            "f1_weighted": 0.022354,
            "precision": 0.00731,
            "precision_weighted": 0.069796,
            "recall": 0.001497,
            "recall_weighted": 0.013575,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.031674,
            "f1": 0.008024,
            "f1_weighted": 0.042699,
            "precision": 0.013889,
            "precision_weighted": 0.100553,
            "recall": 0.006761,
            "recall_weighted": 0.031674,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.031674,
            "f1": 0.010247,
            "f1_weighted": 0.036459,
            "precision": 0.013915,
            "precision_weighted": 0.080227,
            "recall": 0.012956,
            "recall_weighted": 0.031674,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.0181,
            "f1": 0.00462,
            "f1_weighted": 0.02339,
            "precision": 0.020082,
            "precision_weighted": 0.203373,
            "recall": 0.00931,
            "recall_weighted": 0.0181,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.023529,
        "f1": 0.00647,
        "f1_weighted": 0.030706,
        "precision": 0.011205,
        "precision_weighted": 0.080952,
        "recall": 0.006789,
        "recall_weighted": 0.023529,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.023529,
        "hf_subset": "default",
        "languages": [
          "ita-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 6.5389440059661865,
  "kg_co2_emissions": null
}