{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "2.3.3",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.527491,
            "f1": 0.432608,
            "f1_weighted": 0.606428,
            "precision": 0.501756,
            "precision_weighted": 0.782272,
            "recall": 0.503983,
            "recall_weighted": 0.527491,
            "ap": 0.126316,
            "ap_weighted": 0.126316
          },
          {
            "accuracy": 0.475945,
            "f1": 0.408907,
            "f1_weighted": 0.558033,
            "precision": 0.504288,
            "precision_weighted": 0.785136,
            "recall": 0.509716,
            "recall_weighted": 0.475945,
            "ap": 0.127628,
            "ap_weighted": 0.127628
          },
          {
            "accuracy": 0.418385,
            "f1": 0.377883,
            "f1_weighted": 0.496798,
            "precision": 0.508319,
            "precision_weighted": 0.790413,
            "recall": 0.517877,
            "recall_weighted": 0.418385,
            "ap": 0.12955,
            "ap_weighted": 0.12955
          },
          {
            "accuracy": 0.591065,
            "f1": 0.463258,
            "f1_weighted": 0.659469,
            "precision": 0.506443,
            "precision_weighted": 0.785954,
            "recall": 0.513927,
            "recall_weighted": 0.591065,
            "ap": 0.128678,
            "ap_weighted": 0.128678
          },
          {
            "accuracy": 0.504296,
            "f1": 0.421217,
            "f1_weighted": 0.58549,
            "precision": 0.501078,
            "precision_weighted": 0.781677,
            "recall": 0.502456,
            "recall_weighted": 0.504296,
            "ap": 0.125973,
            "ap_weighted": 0.125973
          },
          {
            "accuracy": 0.414089,
            "f1": 0.372401,
            "f1_weighted": 0.493576,
            "precision": 0.501711,
            "precision_weighted": 0.782617,
            "recall": 0.503687,
            "recall_weighted": 0.414089,
            "ap": 0.126247,
            "ap_weighted": 0.126247
          },
          {
            "accuracy": 0.424399,
            "f1": 0.386306,
            "f1_weighted": 0.500847,
            "precision": 0.519596,
            "precision_weighted": 0.803885,
            "recall": 0.541849,
            "recall_weighted": 0.424399,
            "ap": 0.135686,
            "ap_weighted": 0.135686
          },
          {
            "accuracy": 0.457904,
            "f1": 0.398066,
            "f1_weighted": 0.540242,
            "precision": 0.502345,
            "precision_weighted": 0.783159,
            "recall": 0.505268,
            "recall_weighted": 0.457904,
            "ap": 0.126604,
            "ap_weighted": 0.126604
          },
          {
            "accuracy": 0.401203,
            "f1": 0.362443,
            "f1_weighted": 0.480207,
            "precision": 0.496894,
            "precision_weighted": 0.776905,
            "recall": 0.493386,
            "recall_weighted": 0.401203,
            "ap": 0.124005,
            "ap_weighted": 0.124005
          },
          {
            "accuracy": 0.452749,
            "f1": 0.39801,
            "f1_weighted": 0.534,
            "precision": 0.507617,
            "precision_weighted": 0.789056,
            "recall": 0.516989,
            "recall_weighted": 0.452749,
            "ap": 0.12935,
            "ap_weighted": 0.12935
          }
        ],
        "accuracy": 0.466753,
        "f1": 0.40211,
        "f1_weighted": 0.545509,
        "precision": 0.505005,
        "precision_weighted": 0.786107,
        "recall": 0.510914,
        "recall_weighted": 0.466753,
        "ap": 0.128004,
        "ap_weighted": 0.128004,
        "main_score": 0.466753,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 44.657684564590454,
  "kg_co2_emissions": null
}