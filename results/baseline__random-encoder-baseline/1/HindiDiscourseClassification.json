{
  "dataset_revision": "6f183d3e509464fd9d92516d4eff91e11b8ec622",
  "task_name": "HindiDiscourseClassification",
  "mteb_version": "2.3.3",
  "scores": {
    "train": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.186035,
            "f1": 0.142375,
            "f1_weighted": 0.231186,
            "precision": 0.191163,
            "precision_weighted": 0.351266,
            "recall": 0.348313,
            "recall_weighted": 0.186035,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.20459,
            "f1": 0.15298,
            "f1_weighted": 0.243777,
            "precision": 0.194156,
            "precision_weighted": 0.35232,
            "recall": 0.370103,
            "recall_weighted": 0.20459,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.19043,
            "f1": 0.142679,
            "f1_weighted": 0.228383,
            "precision": 0.174583,
            "precision_weighted": 0.315489,
            "recall": 0.354735,
            "recall_weighted": 0.19043,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.188477,
            "f1": 0.145661,
            "f1_weighted": 0.237521,
            "precision": 0.197941,
            "precision_weighted": 0.364855,
            "recall": 0.357833,
            "recall_weighted": 0.188477,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.179199,
            "f1": 0.139671,
            "f1_weighted": 0.218426,
            "precision": 0.183444,
            "precision_weighted": 0.330724,
            "recall": 0.409849,
            "recall_weighted": 0.179199,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.178711,
            "f1": 0.138652,
            "f1_weighted": 0.222226,
            "precision": 0.192645,
            "precision_weighted": 0.354076,
            "recall": 0.371463,
            "recall_weighted": 0.178711,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.168945,
            "f1": 0.131114,
            "f1_weighted": 0.214427,
            "precision": 0.18882,
            "precision_weighted": 0.346973,
            "recall": 0.360435,
            "recall_weighted": 0.168945,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.172363,
            "f1": 0.133342,
            "f1_weighted": 0.213146,
            "precision": 0.175352,
            "precision_weighted": 0.319805,
            "recall": 0.372707,
            "recall_weighted": 0.172363,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.176758,
            "f1": 0.135774,
            "f1_weighted": 0.215009,
            "precision": 0.180025,
            "precision_weighted": 0.327421,
            "recall": 0.362039,
            "recall_weighted": 0.176758,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.204102,
            "f1": 0.153088,
            "f1_weighted": 0.248212,
            "precision": 0.193843,
            "precision_weighted": 0.352564,
            "recall": 0.367861,
            "recall_weighted": 0.204102,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.184961,
        "f1": 0.141534,
        "f1_weighted": 0.227231,
        "precision": 0.187197,
        "precision_weighted": 0.341549,
        "recall": 0.367534,
        "recall_weighted": 0.184961,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.184961,
        "hf_subset": "default",
        "languages": [
          "hin-Deva"
        ]
      }
    ]
  },
  "evaluation_time": 50.606858015060425,
  "kg_co2_emissions": null
}