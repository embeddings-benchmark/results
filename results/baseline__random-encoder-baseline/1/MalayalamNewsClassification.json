{
  "dataset_revision": "666f63bba2387456d8f846ea4d0565181bd47b81",
  "task_name": "MalayalamNewsClassification",
  "mteb_version": "2.3.3",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.329365,
            "f1": 0.329461,
            "f1_weighted": 0.32929,
            "precision": 0.331694,
            "precision_weighted": 0.332827,
            "recall": 0.330779,
            "recall_weighted": 0.329365,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.331746,
            "f1": 0.326327,
            "f1_weighted": 0.328019,
            "precision": 0.333178,
            "precision_weighted": 0.335898,
            "recall": 0.330555,
            "recall_weighted": 0.331746,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.353175,
            "f1": 0.3411,
            "f1_weighted": 0.343273,
            "precision": 0.351232,
            "precision_weighted": 0.352024,
            "recall": 0.348937,
            "recall_weighted": 0.353175,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.335714,
            "f1": 0.335426,
            "f1_weighted": 0.336121,
            "precision": 0.339611,
            "precision_weighted": 0.341667,
            "recall": 0.336209,
            "recall_weighted": 0.335714,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.310317,
            "f1": 0.309733,
            "f1_weighted": 0.309017,
            "precision": 0.315449,
            "precision_weighted": 0.317054,
            "recall": 0.313699,
            "recall_weighted": 0.310317,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.321429,
            "f1": 0.302472,
            "f1_weighted": 0.301487,
            "precision": 0.318774,
            "precision_weighted": 0.319836,
            "recall": 0.322681,
            "recall_weighted": 0.321429,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.337302,
            "f1": 0.329364,
            "f1_weighted": 0.332803,
            "precision": 0.332484,
            "precision_weighted": 0.334275,
            "recall": 0.332202,
            "recall_weighted": 0.337302,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.303175,
            "f1": 0.300935,
            "f1_weighted": 0.300009,
            "precision": 0.303499,
            "precision_weighted": 0.304181,
            "recall": 0.305973,
            "recall_weighted": 0.303175,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.32619,
            "f1": 0.326095,
            "f1_weighted": 0.326349,
            "precision": 0.326484,
            "precision_weighted": 0.327567,
            "recall": 0.326798,
            "recall_weighted": 0.32619,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.326984,
            "f1": 0.324164,
            "f1_weighted": 0.322993,
            "precision": 0.331894,
            "precision_weighted": 0.332585,
            "recall": 0.330245,
            "recall_weighted": 0.326984,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.32754,
        "f1": 0.322508,
        "f1_weighted": 0.322936,
        "precision": 0.32843,
        "precision_weighted": 0.329791,
        "recall": 0.327808,
        "recall_weighted": 0.32754,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.32754,
        "hf_subset": "default",
        "languages": [
          "mal-Mlym"
        ]
      }
    ]
  },
  "evaluation_time": 45.5118293762207,
  "kg_co2_emissions": null
}