{
  "dataset_revision": "1beac1b941da76a9c51e3e5b39d230fde9a80983",
  "task_name": "SentimentAnalysisHindi",
  "mteb_version": "2.3.3",
  "scores": {
    "train": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.289551,
            "f1": 0.282329,
            "f1_weighted": 0.308886,
            "precision": 0.318025,
            "precision_weighted": 0.375733,
            "recall": 0.310798,
            "recall_weighted": 0.289551,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.35791,
            "f1": 0.335157,
            "f1_weighted": 0.373698,
            "precision": 0.348103,
            "precision_weighted": 0.406901,
            "recall": 0.350794,
            "recall_weighted": 0.35791,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.364746,
            "f1": 0.335281,
            "f1_weighted": 0.376031,
            "precision": 0.348654,
            "precision_weighted": 0.409752,
            "recall": 0.347903,
            "recall_weighted": 0.364746,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.352051,
            "f1": 0.321422,
            "f1_weighted": 0.359474,
            "precision": 0.333346,
            "precision_weighted": 0.390264,
            "recall": 0.333769,
            "recall_weighted": 0.352051,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.31543,
            "f1": 0.309029,
            "f1_weighted": 0.333439,
            "precision": 0.345127,
            "precision_weighted": 0.404404,
            "recall": 0.346914,
            "recall_weighted": 0.31543,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.346191,
            "f1": 0.322895,
            "f1_weighted": 0.348433,
            "precision": 0.337846,
            "precision_weighted": 0.388443,
            "recall": 0.347663,
            "recall_weighted": 0.346191,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.374512,
            "f1": 0.324599,
            "f1_weighted": 0.365363,
            "precision": 0.345498,
            "precision_weighted": 0.405518,
            "recall": 0.342648,
            "recall_weighted": 0.374512,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.342285,
            "f1": 0.324736,
            "f1_weighted": 0.356206,
            "precision": 0.340045,
            "precision_weighted": 0.395544,
            "recall": 0.345309,
            "recall_weighted": 0.342285,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.366211,
            "f1": 0.334096,
            "f1_weighted": 0.367795,
            "precision": 0.344379,
            "precision_weighted": 0.39748,
            "recall": 0.350813,
            "recall_weighted": 0.366211,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.361328,
            "f1": 0.340041,
            "f1_weighted": 0.37723,
            "precision": 0.352823,
            "precision_weighted": 0.411515,
            "recall": 0.356738,
            "recall_weighted": 0.361328,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.347021,
        "f1": 0.322959,
        "f1_weighted": 0.356655,
        "precision": 0.341385,
        "precision_weighted": 0.398556,
        "recall": 0.343335,
        "recall_weighted": 0.347021,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.322959,
        "hf_subset": "default",
        "languages": [
          "hin-Deva"
        ]
      }
    ]
  },
  "evaluation_time": 50.079662561416626,
  "kg_co2_emissions": null
}