{
    "dataset_revision": "ed9a4403ed4adbfaf4aab56d5b2709e9f6c3ba33",
    "task_name": "NTREXBitextMining",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "arb_Arab-hun_Latn",
                "languages": [
                    "arb-Arab",
                    "hun-Latn"
                ],
                "accuracy": 0.8307461191787682,
                "f1": 0.7897012184944081,
                "main_score": 0.7897012184944081,
                "precision": 0.7716324486730095,
                "recall": 0.8307461191787682
            },
            {
                "hf_subset": "ben_Beng-hun_Latn",
                "languages": [
                    "ben-Beng",
                    "hun-Latn"
                ],
                "accuracy": 0.812719078617927,
                "f1": 0.7661337243961179,
                "main_score": 0.7661337243961179,
                "precision": 0.745247633354794,
                "recall": 0.812719078617927
            },
            {
                "hf_subset": "deu_Latn-hun_Latn",
                "languages": [
                    "deu-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.9078617926890336,
                "f1": 0.8827073944249707,
                "main_score": 0.8827073944249707,
                "precision": 0.8710565848773161,
                "recall": 0.9078617926890336
            },
            {
                "hf_subset": "ell_Grek-hun_Latn",
                "languages": [
                    "ell-Grek",
                    "hun-Latn"
                ],
                "accuracy": 0.8908362543815723,
                "f1": 0.8619429143715575,
                "main_score": 0.8619429143715575,
                "precision": 0.8485728592889333,
                "recall": 0.8908362543815723
            },
            {
                "hf_subset": "eng_Latn-hun_Latn",
                "languages": [
                    "eng-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.9323985978968453,
                "f1": 0.9140877983642131,
                "main_score": 0.9140877983642131,
                "precision": 0.9057753296611586,
                "recall": 0.9323985978968453
            },
            {
                "hf_subset": "fas_Arab-hun_Latn",
                "languages": [
                    "fas-Arab",
                    "hun-Latn"
                ],
                "accuracy": 0.8637956935403105,
                "f1": 0.8284426639959941,
                "main_score": 0.8284426639959941,
                "precision": 0.8126356200968119,
                "recall": 0.8637956935403105
            },
            {
                "hf_subset": "fin_Latn-hun_Latn",
                "languages": [
                    "fin-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.8542814221331997,
                "f1": 0.8180031952690942,
                "main_score": 0.8180031952690942,
                "precision": 0.801235186112502,
                "recall": 0.8542814221331997
            },
            {
                "hf_subset": "fra_Latn-hun_Latn",
                "languages": [
                    "fra-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.9083625438157236,
                "f1": 0.8831079953263228,
                "main_score": 0.8831079953263228,
                "precision": 0.8711817726589886,
                "recall": 0.9083625438157236
            },
            {
                "hf_subset": "heb_Hebr-hun_Latn",
                "languages": [
                    "heb-Hebr",
                    "hun-Latn"
                ],
                "accuracy": 0.8132198297446169,
                "f1": 0.7649724586880321,
                "main_score": 0.7649724586880321,
                "precision": 0.743578462932494,
                "recall": 0.8132198297446169
            },
            {
                "hf_subset": "hin_Deva-hun_Latn",
                "languages": [
                    "hin-Deva",
                    "hun-Latn"
                ],
                "accuracy": 0.8637956935403105,
                "f1": 0.8283341679185444,
                "main_score": 0.8283341679185444,
                "precision": 0.8121563297326942,
                "recall": 0.8637956935403105
            },
            {
                "hf_subset": "hun_Latn-arb_Arab",
                "languages": [
                    "hun-Latn",
                    "arb-Arab"
                ],
                "accuracy": 0.8222333500250375,
                "f1": 0.7776760378663232,
                "main_score": 0.7776760378663232,
                "precision": 0.7581634356296348,
                "recall": 0.8222333500250375
            },
            {
                "hf_subset": "hun_Latn-ben_Beng",
                "languages": [
                    "hun-Latn",
                    "ben-Beng"
                ],
                "accuracy": 0.7756634952428643,
                "f1": 0.7228537250319925,
                "main_score": 0.7228537250319925,
                "precision": 0.7002032811121446,
                "recall": 0.7756634952428643
            },
            {
                "hf_subset": "hun_Latn-deu_Latn",
                "languages": [
                    "hun-Latn",
                    "deu-Latn"
                ],
                "accuracy": 0.9143715573360041,
                "f1": 0.8927391086629944,
                "main_score": 0.8927391086629944,
                "precision": 0.8824904022700718,
                "recall": 0.9143715573360041
            },
            {
                "hf_subset": "hun_Latn-ell_Grek",
                "languages": [
                    "hun-Latn",
                    "ell-Grek"
                ],
                "accuracy": 0.883825738607912,
                "f1": 0.8536900588978704,
                "main_score": 0.8536900588978704,
                "precision": 0.8398848272408614,
                "recall": 0.883825738607912
            },
            {
                "hf_subset": "hun_Latn-eng_Latn",
                "languages": [
                    "hun-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.942914371557336,
                "f1": 0.926890335503255,
                "main_score": 0.926890335503255,
                "precision": 0.9192121515606744,
                "recall": 0.942914371557336
            },
            {
                "hf_subset": "hun_Latn-fas_Arab",
                "languages": [
                    "hun-Latn",
                    "fas-Arab"
                ],
                "accuracy": 0.8472709063595393,
                "f1": 0.8081622433650475,
                "main_score": 0.8081622433650475,
                "precision": 0.7905524954097813,
                "recall": 0.8472709063595393
            },
            {
                "hf_subset": "hun_Latn-fin_Latn",
                "languages": [
                    "hun-Latn",
                    "fin-Latn"
                ],
                "accuracy": 0.8357536304456685,
                "f1": 0.7932338984667477,
                "main_score": 0.7932338984667477,
                "precision": 0.7745833035267187,
                "recall": 0.8357536304456685
            },
            {
                "hf_subset": "hun_Latn-fra_Latn",
                "languages": [
                    "hun-Latn",
                    "fra-Latn"
                ],
                "accuracy": 0.9048572859288933,
                "f1": 0.8794954336266304,
                "main_score": 0.8794954336266304,
                "precision": 0.8675429811383744,
                "recall": 0.9048572859288933
            },
            {
                "hf_subset": "hun_Latn-heb_Hebr",
                "languages": [
                    "hun-Latn",
                    "heb-Hebr"
                ],
                "accuracy": 0.7721582373560341,
                "f1": 0.7182277384330463,
                "main_score": 0.7182277384330463,
                "precision": 0.6955856403653098,
                "recall": 0.7721582373560341
            },
            {
                "hf_subset": "hun_Latn-hin_Deva",
                "languages": [
                    "hun-Latn",
                    "hin-Deva"
                ],
                "accuracy": 0.8477716574862293,
                "f1": 0.8097423913648251,
                "main_score": 0.8097423913648251,
                "precision": 0.7927265898848274,
                "recall": 0.8477716574862293
            },
            {
                "hf_subset": "hun_Latn-ind_Latn",
                "languages": [
                    "hun-Latn",
                    "ind-Latn"
                ],
                "accuracy": 0.900350525788683,
                "f1": 0.8728592889334,
                "main_score": 0.8728592889334,
                "precision": 0.8599732932732432,
                "recall": 0.900350525788683
            },
            {
                "hf_subset": "hun_Latn-jpn_Jpan",
                "languages": [
                    "hun-Latn",
                    "jpn-Jpan"
                ],
                "accuracy": 0.8437656484727092,
                "f1": 0.8059017097074183,
                "main_score": 0.8059017097074183,
                "precision": 0.7894508429310634,
                "recall": 0.8437656484727092
            },
            {
                "hf_subset": "hun_Latn-kor_Hang",
                "languages": [
                    "hun-Latn",
                    "kor-Hang"
                ],
                "accuracy": 0.8077115673510264,
                "f1": 0.7635683684256542,
                "main_score": 0.7635683684256542,
                "precision": 0.7447361699114328,
                "recall": 0.8077115673510264
            },
            {
                "hf_subset": "hun_Latn-lav_Latn",
                "languages": [
                    "hun-Latn",
                    "lav-Latn"
                ],
                "accuracy": 0.7681522283425137,
                "f1": 0.7124067052960392,
                "main_score": 0.7124067052960392,
                "precision": 0.6894003703968652,
                "recall": 0.7681522283425137
            },
            {
                "hf_subset": "hun_Latn-lit_Latn",
                "languages": [
                    "hun-Latn",
                    "lit-Latn"
                ],
                "accuracy": 0.7731597396094141,
                "f1": 0.7192622266733433,
                "main_score": 0.7192622266733433,
                "precision": 0.6958461501776473,
                "recall": 0.7731597396094141
            },
            {
                "hf_subset": "hun_Latn-nld_Latn",
                "languages": [
                    "hun-Latn",
                    "nld-Latn"
                ],
                "accuracy": 0.9098647971957938,
                "f1": 0.8850275413119679,
                "main_score": 0.8850275413119679,
                "precision": 0.8733683859122017,
                "recall": 0.9098647971957938
            },
            {
                "hf_subset": "hun_Latn-pol_Latn",
                "languages": [
                    "hun-Latn",
                    "pol-Latn"
                ],
                "accuracy": 0.8843264897346019,
                "f1": 0.8533896082218565,
                "main_score": 0.8533896082218565,
                "precision": 0.8390919712902688,
                "recall": 0.8843264897346019
            },
            {
                "hf_subset": "hun_Latn-por_Latn",
                "languages": [
                    "hun-Latn",
                    "por-Latn"
                ],
                "accuracy": 0.9068602904356535,
                "f1": 0.8809046903688867,
                "main_score": 0.8809046903688867,
                "precision": 0.8688449340677683,
                "recall": 0.9068602904356535
            },
            {
                "hf_subset": "hun_Latn-rus_Cyrl",
                "languages": [
                    "hun-Latn",
                    "rus-Cyrl"
                ],
                "accuracy": 0.900350525788683,
                "f1": 0.8735770322149893,
                "main_score": 0.8735770322149893,
                "precision": 0.8610832916040727,
                "recall": 0.900350525788683
            },
            {
                "hf_subset": "hun_Latn-spa_Latn",
                "languages": [
                    "hun-Latn",
                    "spa-Latn"
                ],
                "accuracy": 0.9258888332498748,
                "f1": 0.9064763812385245,
                "main_score": 0.9064763812385245,
                "precision": 0.8975880487397765,
                "recall": 0.9258888332498748
            },
            {
                "hf_subset": "hun_Latn-swa_Latn",
                "languages": [
                    "hun-Latn",
                    "swa-Latn"
                ],
                "accuracy": 0.7260891337005507,
                "f1": 0.6662728580605396,
                "main_score": 0.6662728580605396,
                "precision": 0.6422842597229177,
                "recall": 0.7260891337005507
            },
            {
                "hf_subset": "hun_Latn-swe_Latn",
                "languages": [
                    "hun-Latn",
                    "swe-Latn"
                ],
                "accuracy": 0.8903355032548823,
                "f1": 0.8601569020196962,
                "main_score": 0.8601569020196962,
                "precision": 0.8459105324653647,
                "recall": 0.8903355032548823
            },
            {
                "hf_subset": "hun_Latn-tam_Taml",
                "languages": [
                    "hun-Latn",
                    "tam-Taml"
                ],
                "accuracy": 0.7466199298948424,
                "f1": 0.687971639999682,
                "main_score": 0.687971639999682,
                "precision": 0.6636091041323892,
                "recall": 0.7466199298948424
            },
            {
                "hf_subset": "hun_Latn-tur_Latn",
                "languages": [
                    "hun-Latn",
                    "tur-Latn"
                ],
                "accuracy": 0.8708062093139709,
                "f1": 0.8379736271073277,
                "main_score": 0.8379736271073277,
                "precision": 0.8233278489162315,
                "recall": 0.8708062093139709
            },
            {
                "hf_subset": "hun_Latn-vie_Latn",
                "languages": [
                    "hun-Latn",
                    "vie-Latn"
                ],
                "accuracy": 0.8978467701552328,
                "f1": 0.870288766483058,
                "main_score": 0.870288766483058,
                "precision": 0.8576781839425806,
                "recall": 0.8978467701552328
            },
            {
                "hf_subset": "hun_Latn-zho_Hant",
                "languages": [
                    "hun-Latn",
                    "zho-Hant"
                ],
                "accuracy": 0.8733099649474211,
                "f1": 0.8402103154732097,
                "main_score": 0.8402103154732097,
                "precision": 0.8251877816725088,
                "recall": 0.8733099649474211
            },
            {
                "hf_subset": "hun_Latn-zul_Latn",
                "languages": [
                    "hun-Latn",
                    "zul-Latn"
                ],
                "accuracy": 0.5192789183775663,
                "f1": 0.43912175926815533,
                "main_score": 0.43912175926815533,
                "precision": 0.4109881091478487,
                "recall": 0.5192789183775663
            },
            {
                "hf_subset": "ind_Latn-hun_Latn",
                "languages": [
                    "ind-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.901352028042063,
                "f1": 0.8751722822328731,
                "main_score": 0.8751722822328731,
                "precision": 0.8631280253713904,
                "recall": 0.901352028042063
            },
            {
                "hf_subset": "jpn_Jpan-hun_Latn",
                "languages": [
                    "jpn-Jpan",
                    "hun-Latn"
                ],
                "accuracy": 0.8437656484727092,
                "f1": 0.8056084126189283,
                "main_score": 0.8056084126189283,
                "precision": 0.7884743782340177,
                "recall": 0.8437656484727092
            },
            {
                "hf_subset": "kor_Hang-hun_Latn",
                "languages": [
                    "kor-Hang",
                    "hun-Latn"
                ],
                "accuracy": 0.8347521281922884,
                "f1": 0.7941519421990129,
                "main_score": 0.7941519421990129,
                "precision": 0.7757350311181057,
                "recall": 0.8347521281922884
            },
            {
                "hf_subset": "lav_Latn-hun_Latn",
                "languages": [
                    "lav-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.8212318477716575,
                "f1": 0.7818656556262966,
                "main_score": 0.7818656556262966,
                "precision": 0.7641879485895511,
                "recall": 0.8212318477716575
            },
            {
                "hf_subset": "lit_Latn-hun_Latn",
                "languages": [
                    "lit-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.8167250876314472,
                "f1": 0.7752628943415122,
                "main_score": 0.7752628943415122,
                "precision": 0.7562426973794024,
                "recall": 0.8167250876314472
            },
            {
                "hf_subset": "nld_Latn-hun_Latn",
                "languages": [
                    "nld-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.9103655483224837,
                "f1": 0.8862404718188392,
                "main_score": 0.8862404718188392,
                "precision": 0.8750584209647806,
                "recall": 0.9103655483224837
            },
            {
                "hf_subset": "pol_Latn-hun_Latn",
                "languages": [
                    "pol-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.8873309964947421,
                "f1": 0.8563869613944726,
                "main_score": 0.8563869613944726,
                "precision": 0.8421799365715239,
                "recall": 0.8873309964947421
            },
            {
                "hf_subset": "por_Latn-hun_Latn",
                "languages": [
                    "por-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.9103655483224837,
                "f1": 0.8854782173259889,
                "main_score": 0.8854782173259889,
                "precision": 0.873910866299449,
                "recall": 0.9103655483224837
            },
            {
                "hf_subset": "rus_Cyrl-hun_Latn",
                "languages": [
                    "rus-Cyrl",
                    "hun-Latn"
                ],
                "accuracy": 0.8888332498748123,
                "f1": 0.858447194601426,
                "main_score": 0.858447194601426,
                "precision": 0.8445751961275246,
                "recall": 0.8888332498748123
            },
            {
                "hf_subset": "spa_Latn-hun_Latn",
                "languages": [
                    "spa-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.9213820731096645,
                "f1": 0.89933233183108,
                "main_score": 0.89933233183108,
                "precision": 0.8892004673677182,
                "recall": 0.9213820731096645
            },
            {
                "hf_subset": "swa_Latn-hun_Latn",
                "languages": [
                    "swa-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.757636454682023,
                "f1": 0.7119297994610965,
                "main_score": 0.7119297994610965,
                "precision": 0.6929461652796656,
                "recall": 0.757636454682023
            },
            {
                "hf_subset": "swe_Latn-hun_Latn",
                "languages": [
                    "swe-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.898347521281923,
                "f1": 0.8725779144907837,
                "main_score": 0.8725779144907837,
                "precision": 0.8605408112168252,
                "recall": 0.898347521281923
            },
            {
                "hf_subset": "tam_Taml-hun_Latn",
                "languages": [
                    "tam-Taml",
                    "hun-Latn"
                ],
                "accuracy": 0.7801702553830746,
                "f1": 0.7270886488462852,
                "main_score": 0.7270886488462852,
                "precision": 0.7039064549204759,
                "recall": 0.7801702553830746
            },
            {
                "hf_subset": "tur_Latn-hun_Latn",
                "languages": [
                    "tur-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.8733099649474211,
                "f1": 0.8428094522736486,
                "main_score": 0.8428094522736486,
                "precision": 0.8289100317142379,
                "recall": 0.8733099649474211
            },
            {
                "hf_subset": "vie_Latn-hun_Latn",
                "languages": [
                    "vie-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.8923385077616425,
                "f1": 0.8638290769487564,
                "main_score": 0.8638290769487564,
                "precision": 0.8508763144717073,
                "recall": 0.8923385077616425
            },
            {
                "hf_subset": "zho_Hant-hun_Latn",
                "languages": [
                    "zho-Hant",
                    "hun-Latn"
                ],
                "accuracy": 0.8652979469203805,
                "f1": 0.82964446670005,
                "main_score": 0.82964446670005,
                "precision": 0.8141044900684361,
                "recall": 0.8652979469203805
            },
            {
                "hf_subset": "zul_Latn-hun_Latn",
                "languages": [
                    "zul-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.5498247371056585,
                "f1": 0.4879136275731169,
                "main_score": 0.4879136275731169,
                "precision": 0.4653637850035387,
                "recall": 0.5498247371056585
            }
        ]
    }
}