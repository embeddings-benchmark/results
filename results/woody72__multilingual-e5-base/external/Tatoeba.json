{
    "dataset_revision": "9080400076fbadbb4c4dcb136ff4eddc40b42553",
    "task_name": "Tatoeba",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "sqi-eng",
                "languages": [
                    "sqi-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.921,
                "f1": 0.9006000000000001,
                "precision": 0.8917333333333333,
                "recall": 0.921,
                "main_score": 0.9006000000000001
            },
            {
                "hf_subset": "fry-eng",
                "languages": [
                    "fry-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.5606936416184971,
                "f1": 0.5087508028259473,
                "precision": 0.48973988439306354,
                "recall": 0.5606936416184971,
                "main_score": 0.5087508028259473
            },
            {
                "hf_subset": "kur-eng",
                "languages": [
                    "kur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.573170731707317,
                "f1": 0.5296080139372822,
                "precision": 0.5167861124382864,
                "recall": 0.573170731707317,
                "main_score": 0.5296080139372822
            },
            {
                "hf_subset": "tur-eng",
                "languages": [
                    "tur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.943,
                "f1": 0.9267333333333333,
                "precision": 0.9190833333333334,
                "recall": 0.943,
                "main_score": 0.9267333333333333
            },
            {
                "hf_subset": "deu-eng",
                "languages": [
                    "deu-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.977,
                "f1": 0.9707333333333332,
                "precision": 0.9679500000000002,
                "recall": 0.977,
                "main_score": 0.9707333333333332
            },
            {
                "hf_subset": "nld-eng",
                "languages": [
                    "nld-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9469999999999998,
                "f1": 0.932,
                "precision": 0.9248333333333334,
                "recall": 0.9469999999999998,
                "main_score": 0.932
            },
            {
                "hf_subset": "ron-eng",
                "languages": [
                    "ron-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.929,
                "f1": 0.9126666666666666,
                "precision": 0.9059444444444444,
                "recall": 0.929,
                "main_score": 0.9126666666666666
            },
            {
                "hf_subset": "ang-eng",
                "languages": [
                    "ang-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.34328358208955223,
                "f1": 0.2907418038015053,
                "precision": 0.28068207322920596,
                "recall": 0.34328358208955223,
                "main_score": 0.2907418038015053
            },
            {
                "hf_subset": "ido-eng",
                "languages": [
                    "ido-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.785,
                "f1": 0.743945115995116,
                "precision": 0.7282967843459223,
                "recall": 0.785,
                "main_score": 0.743945115995116
            },
            {
                "hf_subset": "jav-eng",
                "languages": [
                    "jav-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.6634146341463415,
                "f1": 0.612469400518181,
                "precision": 0.5963977756660683,
                "recall": 0.6634146341463415,
                "main_score": 0.612469400518181
            },
            {
                "hf_subset": "isl-eng",
                "languages": [
                    "isl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.809,
                "f1": 0.7690349206349206,
                "precision": 0.7532921568627451,
                "recall": 0.809,
                "main_score": 0.7690349206349206
            },
            {
                "hf_subset": "slv-eng",
                "languages": [
                    "slv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8493317132442284,
                "f1": 0.8192519105034295,
                "precision": 0.8071283920615635,
                "recall": 0.8493317132442284,
                "main_score": 0.8192519105034295
            },
            {
                "hf_subset": "cym-eng",
                "languages": [
                    "cym-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.711304347826087,
                "f1": 0.6522394755003451,
                "precision": 0.6291242236024843,
                "recall": 0.711304347826087,
                "main_score": 0.6522394755003451
            },
            {
                "hf_subset": "kaz-eng",
                "languages": [
                    "kaz-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.7982608695652174,
                "f1": 0.7555693581780538,
                "precision": 0.7379420289855072,
                "recall": 0.7982608695652174,
                "main_score": 0.7555693581780538
            },
            {
                "hf_subset": "est-eng",
                "languages": [
                    "est-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.74,
                "f1": 0.7051022222222223,
                "precision": 0.6929673599347512,
                "recall": 0.74,
                "main_score": 0.7051022222222223
            },
            {
                "hf_subset": "heb-eng",
                "languages": [
                    "heb-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.787,
                "f1": 0.7414238095238095,
                "precision": 0.7227214285714285,
                "recall": 0.787,
                "main_score": 0.7414238095238095
            },
            {
                "hf_subset": "gla-eng",
                "languages": [
                    "gla-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.48974668275030164,
                "f1": 0.43080330405420875,
                "precision": 0.41365054995935574,
                "recall": 0.48974668275030164,
                "main_score": 0.43080330405420875
            },
            {
                "hf_subset": "mar-eng",
                "languages": [
                    "mar-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.8960000000000001,
                "f1": 0.8662333333333333,
                "precision": 0.85225,
                "recall": 0.8960000000000001,
                "main_score": 0.8662333333333333
            },
            {
                "hf_subset": "lat-eng",
                "languages": [
                    "lat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.452,
                "f1": 0.395761253006253,
                "precision": 0.37991358436312,
                "recall": 0.452,
                "main_score": 0.395761253006253
            },
            {
                "hf_subset": "bel-eng",
                "languages": [
                    "bel-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.895,
                "f1": 0.8670333333333333,
                "precision": 0.8553166666666666,
                "recall": 0.895,
                "main_score": 0.8670333333333333
            },
            {
                "hf_subset": "pms-eng",
                "languages": [
                    "pms-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.5009523809523809,
                "f1": 0.4460650460650461,
                "precision": 0.42774116796477046,
                "recall": 0.5009523809523809,
                "main_score": 0.4460650460650461
            },
            {
                "hf_subset": "gle-eng",
                "languages": [
                    "gle-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.634,
                "f1": 0.5835967261904762,
                "precision": 0.5654857142857143,
                "recall": 0.634,
                "main_score": 0.5835967261904762
            },
            {
                "hf_subset": "pes-eng",
                "languages": [
                    "pes-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.892,
                "f1": 0.87075,
                "precision": 0.8612095238095239,
                "recall": 0.892,
                "main_score": 0.87075
            },
            {
                "hf_subset": "nob-eng",
                "languages": [
                    "nob-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.968,
                "f1": 0.9590333333333334,
                "precision": 0.9550833333333333,
                "recall": 0.968,
                "main_score": 0.9590333333333334
            },
            {
                "hf_subset": "bul-eng",
                "languages": [
                    "bul-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.909,
                "f1": 0.8862888888888889,
                "precision": 0.8761607142857142,
                "recall": 0.909,
                "main_score": 0.8862888888888889
            },
            {
                "hf_subset": "cbk-eng",
                "languages": [
                    "cbk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.652,
                "f1": 0.6054377630539395,
                "precision": 0.5889434482711381,
                "recall": 0.652,
                "main_score": 0.6054377630539395
            },
            {
                "hf_subset": "hun-eng",
                "languages": [
                    "hun-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.87,
                "f1": 0.8432412698412699,
                "precision": 0.8325527777777778,
                "recall": 0.87,
                "main_score": 0.8432412698412699
            },
            {
                "hf_subset": "uig-eng",
                "languages": [
                    "uig-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.687,
                "f1": 0.6307883541295306,
                "precision": 0.6106117424242425,
                "recall": 0.687,
                "main_score": 0.6307883541295306
            },
            {
                "hf_subset": "rus-eng",
                "languages": [
                    "rus-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.937,
                "f1": 0.9178333333333335,
                "precision": 0.9086666666666667,
                "recall": 0.937,
                "main_score": 0.9178333333333335
            },
            {
                "hf_subset": "spa-eng",
                "languages": [
                    "spa-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.977,
                "f1": 0.9696666666666667,
                "precision": 0.9661666666666667,
                "recall": 0.977,
                "main_score": 0.9696666666666667
            },
            {
                "hf_subset": "hye-eng",
                "languages": [
                    "hye-Armn",
                    "eng-Latn"
                ],
                "accuracy": 0.8827493261455526,
                "f1": 0.8590745732255168,
                "precision": 0.8491389637616052,
                "recall": 0.8827493261455526,
                "main_score": 0.8590745732255168
            },
            {
                "hf_subset": "tel-eng",
                "languages": [
                    "tel-Telu",
                    "eng-Latn"
                ],
                "accuracy": 0.905982905982906,
                "f1": 0.8849002849002849,
                "precision": 0.8757122507122507,
                "recall": 0.905982905982906,
                "main_score": 0.8849002849002849
            },
            {
                "hf_subset": "afr-eng",
                "languages": [
                    "afr-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.895,
                "f1": 0.8690769841269842,
                "precision": 0.8580178571428572,
                "recall": 0.895,
                "main_score": 0.8690769841269842
            },
            {
                "hf_subset": "mon-eng",
                "languages": [
                    "mon-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.825,
                "f1": 0.7836796536796539,
                "precision": 0.7682196969696968,
                "recall": 0.825,
                "main_score": 0.7836796536796539
            },
            {
                "hf_subset": "arz-eng",
                "languages": [
                    "arz-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.7148846960167715,
                "f1": 0.6678771089148448,
                "precision": 0.6498302885095338,
                "recall": 0.7148846960167715,
                "main_score": 0.6678771089148448
            },
            {
                "hf_subset": "hrv-eng",
                "languages": [
                    "hrv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.941,
                "f1": 0.9250333333333333,
                "precision": 0.91775,
                "recall": 0.941,
                "main_score": 0.9250333333333333
            },
            {
                "hf_subset": "nov-eng",
                "languages": [
                    "nov-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7120622568093385,
                "f1": 0.6683278891450098,
                "precision": 0.6535065777283677,
                "recall": 0.7120622568093385,
                "main_score": 0.6683278891450098
            },
            {
                "hf_subset": "gsw-eng",
                "languages": [
                    "gsw-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.48717948717948717,
                "f1": 0.4353146853146853,
                "precision": 0.4204721204721204,
                "recall": 0.48717948717948717,
                "main_score": 0.4353146853146853
            },
            {
                "hf_subset": "nds-eng",
                "languages": [
                    "nds-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.585,
                "f1": 0.538564991863928,
                "precision": 0.5240329436122275,
                "recall": 0.585,
                "main_score": 0.538564991863928
            },
            {
                "hf_subset": "ukr-eng",
                "languages": [
                    "ukr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.9079999999999999,
                "f1": 0.8829,
                "precision": 0.8709166666666667,
                "recall": 0.9079999999999999,
                "main_score": 0.8829
            },
            {
                "hf_subset": "uzb-eng",
                "languages": [
                    "uzb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.6728971962616822,
                "f1": 0.6263425307817831,
                "precision": 0.6098065939771546,
                "recall": 0.6728971962616822,
                "main_score": 0.6263425307817831
            },
            {
                "hf_subset": "lit-eng",
                "languages": [
                    "lit-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.787,
                "f1": 0.755264472455649,
                "precision": 0.7438205086580086,
                "recall": 0.787,
                "main_score": 0.755264472455649
            },
            {
                "hf_subset": "ina-eng",
                "languages": [
                    "ina-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.887,
                "f1": 0.8610809523809525,
                "precision": 0.8507602564102565,
                "recall": 0.887,
                "main_score": 0.8610809523809525
            },
            {
                "hf_subset": "lfn-eng",
                "languages": [
                    "lfn-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.57,
                "f1": 0.5285487521402737,
                "precision": 0.5153985162713104,
                "recall": 0.57,
                "main_score": 0.5285487521402737
            },
            {
                "hf_subset": "zsm-eng",
                "languages": [
                    "zsm-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.94,
                "f1": 0.9245333333333333,
                "precision": 0.9179166666666667,
                "recall": 0.94,
                "main_score": 0.9245333333333333
            },
            {
                "hf_subset": "ita-eng",
                "languages": [
                    "ita-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9230000000000002,
                "f1": 0.9061333333333333,
                "precision": 0.8983333333333331,
                "recall": 0.9230000000000002,
                "main_score": 0.9061333333333333
            },
            {
                "hf_subset": "cmn-eng",
                "languages": [
                    "cmn-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.9469999999999998,
                "f1": 0.9334555555555555,
                "precision": 0.9275416666666668,
                "recall": 0.9469999999999998,
                "main_score": 0.9334555555555555
            },
            {
                "hf_subset": "lvs-eng",
                "languages": [
                    "lvs-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.802,
                "f1": 0.766563035113035,
                "precision": 0.753014652014652,
                "recall": 0.802,
                "main_score": 0.766563035113035
            },
            {
                "hf_subset": "glg-eng",
                "languages": [
                    "glg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.847,
                "f1": 0.8278689263765208,
                "precision": 0.8206705086580086,
                "recall": 0.847,
                "main_score": 0.8278689263765208
            },
            {
                "hf_subset": "ceb-eng",
                "languages": [
                    "ceb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.5033333333333333,
                "f1": 0.4546152366152366,
                "precision": 0.4393545574795575,
                "recall": 0.5033333333333333,
                "main_score": 0.4546152366152366
            },
            {
                "hf_subset": "bre-eng",
                "languages": [
                    "bre-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.066,
                "f1": 0.054421214004464404,
                "precision": 0.051466303854875294,
                "recall": 0.066,
                "main_score": 0.054421214004464404
            },
            {
                "hf_subset": "ben-eng",
                "languages": [
                    "ben-Beng",
                    "eng-Latn"
                ],
                "accuracy": 0.85,
                "f1": 0.8104666666666667,
                "precision": 0.7925,
                "recall": 0.85,
                "main_score": 0.8104666666666667
            },
            {
                "hf_subset": "swg-eng",
                "languages": [
                    "swg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.4732142857142857,
                "f1": 0.42333333333333334,
                "precision": 0.40691964285714294,
                "recall": 0.4732142857142857,
                "main_score": 0.42333333333333334
            },
            {
                "hf_subset": "arq-eng",
                "languages": [
                    "arq-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.30735455543358947,
                "f1": 0.2673616790022338,
                "precision": 0.2539782322045128,
                "recall": 0.30735455543358947,
                "main_score": 0.2673616790022338
            },
            {
                "hf_subset": "kab-eng",
                "languages": [
                    "kab-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.251,
                "f1": 0.21975989896371023,
                "precision": 0.21059885632257203,
                "recall": 0.251,
                "main_score": 0.21975989896371023
            },
            {
                "hf_subset": "fra-eng",
                "languages": [
                    "fra-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.943,
                "f1": 0.9275666666666667,
                "precision": 0.9206166666666665,
                "recall": 0.943,
                "main_score": 0.9275666666666667
            },
            {
                "hf_subset": "por-eng",
                "languages": [
                    "por-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.941,
                "f1": 0.9274,
                "precision": 0.9209166666666667,
                "recall": 0.941,
                "main_score": 0.9274
            },
            {
                "hf_subset": "tat-eng",
                "languages": [
                    "tat-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.713,
                "f1": 0.66922442002442,
                "precision": 0.6538249567099568,
                "recall": 0.713,
                "main_score": 0.66922442002442
            },
            {
                "hf_subset": "oci-eng",
                "languages": [
                    "oci-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.403,
                "f1": 0.3578682789299971,
                "precision": 0.3466425128716588,
                "recall": 0.403,
                "main_score": 0.3578682789299971
            },
            {
                "hf_subset": "pol-eng",
                "languages": [
                    "pol-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.96,
                "f1": 0.9482333333333334,
                "precision": 0.9427833333333333,
                "recall": 0.96,
                "main_score": 0.9482333333333334
            },
            {
                "hf_subset": "war-eng",
                "languages": [
                    "war-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.511,
                "f1": 0.4717907475313358,
                "precision": 0.46064610447024246,
                "recall": 0.511,
                "main_score": 0.4717907475313358
            },
            {
                "hf_subset": "aze-eng",
                "languages": [
                    "aze-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.877,
                "f1": 0.8471,
                "precision": 0.8346166666666668,
                "recall": 0.877,
                "main_score": 0.8471
            },
            {
                "hf_subset": "vie-eng",
                "languages": [
                    "vie-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.958,
                "f1": 0.9468333333333334,
                "precision": 0.9413333333333334,
                "recall": 0.958,
                "main_score": 0.9468333333333334
            },
            {
                "hf_subset": "nno-eng",
                "languages": [
                    "nno-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8539999999999999,
                "f1": 0.825577380952381,
                "precision": 0.8136833333333334,
                "recall": 0.8539999999999999,
                "main_score": 0.825577380952381
            },
            {
                "hf_subset": "cha-eng",
                "languages": [
                    "cha-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.2116788321167883,
                "f1": 0.16948865627297988,
                "precision": 0.15971932568647898,
                "recall": 0.2116788321167883,
                "main_score": 0.16948865627297988
            },
            {
                "hf_subset": "mhr-eng",
                "languages": [
                    "mhr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.069,
                "f1": 0.05515526831658907,
                "precision": 0.05141966366966367,
                "recall": 0.069,
                "main_score": 0.05515526831658907
            },
            {
                "hf_subset": "dan-eng",
                "languages": [
                    "dan-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.932,
                "f1": 0.9139666666666667,
                "precision": 0.9058666666666667,
                "recall": 0.932,
                "main_score": 0.9139666666666667
            },
            {
                "hf_subset": "ell-eng",
                "languages": [
                    "ell-Grek",
                    "eng-Latn"
                ],
                "accuracy": 0.922,
                "f1": 0.8995666666666666,
                "precision": 0.8892833333333333,
                "recall": 0.922,
                "main_score": 0.8995666666666666
            },
            {
                "hf_subset": "amh-eng",
                "languages": [
                    "amh-Ethi",
                    "eng-Latn"
                ],
                "accuracy": 0.7976190476190478,
                "f1": 0.7493386243386244,
                "precision": 0.7311011904761904,
                "recall": 0.7976190476190478,
                "main_score": 0.7493386243386244
            },
            {
                "hf_subset": "pam-eng",
                "languages": [
                    "pam-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.088,
                "f1": 0.06921439712248537,
                "precision": 0.06489885109680683,
                "recall": 0.088,
                "main_score": 0.06921439712248537
            },
            {
                "hf_subset": "hsb-eng",
                "languages": [
                    "hsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.45755693581780543,
                "f1": 0.4034699501312631,
                "precision": 0.3857886764719063,
                "recall": 0.45755693581780543,
                "main_score": 0.4034699501312631
            },
            {
                "hf_subset": "srp-eng",
                "languages": [
                    "srp-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.914,
                "f1": 0.8908333333333333,
                "precision": 0.8801666666666668,
                "recall": 0.914,
                "main_score": 0.8908333333333333
            },
            {
                "hf_subset": "epo-eng",
                "languages": [
                    "epo-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.936,
                "f1": 0.9206690476190477,
                "precision": 0.9145095238095239,
                "recall": 0.936,
                "main_score": 0.9206690476190477
            },
            {
                "hf_subset": "kzj-eng",
                "languages": [
                    "kzj-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.075,
                "f1": 0.06200363129378736,
                "precision": 0.0589115314822466,
                "recall": 0.075,
                "main_score": 0.06200363129378736
            },
            {
                "hf_subset": "awa-eng",
                "languages": [
                    "awa-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.7359307359307359,
                "f1": 0.6838933553219267,
                "precision": 0.6662698412698412,
                "recall": 0.7359307359307359,
                "main_score": 0.6838933553219267
            },
            {
                "hf_subset": "fao-eng",
                "languages": [
                    "fao-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.6984732824427481,
                "f1": 0.6472373682297347,
                "precision": 0.6282834214131924,
                "recall": 0.6984732824427481,
                "main_score": 0.6472373682297347
            },
            {
                "hf_subset": "mal-eng",
                "languages": [
                    "mal-Mlym",
                    "eng-Latn"
                ],
                "accuracy": 0.975254730713246,
                "f1": 0.9672489082969432,
                "precision": 0.9633672974284326,
                "recall": 0.975254730713246,
                "main_score": 0.9672489082969432
            },
            {
                "hf_subset": "ile-eng",
                "languages": [
                    "ile-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7559999999999999,
                "f1": 0.7242746031746033,
                "precision": 0.7114036630036631,
                "recall": 0.7559999999999999,
                "main_score": 0.7242746031746033
            },
            {
                "hf_subset": "bos-eng",
                "languages": [
                    "bos-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9124293785310733,
                "f1": 0.8886064030131826,
                "precision": 0.8773540489642184,
                "recall": 0.9124293785310733,
                "main_score": 0.8886064030131826
            },
            {
                "hf_subset": "cor-eng",
                "languages": [
                    "cor-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.062,
                "f1": 0.043830836597949535,
                "precision": 0.04027861324289673,
                "recall": 0.062,
                "main_score": 0.043830836597949535
            },
            {
                "hf_subset": "cat-eng",
                "languages": [
                    "cat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.868,
                "f1": 0.8409428571428572,
                "precision": 0.8300333333333333,
                "recall": 0.868,
                "main_score": 0.8409428571428572
            },
            {
                "hf_subset": "eus-eng",
                "languages": [
                    "eus-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.607,
                "f1": 0.561584972394755,
                "precision": 0.5471345633090313,
                "recall": 0.607,
                "main_score": 0.561584972394755
            },
            {
                "hf_subset": "yue-eng",
                "languages": [
                    "yue-Hant",
                    "eng-Latn"
                ],
                "accuracy": 0.8420000000000001,
                "f1": 0.8066190476190475,
                "precision": 0.7919690476190476,
                "recall": 0.8420000000000001,
                "main_score": 0.8066190476190475
            },
            {
                "hf_subset": "swe-eng",
                "languages": [
                    "swe-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.932,
                "f1": 0.9133,
                "precision": 0.9045000000000001,
                "recall": 0.932,
                "main_score": 0.9133
            },
            {
                "hf_subset": "dtp-eng",
                "languages": [
                    "dtp-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.063,
                "f1": 0.05126828976748276,
                "precision": 0.048536143289666676,
                "recall": 0.063,
                "main_score": 0.05126828976748276
            },
            {
                "hf_subset": "kat-eng",
                "languages": [
                    "kat-Geor",
                    "eng-Latn"
                ],
                "accuracy": 0.8176943699731903,
                "f1": 0.7782873739308057,
                "precision": 0.7627622452019235,
                "recall": 0.8176943699731903,
                "main_score": 0.7782873739308057
            },
            {
                "hf_subset": "jpn-eng",
                "languages": [
                    "jpn-Jpan",
                    "eng-Latn"
                ],
                "accuracy": 0.9230000000000002,
                "f1": 0.9029666666666665,
                "precision": 0.8940333333333333,
                "recall": 0.9230000000000002,
                "main_score": 0.9029666666666665
            },
            {
                "hf_subset": "csb-eng",
                "languages": [
                    "csb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.2924901185770751,
                "f1": 0.24561866096392948,
                "precision": 0.23356583740215456,
                "recall": 0.2924901185770751,
                "main_score": 0.24561866096392948
            },
            {
                "hf_subset": "xho-eng",
                "languages": [
                    "xho-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7746478873239436,
                "f1": 0.7323943661971832,
                "precision": 0.7166666666666667,
                "recall": 0.7746478873239436,
                "main_score": 0.7323943661971832
            },
            {
                "hf_subset": "orv-eng",
                "languages": [
                    "orv-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.20359281437125748,
                "f1": 0.15997867865075824,
                "precision": 0.14882104658301346,
                "recall": 0.20359281437125748,
                "main_score": 0.15997867865075824
            },
            {
                "hf_subset": "ind-eng",
                "languages": [
                    "ind-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.922,
                "f1": 0.9026,
                "precision": 0.8945333333333335,
                "recall": 0.922,
                "main_score": 0.9026
            },
            {
                "hf_subset": "tuk-eng",
                "languages": [
                    "tuk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.2315270935960591,
                "f1": 0.1965673625772148,
                "precision": 0.1879370529346499,
                "recall": 0.2315270935960591,
                "main_score": 0.1965673625772148
            },
            {
                "hf_subset": "max-eng",
                "languages": [
                    "max-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.5915492957746479,
                "f1": 0.523868463305083,
                "precision": 0.5014938113529662,
                "recall": 0.5915492957746479,
                "main_score": 0.523868463305083
            },
            {
                "hf_subset": "swh-eng",
                "languages": [
                    "swh-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7051282051282051,
                "f1": 0.6680891330891331,
                "precision": 0.6537645687645687,
                "recall": 0.7051282051282051,
                "main_score": 0.6680891330891331
            },
            {
                "hf_subset": "hin-eng",
                "languages": [
                    "hin-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.946,
                "f1": 0.93,
                "precision": 0.9223333333333333,
                "recall": 0.946,
                "main_score": 0.93
            },
            {
                "hf_subset": "dsb-eng",
                "languages": [
                    "dsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.3862212943632568,
                "f1": 0.34327827696258306,
                "precision": 0.3307646935732408,
                "recall": 0.3862212943632568,
                "main_score": 0.34327827696258306
            },
            {
                "hf_subset": "ber-eng",
                "languages": [
                    "ber-Tfng",
                    "eng-Latn"
                ],
                "accuracy": 0.281,
                "f1": 0.23579609223054604,
                "precision": 0.22396227749215553,
                "recall": 0.281,
                "main_score": 0.23579609223054604
            },
            {
                "hf_subset": "tam-eng",
                "languages": [
                    "tam-Taml",
                    "eng-Latn"
                ],
                "accuracy": 0.8827361563517914,
                "f1": 0.8512486427795873,
                "precision": 0.8371335504885994,
                "recall": 0.8827361563517914,
                "main_score": 0.8512486427795873
            },
            {
                "hf_subset": "slk-eng",
                "languages": [
                    "slk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8859999999999999,
                "f1": 0.8639928571428571,
                "precision": 0.854947557997558,
                "recall": 0.8859999999999999,
                "main_score": 0.8639928571428571
            },
            {
                "hf_subset": "tgl-eng",
                "languages": [
                    "tgl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.865,
                "f1": 0.837795238095238,
                "precision": 0.8267602564102564,
                "recall": 0.865,
                "main_score": 0.837795238095238
            },
            {
                "hf_subset": "ast-eng",
                "languages": [
                    "ast-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7952755905511811,
                "f1": 0.7530558680164979,
                "precision": 0.7381889763779528,
                "recall": 0.7952755905511811,
                "main_score": 0.7530558680164979
            },
            {
                "hf_subset": "mkd-eng",
                "languages": [
                    "mkd-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.779,
                "f1": 0.7376261904761905,
                "precision": 0.7211670995670996,
                "recall": 0.779,
                "main_score": 0.7376261904761905
            },
            {
                "hf_subset": "khm-eng",
                "languages": [
                    "khm-Khmr",
                    "eng-Latn"
                ],
                "accuracy": 0.538781163434903,
                "f1": 0.47258040512888155,
                "precision": 0.450603482390186,
                "recall": 0.538781163434903,
                "main_score": 0.47258040512888155
            },
            {
                "hf_subset": "ces-eng",
                "languages": [
                    "ces-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.911,
                "f1": 0.8887999999999999,
                "precision": 0.8796333333333334,
                "recall": 0.911,
                "main_score": 0.8887999999999999
            },
            {
                "hf_subset": "tzl-eng",
                "languages": [
                    "tzl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.3846153846153847,
                "f1": 0.3443978243978244,
                "precision": 0.3342948717948718,
                "recall": 0.3846153846153847,
                "main_score": 0.3443978243978244
            },
            {
                "hf_subset": "urd-eng",
                "languages": [
                    "urd-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.889,
                "f1": 0.8619888888888887,
                "precision": 0.8507440476190475,
                "recall": 0.889,
                "main_score": 0.8619888888888887
            },
            {
                "hf_subset": "ara-eng",
                "languages": [
                    "ara-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.8590000000000001,
                "f1": 0.8258857142857142,
                "precision": 0.8115666666666667,
                "recall": 0.8590000000000001,
                "main_score": 0.8258857142857142
            },
            {
                "hf_subset": "kor-eng",
                "languages": [
                    "kor-Hang",
                    "eng-Latn"
                ],
                "accuracy": 0.868,
                "f1": 0.8336999999999999,
                "precision": 0.8186833333333332,
                "recall": 0.868,
                "main_score": 0.8336999999999999
            },
            {
                "hf_subset": "yid-eng",
                "languages": [
                    "yid-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.6851415094339621,
                "f1": 0.6319500009948124,
                "precision": 0.6139403344297212,
                "recall": 0.6851415094339621,
                "main_score": 0.6319500009948124
            },
            {
                "hf_subset": "fin-eng",
                "languages": [
                    "fin-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.885,
                "f1": 0.8614603174603175,
                "precision": 0.8511620370370371,
                "recall": 0.885,
                "main_score": 0.8614603174603175
            },
            {
                "hf_subset": "tha-eng",
                "languages": [
                    "tha-Thai",
                    "eng-Latn"
                ],
                "accuracy": 0.9562043795620438,
                "f1": 0.9440389294403891,
                "precision": 0.9379562043795621,
                "recall": 0.9562043795620438,
                "main_score": 0.9440389294403891
            },
            {
                "hf_subset": "wuu-eng",
                "languages": [
                    "wuu-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.818,
                "f1": 0.786532178932179,
                "precision": 0.7746348795840176,
                "recall": 0.818,
                "main_score": 0.786532178932179
            }
        ]
    }
}