{
  "dataset_revision": "3d86128a09e091d6018b6d26cad27f2739fc2db7",
  "task_name": "ImdbClassification",
  "mteb_version": "1.36.37",
  "scores": {
    "test": [
      {
        "accuracy": 0.61218,
        "f1": 0.609921,
        "f1_weighted": 0.609921,
        "ap": 0.568429,
        "ap_weighted": 0.568429,
        "scores_per_experiment": [
          {
            "accuracy": 0.64392,
            "f1": 0.641636,
            "f1_weighted": 0.641636,
            "ap": 0.596609,
            "ap_weighted": 0.596609
          },
          {
            "accuracy": 0.62808,
            "f1": 0.626295,
            "f1_weighted": 0.626295,
            "ap": 0.578452,
            "ap_weighted": 0.578452
          },
          {
            "accuracy": 0.5758,
            "f1": 0.570924,
            "f1_weighted": 0.570924,
            "ap": 0.542636,
            "ap_weighted": 0.542636
          },
          {
            "accuracy": 0.61052,
            "f1": 0.609015,
            "f1_weighted": 0.609015,
            "ap": 0.566126,
            "ap_weighted": 0.566126
          },
          {
            "accuracy": 0.59716,
            "f1": 0.59334,
            "f1_weighted": 0.59334,
            "ap": 0.556487,
            "ap_weighted": 0.556487
          },
          {
            "accuracy": 0.60456,
            "f1": 0.602499,
            "f1_weighted": 0.602499,
            "ap": 0.565052,
            "ap_weighted": 0.565052
          },
          {
            "accuracy": 0.62836,
            "f1": 0.626875,
            "f1_weighted": 0.626875,
            "ap": 0.578811,
            "ap_weighted": 0.578811
          },
          {
            "accuracy": 0.62084,
            "f1": 0.620334,
            "f1_weighted": 0.620334,
            "ap": 0.574028,
            "ap_weighted": 0.574028
          },
          {
            "accuracy": 0.60524,
            "f1": 0.602117,
            "f1_weighted": 0.602117,
            "ap": 0.562028,
            "ap_weighted": 0.562028
          },
          {
            "accuracy": 0.60732,
            "f1": 0.606177,
            "f1_weighted": 0.606177,
            "ap": 0.564057,
            "ap_weighted": 0.564057
          }
        ],
        "main_score": 0.61218,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 93.52223753929138,
  "kg_co2_emissions": null
}