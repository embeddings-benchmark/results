{
  "dataset_revision": "f8650438298df086750ff4973661bb58a201a5ee",
  "evaluation_time": 2517.1077768802643,
  "kg_co2_emissions": 0.21261099745687223,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.6264822134387352,
        "f1": 0.5778722769638918,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.5778722769638918,
        "precision": 0.5622834982754111,
        "recall": 0.6264822134387352
      },
      {
        "accuracy": 0.6699604743083004,
        "f1": 0.6033699502078949,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ],
        "main_score": 0.6033699502078949,
        "precision": 0.5779997176736308,
        "recall": 0.6699604743083004
      },
      {
        "accuracy": 0.33300395256917,
        "f1": 0.29547938293830966,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ],
        "main_score": 0.29547938293830966,
        "precision": 0.285413864380801,
        "recall": 0.33300395256917
      },
      {
        "accuracy": 0.4535573122529644,
        "f1": 0.39059092689103986,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ],
        "main_score": 0.39059092689103986,
        "precision": 0.3701855767368527,
        "recall": 0.4535573122529644
      },
      {
        "accuracy": 0.9229249011857708,
        "f1": 0.903623188405797,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.903623188405797,
        "precision": 0.8958027479766609,
        "recall": 0.9229249011857708
      },
      {
        "accuracy": 0.9436758893280632,
        "f1": 0.9271080368906456,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ],
        "main_score": 0.9271080368906456,
        "precision": 0.9193840579710144,
        "recall": 0.9436758893280632
      },
      {
        "accuracy": 0.2391304347826087,
        "f1": 0.20925896721866685,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ],
        "main_score": 0.20925896721866685,
        "precision": 0.20009302887036526,
        "recall": 0.2391304347826087
      },
      {
        "accuracy": 0.3221343873517787,
        "f1": 0.26781094399671473,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ],
        "main_score": 0.26781094399671473,
        "precision": 0.250807597967989,
        "recall": 0.3221343873517787
      },
      {
        "accuracy": 0.33794466403162055,
        "f1": 0.30238217317583543,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.30238217317583543,
        "precision": 0.2908705023043581,
        "recall": 0.33794466403162055
      },
      {
        "accuracy": 0.41205533596837945,
        "f1": 0.353424442742028,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ],
        "main_score": 0.353424442742028,
        "precision": 0.3359342225078391,
        "recall": 0.41205533596837945
      },
      {
        "accuracy": 0.6432806324110671,
        "f1": 0.6027498065541544,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.6027498065541544,
        "precision": 0.5900426501667492,
        "recall": 0.6432806324110671
      },
      {
        "accuracy": 0.6907114624505929,
        "f1": 0.6319653776175514,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ],
        "main_score": 0.6319653776175514,
        "precision": 0.6104370569044482,
        "recall": 0.6907114624505929
      },
      {
        "accuracy": 0.16403162055335968,
        "f1": 0.1405448983719571,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.1405448983719571,
        "precision": 0.13394850155136148,
        "recall": 0.16403162055335968
      },
      {
        "accuracy": 0.24802371541501977,
        "f1": 0.19490132947397565,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ],
        "main_score": 0.19490132947397565,
        "precision": 0.1787470403920529,
        "recall": 0.24802371541501977
      },
      {
        "accuracy": 0.3231225296442688,
        "f1": 0.29165853811072445,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.29165853811072445,
        "precision": 0.2815558438003481,
        "recall": 0.3231225296442688
      },
      {
        "accuracy": 0.4051383399209486,
        "f1": 0.33131928458015414,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ],
        "main_score": 0.33131928458015414,
        "precision": 0.3076900070724181,
        "recall": 0.4051383399209486
      },
      {
        "accuracy": 0.8359683794466403,
        "f1": 0.801288385527516,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.801288385527516,
        "precision": 0.7882991404730535,
        "recall": 0.8359683794466403
      },
      {
        "accuracy": 0.8547430830039525,
        "f1": 0.8187276491624317,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ],
        "main_score": 0.8187276491624317,
        "precision": 0.8035483291412145,
        "recall": 0.8547430830039525
      },
      {
        "accuracy": 0.4209486166007905,
        "f1": 0.36929543504305967,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ],
        "main_score": 0.36929543504305967,
        "precision": 0.3538358482335582,
        "recall": 0.4209486166007905
      },
      {
        "accuracy": 0.5217391304347826,
        "f1": 0.44585593838504145,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ],
        "main_score": 0.44585593838504145,
        "precision": 0.4183976802183324,
        "recall": 0.5217391304347826
      },
      {
        "accuracy": 0.8102766798418972,
        "f1": 0.7785237306976437,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ],
        "main_score": 0.7785237306976437,
        "precision": 0.7673247207338116,
        "recall": 0.8102766798418972
      },
      {
        "accuracy": 0.8488142292490118,
        "f1": 0.8123764822134387,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ],
        "main_score": 0.8123764822134387,
        "precision": 0.7971179183135706,
        "recall": 0.8488142292490118
      },
      {
        "accuracy": 0.7094861660079052,
        "f1": 0.6706844522870623,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ],
        "main_score": 0.6706844522870623,
        "precision": 0.6572711302750828,
        "recall": 0.7094861660079052
      },
      {
        "accuracy": 0.766798418972332,
        "f1": 0.7176736307171089,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ],
        "main_score": 0.7176736307171089,
        "precision": 0.6978116141653691,
        "recall": 0.766798418972332
      },
      {
        "accuracy": 0.1492094861660079,
        "f1": 0.12750765448693163,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ],
        "main_score": 0.12750765448693163,
        "precision": 0.1207323802679534,
        "recall": 0.1492094861660079
      },
      {
        "accuracy": 0.22035573122529645,
        "f1": 0.16531401733970902,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ],
        "main_score": 0.16531401733970902,
        "precision": 0.150401612185209,
        "recall": 0.22035573122529645
      },
      {
        "accuracy": 0.29347826086956524,
        "f1": 0.2587269340559861,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ],
        "main_score": 0.2587269340559861,
        "precision": 0.2501924189955354,
        "recall": 0.29347826086956524
      },
      {
        "accuracy": 0.391304347826087,
        "f1": 0.31876392162953426,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ],
        "main_score": 0.31876392162953426,
        "precision": 0.29639661611993623,
        "recall": 0.391304347826087
      },
      {
        "accuracy": 0.607707509881423,
        "f1": 0.5566257760453726,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ],
        "main_score": 0.5566257760453726,
        "precision": 0.5399252768632032,
        "recall": 0.607707509881423
      },
      {
        "accuracy": 0.6195652173913043,
        "f1": 0.5506007277746408,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ],
        "main_score": 0.5506007277746408,
        "precision": 0.5248184484597528,
        "recall": 0.6195652173913043
      },
      {
        "accuracy": 0.5662055335968379,
        "f1": 0.527077609401514,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ],
        "main_score": 0.527077609401514,
        "precision": 0.5142489851331018,
        "recall": 0.5662055335968379
      },
      {
        "accuracy": 0.650197628458498,
        "f1": 0.581805633979547,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ],
        "main_score": 0.581805633979547,
        "precision": 0.5558104335278248,
        "recall": 0.650197628458498
      },
      {
        "accuracy": 0.8656126482213439,
        "f1": 0.8385150622711655,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.8385150622711655,
        "precision": 0.828481829161177,
        "recall": 0.8656126482213439
      },
      {
        "accuracy": 0.8932806324110671,
        "f1": 0.8645115753811405,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ],
        "main_score": 0.8645115753811405,
        "precision": 0.8516139657444004,
        "recall": 0.8932806324110671
      },
      {
        "accuracy": 0.8646245059288538,
        "f1": 0.8360400723949898,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ],
        "main_score": 0.8360400723949898,
        "precision": 0.8252440946734425,
        "recall": 0.8646245059288538
      },
      {
        "accuracy": 0.8992094861660079,
        "f1": 0.86933465085639,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ],
        "main_score": 0.86933465085639,
        "precision": 0.8554347826086957,
        "recall": 0.8992094861660079
      },
      {
        "accuracy": 0.08695652173913043,
        "f1": 0.07133488559966333,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ],
        "main_score": 0.07133488559966333,
        "precision": 0.06643043176477409,
        "recall": 0.08695652173913043
      },
      {
        "accuracy": 0.13339920948616601,
        "f1": 0.09338493631971892,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ],
        "main_score": 0.09338493631971892,
        "precision": 0.08227576112951607,
        "recall": 0.13339920948616601
      },
      {
        "accuracy": 0.3201581027667984,
        "f1": 0.2916244835102238,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ],
        "main_score": 0.2916244835102238,
        "precision": 0.28316360143651953,
        "recall": 0.3201581027667984
      },
      {
        "accuracy": 0.3883399209486166,
        "f1": 0.3200011276936006,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ],
        "main_score": 0.3200011276936006,
        "precision": 0.2990503786946475,
        "recall": 0.3883399209486166
      },
      {
        "accuracy": 0.8260869565217391,
        "f1": 0.798942382968848,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ],
        "main_score": 0.798942382968848,
        "precision": 0.7892923838027714,
        "recall": 0.8260869565217391
      },
      {
        "accuracy": 0.8932806324110671,
        "f1": 0.8618906455862978,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ],
        "main_score": 0.8618906455862978,
        "precision": 0.8474472990777338,
        "recall": 0.8932806324110671
      },
      {
        "accuracy": 0.5513833992094862,
        "f1": 0.509511799097684,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ],
        "main_score": 0.509511799097684,
        "precision": 0.4966522919148639,
        "recall": 0.5513833992094862
      },
      {
        "accuracy": 0.6294466403162056,
        "f1": 0.5715304019651846,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ],
        "main_score": 0.5715304019651846,
        "precision": 0.5502678535781304,
        "recall": 0.6294466403162056
      },
      {
        "accuracy": 0.8132411067193676,
        "f1": 0.7791982618933352,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ],
        "main_score": 0.7791982618933352,
        "precision": 0.7664479331488381,
        "recall": 0.8132411067193676
      },
      {
        "accuracy": 0.8695652173913043,
        "f1": 0.8345191040843215,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ],
        "main_score": 0.8345191040843215,
        "precision": 0.8187911725955205,
        "recall": 0.8695652173913043
      },
      {
        "accuracy": 0.8132411067193676,
        "f1": 0.7792070375594403,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ],
        "main_score": 0.7792070375594403,
        "precision": 0.7666411185432925,
        "recall": 0.8132411067193676
      },
      {
        "accuracy": 0.8804347826086957,
        "f1": 0.8464285714285714,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ],
        "main_score": 0.8464285714285714,
        "precision": 0.830862977602108,
        "recall": 0.8804347826086957
      },
      {
        "accuracy": 0.7766798418972332,
        "f1": 0.7426668440255396,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ],
        "main_score": 0.7426668440255396,
        "precision": 0.7315753612311022,
        "recall": 0.7766798418972332
      },
      {
        "accuracy": 0.8300395256916996,
        "f1": 0.7853307923960098,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ],
        "main_score": 0.7853307923960098,
        "precision": 0.7668407679277245,
        "recall": 0.8300395256916996
      },
      {
        "accuracy": 0.3725296442687747,
        "f1": 0.3299105842425861,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ],
        "main_score": 0.3299105842425861,
        "precision": 0.3166880953578916,
        "recall": 0.3725296442687747
      },
      {
        "accuracy": 0.41798418972332013,
        "f1": 0.3551834925754863,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ],
        "main_score": 0.3551834925754863,
        "precision": 0.3351189092237686,
        "recall": 0.41798418972332013
      },
      {
        "accuracy": 0.8280632411067194,
        "f1": 0.7974823685704876,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ],
        "main_score": 0.7974823685704876,
        "precision": 0.7867300724637681,
        "recall": 0.8280632411067194
      },
      {
        "accuracy": 0.8764822134387352,
        "f1": 0.8421277997364954,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ],
        "main_score": 0.8421277997364954,
        "precision": 0.8266963109354414,
        "recall": 0.8764822134387352
      },
      {
        "accuracy": 0.841897233201581,
        "f1": 0.8108283926218709,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ],
        "main_score": 0.8108283926218709,
        "precision": 0.7994518162996425,
        "recall": 0.841897233201581
      },
      {
        "accuracy": 0.8932806324110671,
        "f1": 0.8627140974967061,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ],
        "main_score": 0.8627140974967061,
        "precision": 0.8487648221343874,
        "recall": 0.8932806324110671
      },
      {
        "accuracy": 0.017786561264822136,
        "f1": 0.015349143610013173,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ],
        "main_score": 0.015349143610013173,
        "precision": 0.014674192179719077,
        "recall": 0.017786561264822136
      },
      {
        "accuracy": 0.03260869565217391,
        "f1": 0.011066382793076654,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ],
        "main_score": 0.011066382793076654,
        "precision": 0.007769356636656135,
        "recall": 0.03260869565217391
      }
    ],
    "validation": [
      {
        "accuracy": 0.6670010030090271,
        "f1": 0.6179531260757699,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.6179531260757699,
        "precision": 0.6012501846754604,
        "recall": 0.6670010030090271
      },
      {
        "accuracy": 0.7211634904714143,
        "f1": 0.6697791788062601,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ],
        "main_score": 0.6697791788062601,
        "precision": 0.649996356591141,
        "recall": 0.7211634904714143
      },
      {
        "accuracy": 0.37512537612838515,
        "f1": 0.3285493209702568,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ],
        "main_score": 0.3285493209702568,
        "precision": 0.31575619740409755,
        "recall": 0.37512537612838515
      },
      {
        "accuracy": 0.4663991975927783,
        "f1": 0.4001966261406346,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ],
        "main_score": 0.4001966261406346,
        "precision": 0.3806529483184449,
        "recall": 0.4663991975927783
      },
      {
        "accuracy": 0.9338014042126379,
        "f1": 0.9159669484644409,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.9159669484644409,
        "precision": 0.9084252758274824,
        "recall": 0.9338014042126379
      },
      {
        "accuracy": 0.9498495486459378,
        "f1": 0.9334670678702774,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ],
        "main_score": 0.9334670678702774,
        "precision": 0.9254429956536276,
        "recall": 0.9498495486459378
      },
      {
        "accuracy": 0.2597793380140421,
        "f1": 0.22133705634848108,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ],
        "main_score": 0.22133705634848108,
        "precision": 0.21271100291661976,
        "recall": 0.2597793380140421
      },
      {
        "accuracy": 0.3319959879638917,
        "f1": 0.2736211333916213,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ],
        "main_score": 0.2736211333916213,
        "precision": 0.25676379292108054,
        "recall": 0.3319959879638917
      },
      {
        "accuracy": 0.3420260782347041,
        "f1": 0.2966172643903204,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.2966172643903204,
        "precision": 0.2847028602205275,
        "recall": 0.3420260782347041
      },
      {
        "accuracy": 0.39518555667001004,
        "f1": 0.334765484241544,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ],
        "main_score": 0.334765484241544,
        "precision": 0.3155240358202185,
        "recall": 0.39518555667001004
      },
      {
        "accuracy": 0.6820461384152458,
        "f1": 0.6286093153633855,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.6286093153633855,
        "precision": 0.6110587715527533,
        "recall": 0.6820461384152458
      },
      {
        "accuracy": 0.7001003009027081,
        "f1": 0.6415588757614836,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ],
        "main_score": 0.6415588757614836,
        "precision": 0.6197827971649437,
        "recall": 0.7001003009027081
      },
      {
        "accuracy": 0.17853560682046138,
        "f1": 0.15417464382644383,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.15417464382644383,
        "precision": 0.14762641695759068,
        "recall": 0.17853560682046138
      },
      {
        "accuracy": 0.2678034102306921,
        "f1": 0.21444975931814864,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ],
        "main_score": 0.21444975931814864,
        "precision": 0.1978196856080507,
        "recall": 0.2678034102306921
      },
      {
        "accuracy": 0.3239719157472417,
        "f1": 0.28325748872173406,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.28325748872173406,
        "precision": 0.2721540436160876,
        "recall": 0.3239719157472417
      },
      {
        "accuracy": 0.3971915747241725,
        "f1": 0.3262680387378009,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ],
        "main_score": 0.3262680387378009,
        "precision": 0.3039435841297308,
        "recall": 0.3971915747241725
      },
      {
        "accuracy": 0.8665997993981945,
        "f1": 0.8388545367482177,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.8388545367482177,
        "precision": 0.8284873852326209,
        "recall": 0.8665997993981945
      },
      {
        "accuracy": 0.876629889669007,
        "f1": 0.841925777331996,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ],
        "main_score": 0.841925777331996,
        "precision": 0.8268472082915413,
        "recall": 0.876629889669007
      },
      {
        "accuracy": 0.4744232698094283,
        "f1": 0.42047045721492554,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ],
        "main_score": 0.42047045721492554,
        "precision": 0.40591723954406533,
        "recall": 0.4744232698094283
      },
      {
        "accuracy": 0.5346038114343029,
        "f1": 0.45882635697079033,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ],
        "main_score": 0.45882635697079033,
        "precision": 0.4322547367933527,
        "recall": 0.5346038114343029
      },
      {
        "accuracy": 0.8505516549648947,
        "f1": 0.8173194909403535,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ],
        "main_score": 0.8173194909403535,
        "precision": 0.8047893681043129,
        "recall": 0.8505516549648947
      },
      {
        "accuracy": 0.8545636910732196,
        "f1": 0.8159048574294311,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ],
        "main_score": 0.8159048574294311,
        "precision": 0.7994316282179873,
        "recall": 0.8545636910732196
      },
      {
        "accuracy": 0.7231695085255767,
        "f1": 0.6783255249152943,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ],
        "main_score": 0.6783255249152943,
        "precision": 0.6628889497965766,
        "recall": 0.7231695085255767
      },
      {
        "accuracy": 0.7612838515546639,
        "f1": 0.7069135979366672,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ],
        "main_score": 0.7069135979366672,
        "precision": 0.6844629125471653,
        "recall": 0.7612838515546639
      },
      {
        "accuracy": 0.1584754262788365,
        "f1": 0.1324803397774389,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ],
        "main_score": 0.1324803397774389,
        "precision": 0.12568630887817664,
        "recall": 0.1584754262788365
      },
      {
        "accuracy": 0.2296890672016048,
        "f1": 0.17486476400600576,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ],
        "main_score": 0.17486476400600576,
        "precision": 0.15945633020564712,
        "recall": 0.2296890672016048
      },
      {
        "accuracy": 0.3039117352056169,
        "f1": 0.2623294718645936,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ],
        "main_score": 0.2623294718645936,
        "precision": 0.25071229192893485,
        "recall": 0.3039117352056169
      },
      {
        "accuracy": 0.42527582748244735,
        "f1": 0.3543520533636041,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ],
        "main_score": 0.3543520533636041,
        "precision": 0.3315301395170798,
        "recall": 0.42527582748244735
      },
      {
        "accuracy": 0.6068204613841525,
        "f1": 0.5534453992829119,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ],
        "main_score": 0.5534453992829119,
        "precision": 0.5352134742556007,
        "recall": 0.6068204613841525
      },
      {
        "accuracy": 0.5987963891675026,
        "f1": 0.5254740411711324,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ],
        "main_score": 0.5254740411711324,
        "precision": 0.4983832199228388,
        "recall": 0.5987963891675026
      },
      {
        "accuracy": 0.6138415245737212,
        "f1": 0.5642667101470286,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ],
        "main_score": 0.5642667101470286,
        "precision": 0.5482790369256372,
        "recall": 0.6138415245737212
      },
      {
        "accuracy": 0.6810431293881645,
        "f1": 0.6199541848990194,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ],
        "main_score": 0.6199541848990194,
        "precision": 0.5962482686153698,
        "recall": 0.6810431293881645
      },
      {
        "accuracy": 0.8816449348044132,
        "f1": 0.8504903599687953,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.8504903599687953,
        "precision": 0.8373131298657878,
        "recall": 0.8816449348044132
      },
      {
        "accuracy": 0.9107321965897693,
        "f1": 0.8855757749438792,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ],
        "main_score": 0.8855757749438792,
        "precision": 0.8741223671013039,
        "recall": 0.9107321965897693
      },
      {
        "accuracy": 0.8776328986960883,
        "f1": 0.8477097960548312,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ],
        "main_score": 0.8477097960548312,
        "precision": 0.8356394580567098,
        "recall": 0.8776328986960883
      },
      {
        "accuracy": 0.9007021063189569,
        "f1": 0.8745235707121363,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ],
        "main_score": 0.8745235707121363,
        "precision": 0.8630391173520562,
        "recall": 0.9007021063189569
      },
      {
        "accuracy": 0.08625877632898696,
        "f1": 0.07128300146979616,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ],
        "main_score": 0.07128300146979616,
        "precision": 0.06737981759881184,
        "recall": 0.08625877632898696
      },
      {
        "accuracy": 0.14042126379137412,
        "f1": 0.09862870771331446,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ],
        "main_score": 0.09862870771331446,
        "precision": 0.08862608660714726,
        "recall": 0.14042126379137412
      },
      {
        "accuracy": 0.3520561685055166,
        "f1": 0.30791373709245123,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ],
        "main_score": 0.30791373709245123,
        "precision": 0.29525341284737117,
        "recall": 0.3520561685055166
      },
      {
        "accuracy": 0.39117352056168503,
        "f1": 0.32712798911658647,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ],
        "main_score": 0.32712798911658647,
        "precision": 0.3064795948119833,
        "recall": 0.39117352056168503
      },
      {
        "accuracy": 0.8535606820461384,
        "f1": 0.8239384320126547,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ],
        "main_score": 0.8239384320126547,
        "precision": 0.8128193493790284,
        "recall": 0.8535606820461384
      },
      {
        "accuracy": 0.876629889669007,
        "f1": 0.8444555889891898,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ],
        "main_score": 0.8444555889891898,
        "precision": 0.8311685055165496,
        "recall": 0.876629889669007
      },
      {
        "accuracy": 0.5827482447342026,
        "f1": 0.5371510491069168,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ],
        "main_score": 0.5371510491069168,
        "precision": 0.5227870118291382,
        "recall": 0.5827482447342026
      },
      {
        "accuracy": 0.6218655967903711,
        "f1": 0.5671322993288892,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ],
        "main_score": 0.5671322993288892,
        "precision": 0.5461514886539962,
        "recall": 0.6218655967903711
      },
      {
        "accuracy": 0.8485456369107321,
        "f1": 0.8142688750010716,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ],
        "main_score": 0.8142688750010716,
        "precision": 0.8007478785563038,
        "recall": 0.8485456369107321
      },
      {
        "accuracy": 0.8595787362086259,
        "f1": 0.8221378420977218,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ],
        "main_score": 0.8221378420977218,
        "precision": 0.8061852223336676,
        "recall": 0.8595787362086259
      },
      {
        "accuracy": 0.8505516549648947,
        "f1": 0.8239260729579512,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ],
        "main_score": 0.8239260729579512,
        "precision": 0.8145932024066428,
        "recall": 0.8505516549648947
      },
      {
        "accuracy": 0.8706118355065195,
        "f1": 0.8395504695905899,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ],
        "main_score": 0.8395504695905899,
        "precision": 0.8261455361756264,
        "recall": 0.8706118355065195
      },
      {
        "accuracy": 0.8124373119358074,
        "f1": 0.7701652576777953,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ],
        "main_score": 0.7701652576777953,
        "precision": 0.7538958144274092,
        "recall": 0.8124373119358074
      },
      {
        "accuracy": 0.8495486459378134,
        "f1": 0.807073601757654,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ],
        "main_score": 0.807073601757654,
        "precision": 0.7882815112002676,
        "recall": 0.8495486459378134
      },
      {
        "accuracy": 0.38214643931795383,
        "f1": 0.33139221114619744,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ],
        "main_score": 0.33139221114619744,
        "precision": 0.31676438571909926,
        "recall": 0.38214643931795383
      },
      {
        "accuracy": 0.4262788365095286,
        "f1": 0.36236445664994477,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ],
        "main_score": 0.36236445664994477,
        "precision": 0.3407595753374469,
        "recall": 0.4262788365095286
      },
      {
        "accuracy": 0.8655967903711134,
        "f1": 0.835163181853252,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ],
        "main_score": 0.835163181853252,
        "precision": 0.8231233382687746,
        "recall": 0.8655967903711134
      },
      {
        "accuracy": 0.8736208625877633,
        "f1": 0.839468405215647,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ],
        "main_score": 0.839468405215647,
        "precision": 0.8250346276926016,
        "recall": 0.8736208625877633
      },
      {
        "accuracy": 0.8676028084252758,
        "f1": 0.8363870109607322,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ],
        "main_score": 0.8363870109607322,
        "precision": 0.8239898266227252,
        "recall": 0.8676028084252758
      },
      {
        "accuracy": 0.8846539618856569,
        "f1": 0.852342742513254,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ],
        "main_score": 0.852342742513254,
        "precision": 0.8382982280173854,
        "recall": 0.8846539618856569
      },
      {
        "accuracy": 0.01805416248746239,
        "f1": 0.015716113858817833,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ],
        "main_score": 0.015716113858817833,
        "precision": 0.0150462896168367,
        "recall": 0.01805416248746239
      },
      {
        "accuracy": 0.024072216649949848,
        "f1": 0.009127464121781437,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ],
        "main_score": 0.009127464121781437,
        "precision": 0.007297984270807509,
        "recall": 0.024072216649949848
      }
    ]
  },
  "task_name": "IndicGenBenchFloresBitextMining"
}