{
  "dataset_revision": "9abd46cf7fc8b4c64290f26993c540b92aa145ac",
  "evaluation_time": 40.4672532081604,
  "kg_co2_emissions": 0.0029210610361316996,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.9107421875,
        "f1": 0.9079025943053081,
        "f1_weighted": 0.9079303697245239,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.9107421875,
        "scores_per_experiment": [
          {
            "accuracy": 0.91259765625,
            "f1": 0.9095115273453148,
            "f1_weighted": 0.9095620292427166
          },
          {
            "accuracy": 0.919921875,
            "f1": 0.9171075075677465,
            "f1_weighted": 0.9171463074605388
          },
          {
            "accuracy": 0.9189453125,
            "f1": 0.9164326902619153,
            "f1_weighted": 0.9164548569339687
          },
          {
            "accuracy": 0.8955078125,
            "f1": 0.8917934723362191,
            "f1_weighted": 0.8918072223976807
          },
          {
            "accuracy": 0.91650390625,
            "f1": 0.9146191042606245,
            "f1_weighted": 0.9146685980149947
          },
          {
            "accuracy": 0.91650390625,
            "f1": 0.9143797518081023,
            "f1_weighted": 0.9143809034486805
          },
          {
            "accuracy": 0.91552734375,
            "f1": 0.9133360126784307,
            "f1_weighted": 0.9133674723335949
          },
          {
            "accuracy": 0.90478515625,
            "f1": 0.901706202943453,
            "f1_weighted": 0.9017676987824048
          },
          {
            "accuracy": 0.89794921875,
            "f1": 0.8935279365738298,
            "f1_weighted": 0.8935169814981955
          },
          {
            "accuracy": 0.9091796875,
            "f1": 0.9066117372774467,
            "f1_weighted": 0.9066316271324645
          }
        ]
      }
    ]
  },
  "task_name": "DBpediaClassification"
}