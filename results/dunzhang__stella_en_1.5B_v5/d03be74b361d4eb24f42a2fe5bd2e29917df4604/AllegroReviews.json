{
  "dataset_revision": "b89853e6de927b0e3bfa8ecc0e56fe4e02ceafc6",
  "task_name": "AllegroReviews",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.527634,
        "f1": 0.439296,
        "f1_weighted": 0.522068,
        "scores_per_experiment": [
          {
            "accuracy": 0.573559,
            "f1": 0.475658,
            "f1_weighted": 0.558949
          },
          {
            "accuracy": 0.550696,
            "f1": 0.444383,
            "f1_weighted": 0.532695
          },
          {
            "accuracy": 0.55169,
            "f1": 0.483812,
            "f1_weighted": 0.565594
          },
          {
            "accuracy": 0.552684,
            "f1": 0.456439,
            "f1_weighted": 0.545646
          },
          {
            "accuracy": 0.56163,
            "f1": 0.442359,
            "f1_weighted": 0.538516
          },
          {
            "accuracy": 0.554672,
            "f1": 0.446371,
            "f1_weighted": 0.544934
          },
          {
            "accuracy": 0.409543,
            "f1": 0.366901,
            "f1_weighted": 0.418425
          },
          {
            "accuracy": 0.477137,
            "f1": 0.424994,
            "f1_weighted": 0.495566
          },
          {
            "accuracy": 0.511928,
            "f1": 0.405096,
            "f1_weighted": 0.49354
          },
          {
            "accuracy": 0.532803,
            "f1": 0.446947,
            "f1_weighted": 0.526814
          }
        ],
        "main_score": 0.527634,
        "hf_subset": "default",
        "languages": [
          "pol-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 44.606359004974365,
  "kg_co2_emissions": 0.00372816503903164
}