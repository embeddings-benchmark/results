{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "task_name": "InappropriatenessClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.50791,
        "f1": 0.502381,
        "f1_weighted": 0.502381,
        "ap": 0.505044,
        "ap_weighted": 0.505044,
        "scores_per_experiment": [
          {
            "accuracy": 0.461426,
            "f1": 0.458933,
            "f1_weighted": 0.458933,
            "ap": 0.482435,
            "ap_weighted": 0.482435
          },
          {
            "accuracy": 0.541016,
            "f1": 0.539487,
            "f1_weighted": 0.539487,
            "ap": 0.522016,
            "ap_weighted": 0.522016
          },
          {
            "accuracy": 0.481934,
            "f1": 0.481637,
            "f1_weighted": 0.481637,
            "ap": 0.49131,
            "ap_weighted": 0.49131
          },
          {
            "accuracy": 0.460938,
            "f1": 0.446389,
            "f1_weighted": 0.446389,
            "ap": 0.481621,
            "ap_weighted": 0.481621
          },
          {
            "accuracy": 0.524414,
            "f1": 0.519164,
            "f1_weighted": 0.519164,
            "ap": 0.5127,
            "ap_weighted": 0.5127
          },
          {
            "accuracy": 0.519043,
            "f1": 0.503971,
            "f1_weighted": 0.503971,
            "ap": 0.510078,
            "ap_weighted": 0.510078
          },
          {
            "accuracy": 0.573242,
            "f1": 0.571296,
            "f1_weighted": 0.571296,
            "ap": 0.541348,
            "ap_weighted": 0.541348
          },
          {
            "accuracy": 0.495605,
            "f1": 0.481908,
            "f1_weighted": 0.481908,
            "ap": 0.497817,
            "ap_weighted": 0.497817
          },
          {
            "accuracy": 0.519531,
            "f1": 0.519245,
            "f1_weighted": 0.519245,
            "ap": 0.510129,
            "ap_weighted": 0.510129
          },
          {
            "accuracy": 0.501953,
            "f1": 0.501782,
            "f1_weighted": 0.501782,
            "ap": 0.500981,
            "ap_weighted": 0.500981
          }
        ],
        "main_score": 0.50791,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 8.184034824371338,
  "kg_co2_emissions": 0.00026144902537023404
}