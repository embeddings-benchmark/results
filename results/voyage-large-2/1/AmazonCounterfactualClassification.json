{
  "dataset_revision": "e8379541af4e31359cca9fbcf4b00f2671dba205",
  "task_name": "AmazonCounterfactualClassification",
  "mteb_version": "1.36.10",
  "scores": {
    "test": [
      {
        "accuracy": 0.677761,
        "f1": 0.610175,
        "f1_weighted": 0.70871,
        "ap": 0.293898,
        "ap_weighted": 0.293898,
        "scores_per_experiment": [
          {
            "accuracy": 0.61194,
            "f1": 0.543142,
            "f1_weighted": 0.651102,
            "ap": 0.237485,
            "ap_weighted": 0.237485
          },
          {
            "accuracy": 0.679104,
            "f1": 0.621697,
            "f1_weighted": 0.711438,
            "ap": 0.309108,
            "ap_weighted": 0.309108
          },
          {
            "accuracy": 0.623881,
            "f1": 0.571629,
            "f1_weighted": 0.662735,
            "ap": 0.270047,
            "ap_weighted": 0.270047
          },
          {
            "accuracy": 0.698507,
            "f1": 0.627815,
            "f1_weighted": 0.726591,
            "ap": 0.303509,
            "ap_weighted": 0.303509
          },
          {
            "accuracy": 0.725373,
            "f1": 0.659883,
            "f1_weighted": 0.750767,
            "ap": 0.339386,
            "ap_weighted": 0.339386
          },
          {
            "accuracy": 0.69403,
            "f1": 0.630949,
            "f1_weighted": 0.723862,
            "ap": 0.312668,
            "ap_weighted": 0.312668
          },
          {
            "accuracy": 0.737313,
            "f1": 0.662163,
            "f1_weighted": 0.759193,
            "ap": 0.333833,
            "ap_weighted": 0.333833
          },
          {
            "accuracy": 0.685075,
            "f1": 0.620148,
            "f1_weighted": 0.71578,
            "ap": 0.300975,
            "ap_weighted": 0.300975
          },
          {
            "accuracy": 0.701493,
            "f1": 0.633836,
            "f1_weighted": 0.729683,
            "ap": 0.311549,
            "ap_weighted": 0.311549
          },
          {
            "accuracy": 0.620896,
            "f1": 0.530491,
            "f1_weighted": 0.65595,
            "ap": 0.220415,
            "ap_weighted": 0.220415
          }
        ],
        "main_score": 0.677761,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 21.886011123657227,
  "kg_co2_emissions": null
}