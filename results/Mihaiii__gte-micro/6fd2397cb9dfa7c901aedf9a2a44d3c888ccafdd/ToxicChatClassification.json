{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.782904,
        "f1": 0.657896,
        "f1_weighted": 0.81242,
        "ap": 0.272629,
        "ap_weighted": 0.272629,
        "scores_per_experiment": [
          {
            "accuracy": 0.742268,
            "f1": 0.612267,
            "f1_weighted": 0.780458,
            "ap": 0.223509,
            "ap_weighted": 0.223509
          },
          {
            "accuracy": 0.78866,
            "f1": 0.661171,
            "f1_weighted": 0.816871,
            "ap": 0.272209,
            "ap_weighted": 0.272209
          },
          {
            "accuracy": 0.812715,
            "f1": 0.690196,
            "f1_weighted": 0.836147,
            "ap": 0.307693,
            "ap_weighted": 0.307693
          },
          {
            "accuracy": 0.813574,
            "f1": 0.688826,
            "f1_weighted": 0.836425,
            "ap": 0.303955,
            "ap_weighted": 0.303955
          },
          {
            "accuracy": 0.782646,
            "f1": 0.657841,
            "f1_weighted": 0.812649,
            "ap": 0.271264,
            "ap_weighted": 0.271264
          },
          {
            "accuracy": 0.77921,
            "f1": 0.657979,
            "f1_weighted": 0.810523,
            "ap": 0.274791,
            "ap_weighted": 0.274791
          },
          {
            "accuracy": 0.83677,
            "f1": 0.697371,
            "f1_weighted": 0.851239,
            "ap": 0.300658,
            "ap_weighted": 0.300658
          },
          {
            "accuracy": 0.724227,
            "f1": 0.622151,
            "f1_weighted": 0.769275,
            "ap": 0.258046,
            "ap_weighted": 0.258046
          },
          {
            "accuracy": 0.793814,
            "f1": 0.647449,
            "f1_weighted": 0.817623,
            "ap": 0.244129,
            "ap_weighted": 0.244129
          },
          {
            "accuracy": 0.755155,
            "f1": 0.64371,
            "f1_weighted": 0.792988,
            "ap": 0.270036,
            "ap_weighted": 0.270036
          }
        ],
        "main_score": 0.782904,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 6.676954507827759,
  "kg_co2_emissions": 0.0001916674032653097
}