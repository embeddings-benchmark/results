{
  "dataset_revision": "fb29d6137c54550329411023aee45a8ff29f0c62",
  "task_name": "GeoreviewClassification",
  "mteb_version": "2.3.4",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.537109,
            "f1": 0.501355,
            "f1_weighted": 0.501284,
            "precision": 0.509473,
            "precision_weighted": 0.509473,
            "recall": 0.537241,
            "recall_weighted": 0.537109,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.54541,
            "f1": 0.510002,
            "f1_weighted": 0.509914,
            "precision": 0.504513,
            "precision_weighted": 0.504484,
            "recall": 0.545547,
            "recall_weighted": 0.54541,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.549316,
            "f1": 0.516294,
            "f1_weighted": 0.516223,
            "precision": 0.520336,
            "precision_weighted": 0.520311,
            "recall": 0.549417,
            "recall_weighted": 0.549316,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.544922,
            "f1": 0.527315,
            "f1_weighted": 0.527252,
            "precision": 0.525847,
            "precision_weighted": 0.525797,
            "recall": 0.545003,
            "recall_weighted": 0.544922,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.558105,
            "f1": 0.534073,
            "f1_weighted": 0.534009,
            "precision": 0.534557,
            "precision_weighted": 0.53452,
            "recall": 0.558192,
            "recall_weighted": 0.558105,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.521484,
            "f1": 0.502701,
            "f1_weighted": 0.50267,
            "precision": 0.497954,
            "precision_weighted": 0.497936,
            "recall": 0.521516,
            "recall_weighted": 0.521484,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.557129,
            "f1": 0.536645,
            "f1_weighted": 0.536578,
            "precision": 0.538653,
            "precision_weighted": 0.538633,
            "recall": 0.557232,
            "recall_weighted": 0.557129,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.567871,
            "f1": 0.545748,
            "f1_weighted": 0.545684,
            "precision": 0.547654,
            "precision_weighted": 0.547632,
            "recall": 0.567965,
            "recall_weighted": 0.567871,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.519043,
            "f1": 0.501067,
            "f1_weighted": 0.501052,
            "precision": 0.503566,
            "precision_weighted": 0.503515,
            "recall": 0.519007,
            "recall_weighted": 0.519043,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.544434,
            "f1": 0.515853,
            "f1_weighted": 0.515803,
            "precision": 0.513561,
            "precision_weighted": 0.513555,
            "recall": 0.54452,
            "recall_weighted": 0.544434,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.544482,
        "f1": 0.519105,
        "f1_weighted": 0.519047,
        "precision": 0.519611,
        "precision_weighted": 0.519586,
        "recall": 0.544564,
        "recall_weighted": 0.544482,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.544482,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 49.4753315448761,
  "kg_co2_emissions": null
}