{
  "dataset_revision": "2fe05ee6b5832cda29f2ef7aaad7b7fe6a3609eb",
  "task_name": "HeadlineClassification",
  "mteb_version": "2.3.4",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.769043,
            "f1": 0.769527,
            "f1_weighted": 0.769523,
            "precision": 0.776869,
            "precision_weighted": 0.776873,
            "recall": 0.769049,
            "recall_weighted": 0.769043,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.792969,
            "f1": 0.791675,
            "f1_weighted": 0.791671,
            "precision": 0.793388,
            "precision_weighted": 0.793379,
            "recall": 0.792971,
            "recall_weighted": 0.792969,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.791992,
            "f1": 0.792718,
            "f1_weighted": 0.792704,
            "precision": 0.799299,
            "precision_weighted": 0.799277,
            "recall": 0.791996,
            "recall_weighted": 0.791992,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.814453,
            "f1": 0.814141,
            "f1_weighted": 0.814129,
            "precision": 0.814967,
            "precision_weighted": 0.814951,
            "recall": 0.814463,
            "recall_weighted": 0.814453,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.810059,
            "f1": 0.811016,
            "f1_weighted": 0.811002,
            "precision": 0.818178,
            "precision_weighted": 0.818196,
            "recall": 0.810106,
            "recall_weighted": 0.810059,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.800781,
            "f1": 0.800846,
            "f1_weighted": 0.800832,
            "precision": 0.808058,
            "precision_weighted": 0.807995,
            "recall": 0.800742,
            "recall_weighted": 0.800781,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.783691,
            "f1": 0.78295,
            "f1_weighted": 0.782946,
            "precision": 0.795266,
            "precision_weighted": 0.795205,
            "recall": 0.783644,
            "recall_weighted": 0.783691,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.801758,
            "f1": 0.801173,
            "f1_weighted": 0.801156,
            "precision": 0.806093,
            "precision_weighted": 0.80605,
            "recall": 0.801754,
            "recall_weighted": 0.801758,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.770996,
            "f1": 0.766988,
            "f1_weighted": 0.767011,
            "precision": 0.777094,
            "precision_weighted": 0.777045,
            "recall": 0.770904,
            "recall_weighted": 0.770996,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.819824,
            "f1": 0.819023,
            "f1_weighted": 0.819029,
            "precision": 0.820564,
            "precision_weighted": 0.820573,
            "recall": 0.819821,
            "recall_weighted": 0.819824,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.795557,
        "f1": 0.795006,
        "f1_weighted": 0.795,
        "precision": 0.800978,
        "precision_weighted": 0.800954,
        "recall": 0.795545,
        "recall_weighted": 0.795557,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.795557,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 39.85558819770813,
  "kg_co2_emissions": null
}