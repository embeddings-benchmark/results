{"name": "google/embeddinggemma-300m", "revision": "64614b0b8b64f0c6c1e52b07e4e9a4e8fe4d2da2", "release_date": "2025-09-04", "languages": ["arb-Arab", "ben-Beng", "eng-Latn", "spa-Latn", "deu-Latn", "pes-Arab", "fin-Latn", "fra-Latn", "hin-Deva", "ind-Latn", "jpn-Jpan", "kor-Hang", "rus-Cyrl", "swh-Latn", "tel-Telu", "tha-Thai", "yor-Latn", "zho-Hant", "zho-Hans"], "n_parameters": 307581696, "memory_usage_mb": 578.0, "max_tokens": 2048.0, "embed_dim": 768, "license": "gemma", "open_weights": true, "public_training_code": null, "public_training_data": null, "framework": ["Sentence Transformers", "PyTorch"], "reference": "https://ai.google.dev/gemma/docs/embeddinggemma/model_card", "similarity_fn_name": "cosine", "use_instructions": true, "training_datasets": {"NQHardNegatives": ["train"], "FEVERHardNegatives": ["train"], "HotpotQAHardNegatives": ["train"], "MIRACLRetrievalHardNegatives": ["train"]}, "adapted_from": null, "superseded_by": null, "is_cross_encoder": null, "modalities": ["text"], "loader": "gemma_embedding_loader"}