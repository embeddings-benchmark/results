{
  "dataset_revision": "17fb326752c26b58491de360f0a6a152c9bfe19d",
  "task_name": "VoxCelebSA",
  "mteb_version": "2.4.2",
  "scores": {
    "train": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.265217,
            "f1": 0.188437,
            "f1_weighted": 0.303309,
            "precision": 0.280534,
            "precision_weighted": 0.641875,
            "recall": 0.257435,
            "recall_weighted": 0.265217,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.457971,
            "f1": 0.212079,
            "f1_weighted": 0.514728,
            "precision": 0.264394,
            "precision_weighted": 0.605465,
            "recall": 0.249225,
            "recall_weighted": 0.457971,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.108696,
            "f1": 0.10083,
            "f1_weighted": 0.094345,
            "precision": 0.246375,
            "precision_weighted": 0.564551,
            "recall": 0.301037,
            "recall_weighted": 0.108696,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.243478,
            "f1": 0.175656,
            "f1_weighted": 0.321865,
            "precision": 0.277467,
            "precision_weighted": 0.650086,
            "recall": 0.307127,
            "recall_weighted": 0.243478,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.078374,
            "f1": 0.078751,
            "f1_weighted": 0.085272,
            "precision": 0.219404,
            "precision_weighted": 0.52465,
            "recall": 0.207127,
            "recall_weighted": 0.078374,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.230747,
        "f1": 0.151151,
        "f1_weighted": 0.263904,
        "precision": 0.257635,
        "precision_weighted": 0.597325,
        "recall": 0.26439,
        "recall_weighted": 0.230747,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.230747,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 223.75621795654297,
  "kg_co2_emissions": null
}