{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.752749,
        "f1": 0.636673,
        "f1_weighted": 0.789433,
        "ap": 0.261006,
        "ap_weighted": 0.261006,
        "scores_per_experiment": [
          {
            "accuracy": 0.680412,
            "f1": 0.579602,
            "f1_weighted": 0.733824,
            "ap": 0.21844,
            "ap_weighted": 0.21844
          },
          {
            "accuracy": 0.713918,
            "f1": 0.596979,
            "f1_weighted": 0.759611,
            "ap": 0.220052,
            "ap_weighted": 0.220052
          },
          {
            "accuracy": 0.794674,
            "f1": 0.683931,
            "f1_weighted": 0.824087,
            "ap": 0.313933,
            "ap_weighted": 0.313933
          },
          {
            "accuracy": 0.816151,
            "f1": 0.699094,
            "f1_weighted": 0.839692,
            "ap": 0.323449,
            "ap_weighted": 0.323449
          },
          {
            "accuracy": 0.707045,
            "f1": 0.589432,
            "f1_weighted": 0.754052,
            "ap": 0.212919,
            "ap_weighted": 0.212919
          },
          {
            "accuracy": 0.767182,
            "f1": 0.65234,
            "f1_weighted": 0.80203,
            "ap": 0.275392,
            "ap_weighted": 0.275392
          },
          {
            "accuracy": 0.84622,
            "f1": 0.698328,
            "f1_weighted": 0.856563,
            "ap": 0.29618,
            "ap_weighted": 0.29618
          },
          {
            "accuracy": 0.662371,
            "f1": 0.575319,
            "f1_weighted": 0.71936,
            "ap": 0.228246,
            "ap_weighted": 0.228246
          },
          {
            "accuracy": 0.786082,
            "f1": 0.650493,
            "f1_weighted": 0.813574,
            "ap": 0.254787,
            "ap_weighted": 0.254787
          },
          {
            "accuracy": 0.753436,
            "f1": 0.64121,
            "f1_weighted": 0.791535,
            "ap": 0.266658,
            "ap_weighted": 0.266658
          }
        ],
        "main_score": 0.752749,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 11.658154487609863,
  "kg_co2_emissions": null
}