{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "evaluation_time": 453.31141114234924,
  "kg_co2_emissions": 0.02072964250602314,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.037060546875,
        "f1": 0.02010193036582834,
        "f1_weighted": 0.026821008973189824,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ],
        "main_score": 0.037060546875,
        "scores_per_experiment": [
          {
            "accuracy": 0.0341796875,
            "f1": 0.0193410640663421,
            "f1_weighted": 0.024567981693673795
          },
          {
            "accuracy": 0.0380859375,
            "f1": 0.02255769621462297,
            "f1_weighted": 0.027133105440533007
          },
          {
            "accuracy": 0.03955078125,
            "f1": 0.022079192941503605,
            "f1_weighted": 0.03111866123608485
          },
          {
            "accuracy": 0.033203125,
            "f1": 0.019157204815455187,
            "f1_weighted": 0.024711808649751386
          },
          {
            "accuracy": 0.03662109375,
            "f1": 0.019324305690419114,
            "f1_weighted": 0.026035653779939053
          },
          {
            "accuracy": 0.0439453125,
            "f1": 0.02103461320337841,
            "f1_weighted": 0.03410098325793298
          },
          {
            "accuracy": 0.0380859375,
            "f1": 0.021919821903000125,
            "f1_weighted": 0.02851960647774119
          },
          {
            "accuracy": 0.03564453125,
            "f1": 0.018070267453833156,
            "f1_weighted": 0.023472319845062872
          },
          {
            "accuracy": 0.03955078125,
            "f1": 0.023571949206855673,
            "f1_weighted": 0.026635528321532376
          },
          {
            "accuracy": 0.03173828125,
            "f1": 0.013963188162873076,
            "f1_weighted": 0.02191444102964675
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.035595703125,
        "f1": 0.018080109896031583,
        "f1_weighted": 0.026285988499576418,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ],
        "main_score": 0.035595703125,
        "scores_per_experiment": [
          {
            "accuracy": 0.0322265625,
            "f1": 0.014574712068282219,
            "f1_weighted": 0.023418974593265036
          },
          {
            "accuracy": 0.03466796875,
            "f1": 0.017643094327580464,
            "f1_weighted": 0.024245465803820525
          },
          {
            "accuracy": 0.0341796875,
            "f1": 0.018376872671575364,
            "f1_weighted": 0.026950294474334294
          },
          {
            "accuracy": 0.03466796875,
            "f1": 0.02017966831674024,
            "f1_weighted": 0.025903926093826923
          },
          {
            "accuracy": 0.0341796875,
            "f1": 0.017947313180848082,
            "f1_weighted": 0.027637035615960773
          },
          {
            "accuracy": 0.037109375,
            "f1": 0.01780086118990075,
            "f1_weighted": 0.026942717534790862
          },
          {
            "accuracy": 0.0380859375,
            "f1": 0.01780039858454658,
            "f1_weighted": 0.027017034783734736
          },
          {
            "accuracy": 0.0380859375,
            "f1": 0.019355569612124278,
            "f1_weighted": 0.027000062515891775
          },
          {
            "accuracy": 0.03369140625,
            "f1": 0.015324738838447257,
            "f1_weighted": 0.023304870974276644
          },
          {
            "accuracy": 0.0390625,
            "f1": 0.021797870170270586,
            "f1_weighted": 0.030439502605862624
          }
        ]
      }
    ]
  },
  "task_name": "GreekLegalCodeClassification"
}