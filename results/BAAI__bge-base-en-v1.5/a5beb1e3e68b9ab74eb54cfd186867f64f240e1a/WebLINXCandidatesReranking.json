{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 4189.33407330513,
  "kg_co2_emissions": 0.25685744813747136,
  "mteb_version": "1.18.0",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.0658940832887462,
        "map": 0.08228549003590785,
        "mrr": 0.0658940832887462,
        "nAUC_map_diff1": 0.030841033759126148,
        "nAUC_map_max": 0.07036003207019446,
        "nAUC_map_std": 0.24379553313241406,
        "nAUC_mrr_diff1": 0.02320711264692053,
        "nAUC_mrr_max": 0.05630445918124481,
        "nAUC_mrr_std": 0.2091063965182897
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09534989409379158,
        "map": 0.11470747310334545,
        "mrr": 0.09534989409379158,
        "nAUC_map_diff1": 0.07083894155639192,
        "nAUC_map_max": -0.06035838072215442,
        "nAUC_map_std": -0.022775200287271247,
        "nAUC_mrr_diff1": 0.060415589744347,
        "nAUC_mrr_max": -0.06920569695811514,
        "nAUC_mrr_std": -0.030743517943368284
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09155048899485616,
        "map": 0.10843137988276355,
        "mrr": 0.09155048899485616,
        "nAUC_map_diff1": 0.14222734456575706,
        "nAUC_map_max": 0.1298430504707355,
        "nAUC_map_std": 0.15444741757292466,
        "nAUC_mrr_diff1": 0.1516965725235197,
        "nAUC_mrr_max": 0.13463227143730955,
        "nAUC_mrr_std": 0.1422495247148
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.0864158083014255,
        "map": 0.10479780894553696,
        "mrr": 0.0864158083014255,
        "nAUC_map_diff1": 0.07299194434802307,
        "nAUC_map_max": 0.050157595141410306,
        "nAUC_map_std": 0.17225621181591247,
        "nAUC_mrr_diff1": 0.07159777083815018,
        "nAUC_mrr_max": 0.05270028554168289,
        "nAUC_mrr_std": 0.14774766513533313
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07235563330506078,
        "map": 0.08937048490177442,
        "mrr": 0.07235563330506078,
        "nAUC_map_diff1": 0.102496811452597,
        "nAUC_map_max": 0.06509017562420565,
        "nAUC_map_std": 0.16083596259891603,
        "nAUC_mrr_diff1": 0.11233560686635816,
        "nAUC_mrr_max": 0.07474085657744828,
        "nAUC_mrr_std": 0.15242653217754984
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.11722850554518503,
        "map": 0.13451466086767017,
        "mrr": 0.11722850554518503,
        "nAUC_map_diff1": 0.13484446398582,
        "nAUC_map_max": -0.09093864590979012,
        "nAUC_map_std": -0.05296637324160656,
        "nAUC_mrr_diff1": 0.13490840703155976,
        "nAUC_mrr_max": -0.08991821076631264,
        "nAUC_mrr_std": -0.06182698241955963
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}