{
  "dataset_revision": "f8650438298df086750ff4973661bb58a201a5ee",
  "evaluation_time": 337.0858750343323,
  "kg_co2_emissions": 0.015261837669360704,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.02766798418972332,
        "f1": 0.021156396437556485,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.021156396437556485,
        "precision": 0.020381200294611587,
        "recall": 0.02766798418972332
      },
      {
        "accuracy": 0.07114624505928854,
        "f1": 0.035237134743253136,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ],
        "main_score": 0.035237134743253136,
        "precision": 0.027903512996797605,
        "recall": 0.07114624505928854
      },
      {
        "accuracy": 0.046442687747035576,
        "f1": 0.037921136834180316,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ],
        "main_score": 0.037921136834180316,
        "precision": 0.03639269028984576,
        "recall": 0.046442687747035576
      },
      {
        "accuracy": 0.09881422924901186,
        "f1": 0.05221893598352948,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ],
        "main_score": 0.05221893598352948,
        "precision": 0.0440250318660971,
        "recall": 0.09881422924901186
      },
      {
        "accuracy": 0.020750988142292492,
        "f1": 0.01664826677570306,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01664826677570306,
        "precision": 0.01565289051048638,
        "recall": 0.020750988142292492
      },
      {
        "accuracy": 0.07806324110671936,
        "f1": 0.04209944897786376,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ],
        "main_score": 0.04209944897786376,
        "precision": 0.03589006152743737,
        "recall": 0.07806324110671936
      },
      {
        "accuracy": 0.058300395256917,
        "f1": 0.049003072824989,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ],
        "main_score": 0.049003072824989,
        "precision": 0.04716621409324053,
        "recall": 0.058300395256917
      },
      {
        "accuracy": 0.116600790513834,
        "f1": 0.06271923962648217,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ],
        "main_score": 0.06271923962648217,
        "precision": 0.052645201036162576,
        "recall": 0.116600790513834
      },
      {
        "accuracy": 0.04841897233201581,
        "f1": 0.03750683999930427,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.03750683999930427,
        "precision": 0.0350510598540031,
        "recall": 0.04841897233201581
      },
      {
        "accuracy": 0.10573122529644269,
        "f1": 0.05592190850108225,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ],
        "main_score": 0.05592190850108225,
        "precision": 0.04615601754043819,
        "recall": 0.10573122529644269
      },
      {
        "accuracy": 0.017786561264822136,
        "f1": 0.014968858545933645,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.014968858545933645,
        "precision": 0.01440618790436248,
        "recall": 0.017786561264822136
      },
      {
        "accuracy": 0.07509881422924901,
        "f1": 0.04045190840726827,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ],
        "main_score": 0.04045190840726827,
        "precision": 0.033911580659544464,
        "recall": 0.07509881422924901
      },
      {
        "accuracy": 0.03162055335968379,
        "f1": 0.026519024516260113,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.026519024516260113,
        "precision": 0.025649820144149064,
        "recall": 0.03162055335968379
      },
      {
        "accuracy": 0.0800395256916996,
        "f1": 0.040423088752407584,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ],
        "main_score": 0.040423088752407584,
        "precision": 0.03371884329003099,
        "recall": 0.0800395256916996
      },
      {
        "accuracy": 0.09090909090909091,
        "f1": 0.07850106754658386,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.07850106754658386,
        "precision": 0.07509226015121373,
        "recall": 0.09090909090909091
      },
      {
        "accuracy": 0.1600790513833992,
        "f1": 0.0825700224443653,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ],
        "main_score": 0.0825700224443653,
        "precision": 0.06703783014306729,
        "recall": 0.1600790513833992
      },
      {
        "accuracy": 0.012845849802371542,
        "f1": 0.010459957466458236,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.010459957466458236,
        "precision": 0.010023646348053887,
        "recall": 0.012845849802371542
      },
      {
        "accuracy": 0.05731225296442688,
        "f1": 0.02957024140329558,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ],
        "main_score": 0.02957024140329558,
        "precision": 0.025273711850290106,
        "recall": 0.05731225296442688
      },
      {
        "accuracy": 0.029644268774703556,
        "f1": 0.023316238590638104,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ],
        "main_score": 0.023316238590638104,
        "precision": 0.0219300928540059,
        "recall": 0.029644268774703556
      },
      {
        "accuracy": 0.06027667984189723,
        "f1": 0.029598959032814662,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ],
        "main_score": 0.029598959032814662,
        "precision": 0.02450589639606072,
        "recall": 0.06027667984189723
      },
      {
        "accuracy": 0.022727272727272728,
        "f1": 0.01928695753460824,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01928695753460824,
        "precision": 0.018619158079283583,
        "recall": 0.022727272727272728
      },
      {
        "accuracy": 0.09486166007905138,
        "f1": 0.04989969307241208,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ],
        "main_score": 0.04989969307241208,
        "precision": 0.04221832130505417,
        "recall": 0.09486166007905138
      },
      {
        "accuracy": 0.042490118577075096,
        "f1": 0.03723928995510759,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ],
        "main_score": 0.03723928995510759,
        "precision": 0.036274485357010965,
        "recall": 0.042490118577075096
      },
      {
        "accuracy": 0.1007905138339921,
        "f1": 0.05570793166383541,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ],
        "main_score": 0.05570793166383541,
        "precision": 0.04881196716994823,
        "recall": 0.1007905138339921
      },
      {
        "accuracy": 0.05632411067193676,
        "f1": 0.04637720688922062,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ],
        "main_score": 0.04637720688922062,
        "precision": 0.04419002205947817,
        "recall": 0.05632411067193676
      },
      {
        "accuracy": 0.11956521739130435,
        "f1": 0.06143104516328247,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ],
        "main_score": 0.06143104516328247,
        "precision": 0.050431750348400625,
        "recall": 0.11956521739130435
      },
      {
        "accuracy": 0.05731225296442688,
        "f1": 0.04772382777696937,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ],
        "main_score": 0.04772382777696937,
        "precision": 0.04550360401809677,
        "recall": 0.05731225296442688
      },
      {
        "accuracy": 0.12055335968379446,
        "f1": 0.06380032370398614,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ],
        "main_score": 0.06380032370398614,
        "precision": 0.053358626030802415,
        "recall": 0.12055335968379446
      },
      {
        "accuracy": 0.08102766798418973,
        "f1": 0.07438574514526852,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ],
        "main_score": 0.07438574514526852,
        "precision": 0.07223944446025747,
        "recall": 0.08102766798418973
      },
      {
        "accuracy": 0.1541501976284585,
        "f1": 0.08396404621737709,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ],
        "main_score": 0.08396404621737709,
        "precision": 0.06983381647213617,
        "recall": 0.1541501976284585
      },
      {
        "accuracy": 0.04743083003952569,
        "f1": 0.040888609475929524,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ],
        "main_score": 0.040888609475929524,
        "precision": 0.03918060030453841,
        "recall": 0.04743083003952569
      },
      {
        "accuracy": 0.08498023715415019,
        "f1": 0.04861239725117336,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ],
        "main_score": 0.04861239725117336,
        "precision": 0.04231127428485598,
        "recall": 0.08498023715415019
      },
      {
        "accuracy": 0.018774703557312252,
        "f1": 0.01549655014390038,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01549655014390038,
        "precision": 0.015027627466460752,
        "recall": 0.018774703557312252
      },
      {
        "accuracy": 0.06126482213438735,
        "f1": 0.03345211922059737,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ],
        "main_score": 0.03345211922059737,
        "precision": 0.028880327592912747,
        "recall": 0.06126482213438735
      },
      {
        "accuracy": 0.007905138339920948,
        "f1": 0.004906778301290062,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ],
        "main_score": 0.004906778301290062,
        "precision": 0.004617918802786168,
        "recall": 0.007905138339920948
      },
      {
        "accuracy": 0.06818181818181818,
        "f1": 0.036930933659892036,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ],
        "main_score": 0.036930933659892036,
        "precision": 0.03044379926738433,
        "recall": 0.06818181818181818
      },
      {
        "accuracy": 0.008893280632411068,
        "f1": 0.006657798336109542,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ],
        "main_score": 0.006657798336109542,
        "precision": 0.006459234400740452,
        "recall": 0.008893280632411068
      },
      {
        "accuracy": 0.05237154150197629,
        "f1": 0.019879095796060644,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ],
        "main_score": 0.019879095796060644,
        "precision": 0.015286968899272464,
        "recall": 0.05237154150197629
      },
      {
        "accuracy": 0.023715415019762844,
        "f1": 0.0189948755772967,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ],
        "main_score": 0.0189948755772967,
        "precision": 0.01833016299619548,
        "recall": 0.023715415019762844
      },
      {
        "accuracy": 0.09288537549407115,
        "f1": 0.05016458175618026,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ],
        "main_score": 0.05016458175618026,
        "precision": 0.042069459122709735,
        "recall": 0.09288537549407115
      },
      {
        "accuracy": 0.022727272727272728,
        "f1": 0.01776125444603705,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01776125444603705,
        "precision": 0.016644696533380982,
        "recall": 0.022727272727272728
      },
      {
        "accuracy": 0.07312252964426877,
        "f1": 0.03505517894230283,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ],
        "main_score": 0.03505517894230283,
        "precision": 0.02895101289262759,
        "recall": 0.07312252964426877
      },
      {
        "accuracy": 0.03557312252964427,
        "f1": 0.030397798547570347,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ],
        "main_score": 0.030397798547570347,
        "precision": 0.02938742642088014,
        "recall": 0.03557312252964427
      },
      {
        "accuracy": 0.11758893280632411,
        "f1": 0.063167604552564,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ],
        "main_score": 0.063167604552564,
        "precision": 0.05382950701837235,
        "recall": 0.11758893280632411
      },
      {
        "accuracy": 0.011857707509881422,
        "f1": 0.008204782162990677,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ],
        "main_score": 0.008204782162990677,
        "precision": 0.007584457468689016,
        "recall": 0.011857707509881422
      },
      {
        "accuracy": 0.05731225296442688,
        "f1": 0.029517981906691516,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ],
        "main_score": 0.029517981906691516,
        "precision": 0.02515880297542916,
        "recall": 0.05731225296442688
      },
      {
        "accuracy": 0.02865612648221344,
        "f1": 0.024911249890767474,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ],
        "main_score": 0.024911249890767474,
        "precision": 0.024324325792021446,
        "recall": 0.02865612648221344
      },
      {
        "accuracy": 0.09683794466403162,
        "f1": 0.05812477405214309,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ],
        "main_score": 0.05812477405214309,
        "precision": 0.05119614455260004,
        "recall": 0.09683794466403162
      },
      {
        "accuracy": 0.024703557312252964,
        "f1": 0.019994490145760015,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ],
        "main_score": 0.019994490145760015,
        "precision": 0.018904270834895785,
        "recall": 0.024703557312252964
      },
      {
        "accuracy": 0.09782608695652174,
        "f1": 0.056195274831428985,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ],
        "main_score": 0.056195274831428985,
        "precision": 0.04958965378386196,
        "recall": 0.09782608695652174
      },
      {
        "accuracy": 0.039525691699604744,
        "f1": 0.028670133103315284,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ],
        "main_score": 0.028670133103315284,
        "precision": 0.026495560541037803,
        "recall": 0.039525691699604744
      },
      {
        "accuracy": 0.09486166007905138,
        "f1": 0.0534851540406414,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ],
        "main_score": 0.0534851540406414,
        "precision": 0.04465889542941865,
        "recall": 0.09486166007905138
      },
      {
        "accuracy": 0.011857707509881422,
        "f1": 0.008494604850352085,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ],
        "main_score": 0.008494604850352085,
        "precision": 0.008052853652543799,
        "recall": 0.011857707509881422
      },
      {
        "accuracy": 0.06422924901185771,
        "f1": 0.0336050392233868,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ],
        "main_score": 0.0336050392233868,
        "precision": 0.02831818606316476,
        "recall": 0.06422924901185771
      },
      {
        "accuracy": 0.018774703557312252,
        "f1": 0.015493191874094678,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ],
        "main_score": 0.015493191874094678,
        "precision": 0.015025933664340926,
        "recall": 0.018774703557312252
      },
      {
        "accuracy": 0.09189723320158102,
        "f1": 0.052258167138722576,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ],
        "main_score": 0.052258167138722576,
        "precision": 0.04562523140501758,
        "recall": 0.09189723320158102
      },
      {
        "accuracy": 0.011857707509881422,
        "f1": 0.009883458233433605,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ],
        "main_score": 0.009883458233433605,
        "precision": 0.009388370482050446,
        "recall": 0.011857707509881422
      },
      {
        "accuracy": 0.025691699604743084,
        "f1": 0.007700792134654703,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ],
        "main_score": 0.007700792134654703,
        "precision": 0.005841436155301732,
        "recall": 0.025691699604743084
      }
    ],
    "validation": [
      {
        "accuracy": 0.025075225677031094,
        "f1": 0.020916453062892377,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.020916453062892377,
        "precision": 0.020326571265101144,
        "recall": 0.025075225677031094
      },
      {
        "accuracy": 0.0641925777331996,
        "f1": 0.027516740096627468,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ],
        "main_score": 0.027516740096627468,
        "precision": 0.021324313637330682,
        "recall": 0.0641925777331996
      },
      {
        "accuracy": 0.048144433299899696,
        "f1": 0.03972202731226584,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ],
        "main_score": 0.03972202731226584,
        "precision": 0.03737466777920517,
        "recall": 0.048144433299899696
      },
      {
        "accuracy": 0.10230692076228685,
        "f1": 0.050298661323392696,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ],
        "main_score": 0.050298661323392696,
        "precision": 0.0420384013611648,
        "recall": 0.10230692076228685
      },
      {
        "accuracy": 0.006018054162487462,
        "f1": 0.0043484131608117,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.0043484131608117,
        "precision": 0.004013057502446239,
        "recall": 0.006018054162487462
      },
      {
        "accuracy": 0.07923771313941826,
        "f1": 0.04220801790000233,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ],
        "main_score": 0.04220801790000233,
        "precision": 0.03589425400097242,
        "recall": 0.07923771313941826
      },
      {
        "accuracy": 0.06519558676028084,
        "f1": 0.05322819238651867,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ],
        "main_score": 0.05322819238651867,
        "precision": 0.05005200231263638,
        "recall": 0.06519558676028084
      },
      {
        "accuracy": 0.11634904714142427,
        "f1": 0.061217365373621324,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ],
        "main_score": 0.061217365373621324,
        "precision": 0.050631751606267573,
        "recall": 0.11634904714142427
      },
      {
        "accuracy": 0.06519558676028084,
        "f1": 0.05201420843371204,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.05201420843371204,
        "precision": 0.049001063425621626,
        "recall": 0.06519558676028084
      },
      {
        "accuracy": 0.12537612838515547,
        "f1": 0.06800604119010951,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ],
        "main_score": 0.06800604119010951,
        "precision": 0.05699056505352294,
        "recall": 0.12537612838515547
      },
      {
        "accuracy": 0.012036108324974924,
        "f1": 0.009920730081211526,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.009920730081211526,
        "precision": 0.009655005599524901,
        "recall": 0.012036108324974924
      },
      {
        "accuracy": 0.07823470411233702,
        "f1": 0.040060020072849034,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ],
        "main_score": 0.040060020072849034,
        "precision": 0.03479479592158206,
        "recall": 0.07823470411233702
      },
      {
        "accuracy": 0.04312938816449348,
        "f1": 0.039749284483487086,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.039749284483487086,
        "precision": 0.0387765861687627,
        "recall": 0.04312938816449348
      },
      {
        "accuracy": 0.08525576730190572,
        "f1": 0.04709341956089538,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ],
        "main_score": 0.04709341956089538,
        "precision": 0.040520477331510686,
        "recall": 0.08525576730190572
      },
      {
        "accuracy": 0.09628886659979939,
        "f1": 0.08830159947630004,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.08830159947630004,
        "precision": 0.08622752293322829,
        "recall": 0.09628886659979939
      },
      {
        "accuracy": 0.14142427281845538,
        "f1": 0.07170149056927037,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ],
        "main_score": 0.07170149056927037,
        "precision": 0.0572224483996565,
        "recall": 0.14142427281845538
      },
      {
        "accuracy": 0.013039117352056168,
        "f1": 0.01106929437200821,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.01106929437200821,
        "precision": 0.011051491142991235,
        "recall": 0.013039117352056168
      },
      {
        "accuracy": 0.08124373119358075,
        "f1": 0.04466920050912836,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ],
        "main_score": 0.04466920050912836,
        "precision": 0.03881470452185528,
        "recall": 0.08124373119358075
      },
      {
        "accuracy": 0.037111334002006016,
        "f1": 0.032703666555221216,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ],
        "main_score": 0.032703666555221216,
        "precision": 0.03156254477718871,
        "recall": 0.037111334002006016
      },
      {
        "accuracy": 0.0732196589769308,
        "f1": 0.03727279656747074,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ],
        "main_score": 0.03727279656747074,
        "precision": 0.030727030235594293,
        "recall": 0.0732196589769308
      },
      {
        "accuracy": 0.011033099297893681,
        "f1": 0.009096003858570246,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ],
        "main_score": 0.009096003858570246,
        "precision": 0.008777357714168146,
        "recall": 0.011033099297893681
      },
      {
        "accuracy": 0.09227683049147442,
        "f1": 0.05344856285861166,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ],
        "main_score": 0.05344856285861166,
        "precision": 0.04783941631916997,
        "recall": 0.09227683049147442
      },
      {
        "accuracy": 0.020060180541624874,
        "f1": 0.017499058020622713,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ],
        "main_score": 0.017499058020622713,
        "precision": 0.01711121650094851,
        "recall": 0.020060180541624874
      },
      {
        "accuracy": 0.10330992978936811,
        "f1": 0.05463266794338708,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ],
        "main_score": 0.05463266794338708,
        "precision": 0.04576563251809693,
        "recall": 0.10330992978936811
      },
      {
        "accuracy": 0.06519558676028084,
        "f1": 0.054359905112161884,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ],
        "main_score": 0.054359905112161884,
        "precision": 0.05101185910673196,
        "recall": 0.06519558676028084
      },
      {
        "accuracy": 0.12738214643931794,
        "f1": 0.06396226718205919,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ],
        "main_score": 0.06396226718205919,
        "precision": 0.05188510277572837,
        "recall": 0.12738214643931794
      },
      {
        "accuracy": 0.06519558676028084,
        "f1": 0.057465031991242206,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ],
        "main_score": 0.057465031991242206,
        "precision": 0.05550659916256706,
        "recall": 0.06519558676028084
      },
      {
        "accuracy": 0.119358074222668,
        "f1": 0.06277622517898407,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ],
        "main_score": 0.06277622517898407,
        "precision": 0.05194001249894214,
        "recall": 0.119358074222668
      },
      {
        "accuracy": 0.11133400200601805,
        "f1": 0.10234269241290304,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ],
        "main_score": 0.10234269241290304,
        "precision": 0.09950327171992167,
        "recall": 0.11133400200601805
      },
      {
        "accuracy": 0.18756268806419257,
        "f1": 0.10952168415696048,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ],
        "main_score": 0.10952168415696048,
        "precision": 0.09261947208035473,
        "recall": 0.18756268806419257
      },
      {
        "accuracy": 0.031093279839518557,
        "f1": 0.028240836236251052,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ],
        "main_score": 0.028240836236251052,
        "precision": 0.027667470819571844,
        "recall": 0.031093279839518557
      },
      {
        "accuracy": 0.08224674022066199,
        "f1": 0.040608602339619576,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ],
        "main_score": 0.040608602339619576,
        "precision": 0.03412835923596924,
        "recall": 0.08224674022066199
      },
      {
        "accuracy": 0.003009027081243731,
        "f1": 0.0014062389187765318,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.0014062389187765318,
        "precision": 0.0012547754486918795,
        "recall": 0.003009027081243731
      },
      {
        "accuracy": 0.07021063189568706,
        "f1": 0.038762510107643915,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ],
        "main_score": 0.038762510107643915,
        "precision": 0.033307636442050856,
        "recall": 0.07021063189568706
      },
      {
        "accuracy": 0.010030090270812437,
        "f1": 0.008312705791178168,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ],
        "main_score": 0.008312705791178168,
        "precision": 0.00819227122958461,
        "recall": 0.010030090270812437
      },
      {
        "accuracy": 0.07622868605817452,
        "f1": 0.04613920478057739,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ],
        "main_score": 0.04613920478057739,
        "precision": 0.04082625780335695,
        "recall": 0.07622868605817452
      },
      {
        "accuracy": 0.022066198595787363,
        "f1": 0.017870368619675364,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ],
        "main_score": 0.017870368619675364,
        "precision": 0.017629853277184936,
        "recall": 0.022066198595787363
      },
      {
        "accuracy": 0.05416248746238716,
        "f1": 0.023370309266980707,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ],
        "main_score": 0.023370309266980707,
        "precision": 0.019725020400068693,
        "recall": 0.05416248746238716
      },
      {
        "accuracy": 0.010030090270812437,
        "f1": 0.007511787388204203,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ],
        "main_score": 0.007511787388204203,
        "precision": 0.007105687740954537,
        "recall": 0.010030090270812437
      },
      {
        "accuracy": 0.07622868605817452,
        "f1": 0.03873430521502566,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ],
        "main_score": 0.03873430521502566,
        "precision": 0.03293468051145015,
        "recall": 0.07622868605817452
      },
      {
        "accuracy": 0.007021063189568706,
        "f1": 0.005418281186009904,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ],
        "main_score": 0.005418281186009904,
        "precision": 0.005266814642711094,
        "recall": 0.007021063189568706
      },
      {
        "accuracy": 0.07221664994984955,
        "f1": 0.03873699315873108,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ],
        "main_score": 0.03873699315873108,
        "precision": 0.032683596388016385,
        "recall": 0.07221664994984955
      },
      {
        "accuracy": 0.017051153460381142,
        "f1": 0.01499947141684068,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ],
        "main_score": 0.01499947141684068,
        "precision": 0.014711849335090786,
        "recall": 0.017051153460381142
      },
      {
        "accuracy": 0.10932798395185557,
        "f1": 0.059309987847674536,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ],
        "main_score": 0.059309987847674536,
        "precision": 0.05046422614066949,
        "recall": 0.10932798395185557
      },
      {
        "accuracy": 0.0020060180541624875,
        "f1": 0.000403227847012278,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ],
        "main_score": 0.000403227847012278,
        "precision": 0.00025176539720170614,
        "recall": 0.0020060180541624875
      },
      {
        "accuracy": 0.06720160481444333,
        "f1": 0.03698256755049356,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ],
        "main_score": 0.03698256755049356,
        "precision": 0.030874695886117347,
        "recall": 0.06720160481444333
      },
      {
        "accuracy": 0.011033099297893681,
        "f1": 0.009095991300445803,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ],
        "main_score": 0.009095991300445803,
        "precision": 0.00877735142225862,
        "recall": 0.011033099297893681
      },
      {
        "accuracy": 0.08926780341023069,
        "f1": 0.04808596847996534,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ],
        "main_score": 0.04808596847996534,
        "precision": 0.041134946521826185,
        "recall": 0.08926780341023069
      },
      {
        "accuracy": 0.010030090270812437,
        "f1": 0.008692744901370779,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ],
        "main_score": 0.008692744901370779,
        "precision": 0.008191240387830156,
        "recall": 0.010030090270812437
      },
      {
        "accuracy": 0.08826479438314945,
        "f1": 0.05007939499339376,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ],
        "main_score": 0.05007939499339376,
        "precision": 0.043737802064734904,
        "recall": 0.08826479438314945
      },
      {
        "accuracy": 0.05215646940822467,
        "f1": 0.044695833532343056,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ],
        "main_score": 0.044695833532343056,
        "precision": 0.04312545479575983,
        "recall": 0.05215646940822467
      },
      {
        "accuracy": 0.10230692076228685,
        "f1": 0.05519701695462753,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ],
        "main_score": 0.05519701695462753,
        "precision": 0.046228950331602625,
        "recall": 0.10230692076228685
      },
      {
        "accuracy": 0.006018054162487462,
        "f1": 0.004300652904305,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ],
        "main_score": 0.004300652904305,
        "precision": 0.004180226714802921,
        "recall": 0.006018054162487462
      },
      {
        "accuracy": 0.0712136409227683,
        "f1": 0.04126610321309887,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ],
        "main_score": 0.04126610321309887,
        "precision": 0.03556595728785251,
        "recall": 0.0712136409227683
      },
      {
        "accuracy": 0.006018054162487462,
        "f1": 0.004080946165039586,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ],
        "main_score": 0.004080946165039586,
        "precision": 0.003762306286852402,
        "recall": 0.006018054162487462
      },
      {
        "accuracy": 0.0732196589769308,
        "f1": 0.03970896931449919,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ],
        "main_score": 0.03970896931449919,
        "precision": 0.03371296295150631,
        "recall": 0.0732196589769308
      },
      {
        "accuracy": 0.010030090270812437,
        "f1": 0.008027990220661986,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ],
        "main_score": 0.008027990220661986,
        "precision": 0.007524530538778763,
        "recall": 0.010030090270812437
      },
      {
        "accuracy": 0.01905717151454363,
        "f1": 0.003706830423160746,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ],
        "main_score": 0.003706830423160746,
        "precision": 0.0023281272193300634,
        "recall": 0.01905717151454363
      }
    ]
  },
  "task_name": "IndicGenBenchFloresBitextMining"
}