{
  "dataset_revision": "ba52e9d114a4a145d79b4293afab31304a999a4c",
  "evaluation_time": 139.79957938194275,
  "kg_co2_emissions": 0.006658669429679676,
  "mteb_version": "1.18.0",
  "scores": {
    "train": [
      {
        "accuracy": 0.738,
        "f1": 0.6835279220779221,
        "hf_subset": "ind-abs",
        "languages": [
          "ind-Latn",
          "abs-Latn"
        ],
        "main_score": 0.6835279220779221,
        "precision": 0.6619234839234839,
        "recall": 0.738
      },
      {
        "accuracy": 0.5675757575757576,
        "f1": 0.5034556020766575,
        "hf_subset": "ind-btk",
        "languages": [
          "ind-Latn",
          "bbc-Latn"
        ],
        "main_score": 0.5034556020766575,
        "precision": 0.4826594080117815,
        "recall": 0.5675757575757576
      },
      {
        "accuracy": 0.7086363636363636,
        "f1": 0.6541939477304896,
        "hf_subset": "ind-bew",
        "languages": [
          "ind-Latn",
          "bew-Latn"
        ],
        "main_score": 0.6541939477304896,
        "precision": 0.6349404157937834,
        "recall": 0.7086363636363636
      },
      {
        "accuracy": 0.617,
        "f1": 0.5667160173160173,
        "hf_subset": "ind-bhp",
        "languages": [
          "ind-Latn",
          "bhp-Latn"
        ],
        "main_score": 0.5667160173160173,
        "precision": 0.5495785049988374,
        "recall": 0.617
      },
      {
        "accuracy": 0.5712121212121212,
        "f1": 0.5096868362605229,
        "hf_subset": "ind-jav",
        "languages": [
          "ind-Latn",
          "jav-Latn"
        ],
        "main_score": 0.5096868362605229,
        "precision": 0.4895454494002834,
        "recall": 0.5712121212121212
      },
      {
        "accuracy": 0.5163636363636364,
        "f1": 0.4556444256771764,
        "hf_subset": "ind-mad",
        "languages": [
          "ind-Latn",
          "mad-Latn"
        ],
        "main_score": 0.4556444256771764,
        "precision": 0.43669994690627334,
        "recall": 0.5163636363636364
      },
      {
        "accuracy": 0.3793939393939394,
        "f1": 0.31896988836361184,
        "hf_subset": "ind-mak",
        "languages": [
          "ind-Latn",
          "mak-Latn"
        ],
        "main_score": 0.31896988836361184,
        "precision": 0.3019127269357028,
        "recall": 0.3793939393939394
      },
      {
        "accuracy": 0.7824242424242425,
        "f1": 0.7374764720261132,
        "hf_subset": "ind-min",
        "languages": [
          "ind-Latn",
          "min-Latn"
        ],
        "main_score": 0.7374764720261132,
        "precision": 0.7200948013021811,
        "recall": 0.7824242424242425
      },
      {
        "accuracy": 0.926,
        "f1": 0.9067190476190476,
        "hf_subset": "ind-mui",
        "languages": [
          "ind-Latn",
          "mui-Latn"
        ],
        "main_score": 0.9067190476190476,
        "precision": 0.898361111111111,
        "recall": 0.926
      },
      {
        "accuracy": 0.649,
        "f1": 0.5868259018759018,
        "hf_subset": "ind-rej",
        "languages": [
          "ind-Latn",
          "rej-Latn"
        ],
        "main_score": 0.5868259018759018,
        "precision": 0.564490884115884,
        "recall": 0.649
      },
      {
        "accuracy": 0.6257575757575757,
        "f1": 0.567196639961742,
        "hf_subset": "ind-sun",
        "languages": [
          "ind-Latn",
          "sun-Latn"
        ],
        "main_score": 0.567196639961742,
        "precision": 0.5470933765154941,
        "recall": 0.6257575757575757
      }
    ]
  },
  "task_name": "NusaTranslationBitextMining"
}