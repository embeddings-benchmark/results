{
  "dataset_revision": "1cd02f1579dab39fedc95de8cc15fd620557a9f2",
  "task_name": "IconclassClassification",
  "mteb_version": "2.1.14",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.445545,
            "f1": 0.438804,
            "f1_weighted": 0.43982,
            "precision": 0.446383,
            "precision_weighted": 0.447861,
            "recall": 0.445103,
            "recall_weighted": 0.445545,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.465347,
            "f1": 0.447904,
            "f1_weighted": 0.450417,
            "precision": 0.447451,
            "precision_weighted": 0.449253,
            "recall": 0.462011,
            "recall_weighted": 0.465347,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.440594,
            "f1": 0.425966,
            "f1_weighted": 0.427214,
            "precision": 0.423254,
            "precision_weighted": 0.424388,
            "recall": 0.439394,
            "recall_weighted": 0.440594,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.465347,
            "f1": 0.45287,
            "f1_weighted": 0.454997,
            "precision": 0.453263,
            "precision_weighted": 0.45518,
            "recall": 0.463109,
            "recall_weighted": 0.465347,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.460396,
            "f1": 0.449984,
            "f1_weighted": 0.451794,
            "precision": 0.451018,
            "precision_weighted": 0.4527,
            "recall": 0.458278,
            "recall_weighted": 0.460396,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.450495,
            "f1": 0.436936,
            "f1_weighted": 0.438437,
            "precision": 0.460528,
            "precision_weighted": 0.460941,
            "recall": 0.448397,
            "recall_weighted": 0.450495,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.475248,
            "f1": 0.468629,
            "f1_weighted": 0.47066,
            "precision": 0.467951,
            "precision_weighted": 0.46975,
            "recall": 0.472991,
            "recall_weighted": 0.475248,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.460396,
            "f1": 0.442968,
            "f1_weighted": 0.445597,
            "precision": 0.452877,
            "precision_weighted": 0.454754,
            "recall": 0.457181,
            "recall_weighted": 0.460396,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.445545,
            "f1": 0.438878,
            "f1_weighted": 0.440615,
            "precision": 0.447688,
            "precision_weighted": 0.449053,
            "recall": 0.443347,
            "recall_weighted": 0.445545,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.450495,
            "f1": 0.434554,
            "f1_weighted": 0.436741,
            "precision": 0.432242,
            "precision_weighted": 0.433653,
            "recall": 0.447519,
            "recall_weighted": 0.450495,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.455941,
        "f1": 0.443749,
        "f1_weighted": 0.445629,
        "precision": 0.448265,
        "precision_weighted": 0.449753,
        "recall": 0.453733,
        "recall_weighted": 0.455941,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.443749,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 17.71418261528015,
  "kg_co2_emissions": null
}