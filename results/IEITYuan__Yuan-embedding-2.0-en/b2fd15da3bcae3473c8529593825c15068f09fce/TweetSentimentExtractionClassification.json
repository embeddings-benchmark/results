{
  "dataset_revision": "d604517c81ca91fe16a244d1248fc021f9ecee7a",
  "task_name": "TweetSentimentExtractionClassification",
  "mteb_version": "1.38.9",
  "scores": {
    "test": [
      {
        "accuracy": 0.74867,
        "f1": 0.751915,
        "f1_weighted": 0.746546,
        "scores_per_experiment": [
          {
            "accuracy": 0.745614,
            "f1": 0.748204,
            "f1_weighted": 0.741855
          },
          {
            "accuracy": 0.747595,
            "f1": 0.75101,
            "f1_weighted": 0.744655
          },
          {
            "accuracy": 0.753254,
            "f1": 0.756642,
            "f1_weighted": 0.752276
          },
          {
            "accuracy": 0.745614,
            "f1": 0.749462,
            "f1_weighted": 0.743413
          },
          {
            "accuracy": 0.75099,
            "f1": 0.754449,
            "f1_weighted": 0.749796
          },
          {
            "accuracy": 0.752971,
            "f1": 0.756174,
            "f1_weighted": 0.750644
          },
          {
            "accuracy": 0.745614,
            "f1": 0.74826,
            "f1_weighted": 0.744399
          },
          {
            "accuracy": 0.754103,
            "f1": 0.757457,
            "f1_weighted": 0.752954
          },
          {
            "accuracy": 0.740238,
            "f1": 0.743256,
            "f1_weighted": 0.736858
          },
          {
            "accuracy": 0.750707,
            "f1": 0.754235,
            "f1_weighted": 0.748606
          }
        ],
        "main_score": 0.74867,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 5.381778240203857,
  "kg_co2_emissions": null
}