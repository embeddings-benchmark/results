{
  "dataset_revision": "edfaf9da55d3dd50d43143d90c1ac476895ae6de",
  "task_name": "ToxicConversationsClassification",
  "mteb_version": "1.38.9",
  "scores": {
    "test": [
      {
        "accuracy": 0.808838,
        "f1": 0.63418,
        "f1_weighted": 0.846334,
        "ap": 0.210774,
        "ap_weighted": 0.210774,
        "scores_per_experiment": [
          {
            "accuracy": 0.808594,
            "f1": 0.631381,
            "f1_weighted": 0.846532,
            "ap": 0.205928,
            "ap_weighted": 0.205928
          },
          {
            "accuracy": 0.802246,
            "f1": 0.631078,
            "f1_weighted": 0.842615,
            "ap": 0.211861,
            "ap_weighted": 0.211861
          },
          {
            "accuracy": 0.860352,
            "f1": 0.680804,
            "f1_weighted": 0.882327,
            "ap": 0.249129,
            "ap_weighted": 0.249129
          },
          {
            "accuracy": 0.847168,
            "f1": 0.662455,
            "f1_weighted": 0.87265,
            "ap": 0.226715,
            "ap_weighted": 0.226715
          },
          {
            "accuracy": 0.785645,
            "f1": 0.618759,
            "f1_weighted": 0.831092,
            "ap": 0.204588,
            "ap_weighted": 0.204588
          },
          {
            "accuracy": 0.713867,
            "f1": 0.56802,
            "f1_weighted": 0.779314,
            "ap": 0.176737,
            "ap_weighted": 0.176737
          },
          {
            "accuracy": 0.852051,
            "f1": 0.669003,
            "f1_weighted": 0.876209,
            "ap": 0.234422,
            "ap_weighted": 0.234422
          },
          {
            "accuracy": 0.762695,
            "f1": 0.600056,
            "f1_weighted": 0.81475,
            "ap": 0.190974,
            "ap_weighted": 0.190974
          },
          {
            "accuracy": 0.820312,
            "f1": 0.633585,
            "f1_weighted": 0.853775,
            "ap": 0.198849,
            "ap_weighted": 0.198849
          },
          {
            "accuracy": 0.835449,
            "f1": 0.646661,
            "f1_weighted": 0.864076,
            "ap": 0.208534,
            "ap_weighted": 0.208534
          }
        ],
        "main_score": 0.808838,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 7.736439228057861,
  "kg_co2_emissions": null
}