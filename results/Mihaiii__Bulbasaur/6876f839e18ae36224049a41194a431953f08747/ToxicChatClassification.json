{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.736082,
        "f1": 0.625932,
        "f1_weighted": 0.777132,
        "ap": 0.25514,
        "ap_weighted": 0.25514,
        "scores_per_experiment": [
          {
            "accuracy": 0.649485,
            "f1": 0.554262,
            "f1_weighted": 0.7086,
            "ap": 0.201322,
            "ap_weighted": 0.201322
          },
          {
            "accuracy": 0.7689,
            "f1": 0.646465,
            "f1_weighted": 0.802324,
            "ap": 0.261991,
            "ap_weighted": 0.261991
          },
          {
            "accuracy": 0.777491,
            "f1": 0.66675,
            "f1_weighted": 0.810664,
            "ap": 0.295117,
            "ap_weighted": 0.295117
          },
          {
            "accuracy": 0.780928,
            "f1": 0.663823,
            "f1_weighted": 0.812463,
            "ap": 0.285025,
            "ap_weighted": 0.285025
          },
          {
            "accuracy": 0.744845,
            "f1": 0.622312,
            "f1_weighted": 0.783472,
            "ap": 0.238534,
            "ap_weighted": 0.238534
          },
          {
            "accuracy": 0.733677,
            "f1": 0.622344,
            "f1_weighted": 0.775955,
            "ap": 0.248697,
            "ap_weighted": 0.248697
          },
          {
            "accuracy": 0.812715,
            "f1": 0.69347,
            "f1_weighted": 0.836695,
            "ap": 0.314752,
            "ap_weighted": 0.314752
          },
          {
            "accuracy": 0.650344,
            "f1": 0.566056,
            "f1_weighted": 0.709328,
            "ap": 0.22249,
            "ap_weighted": 0.22249
          },
          {
            "accuracy": 0.734536,
            "f1": 0.611527,
            "f1_weighted": 0.775289,
            "ap": 0.228205,
            "ap_weighted": 0.228205
          },
          {
            "accuracy": 0.707904,
            "f1": 0.612309,
            "f1_weighted": 0.756528,
            "ap": 0.255267,
            "ap_weighted": 0.255267
          }
        ],
        "main_score": 0.736082,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 7.081789255142212,
  "kg_co2_emissions": 0.00020337668772735422
}