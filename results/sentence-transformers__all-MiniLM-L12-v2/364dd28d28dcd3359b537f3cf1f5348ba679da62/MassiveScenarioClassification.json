{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "task_name": "MassiveScenarioClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.513035,
        "f1": 0.493909,
        "f1_weighted": 0.503789,
        "scores_per_experiment": [
          {
            "accuracy": 0.516478,
            "f1": 0.499216,
            "f1_weighted": 0.502576
          },
          {
            "accuracy": 0.523856,
            "f1": 0.500947,
            "f1_weighted": 0.517314
          },
          {
            "accuracy": 0.520413,
            "f1": 0.50443,
            "f1_weighted": 0.509179
          },
          {
            "accuracy": 0.473684,
            "f1": 0.46513,
            "f1_weighted": 0.467473
          },
          {
            "accuracy": 0.53271,
            "f1": 0.510563,
            "f1_weighted": 0.526309
          },
          {
            "accuracy": 0.492376,
            "f1": 0.47542,
            "f1_weighted": 0.482427
          },
          {
            "accuracy": 0.519429,
            "f1": 0.495878,
            "f1_weighted": 0.508436
          },
          {
            "accuracy": 0.517954,
            "f1": 0.506384,
            "f1_weighted": 0.515993
          },
          {
            "accuracy": 0.497787,
            "f1": 0.476318,
            "f1_weighted": 0.486318
          },
          {
            "accuracy": 0.535662,
            "f1": 0.504809,
            "f1_weighted": 0.52187
          }
        ],
        "main_score": 0.513035,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.520881,
        "f1": 0.494821,
        "f1_weighted": 0.513845,
        "scores_per_experiment": [
          {
            "accuracy": 0.511096,
            "f1": 0.487781,
            "f1_weighted": 0.502591
          },
          {
            "accuracy": 0.538332,
            "f1": 0.509203,
            "f1_weighted": 0.534576
          },
          {
            "accuracy": 0.539005,
            "f1": 0.516751,
            "f1_weighted": 0.529797
          },
          {
            "accuracy": 0.495629,
            "f1": 0.467918,
            "f1_weighted": 0.48754
          },
          {
            "accuracy": 0.544048,
            "f1": 0.513191,
            "f1_weighted": 0.537741
          },
          {
            "accuracy": 0.512777,
            "f1": 0.489578,
            "f1_weighted": 0.505544
          },
          {
            "accuracy": 0.536315,
            "f1": 0.502994,
            "f1_weighted": 0.534072
          },
          {
            "accuracy": 0.523874,
            "f1": 0.509471,
            "f1_weighted": 0.521969
          },
          {
            "accuracy": 0.48924,
            "f1": 0.467417,
            "f1_weighted": 0.47912
          },
          {
            "accuracy": 0.518494,
            "f1": 0.483907,
            "f1_weighted": 0.505498
          }
        ],
        "main_score": 0.520881,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 8.283345460891724,
  "kg_co2_emissions": null
}