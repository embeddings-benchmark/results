{
    "dataset_revision": "6b8164f3af61f3bb7728724229ba36213fb46c25",
    "mteb_dataset_name": "CQADupstackStats-VN",
    "mteb_version": "1.38.41",
    "task_name": "CQADupstackStats-VN",
    "scores": {
        "test": [
            {
                "evaluation_time": 24.82,
                "map_at_1": 0.07892,
                "map_at_10": 0.11948,
                "map_at_100": 0.12505,
                "map_at_1000": 0.12578,
                "map_at_20": 0.12291,
                "map_at_3": 0.1043,
                "map_at_5": 0.11242,
                "mrr_at_1": 0.09272,
                "mrr_at_10": 0.13704,
                "mrr_at_100": 0.14303,
                "mrr_at_1000": 0.14373,
                "mrr_at_20": 0.14103,
                "mrr_at_3": 0.12031,
                "mrr_at_5": 0.12875,
                "ndcg_at_1": 0.09272,
                "ndcg_at_10": 0.14797,
                "ndcg_at_100": 0.17697,
                "ndcg_at_1000": 0.19985,
                "ndcg_at_20": 0.16089,
                "ndcg_at_3": 0.1155,
                "ndcg_at_5": 0.13081,
                "precision_at_1": 0.09272,
                "precision_at_10": 0.0255,
                "precision_at_100": 0.0043,
                "precision_at_1000": 0.00069,
                "precision_at_20": 0.01589,
                "precision_at_3": 0.05188,
                "precision_at_5": 0.03907,
                "recall_at_1": 0.07892,
                "recall_at_10": 0.22208,
                "recall_at_100": 0.35478,
                "recall_at_1000": 0.52963,
                "recall_at_20": 0.27078,
                "recall_at_3": 0.13355,
                "recall_at_5": 0.17163,
                "hf_subset": "default",
                "main_score": 0.14797,
                "languages": [
                    "vie-Latn"
                ]
            }
        ]
    },
    "evaluation_time": null
}