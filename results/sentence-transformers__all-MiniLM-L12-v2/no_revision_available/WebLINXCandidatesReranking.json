{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 998.6593120098114,
  "kg_co2_emissions": 0.05008284934955373,
  "mteb_version": "1.12.41",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.0465829543427858,
        "map": 0.062261466584615045,
        "mrr": 0.0465829543427858,
        "nAUC_map_diff1": -0.01892859243433936,
        "nAUC_map_max": -0.12045351850904387,
        "nAUC_map_std": 0.08908688418352664,
        "nAUC_mrr_diff1": -0.03227416549577134,
        "nAUC_mrr_max": -0.12627883966008074,
        "nAUC_mrr_std": 0.08735876353795523
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.0589996060805662,
        "map": 0.07605540030538403,
        "mrr": 0.0589996060805662,
        "nAUC_map_diff1": 0.11635296498201607,
        "nAUC_map_max": -0.06253360980568388,
        "nAUC_map_std": 4.4695967978582046e-05,
        "nAUC_mrr_diff1": 0.12319311313074974,
        "nAUC_mrr_max": -0.06868616986582964,
        "nAUC_mrr_std": -0.004134793630692073
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.04637807139545665,
        "map": 0.062196898798318224,
        "mrr": 0.04637807139545665,
        "nAUC_map_diff1": 0.08240474118017718,
        "nAUC_map_max": 0.01624744306574406,
        "nAUC_map_std": 0.08343393259931882,
        "nAUC_mrr_diff1": 0.09821289959776126,
        "nAUC_mrr_max": 0.023341857771297554,
        "nAUC_mrr_std": 0.07711802238867696
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.04777941061916146,
        "map": 0.06423838156021841,
        "mrr": 0.04777941061916146,
        "nAUC_map_diff1": 0.015648478106555883,
        "nAUC_map_max": 0.016676340505853714,
        "nAUC_map_std": 0.07474919378227654,
        "nAUC_mrr_diff1": 0.011494554983654096,
        "nAUC_mrr_max": 0.021268528923402454,
        "nAUC_mrr_std": 0.0743103703634384
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.04094028938971686,
        "map": 0.05609140523915738,
        "mrr": 0.04094028938971686,
        "nAUC_map_diff1": -0.07783956928103114,
        "nAUC_map_max": -0.08531590487923917,
        "nAUC_map_std": -0.15664290520761573,
        "nAUC_mrr_diff1": -0.05996224047553799,
        "nAUC_mrr_max": -0.10576918330921342,
        "nAUC_mrr_std": -0.15052043960385336
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.05212839939972915,
        "map": 0.0707896399680347,
        "mrr": 0.05212839939972915,
        "nAUC_map_diff1": 0.0940483232103494,
        "nAUC_map_max": -0.021544595509571857,
        "nAUC_map_std": -0.023566474581529273,
        "nAUC_mrr_diff1": 0.11397232278443448,
        "nAUC_mrr_max": -0.008211919607166862,
        "nAUC_mrr_std": -0.04937834412427377
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}