{
  "dataset_revision": "8b6510b0b1fa4e4c4f879467980e9be563ec1cdf",
  "evaluation_time": 17.717024326324463,
  "kg_co2_emissions": 0.0005577039803747544,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.884367601971514,
          "accuracy_threshold": 0.6715601682662964,
          "ap": 0.8477319763778848,
          "f1": 0.7713502703503444,
          "f1_threshold": 0.6405660510063171,
          "precision": 0.7431140288283146,
          "recall": 0.8018170619032954
        },
        "dot": {
          "accuracy": 0.884367601971514,
          "accuracy_threshold": 0.6715601086616516,
          "ap": 0.8477318566465304,
          "f1": 0.7713502703503444,
          "f1_threshold": 0.6405660510063171,
          "precision": 0.7431140288283146,
          "recall": 0.8018170619032954
        },
        "euclidean": {
          "accuracy": 0.884367601971514,
          "accuracy_threshold": 0.810481071472168,
          "ap": 0.8477319997773203,
          "f1": 0.7713502703503444,
          "f1_threshold": 0.8478608131408691,
          "precision": 0.7431140288283146,
          "recall": 0.8018170619032954
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8477319997773203,
        "manhattan": {
          "accuracy": 0.8843287926417511,
          "accuracy_threshold": 12.546772003173828,
          "ap": 0.8471102255724698,
          "f1": 0.7708356453223837,
          "f1_threshold": 13.208250045776367,
          "precision": 0.7418298326806693,
          "recall": 0.8022020326455189
        },
        "max": {
          "accuracy": 0.884367601971514,
          "ap": 0.8477319997773203,
          "f1": 0.7713502703503444
        },
        "similarity": {
          "accuracy": 0.884367601971514,
          "accuracy_threshold": 0.6715602874755859,
          "ap": 0.8477319897729817,
          "f1": 0.7713502703503444,
          "f1_threshold": 0.6405661106109619,
          "precision": 0.7431140288283146,
          "recall": 0.8018170619032954
        }
      }
    ]
  },
  "task_name": "TwitterURLCorpus"
}