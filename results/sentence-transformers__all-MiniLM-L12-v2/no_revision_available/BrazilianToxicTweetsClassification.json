{
  "dataset_revision": "fb4f11a5bc68b99891852d20f1ec074be6289768",
  "evaluation_time": 3.269745349884033,
  "kg_co2_emissions": 9.309381685449481e-05,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.193310546875,
        "f1": 0.15708917375385717,
        "hf_subset": "default",
        "languages": [
          "por-Latn"
        ],
        "lrap": 0.8009908040364595,
        "main_score": 0.193310546875,
        "scores_per_experiment": [
          {
            "accuracy": 0.18359375,
            "f1": 0.14449986904669684,
            "lrap": 0.8037109375000011
          },
          {
            "accuracy": 0.14501953125,
            "f1": 0.145055242709551,
            "lrap": 0.7985026041666679
          },
          {
            "accuracy": 0.1923828125,
            "f1": 0.1579259101121662,
            "lrap": 0.8064778645833344
          },
          {
            "accuracy": 0.21728515625,
            "f1": 0.13293012631486487,
            "lrap": 0.8147108289930565
          },
          {
            "accuracy": 0.2265625,
            "f1": 0.15763378821088267,
            "lrap": 0.8030734592013904
          },
          {
            "accuracy": 0.17138671875,
            "f1": 0.1777087842455595,
            "lrap": 0.8038330078125013
          },
          {
            "accuracy": 0.1552734375,
            "f1": 0.15146540272141967,
            "lrap": 0.7742648654513902
          },
          {
            "accuracy": 0.1240234375,
            "f1": 0.16463118016445863,
            "lrap": 0.8024698893229171
          },
          {
            "accuracy": 0.2587890625,
            "f1": 0.16340649228714557,
            "lrap": 0.7950439453125013
          },
          {
            "accuracy": 0.2587890625,
            "f1": 0.17563494172582664,
            "lrap": 0.8078206380208344
          }
        ]
      }
    ]
  },
  "task_name": "BrazilianToxicTweetsClassification"
}