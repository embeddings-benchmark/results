{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 26.207862377166748,
  "kg_co2_emissions": 0.00071339962285978,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.9972871287128713,
          "accuracy_threshold": 0.742490291595459,
          "ap": 0.924455482012236,
          "f1": 0.8589083419155509,
          "f1_threshold": 0.7408154010772705,
          "precision": 0.8853503184713376,
          "recall": 0.834
        },
        "dot": {
          "accuracy": 0.9972871287128713,
          "accuracy_threshold": 0.7424901723861694,
          "ap": 0.9244554820122362,
          "f1": 0.8589083419155509,
          "f1_threshold": 0.7408151626586914,
          "precision": 0.8853503184713376,
          "recall": 0.834
        },
        "euclidean": {
          "accuracy": 0.9972871287128713,
          "accuracy_threshold": 0.7176481485366821,
          "ap": 0.9244554820122362,
          "f1": 0.8589083419155509,
          "f1_threshold": 0.7199785113334656,
          "precision": 0.8853503184713376,
          "recall": 0.834
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.9257860510428625,
        "manhattan": {
          "accuracy": 0.9973267326732673,
          "accuracy_threshold": 11.244457244873047,
          "ap": 0.9257860510428625,
          "f1": 0.8620170597089813,
          "f1_threshold": 11.408807754516602,
          "precision": 0.865055387713998,
          "recall": 0.859
        },
        "max": {
          "accuracy": 0.9973267326732673,
          "ap": 0.9257860510428625,
          "f1": 0.8620170597089813
        },
        "similarity": {
          "accuracy": 0.9972871287128713,
          "accuracy_threshold": 0.7424903512001038,
          "ap": 0.9244554820122362,
          "f1": 0.8589083419155509,
          "f1_threshold": 0.7408154606819153,
          "precision": 0.8853503184713376,
          "recall": 0.834
        }
      }
    ],
    "validation": [
      {
        "cosine": {
          "accuracy": 0.9974653465346535,
          "accuracy_threshold": 0.7217644453048706,
          "ap": 0.9341102623930363,
          "f1": 0.8707893413775767,
          "f1_threshold": 0.7175433039665222,
          "precision": 0.8756319514661274,
          "recall": 0.866
        },
        "dot": {
          "accuracy": 0.9974653465346535,
          "accuracy_threshold": 0.7217644453048706,
          "ap": 0.9341101997102945,
          "f1": 0.8707893413775767,
          "f1_threshold": 0.7175433039665222,
          "precision": 0.8756319514661274,
          "recall": 0.866
        },
        "euclidean": {
          "accuracy": 0.9974653465346535,
          "accuracy_threshold": 0.7459697723388672,
          "ap": 0.9341101990332019,
          "f1": 0.8707893413775767,
          "f1_threshold": 0.7516071796417236,
          "precision": 0.8756319514661274,
          "recall": 0.866
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.9354870311915034,
        "manhattan": {
          "accuracy": 0.9975247524752475,
          "accuracy_threshold": 11.510499000549316,
          "ap": 0.9354870311915034,
          "f1": 0.8723186925434115,
          "f1_threshold": 11.510499000549316,
          "precision": 0.8914405010438413,
          "recall": 0.854
        },
        "max": {
          "accuracy": 0.9975247524752475,
          "ap": 0.9354870311915034,
          "f1": 0.8723186925434115
        },
        "similarity": {
          "accuracy": 0.9974653465346535,
          "accuracy_threshold": 0.7217645645141602,
          "ap": 0.9341102630701288,
          "f1": 0.8707893413775767,
          "f1_threshold": 0.7175434231758118,
          "precision": 0.8756319514661274,
          "recall": 0.866
        }
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}