{
  "dataset_revision": "061ca1525717eebaaa9bada240f6cbb31eb3aa87",
  "task_name": "TswanaNewsClassification",
  "mteb_version": "2.3.11",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.540041,
            "f1": 0.548063,
            "f1_weighted": 0.534191,
            "precision": 0.555024,
            "precision_weighted": 0.545786,
            "recall": 0.556618,
            "recall_weighted": 0.540041,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.542094,
            "f1": 0.560368,
            "f1_weighted": 0.536526,
            "precision": 0.553395,
            "precision_weighted": 0.545284,
            "recall": 0.581412,
            "recall_weighted": 0.542094,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.527721,
            "f1": 0.529367,
            "f1_weighted": 0.515742,
            "precision": 0.519397,
            "precision_weighted": 0.551009,
            "recall": 0.57509,
            "recall_weighted": 0.527721,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.507187,
            "f1": 0.517292,
            "f1_weighted": 0.48199,
            "precision": 0.520823,
            "precision_weighted": 0.515814,
            "recall": 0.555872,
            "recall_weighted": 0.507187,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.525667,
            "f1": 0.544203,
            "f1_weighted": 0.513035,
            "precision": 0.531833,
            "precision_weighted": 0.526267,
            "recall": 0.578007,
            "recall_weighted": 0.525667,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.544148,
            "f1": 0.56242,
            "f1_weighted": 0.534107,
            "precision": 0.553886,
            "precision_weighted": 0.548599,
            "recall": 0.596331,
            "recall_weighted": 0.544148,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.49076,
            "f1": 0.496919,
            "f1_weighted": 0.474048,
            "precision": 0.480552,
            "precision_weighted": 0.483096,
            "recall": 0.533925,
            "recall_weighted": 0.49076,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.548255,
            "f1": 0.561434,
            "f1_weighted": 0.530456,
            "precision": 0.556971,
            "precision_weighted": 0.561217,
            "recall": 0.599556,
            "recall_weighted": 0.548255,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.529774,
            "f1": 0.542355,
            "f1_weighted": 0.521514,
            "precision": 0.525633,
            "precision_weighted": 0.526511,
            "recall": 0.57133,
            "recall_weighted": 0.529774,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.50924,
            "f1": 0.509993,
            "f1_weighted": 0.491484,
            "precision": 0.507909,
            "precision_weighted": 0.533465,
            "recall": 0.557237,
            "recall_weighted": 0.50924,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.526489,
        "f1": 0.537241,
        "f1_weighted": 0.513309,
        "precision": 0.530542,
        "precision_weighted": 0.533705,
        "recall": 0.570538,
        "recall_weighted": 0.526489,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.526489,
        "hf_subset": "default",
        "languages": [
          "tsn-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 9.96636414527893,
  "kg_co2_emissions": null
}