{
  "dataset_revision": "9157397f05a127b3ac93b93dd88abf1bdf710c22",
  "task_name": "EstonianValenceClassification",
  "mteb_version": "2.3.11",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.49511,
            "f1": 0.428121,
            "f1_weighted": 0.507544,
            "precision": 0.436679,
            "precision_weighted": 0.529548,
            "recall": 0.432636,
            "recall_weighted": 0.49511,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.506112,
            "f1": 0.425521,
            "f1_weighted": 0.512344,
            "precision": 0.427737,
            "precision_weighted": 0.520907,
            "recall": 0.42648,
            "recall_weighted": 0.506112,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.508557,
            "f1": 0.428436,
            "f1_weighted": 0.5152,
            "precision": 0.43714,
            "precision_weighted": 0.526829,
            "recall": 0.426921,
            "recall_weighted": 0.508557,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.48044,
            "f1": 0.414332,
            "f1_weighted": 0.49474,
            "precision": 0.422512,
            "precision_weighted": 0.519815,
            "recall": 0.419433,
            "recall_weighted": 0.48044,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.515892,
            "f1": 0.443147,
            "f1_weighted": 0.52754,
            "precision": 0.448318,
            "precision_weighted": 0.54714,
            "recall": 0.448885,
            "recall_weighted": 0.515892,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.525672,
            "f1": 0.46352,
            "f1_weighted": 0.538354,
            "precision": 0.474462,
            "precision_weighted": 0.561731,
            "recall": 0.468438,
            "recall_weighted": 0.525672,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.523227,
            "f1": 0.441127,
            "f1_weighted": 0.527457,
            "precision": 0.449226,
            "precision_weighted": 0.537808,
            "recall": 0.440704,
            "recall_weighted": 0.523227,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.51467,
            "f1": 0.438338,
            "f1_weighted": 0.521098,
            "precision": 0.441414,
            "precision_weighted": 0.53079,
            "recall": 0.440125,
            "recall_weighted": 0.51467,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.518337,
            "f1": 0.444872,
            "f1_weighted": 0.527191,
            "precision": 0.44916,
            "precision_weighted": 0.542758,
            "recall": 0.449339,
            "recall_weighted": 0.518337,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.511002,
            "f1": 0.428784,
            "f1_weighted": 0.515783,
            "precision": 0.447717,
            "precision_weighted": 0.5348,
            "recall": 0.429842,
            "recall_weighted": 0.511002,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.509902,
        "f1": 0.43562,
        "f1_weighted": 0.518725,
        "precision": 0.443437,
        "precision_weighted": 0.535212,
        "recall": 0.43828,
        "recall_weighted": 0.509902,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.509902,
        "hf_subset": "default",
        "languages": [
          "est-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 41.82382345199585,
  "kg_co2_emissions": null
}