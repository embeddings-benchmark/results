{
  "dataset_revision": "1a5f2fa2914bfeff4fcdc6fff4194fa8ec8fa19e",
  "task_name": "GujaratiNewsClassification",
  "mteb_version": "2.3.11",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.91047,
            "f1": 0.890531,
            "f1_weighted": 0.911104,
            "precision": 0.886327,
            "precision_weighted": 0.912681,
            "recall": 0.896094,
            "recall_weighted": 0.91047,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.908953,
            "f1": 0.889634,
            "f1_weighted": 0.909477,
            "precision": 0.885505,
            "precision_weighted": 0.911734,
            "recall": 0.896013,
            "recall_weighted": 0.908953,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.908953,
            "f1": 0.88706,
            "f1_weighted": 0.909004,
            "precision": 0.885558,
            "precision_weighted": 0.909324,
            "recall": 0.888869,
            "recall_weighted": 0.908953,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.914264,
            "f1": 0.893682,
            "f1_weighted": 0.914449,
            "precision": 0.892657,
            "precision_weighted": 0.91466,
            "recall": 0.894746,
            "recall_weighted": 0.914264,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.913505,
            "f1": 0.893929,
            "f1_weighted": 0.913809,
            "precision": 0.891825,
            "precision_weighted": 0.914259,
            "recall": 0.896254,
            "recall_weighted": 0.913505,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.91047,
            "f1": 0.889442,
            "f1_weighted": 0.910846,
            "precision": 0.886765,
            "precision_weighted": 0.911587,
            "recall": 0.892629,
            "recall_weighted": 0.91047,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.903642,
            "f1": 0.882389,
            "f1_weighted": 0.90433,
            "precision": 0.87925,
            "precision_weighted": 0.905314,
            "recall": 0.885999,
            "recall_weighted": 0.903642,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.913505,
            "f1": 0.892673,
            "f1_weighted": 0.913696,
            "precision": 0.891453,
            "precision_weighted": 0.913929,
            "recall": 0.893959,
            "recall_weighted": 0.913505,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.908194,
            "f1": 0.885916,
            "f1_weighted": 0.908418,
            "precision": 0.884215,
            "precision_weighted": 0.908792,
            "recall": 0.887821,
            "recall_weighted": 0.908194,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.908953,
            "f1": 0.887447,
            "f1_weighted": 0.909494,
            "precision": 0.883978,
            "precision_weighted": 0.910701,
            "recall": 0.891856,
            "recall_weighted": 0.908953,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.910091,
        "f1": 0.88927,
        "f1_weighted": 0.910463,
        "precision": 0.886753,
        "precision_weighted": 0.911298,
        "recall": 0.892424,
        "recall_weighted": 0.910091,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.910091,
        "hf_subset": "default",
        "languages": [
          "guj-Gujr"
        ]
      }
    ]
  },
  "evaluation_time": 42.55501437187195,
  "kg_co2_emissions": null
}