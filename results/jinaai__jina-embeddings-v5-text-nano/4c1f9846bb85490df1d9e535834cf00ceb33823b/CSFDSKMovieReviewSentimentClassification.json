{
  "dataset_revision": "23a20c659d868740ef9c54854de631fe19cd5c17",
  "task_name": "CSFDSKMovieReviewSentimentClassification",
  "mteb_version": "2.3.11",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.413574,
            "f1": 0.389239,
            "f1_weighted": 0.392168,
            "precision": 0.393802,
            "precision_weighted": 0.395208,
            "recall": 0.409213,
            "recall_weighted": 0.413574,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.385742,
            "f1": 0.345075,
            "f1_weighted": 0.348817,
            "precision": 0.356535,
            "precision_weighted": 0.358079,
            "recall": 0.379124,
            "recall_weighted": 0.385742,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.421387,
            "f1": 0.392979,
            "f1_weighted": 0.39619,
            "precision": 0.400043,
            "precision_weighted": 0.401495,
            "recall": 0.416221,
            "recall_weighted": 0.421387,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.403809,
            "f1": 0.396222,
            "f1_weighted": 0.397608,
            "precision": 0.402636,
            "precision_weighted": 0.403983,
            "recall": 0.402439,
            "recall_weighted": 0.403809,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.41748,
            "f1": 0.383619,
            "f1_weighted": 0.386575,
            "precision": 0.39918,
            "precision_weighted": 0.400303,
            "recall": 0.412562,
            "recall_weighted": 0.41748,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.40918,
            "f1": 0.376825,
            "f1_weighted": 0.380135,
            "precision": 0.401154,
            "precision_weighted": 0.402534,
            "recall": 0.403906,
            "recall_weighted": 0.40918,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.394531,
            "f1": 0.360579,
            "f1_weighted": 0.364443,
            "precision": 0.374021,
            "precision_weighted": 0.375814,
            "recall": 0.388955,
            "recall_weighted": 0.394531,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.403809,
            "f1": 0.387163,
            "f1_weighted": 0.390544,
            "precision": 0.402119,
            "precision_weighted": 0.40391,
            "recall": 0.398601,
            "recall_weighted": 0.403809,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.412109,
            "f1": 0.388932,
            "f1_weighted": 0.392143,
            "precision": 0.394656,
            "precision_weighted": 0.395888,
            "recall": 0.406246,
            "recall_weighted": 0.412109,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.394531,
            "f1": 0.350113,
            "f1_weighted": 0.35303,
            "precision": 0.371738,
            "precision_weighted": 0.37313,
            "recall": 0.389984,
            "recall_weighted": 0.394531,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.405615,
        "f1": 0.377075,
        "f1_weighted": 0.380165,
        "precision": 0.389589,
        "precision_weighted": 0.391034,
        "recall": 0.400725,
        "recall_weighted": 0.405615,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.405615,
        "hf_subset": "default",
        "languages": [
          "slk-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 19.22912359237671,
  "kg_co2_emissions": null
}