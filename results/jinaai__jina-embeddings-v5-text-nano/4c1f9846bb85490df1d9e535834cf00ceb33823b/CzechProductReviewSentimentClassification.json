{
  "dataset_revision": "2e6fedf42c9c104e83dfd95c3a453721e683e244",
  "task_name": "CzechProductReviewSentimentClassification",
  "mteb_version": "2.3.11",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.640137,
            "f1": 0.602039,
            "f1_weighted": 0.601975,
            "precision": 0.636708,
            "precision_weighted": 0.636703,
            "recall": 0.640237,
            "recall_weighted": 0.640137,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.63916,
            "f1": 0.603706,
            "f1_weighted": 0.603642,
            "precision": 0.630413,
            "precision_weighted": 0.630406,
            "recall": 0.639265,
            "recall_weighted": 0.63916,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.606445,
            "f1": 0.568447,
            "f1_weighted": 0.568364,
            "precision": 0.579471,
            "precision_weighted": 0.579434,
            "recall": 0.606563,
            "recall_weighted": 0.606445,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.60498,
            "f1": 0.54264,
            "f1_weighted": 0.542562,
            "precision": 0.579625,
            "precision_weighted": 0.579611,
            "recall": 0.605094,
            "recall_weighted": 0.60498,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.632812,
            "f1": 0.60772,
            "f1_weighted": 0.607658,
            "precision": 0.625305,
            "precision_weighted": 0.625257,
            "recall": 0.632869,
            "recall_weighted": 0.632812,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.623535,
            "f1": 0.575631,
            "f1_weighted": 0.575555,
            "precision": 0.608202,
            "precision_weighted": 0.608195,
            "recall": 0.623662,
            "recall_weighted": 0.623535,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.650391,
            "f1": 0.626785,
            "f1_weighted": 0.626728,
            "precision": 0.638293,
            "precision_weighted": 0.638272,
            "recall": 0.650474,
            "recall_weighted": 0.650391,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.63916,
            "f1": 0.60006,
            "f1_weighted": 0.599993,
            "precision": 0.629325,
            "precision_weighted": 0.62931,
            "recall": 0.639257,
            "recall_weighted": 0.63916,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.618164,
            "f1": 0.572529,
            "f1_weighted": 0.572456,
            "precision": 0.601303,
            "precision_weighted": 0.601298,
            "recall": 0.61829,
            "recall_weighted": 0.618164,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.617676,
            "f1": 0.578845,
            "f1_weighted": 0.578772,
            "precision": 0.598972,
            "precision_weighted": 0.598962,
            "recall": 0.617806,
            "recall_weighted": 0.617676,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.627246,
        "f1": 0.58784,
        "f1_weighted": 0.58777,
        "precision": 0.612762,
        "precision_weighted": 0.612745,
        "recall": 0.627352,
        "recall_weighted": 0.627246,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.627246,
        "hf_subset": "default",
        "languages": [
          "ces-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 49.993306159973145,
  "kg_co2_emissions": null
}