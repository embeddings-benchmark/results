{
  "dataset_revision": "fafcfc4fee815f7017848e54b26c47ece8ff1626",
  "task_name": "ItaCaseholdClassification",
  "mteb_version": "2.3.11",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.873303,
            "f1": 0.601785,
            "f1_weighted": 0.840198,
            "precision": 0.577888,
            "precision_weighted": 0.821684,
            "recall": 0.645916,
            "recall_weighted": 0.873303,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.877828,
            "f1": 0.603001,
            "f1_weighted": 0.84275,
            "precision": 0.577687,
            "precision_weighted": 0.820933,
            "recall": 0.648148,
            "recall_weighted": 0.877828,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.877828,
            "f1": 0.609793,
            "f1_weighted": 0.841541,
            "precision": 0.587215,
            "precision_weighted": 0.817455,
            "recall": 0.649172,
            "recall_weighted": 0.877828,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.873303,
            "f1": 0.60713,
            "f1_weighted": 0.836478,
            "precision": 0.582917,
            "precision_weighted": 0.812327,
            "recall": 0.648772,
            "recall_weighted": 0.873303,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.873303,
            "f1": 0.605822,
            "f1_weighted": 0.838738,
            "precision": 0.582993,
            "precision_weighted": 0.817595,
            "recall": 0.647748,
            "recall_weighted": 0.873303,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.873303,
            "f1": 0.606366,
            "f1_weighted": 0.83844,
            "precision": 0.582904,
            "precision_weighted": 0.817738,
            "recall": 0.64891,
            "recall_weighted": 0.873303,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.877828,
            "f1": 0.605851,
            "f1_weighted": 0.843657,
            "precision": 0.582253,
            "precision_weighted": 0.822882,
            "recall": 0.64931,
            "recall_weighted": 0.877828,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.868778,
            "f1": 0.601589,
            "f1_weighted": 0.835563,
            "precision": 0.576531,
            "precision_weighted": 0.817526,
            "recall": 0.647209,
            "recall_weighted": 0.868778,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.873303,
            "f1": 0.607771,
            "f1_weighted": 0.83829,
            "precision": 0.584428,
            "precision_weighted": 0.816221,
            "recall": 0.64891,
            "recall_weighted": 0.873303,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.873303,
            "f1": 0.605737,
            "f1_weighted": 0.838685,
            "precision": 0.583345,
            "precision_weighted": 0.818615,
            "recall": 0.647748,
            "recall_weighted": 0.873303,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.874208,
        "f1": 0.605484,
        "f1_weighted": 0.839434,
        "precision": 0.581816,
        "precision_weighted": 0.818298,
        "recall": 0.648184,
        "recall_weighted": 0.874208,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.874208,
        "hf_subset": "default",
        "languages": [
          "ita-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 53.547192335128784,
  "kg_co2_emissions": null
}