{
  "dataset_revision": "79125f20d858a08f71ec4923169a6545221725c4",
  "task_name": "NepaliNewsClassification",
  "mteb_version": "2.3.11",
  "scores": {
    "train": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.988281,
            "f1": 0.988503,
            "f1_weighted": 0.988275,
            "precision": 0.988506,
            "precision_weighted": 0.988334,
            "recall": 0.988563,
            "recall_weighted": 0.988281,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.956055,
            "f1": 0.955645,
            "f1_weighted": 0.956411,
            "precision": 0.956468,
            "precision_weighted": 0.959146,
            "recall": 0.957318,
            "recall_weighted": 0.956055,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.982422,
            "f1": 0.98215,
            "f1_weighted": 0.982402,
            "precision": 0.983346,
            "precision_weighted": 0.982746,
            "recall": 0.981328,
            "recall_weighted": 0.982422,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.98291,
            "f1": 0.982682,
            "f1_weighted": 0.982922,
            "precision": 0.982465,
            "precision_weighted": 0.983142,
            "recall": 0.9831,
            "recall_weighted": 0.98291,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.990234,
            "f1": 0.990138,
            "f1_weighted": 0.990236,
            "precision": 0.989717,
            "precision_weighted": 0.990457,
            "recall": 0.990782,
            "recall_weighted": 0.990234,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.993164,
            "f1": 0.993068,
            "f1_weighted": 0.993168,
            "precision": 0.99304,
            "precision_weighted": 0.99318,
            "recall": 0.993104,
            "recall_weighted": 0.993164,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.97998,
            "f1": 0.979713,
            "f1_weighted": 0.979976,
            "precision": 0.981239,
            "precision_weighted": 0.980508,
            "recall": 0.978734,
            "recall_weighted": 0.97998,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.996094,
            "f1": 0.996122,
            "f1_weighted": 0.996094,
            "precision": 0.996093,
            "precision_weighted": 0.996119,
            "recall": 0.996175,
            "recall_weighted": 0.996094,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.997559,
            "f1": 0.997505,
            "f1_weighted": 0.997558,
            "precision": 0.997534,
            "precision_weighted": 0.997558,
            "recall": 0.997477,
            "recall_weighted": 0.997559,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.990234,
            "f1": 0.99047,
            "f1_weighted": 0.990235,
            "precision": 0.99043,
            "precision_weighted": 0.990343,
            "recall": 0.990612,
            "recall_weighted": 0.990234,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.985693,
        "f1": 0.9856,
        "f1_weighted": 0.985728,
        "precision": 0.985884,
        "precision_weighted": 0.986153,
        "recall": 0.985719,
        "recall_weighted": 0.985693,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.985693,
        "hf_subset": "default",
        "languages": [
          "nep-Deva"
        ]
      }
    ]
  },
  "evaluation_time": 9.120634078979492,
  "kg_co2_emissions": null
}