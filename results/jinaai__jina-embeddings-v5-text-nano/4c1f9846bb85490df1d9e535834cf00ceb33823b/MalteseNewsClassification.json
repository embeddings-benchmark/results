{
  "dataset_revision": "6bb0321659c4f07c4c2176c30c98c971be6571b4",
  "task_name": "MalteseNewsClassification",
  "mteb_version": "2.3.11",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.568132,
            "lrap": 0.666221,
            "f1": 0.388333,
            "hamming": 0.63641
          },
          {
            "accuracy": 0.578581,
            "lrap": 0.690255,
            "f1": 0.388012,
            "hamming": 0.664845
          },
          {
            "accuracy": 0.569003,
            "lrap": 0.676548,
            "f1": 0.370672,
            "hamming": 0.649195
          },
          {
            "accuracy": 0.531128,
            "lrap": 0.661573,
            "f1": 0.374064,
            "hamming": 0.636584
          },
          {
            "accuracy": 0.559861,
            "lrap": 0.669411,
            "f1": 0.382267,
            "hamming": 0.641598
          },
          {
            "accuracy": 0.565956,
            "lrap": 0.675819,
            "f1": 0.377555,
            "hamming": 0.649412
          },
          {
            "accuracy": 0.573357,
            "lrap": 0.678612,
            "f1": 0.368625,
            "hamming": 0.652024
          },
          {
            "accuracy": 0.543317,
            "lrap": 0.666442,
            "f1": 0.386497,
            "hamming": 0.641286
          },
          {
            "accuracy": 0.537658,
            "lrap": 0.670358,
            "f1": 0.412791,
            "hamming": 0.646764
          },
          {
            "accuracy": 0.597301,
            "lrap": 0.703242,
            "f1": 0.407728,
            "hamming": 0.677449
          }
        ],
        "accuracy": 0.562429,
        "lrap": 0.675848,
        "f1": 0.385655,
        "hamming": 0.649557,
        "main_score": 0.562429,
        "hf_subset": "default",
        "languages": [
          "mlt-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 31.68807864189148,
  "kg_co2_emissions": null
}