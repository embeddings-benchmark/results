{
  "dataset_revision": "d604517c81ca91fe16a244d1248fc021f9ecee7a",
  "task_name": "TweetSentimentExtractionClassification",
  "mteb_version": "2.3.11",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.709394,
            "f1": 0.711833,
            "f1_weighted": 0.70358,
            "precision": 0.709194,
            "precision_weighted": 0.710713,
            "recall": 0.726435,
            "recall_weighted": 0.709394,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.71562,
            "f1": 0.718632,
            "f1_weighted": 0.711192,
            "precision": 0.716023,
            "precision_weighted": 0.71843,
            "recall": 0.732348,
            "recall_weighted": 0.71562,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.727221,
            "f1": 0.731026,
            "f1_weighted": 0.724201,
            "precision": 0.727423,
            "precision_weighted": 0.728241,
            "recall": 0.741476,
            "recall_weighted": 0.727221,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.710809,
            "f1": 0.713586,
            "f1_weighted": 0.705369,
            "precision": 0.713009,
            "precision_weighted": 0.716437,
            "recall": 0.730015,
            "recall_weighted": 0.710809,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.718449,
            "f1": 0.7212,
            "f1_weighted": 0.713663,
            "precision": 0.718426,
            "precision_weighted": 0.720858,
            "recall": 0.735209,
            "recall_weighted": 0.718449,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.724109,
            "f1": 0.72775,
            "f1_weighted": 0.721625,
            "precision": 0.724125,
            "precision_weighted": 0.724873,
            "recall": 0.736914,
            "recall_weighted": 0.724109,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.709677,
            "f1": 0.712125,
            "f1_weighted": 0.705285,
            "precision": 0.709474,
            "precision_weighted": 0.709242,
            "recall": 0.722594,
            "recall_weighted": 0.709677,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.713639,
            "f1": 0.715273,
            "f1_weighted": 0.706856,
            "precision": 0.714973,
            "precision_weighted": 0.718749,
            "recall": 0.732774,
            "recall_weighted": 0.713639,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.7176,
            "f1": 0.72016,
            "f1_weighted": 0.712602,
            "precision": 0.719176,
            "precision_weighted": 0.722997,
            "recall": 0.735817,
            "recall_weighted": 0.7176,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.703452,
            "f1": 0.705144,
            "f1_weighted": 0.696132,
            "precision": 0.704584,
            "precision_weighted": 0.708247,
            "recall": 0.723815,
            "recall_weighted": 0.703452,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.714997,
        "f1": 0.717673,
        "f1_weighted": 0.710051,
        "precision": 0.715641,
        "precision_weighted": 0.717879,
        "recall": 0.73174,
        "recall_weighted": 0.714997,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.714997,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 57.18669295310974,
  "kg_co2_emissions": null
}