{
  "dataset_revision": "ed9a4403ed4adbfaf4aab56d5b2709e9f6c3ba33",
  "evaluation_time": 199.68929600715637,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test": [
      {
        "accuracy": 0.00100150225338007,
        "f1": 0.00016742697209425815,
        "hf_subset": "arb_Arab-rus_Cyrl",
        "languages": [
          "arb-Arab",
          "rus-Cyrl"
        ],
        "main_score": 0.00016742697209425815,
        "precision": 0.0001004053201554752,
        "recall": 0.00100150225338007
      },
      {
        "accuracy": 0.0500751126690035,
        "f1": 0.0337715250378964,
        "hf_subset": "bel_Cyrl-rus_Cyrl",
        "languages": [
          "bel-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.0337715250378964,
        "precision": 0.030975322500134517,
        "recall": 0.0500751126690035
      },
      {
        "accuracy": 0.000500751126690035,
        "f1": 5.186443570067685e-07,
        "hf_subset": "ben_Beng-rus_Cyrl",
        "languages": [
          "ben-Beng",
          "rus-Cyrl"
        ],
        "main_score": 5.186443570067685e-07,
        "precision": 2.5945654232644303e-07,
        "recall": 0.000500751126690035
      },
      {
        "accuracy": 0.005007511266900351,
        "f1": 0.0021596470290869982,
        "hf_subset": "bos_Latn-rus_Cyrl",
        "languages": [
          "bos-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0021596470290869982,
        "precision": 0.0019402520127302244,
        "recall": 0.005007511266900351
      },
      {
        "accuracy": 0.03455182774161242,
        "f1": 0.027389627638430607,
        "hf_subset": "bul_Cyrl-rus_Cyrl",
        "languages": [
          "bul-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.027389627638430607,
        "precision": 0.026186885496696184,
        "recall": 0.03455182774161242
      },
      {
        "accuracy": 0.006509764646970456,
        "f1": 0.0034316613364716074,
        "hf_subset": "ces_Latn-rus_Cyrl",
        "languages": [
          "ces-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0034316613364716074,
        "precision": 0.0031602863754935707,
        "recall": 0.006509764646970456
      },
      {
        "accuracy": 0.011517275913870806,
        "f1": 0.007050750451909607,
        "hf_subset": "deu_Latn-rus_Cyrl",
        "languages": [
          "deu-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.007050750451909607,
        "precision": 0.006243429497049175,
        "recall": 0.011517275913870806
      },
      {
        "accuracy": 0.00100150225338007,
        "f1": 0.0005012538888654267,
        "hf_subset": "ell_Grek-rus_Cyrl",
        "languages": [
          "ell-Grek",
          "rus-Cyrl"
        ],
        "main_score": 0.0005012538888654267,
        "precision": 0.0005010026340364389,
        "recall": 0.00100150225338007
      },
      {
        "accuracy": 0.06159238858287431,
        "f1": 0.043428649565294235,
        "hf_subset": "eng_Latn-rus_Cyrl",
        "languages": [
          "eng-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.043428649565294235,
        "precision": 0.0404725123719726,
        "recall": 0.06159238858287431
      },
      {
        "accuracy": 0.00100150225338007,
        "f1": 7.757998744478464e-05,
        "hf_subset": "fas_Arab-rus_Cyrl",
        "languages": [
          "fas-Arab",
          "rus-Cyrl"
        ],
        "main_score": 7.757998744478464e-05,
        "precision": 4.2000083232835545e-05,
        "recall": 0.00100150225338007
      },
      {
        "accuracy": 0.00400600901352028,
        "f1": 0.0024381642886865504,
        "hf_subset": "fin_Latn-rus_Cyrl",
        "languages": [
          "fin-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0024381642886865504,
        "precision": 0.002159382172880681,
        "recall": 0.00400600901352028
      },
      {
        "accuracy": 0.00400600901352028,
        "f1": 0.0028625368630498275,
        "hf_subset": "fra_Latn-rus_Cyrl",
        "languages": [
          "fra-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0028625368630498275,
        "precision": 0.0027669102406208063,
        "recall": 0.00400600901352028
      },
      {
        "accuracy": 0.00100150225338007,
        "f1": 0.0005012564255060693,
        "hf_subset": "heb_Hebr-rus_Cyrl",
        "languages": [
          "heb-Hebr",
          "rus-Cyrl"
        ],
        "main_score": 0.0005012564255060693,
        "precision": 0.000501003903634351,
        "recall": 0.00100150225338007
      },
      {
        "accuracy": 0.00100150225338007,
        "f1": 0.0001117910928323268,
        "hf_subset": "hin_Deva-rus_Cyrl",
        "languages": [
          "hin-Deva",
          "rus-Cyrl"
        ],
        "main_score": 0.0001117910928323268,
        "precision": 6.285055466336357e-05,
        "recall": 0.00100150225338007
      },
      {
        "accuracy": 0.006009013520280421,
        "f1": 0.0022108290556515004,
        "hf_subset": "hrv_Latn-rus_Cyrl",
        "languages": [
          "hrv-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0022108290556515004,
        "precision": 0.0019313329112702568,
        "recall": 0.006009013520280421
      },
      {
        "accuracy": 0.005508262393590386,
        "f1": 0.0014441881010621657,
        "hf_subset": "hun_Latn-rus_Cyrl",
        "languages": [
          "hun-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0014441881010621657,
        "precision": 0.0012450043081851318,
        "recall": 0.005508262393590386
      },
      {
        "accuracy": 0.00400600901352028,
        "f1": 0.0022539888860646593,
        "hf_subset": "ind_Latn-rus_Cyrl",
        "languages": [
          "ind-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0022539888860646593,
        "precision": 0.002004626866025764,
        "recall": 0.00400600901352028
      },
      {
        "accuracy": 0.00100150225338007,
        "f1": 0.0005028507121268905,
        "hf_subset": "jpn_Jpan-rus_Cyrl",
        "languages": [
          "jpn-Jpan",
          "rus-Cyrl"
        ],
        "main_score": 0.0005028507121268905,
        "precision": 0.0005018031248553502,
        "recall": 0.00100150225338007
      },
      {
        "accuracy": 0.0025037556334501754,
        "f1": 0.0018366077254409754,
        "hf_subset": "kor_Hang-rus_Cyrl",
        "languages": [
          "kor-Hang",
          "rus-Cyrl"
        ],
        "main_score": 0.0018366077254409754,
        "precision": 0.0017528892090734854,
        "recall": 0.0025037556334501754
      },
      {
        "accuracy": 0.00400600901352028,
        "f1": 0.0022003967128917832,
        "hf_subset": "lit_Latn-rus_Cyrl",
        "languages": [
          "lit-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0022003967128917832,
        "precision": 0.0020186413605102005,
        "recall": 0.00400600901352028
      },
      {
        "accuracy": 0.01602403605408112,
        "f1": 0.00939710409519316,
        "hf_subset": "mkd_Cyrl-rus_Cyrl",
        "languages": [
          "mkd-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.00939710409519316,
        "precision": 0.008622065078022406,
        "recall": 0.01602403605408112
      },
      {
        "accuracy": 0.005007511266900351,
        "f1": 0.0035823192369156824,
        "hf_subset": "nld_Latn-rus_Cyrl",
        "languages": [
          "nld-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0035823192369156824,
        "precision": 0.003545786603185201,
        "recall": 0.005007511266900351
      },
      {
        "accuracy": 0.005508262393590386,
        "f1": 0.002242507934340797,
        "hf_subset": "pol_Latn-rus_Cyrl",
        "languages": [
          "pol-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.002242507934340797,
        "precision": 0.00198924763930766,
        "recall": 0.005508262393590386
      },
      {
        "accuracy": 0.009013520280420632,
        "f1": 0.0058006016763324205,
        "hf_subset": "por_Latn-rus_Cyrl",
        "languages": [
          "por-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0058006016763324205,
        "precision": 0.005528587736038129,
        "recall": 0.009013520280420632
      },
      {
        "accuracy": 0.0015022533800701052,
        "f1": 2.0266425945985523e-05,
        "hf_subset": "rus_Cyrl-arb_Arab",
        "languages": [
          "rus-Cyrl",
          "arb-Arab"
        ],
        "main_score": 2.0266425945985523e-05,
        "precision": 1.02290746095833e-05,
        "recall": 0.0015022533800701052
      },
      {
        "accuracy": 0.016524787180771158,
        "f1": 0.007125272059049799,
        "hf_subset": "rus_Cyrl-bel_Cyrl",
        "languages": [
          "rus-Cyrl",
          "bel-Cyrl"
        ],
        "main_score": 0.007125272059049799,
        "precision": 0.006221192300291406,
        "recall": 0.016524787180771158
      },
      {
        "accuracy": 0.000500751126690035,
        "f1": 6.06971062654588e-06,
        "hf_subset": "rus_Cyrl-ben_Beng",
        "languages": [
          "rus-Cyrl",
          "ben-Beng"
        ],
        "main_score": 6.06971062654588e-06,
        "precision": 3.0533605285977747e-06,
        "recall": 0.000500751126690035
      },
      {
        "accuracy": 0.007511266900350526,
        "f1": 0.002036880343326832,
        "hf_subset": "rus_Cyrl-bos_Latn",
        "languages": [
          "rus-Cyrl",
          "bos-Latn"
        ],
        "main_score": 0.002036880343326832,
        "precision": 0.00166457989035658,
        "recall": 0.007511266900350526
      },
      {
        "accuracy": 0.07361041562343515,
        "f1": 0.04436265083234472,
        "hf_subset": "rus_Cyrl-bul_Cyrl",
        "languages": [
          "rus-Cyrl",
          "bul-Cyrl"
        ],
        "main_score": 0.04436265083234472,
        "precision": 0.03940355208055305,
        "recall": 0.07361041562343515
      },
      {
        "accuracy": 0.007010515773660491,
        "f1": 0.0028801621273912446,
        "hf_subset": "rus_Cyrl-ces_Latn",
        "languages": [
          "rus-Cyrl",
          "ces-Latn"
        ],
        "main_score": 0.0028801621273912446,
        "precision": 0.0024329356516281674,
        "recall": 0.007010515773660491
      },
      {
        "accuracy": 0.012518778167250876,
        "f1": 0.004105558592719256,
        "hf_subset": "rus_Cyrl-deu_Latn",
        "languages": [
          "rus-Cyrl",
          "deu-Latn"
        ],
        "main_score": 0.004105558592719256,
        "precision": 0.0038285661315891704,
        "recall": 0.012518778167250876
      },
      {
        "accuracy": 0.0015022533800701052,
        "f1": 0.0005639429293193522,
        "hf_subset": "rus_Cyrl-ell_Grek",
        "languages": [
          "rus-Cyrl",
          "ell-Grek"
        ],
        "main_score": 0.0005639429293193522,
        "precision": 0.0005344336696203205,
        "recall": 0.0015022533800701052
      },
      {
        "accuracy": 0.1271907861792689,
        "f1": 0.08670229337040089,
        "hf_subset": "rus_Cyrl-eng_Latn",
        "languages": [
          "rus-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.08670229337040089,
        "precision": 0.07867939657413803,
        "recall": 0.1271907861792689
      },
      {
        "accuracy": 0.0015022533800701052,
        "f1": 9.575082575940628e-06,
        "hf_subset": "rus_Cyrl-fas_Arab",
        "languages": [
          "rus-Cyrl",
          "fas-Arab"
        ],
        "main_score": 9.575082575940628e-06,
        "precision": 4.810231063112032e-06,
        "recall": 0.0015022533800701052
      },
      {
        "accuracy": 0.007010515773660491,
        "f1": 0.00247491324338748,
        "hf_subset": "rus_Cyrl-fin_Latn",
        "languages": [
          "rus-Cyrl",
          "fin-Latn"
        ],
        "main_score": 0.00247491324338748,
        "precision": 0.002136072456141871,
        "recall": 0.007010515773660491
      },
      {
        "accuracy": 0.01702553830746119,
        "f1": 0.005621558203989358,
        "hf_subset": "rus_Cyrl-fra_Latn",
        "languages": [
          "rus-Cyrl",
          "fra-Latn"
        ],
        "main_score": 0.005621558203989358,
        "precision": 0.004519226580857456,
        "recall": 0.01702553830746119
      },
      {
        "accuracy": 0.00200300450676014,
        "f1": 0.0005555891895159005,
        "hf_subset": "rus_Cyrl-heb_Hebr",
        "languages": [
          "rus-Cyrl",
          "heb-Hebr"
        ],
        "main_score": 0.0005555891895159005,
        "precision": 0.0005294967660142913,
        "recall": 0.00200300450676014
      },
      {
        "accuracy": 0.0030045067601402104,
        "f1": 0.0002965866712944904,
        "hf_subset": "rus_Cyrl-hin_Deva",
        "languages": [
          "rus-Cyrl",
          "hin-Deva"
        ],
        "main_score": 0.0002965866712944904,
        "precision": 0.00016536949273254453,
        "recall": 0.0030045067601402104
      },
      {
        "accuracy": 0.007511266900350526,
        "f1": 0.0032838524881585443,
        "hf_subset": "rus_Cyrl-hrv_Latn",
        "languages": [
          "rus-Cyrl",
          "hrv-Latn"
        ],
        "main_score": 0.0032838524881585443,
        "precision": 0.0026925927422696903,
        "recall": 0.007511266900350526
      },
      {
        "accuracy": 0.007010515773660491,
        "f1": 0.002338517707057828,
        "hf_subset": "rus_Cyrl-hun_Latn",
        "languages": [
          "rus-Cyrl",
          "hun-Latn"
        ],
        "main_score": 0.002338517707057828,
        "precision": 0.00203135915492081,
        "recall": 0.007010515773660491
      },
      {
        "accuracy": 0.013520280420630946,
        "f1": 0.0038315549312533176,
        "hf_subset": "rus_Cyrl-ind_Latn",
        "languages": [
          "rus-Cyrl",
          "ind-Latn"
        ],
        "main_score": 0.0038315549312533176,
        "precision": 0.002985568875928657,
        "recall": 0.013520280420630946
      },
      {
        "accuracy": 0.0025037556334501754,
        "f1": 0.0004389532358969497,
        "hf_subset": "rus_Cyrl-jpn_Jpan",
        "languages": [
          "rus-Cyrl",
          "jpn-Jpan"
        ],
        "main_score": 0.0004389532358969497,
        "precision": 0.0003056611422669022,
        "recall": 0.0025037556334501754
      },
      {
        "accuracy": 0.00200300450676014,
        "f1": 0.00013252645041352868,
        "hf_subset": "rus_Cyrl-kor_Hang",
        "languages": [
          "rus-Cyrl",
          "kor-Hang"
        ],
        "main_score": 0.00013252645041352868,
        "precision": 7.521638759653509e-05,
        "recall": 0.00200300450676014
      },
      {
        "accuracy": 0.00801201802704056,
        "f1": 0.0015697902813797627,
        "hf_subset": "rus_Cyrl-lit_Latn",
        "languages": [
          "rus-Cyrl",
          "lit-Latn"
        ],
        "main_score": 0.0015697902813797627,
        "precision": 0.0011828836379553264,
        "recall": 0.00801201802704056
      },
      {
        "accuracy": 0.03355032548823235,
        "f1": 0.018208334106060955,
        "hf_subset": "rus_Cyrl-mkd_Cyrl",
        "languages": [
          "rus-Cyrl",
          "mkd-Cyrl"
        ],
        "main_score": 0.018208334106060955,
        "precision": 0.01626183511956988,
        "recall": 0.03355032548823235
      },
      {
        "accuracy": 0.013520280420630946,
        "f1": 0.004374346495796142,
        "hf_subset": "rus_Cyrl-nld_Latn",
        "languages": [
          "rus-Cyrl",
          "nld-Latn"
        ],
        "main_score": 0.004374346495796142,
        "precision": 0.0036075642526103764,
        "recall": 0.013520280420630946
      },
      {
        "accuracy": 0.005508262393590386,
        "f1": 0.0011941396980322504,
        "hf_subset": "rus_Cyrl-pol_Latn",
        "languages": [
          "rus-Cyrl",
          "pol-Latn"
        ],
        "main_score": 0.0011941396980322504,
        "precision": 0.0009755146365586954,
        "recall": 0.005508262393590386
      },
      {
        "accuracy": 0.012518778167250876,
        "f1": 0.0034334154738721824,
        "hf_subset": "rus_Cyrl-por_Latn",
        "languages": [
          "rus-Cyrl",
          "por-Latn"
        ],
        "main_score": 0.0034334154738721824,
        "precision": 0.0027588690261675652,
        "recall": 0.012518778167250876
      },
      {
        "accuracy": 0.007010515773660491,
        "f1": 0.002535483709112279,
        "hf_subset": "rus_Cyrl-slk_Latn",
        "languages": [
          "rus-Cyrl",
          "slk-Latn"
        ],
        "main_score": 0.002535483709112279,
        "precision": 0.002177488186086728,
        "recall": 0.007010515773660491
      },
      {
        "accuracy": 0.009013520280420632,
        "f1": 0.0025942503033112574,
        "hf_subset": "rus_Cyrl-slv_Latn",
        "languages": [
          "rus-Cyrl",
          "slv-Latn"
        ],
        "main_score": 0.0025942503033112574,
        "precision": 0.0021810999252846767,
        "recall": 0.009013520280420632
      },
      {
        "accuracy": 0.014521782674011016,
        "f1": 0.005827985994107198,
        "hf_subset": "rus_Cyrl-spa_Latn",
        "languages": [
          "rus-Cyrl",
          "spa-Latn"
        ],
        "main_score": 0.005827985994107198,
        "precision": 0.005272918560493829,
        "recall": 0.014521782674011016
      },
      {
        "accuracy": 0.030045067601402103,
        "f1": 0.018360943362801634,
        "hf_subset": "rus_Cyrl-srp_Cyrl",
        "languages": [
          "rus-Cyrl",
          "srp-Cyrl"
        ],
        "main_score": 0.018360943362801634,
        "precision": 0.016422930037064413,
        "recall": 0.030045067601402103
      },
      {
        "accuracy": 0.00801201802704056,
        "f1": 0.003057128251538546,
        "hf_subset": "rus_Cyrl-srp_Latn",
        "languages": [
          "rus-Cyrl",
          "srp-Latn"
        ],
        "main_score": 0.003057128251538546,
        "precision": 0.002712608399806361,
        "recall": 0.00801201802704056
      },
      {
        "accuracy": 0.006509764646970456,
        "f1": 0.002708336937241584,
        "hf_subset": "rus_Cyrl-swa_Latn",
        "languages": [
          "rus-Cyrl",
          "swa-Latn"
        ],
        "main_score": 0.002708336937241584,
        "precision": 0.0024823153988192902,
        "recall": 0.006509764646970456
      },
      {
        "accuracy": 0.010515773660490736,
        "f1": 0.004159118615517679,
        "hf_subset": "rus_Cyrl-swe_Latn",
        "languages": [
          "rus-Cyrl",
          "swe-Latn"
        ],
        "main_score": 0.004159118615517679,
        "precision": 0.003735021603756512,
        "recall": 0.010515773660490736
      },
      {
        "accuracy": 0.0025037556334501754,
        "f1": 9.864734208983688e-05,
        "hf_subset": "rus_Cyrl-tam_Taml",
        "languages": [
          "rus-Cyrl",
          "tam-Taml"
        ],
        "main_score": 9.864734208983688e-05,
        "precision": 5.1337047222776674e-05,
        "recall": 0.0025037556334501754
      },
      {
        "accuracy": 0.004506760140210316,
        "f1": 0.0018492273345026896,
        "hf_subset": "rus_Cyrl-tur_Latn",
        "languages": [
          "rus-Cyrl",
          "tur-Latn"
        ],
        "main_score": 0.0018492273345026896,
        "precision": 0.0015088516769922976,
        "recall": 0.004506760140210316
      },
      {
        "accuracy": 0.14171256885327993,
        "f1": 0.11179589132437066,
        "hf_subset": "rus_Cyrl-ukr_Cyrl",
        "languages": [
          "rus-Cyrl",
          "ukr-Cyrl"
        ],
        "main_score": 0.11179589132437066,
        "precision": 0.10561575615232636,
        "recall": 0.14171256885327993
      },
      {
        "accuracy": 0.005007511266900351,
        "f1": 0.00032671506484581735,
        "hf_subset": "rus_Cyrl-vie_Latn",
        "languages": [
          "rus-Cyrl",
          "vie-Latn"
        ],
        "main_score": 0.00032671506484581735,
        "precision": 0.00019066397601546494,
        "recall": 0.005007511266900351
      },
      {
        "accuracy": 0.005007511266900351,
        "f1": 0.000754188279430649,
        "hf_subset": "rus_Cyrl-zho_Hant",
        "languages": [
          "rus-Cyrl",
          "zho-Hant"
        ],
        "main_score": 0.000754188279430649,
        "precision": 0.0006408151802943633,
        "recall": 0.005007511266900351
      },
      {
        "accuracy": 0.013019529293940912,
        "f1": 0.004683151685844592,
        "hf_subset": "rus_Cyrl-zul_Latn",
        "languages": [
          "rus-Cyrl",
          "zul-Latn"
        ],
        "main_score": 0.004683151685844592,
        "precision": 0.0038965810430294846,
        "recall": 0.013019529293940912
      },
      {
        "accuracy": 0.007010515773660491,
        "f1": 0.0033611734157092225,
        "hf_subset": "slk_Latn-rus_Cyrl",
        "languages": [
          "slk-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0033611734157092225,
        "precision": 0.0028982129334147524,
        "recall": 0.007010515773660491
      },
      {
        "accuracy": 0.009013520280420632,
        "f1": 0.0042662349106994376,
        "hf_subset": "slv_Latn-rus_Cyrl",
        "languages": [
          "slv-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0042662349106994376,
        "precision": 0.003825626857990244,
        "recall": 0.009013520280420632
      },
      {
        "accuracy": 0.007010515773660491,
        "f1": 0.003999497884969718,
        "hf_subset": "spa_Latn-rus_Cyrl",
        "languages": [
          "spa-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.003999497884969718,
        "precision": 0.0037952219941612625,
        "recall": 0.007010515773660491
      },
      {
        "accuracy": 0.01902854281422133,
        "f1": 0.012851091272167495,
        "hf_subset": "srp_Cyrl-rus_Cyrl",
        "languages": [
          "srp-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.012851091272167495,
        "precision": 0.012200616991680456,
        "recall": 0.01902854281422133
      },
      {
        "accuracy": 0.005508262393590386,
        "f1": 0.001799580784325629,
        "hf_subset": "srp_Latn-rus_Cyrl",
        "languages": [
          "srp-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.001799580784325629,
        "precision": 0.001375834869876183,
        "recall": 0.005508262393590386
      },
      {
        "accuracy": 0.005508262393590386,
        "f1": 0.0016795541567138795,
        "hf_subset": "swa_Latn-rus_Cyrl",
        "languages": [
          "swa-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0016795541567138795,
        "precision": 0.0014028033336554468,
        "recall": 0.005508262393590386
      },
      {
        "accuracy": 0.0035052578868302454,
        "f1": 0.0022234308400399644,
        "hf_subset": "swe_Latn-rus_Cyrl",
        "languages": [
          "swe-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0022234308400399644,
        "precision": 0.0021313738808976077,
        "recall": 0.0035052578868302454
      },
      {
        "accuracy": 0.0015022533800701052,
        "f1": 0.0005848720170202393,
        "hf_subset": "tam_Taml-rus_Cyrl",
        "languages": [
          "tam-Taml",
          "rus-Cyrl"
        ],
        "main_score": 0.0005848720170202393,
        "precision": 0.00041762400936502916,
        "recall": 0.0015022533800701052
      },
      {
        "accuracy": 0.006009013520280421,
        "f1": 0.0030031826742413174,
        "hf_subset": "tur_Latn-rus_Cyrl",
        "languages": [
          "tur-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0030031826742413174,
        "precision": 0.0028104869143743445,
        "recall": 0.006009013520280421
      },
      {
        "accuracy": 0.5443164747120681,
        "f1": 0.48578718926208225,
        "hf_subset": "ukr_Cyrl-rus_Cyrl",
        "languages": [
          "ukr-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.48578718926208225,
        "precision": 0.46692669045732743,
        "recall": 0.5443164747120681
      },
      {
        "accuracy": 0.0025037556334501754,
        "f1": 0.0013776492244193797,
        "hf_subset": "vie_Latn-rus_Cyrl",
        "languages": [
          "vie-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0013776492244193797,
        "precision": 0.001240247154015766,
        "recall": 0.0025037556334501754
      },
      {
        "accuracy": 0.00200300450676014,
        "f1": 0.0011689276723884474,
        "hf_subset": "zho_Hant-rus_Cyrl",
        "languages": [
          "zho-Hant",
          "rus-Cyrl"
        ],
        "main_score": 0.0011689276723884474,
        "precision": 0.0011019067962024297,
        "recall": 0.00200300450676014
      },
      {
        "accuracy": 0.005007511266900351,
        "f1": 0.0023327391515051425,
        "hf_subset": "zul_Latn-rus_Cyrl",
        "languages": [
          "zul-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0023327391515051425,
        "precision": 0.0021978208569023642,
        "recall": 0.005007511266900351
      }
    ]
  },
  "task_name": "NTREXBitextMining"
}