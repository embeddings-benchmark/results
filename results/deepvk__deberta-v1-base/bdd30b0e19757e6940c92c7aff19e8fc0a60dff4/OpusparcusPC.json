{
  "dataset_revision": "9e9b1f8ef51616073f47f306f7f47dd91663f86a",
  "evaluation_time": 4.738001108169556,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test.full": [
      {
        "cosine_accuracy": 0.6936274509803921,
        "cosine_accuracy_threshold": 0.6943374872207642,
        "cosine_ap": 0.8330868057881049,
        "cosine_f1": 0.7943049831397527,
        "cosine_f1_threshold": 0.5001394748687744,
        "cosine_precision": 0.6620861961274204,
        "cosine_recall": 0.9925093632958801,
        "dot_accuracy": 0.6770833333333334,
        "dot_accuracy_threshold": 75.25826263427734,
        "dot_ap": 0.7485513459984103,
        "dot_f1": 0.7923646279703934,
        "dot_f1_threshold": 70.9552001953125,
        "dot_precision": 0.6784523015343562,
        "dot_recall": 0.952247191011236,
        "euclidean_accuracy": 0.6979166666666666,
        "euclidean_accuracy_threshold": 8.81930160522461,
        "euclidean_ap": 0.8353061597392519,
        "euclidean_f1": 0.7956176803928976,
        "euclidean_f1_threshold": 11.170427322387695,
        "euclidean_precision": 0.6668777707409753,
        "euclidean_recall": 0.9859550561797753,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.8368599694020211,
        "manhattan_accuracy": 0.7022058823529411,
        "manhattan_accuracy_threshold": 187.437744140625,
        "manhattan_ap": 0.8368599694020211,
        "manhattan_f1": 0.7992322456813818,
        "manhattan_f1_threshold": 225.73403930664062,
        "manhattan_precision": 0.6772934287573195,
        "manhattan_recall": 0.9747191011235955,
        "max_ap": 0.8368599694020211,
        "max_f1": 0.7992322456813818,
        "max_precision": 0.6784523015343562,
        "max_recall": 0.9925093632958801,
        "similarity_accuracy": 0.6936274509803921,
        "similarity_accuracy_threshold": 0.6943374276161194,
        "similarity_ap": 0.8330868057881049,
        "similarity_f1": 0.7943049831397527,
        "similarity_f1_threshold": 0.5001394152641296,
        "similarity_precision": 0.6620861961274204,
        "similarity_recall": 0.9925093632958801
      }
    ],
    "validation.full": [
      {
        "cosine_accuracy": 0.7152690863579474,
        "cosine_accuracy_threshold": 0.6873586773872375,
        "cosine_ap": 0.8368594522642037,
        "cosine_f1": 0.7979094076655052,
        "cosine_f1_threshold": 0.6760851144790649,
        "cosine_precision": 0.7178683385579937,
        "cosine_recall": 0.8980392156862745,
        "dot_accuracy": 0.6789737171464331,
        "dot_accuracy_threshold": 78.70634460449219,
        "dot_ap": 0.7211918191508312,
        "dot_f1": 0.7855382087099425,
        "dot_f1_threshold": 73.5989990234375,
        "dot_precision": 0.6760961810466761,
        "dot_recall": 0.9372549019607843,
        "euclidean_accuracy": 0.7165206508135169,
        "euclidean_accuracy_threshold": 8.156304359436035,
        "euclidean_ap": 0.8390535927168178,
        "euclidean_f1": 0.7935258092738408,
        "euclidean_f1_threshold": 9.015018463134766,
        "euclidean_precision": 0.7164296998420221,
        "euclidean_recall": 0.8892156862745098,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.8422346798031375,
        "manhattan_accuracy": 0.7221526908635795,
        "manhattan_accuracy_threshold": 174.17137145996094,
        "manhattan_ap": 0.8422346798031375,
        "manhattan_f1": 0.7961588825840245,
        "manhattan_f1_threshold": 193.62387084960938,
        "manhattan_precision": 0.7175452399685287,
        "manhattan_recall": 0.8941176470588236,
        "max_ap": 0.8422346798031375,
        "max_f1": 0.7979094076655052,
        "max_precision": 0.7178683385579937,
        "max_recall": 0.9372549019607843,
        "similarity_accuracy": 0.7152690863579474,
        "similarity_accuracy_threshold": 0.6873587369918823,
        "similarity_ap": 0.8368594522642037,
        "similarity_f1": 0.7979094076655052,
        "similarity_f1_threshold": 0.6760851740837097,
        "similarity_precision": 0.7178683385579937,
        "similarity_recall": 0.8980392156862745
      }
    ]
  },
  "task_name": "OpusparcusPC"
}