{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 14.501518487930298,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test": [
      {
        "accuracy": 0.6471418964357767,
        "f1": 0.6443593471772247,
        "f1_weighted": 0.6493122127672156,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6471418964357767,
        "scores_per_experiment": [
          {
            "accuracy": 0.648285137861466,
            "f1": 0.6468607430413067,
            "f1_weighted": 0.6492610684602502
          },
          {
            "accuracy": 0.6620712844653666,
            "f1": 0.6697061809802274,
            "f1_weighted": 0.6615862186345612
          },
          {
            "accuracy": 0.6466039004707465,
            "f1": 0.6470152020934636,
            "f1_weighted": 0.6488352309805674
          },
          {
            "accuracy": 0.6240753194351042,
            "f1": 0.6185402032507938,
            "f1_weighted": 0.6283553341857121
          },
          {
            "accuracy": 0.6476126429051782,
            "f1": 0.6382794748315307,
            "f1_weighted": 0.6503969834979577
          },
          {
            "accuracy": 0.6217215870880969,
            "f1": 0.6157638654059316,
            "f1_weighted": 0.6223722198630367
          },
          {
            "accuracy": 0.6543375924680565,
            "f1": 0.6499667811573878,
            "f1_weighted": 0.6591532298542727
          },
          {
            "accuracy": 0.6395427034297243,
            "f1": 0.6417269334009872,
            "f1_weighted": 0.6402872371301073
          },
          {
            "accuracy": 0.6587088096839274,
            "f1": 0.6560560355903275,
            "f1_weighted": 0.6603888928091084
          },
          {
            "accuracy": 0.6684599865501009,
            "f1": 0.6596780520202898,
            "f1_weighted": 0.6724857122565816
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6432365961633055,
        "f1": 0.6435043889131761,
        "f1_weighted": 0.6452750123524902,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6432365961633055,
        "scores_per_experiment": [
          {
            "accuracy": 0.6655189375307428,
            "f1": 0.6654562719570203,
            "f1_weighted": 0.6661295812456345
          },
          {
            "accuracy": 0.6551893753074275,
            "f1": 0.661185154926957,
            "f1_weighted": 0.6544067327899334
          },
          {
            "accuracy": 0.6305951795376291,
            "f1": 0.6296029770391427,
            "f1_weighted": 0.6344881004075875
          },
          {
            "accuracy": 0.617314313821938,
            "f1": 0.6226558509800125,
            "f1_weighted": 0.6208889806937905
          },
          {
            "accuracy": 0.6483030004918839,
            "f1": 0.6462735706002231,
            "f1_weighted": 0.6542003027816827
          },
          {
            "accuracy": 0.6301032956222331,
            "f1": 0.6336901487745012,
            "f1_weighted": 0.6302085441590383
          },
          {
            "accuracy": 0.6394490900147565,
            "f1": 0.6403191937206097,
            "f1_weighted": 0.6447602674576433
          },
          {
            "accuracy": 0.631578947368421,
            "f1": 0.6364131398697345,
            "f1_weighted": 0.6314848786680275
          },
          {
            "accuracy": 0.6483030004918839,
            "f1": 0.64895243735273,
            "f1_weighted": 0.6476746628427659
          },
          {
            "accuracy": 0.6660108214461387,
            "f1": 0.650495143910829,
            "f1_weighted": 0.6685080724787977
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}