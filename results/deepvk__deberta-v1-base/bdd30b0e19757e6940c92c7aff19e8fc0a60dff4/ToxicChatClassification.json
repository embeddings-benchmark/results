{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.64201,
        "f1": 0.536822,
        "f1_weighted": 0.699151,
        "ap": 0.182876,
        "ap_weighted": 0.182876,
        "scores_per_experiment": [
          {
            "accuracy": 0.553265,
            "f1": 0.479576,
            "f1_weighted": 0.62628,
            "ap": 0.162225,
            "ap_weighted": 0.162225
          },
          {
            "accuracy": 0.774914,
            "f1": 0.622235,
            "f1_weighted": 0.802149,
            "ap": 0.217491,
            "ap_weighted": 0.217491
          },
          {
            "accuracy": 0.530928,
            "f1": 0.472971,
            "f1_weighted": 0.603899,
            "ap": 0.171603,
            "ap_weighted": 0.171603
          },
          {
            "accuracy": 0.604811,
            "f1": 0.510923,
            "f1_weighted": 0.671453,
            "ap": 0.168957,
            "ap_weighted": 0.168957
          },
          {
            "accuracy": 0.578179,
            "f1": 0.493045,
            "f1_weighted": 0.648677,
            "ap": 0.162828,
            "ap_weighted": 0.162828
          },
          {
            "accuracy": 0.756014,
            "f1": 0.615287,
            "f1_weighted": 0.789596,
            "ap": 0.218749,
            "ap_weighted": 0.218749
          },
          {
            "accuracy": 0.676117,
            "f1": 0.542532,
            "f1_weighted": 0.727724,
            "ap": 0.167449,
            "ap_weighted": 0.167449
          },
          {
            "accuracy": 0.617698,
            "f1": 0.515305,
            "f1_weighted": 0.682196,
            "ap": 0.166418,
            "ap_weighted": 0.166418
          },
          {
            "accuracy": 0.685567,
            "f1": 0.565438,
            "f1_weighted": 0.736602,
            "ap": 0.191443,
            "ap_weighted": 0.191443
          },
          {
            "accuracy": 0.642612,
            "f1": 0.550904,
            "f1_weighted": 0.702936,
            "ap": 0.201594,
            "ap_weighted": 0.201594
          }
        ],
        "main_score": 0.64201,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 8.185313940048218,
  "kg_co2_emissions": 0.0002931095704548976
}