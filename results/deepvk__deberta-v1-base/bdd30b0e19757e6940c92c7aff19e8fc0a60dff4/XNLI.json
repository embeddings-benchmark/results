{
  "dataset_revision": "09698e0180d87dc247ca447d3a1248b931ac0cdb",
  "evaluation_time": 5.3878538608551025,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.5772893772893772,
        "cosine_accuracy_threshold": 0.7099871635437012,
        "cosine_ap": 0.5869829025717166,
        "cosine_f1": 0.6673490276356192,
        "cosine_f1_threshold": 0.6183316707611084,
        "cosine_precision": 0.5125786163522013,
        "cosine_recall": 0.9560117302052786,
        "dot_accuracy": 0.558974358974359,
        "dot_accuracy_threshold": 73.20841979980469,
        "dot_ap": 0.5614561448937075,
        "dot_f1": 0.6736736736736736,
        "dot_f1_threshold": 57.148773193359375,
        "dot_precision": 0.5113981762917933,
        "dot_recall": 0.9868035190615836,
        "euclidean_accuracy": 0.558974358974359,
        "euclidean_accuracy_threshold": 7.750351905822754,
        "euclidean_ap": 0.576015658122321,
        "euclidean_f1": 0.6669950738916257,
        "euclidean_f1_threshold": 10.248541831970215,
        "euclidean_precision": 0.5022255192878339,
        "euclidean_recall": 0.9926686217008798,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.5869829025717166,
        "manhattan_accuracy": 0.5663003663003663,
        "manhattan_accuracy_threshold": 143.30435180664062,
        "manhattan_ap": 0.583387880441391,
        "manhattan_f1": 0.6687306501547987,
        "manhattan_f1_threshold": 190.03375244140625,
        "manhattan_precision": 0.5159235668789809,
        "manhattan_recall": 0.9501466275659824,
        "max_ap": 0.5869829025717166,
        "max_f1": 0.6736736736736736,
        "max_precision": 0.5159235668789809,
        "max_recall": 0.9926686217008798,
        "similarity_accuracy": 0.5772893772893772,
        "similarity_accuracy_threshold": 0.7099870443344116,
        "similarity_ap": 0.5869829025717166,
        "similarity_f1": 0.6673490276356192,
        "similarity_f1_threshold": 0.6183316707611084,
        "similarity_precision": 0.5125786163522013,
        "similarity_recall": 0.9560117302052786
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.5787545787545788,
        "cosine_accuracy_threshold": 0.7968451976776123,
        "cosine_ap": 0.6049308314715137,
        "cosine_f1": 0.6676602086438153,
        "cosine_f1_threshold": 0.5777028799057007,
        "cosine_precision": 0.5048835462058603,
        "cosine_recall": 0.9853372434017595,
        "dot_accuracy": 0.5611721611721612,
        "dot_accuracy_threshold": 74.95662689208984,
        "dot_ap": 0.5668001542350328,
        "dot_f1": 0.6683070866141733,
        "dot_f1_threshold": 52.142398834228516,
        "dot_precision": 0.502962962962963,
        "dot_recall": 0.9956011730205279,
        "euclidean_accuracy": 0.5714285714285714,
        "euclidean_accuracy_threshold": 6.929468154907227,
        "euclidean_ap": 0.5947622243730392,
        "euclidean_f1": 0.6663361427863164,
        "euclidean_f1_threshold": 9.794191360473633,
        "euclidean_precision": 0.503370786516854,
        "euclidean_recall": 0.9853372434017595,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6049308314715137,
        "manhattan_accuracy": 0.5736263736263736,
        "manhattan_accuracy_threshold": 151.68548583984375,
        "manhattan_ap": 0.5974260102988791,
        "manhattan_f1": 0.6686596910812157,
        "manhattan_f1_threshold": 204.6365203857422,
        "manhattan_precision": 0.5064150943396226,
        "manhattan_recall": 0.9838709677419355,
        "max_ap": 0.6049308314715137,
        "max_f1": 0.6686596910812157,
        "max_precision": 0.5064150943396226,
        "max_recall": 0.9956011730205279,
        "similarity_accuracy": 0.5787545787545788,
        "similarity_accuracy_threshold": 0.7968451976776123,
        "similarity_ap": 0.6049308314715137,
        "similarity_f1": 0.6676602086438153,
        "similarity_f1_threshold": 0.5777028203010559,
        "similarity_precision": 0.5048835462058603,
        "similarity_recall": 0.9853372434017595
      }
    ]
  },
  "task_name": "XNLI"
}