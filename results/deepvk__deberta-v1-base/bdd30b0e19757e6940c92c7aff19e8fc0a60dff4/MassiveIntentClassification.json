{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 29.530900239944458,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test": [
      {
        "accuracy": 0.6132481506388703,
        "f1": 0.5734557679322055,
        "f1_weighted": 0.6198417131764273,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6132481506388703,
        "scores_per_experiment": [
          {
            "accuracy": 0.5958305312710155,
            "f1": 0.5751944181553894,
            "f1_weighted": 0.6009335544744082
          },
          {
            "accuracy": 0.621049092131809,
            "f1": 0.5755468086292409,
            "f1_weighted": 0.6271998020777511
          },
          {
            "accuracy": 0.6082716879623403,
            "f1": 0.5662603949829826,
            "f1_weighted": 0.6135907100113686
          },
          {
            "accuracy": 0.6294552790854069,
            "f1": 0.5771458955732456,
            "f1_weighted": 0.6347630075948935
          },
          {
            "accuracy": 0.6348352387357095,
            "f1": 0.5867551789539255,
            "f1_weighted": 0.6371840383355011
          },
          {
            "accuracy": 0.5880968392737055,
            "f1": 0.5553322695189008,
            "f1_weighted": 0.5977006198744983
          },
          {
            "accuracy": 0.6116341627437795,
            "f1": 0.5797688283835611,
            "f1_weighted": 0.6180657342757512
          },
          {
            "accuracy": 0.6136516476126429,
            "f1": 0.5705442283707478,
            "f1_weighted": 0.6224591800541175
          },
          {
            "accuracy": 0.5995292535305985,
            "f1": 0.5659102419132643,
            "f1_weighted": 0.6102648423596904
          },
          {
            "accuracy": 0.6301277740416947,
            "f1": 0.5820994148407966,
            "f1_weighted": 0.6362556427062938
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6116084604033448,
        "f1": 0.5745781426355071,
        "f1_weighted": 0.618210759952196,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6116084604033448,
        "scores_per_experiment": [
          {
            "accuracy": 0.6079685194294147,
            "f1": 0.5775303282228312,
            "f1_weighted": 0.6115764017427323
          },
          {
            "accuracy": 0.6153467781603541,
            "f1": 0.5746706368402976,
            "f1_weighted": 0.6214219975074585
          },
          {
            "accuracy": 0.6232169208066897,
            "f1": 0.5858888365216957,
            "f1_weighted": 0.6276109817688045
          },
          {
            "accuracy": 0.6251844564682735,
            "f1": 0.5753023222359045,
            "f1_weighted": 0.6286378921826172
          },
          {
            "accuracy": 0.6217412690605018,
            "f1": 0.5856912995184336,
            "f1_weighted": 0.6273609884384163
          },
          {
            "accuracy": 0.5873093949827841,
            "f1": 0.5562049318646142,
            "f1_weighted": 0.598077119162789
          },
          {
            "accuracy": 0.5981308411214953,
            "f1": 0.57848396376557,
            "f1_weighted": 0.6047338565872232
          },
          {
            "accuracy": 0.6060009837678307,
            "f1": 0.5602985851001226,
            "f1_weighted": 0.6143027363882422
          },
          {
            "accuracy": 0.5981308411214953,
            "f1": 0.5649125118141837,
            "f1_weighted": 0.6094923286329683
          },
          {
            "accuracy": 0.6330545991146089,
            "f1": 0.5867980104714187,
            "f1_weighted": 0.6388932971107084
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}