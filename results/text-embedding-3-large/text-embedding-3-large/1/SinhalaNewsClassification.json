{
  "dataset_revision": "7fb2f514ea683c5282dfec0a9672ece8de90ac50",
  "evaluation_time": 84.91087484359741,
  "kg_co2_emissions": null,
  "mteb_version": "1.18.0",
  "scores": {
    "train": [
      {
        "accuracy": 0.496728515625,
        "f1": 0.4850520754227802,
        "f1_weighted": 0.49950710817637933,
        "hf_subset": "default",
        "languages": [
          "sin-Sinh"
        ],
        "main_score": 0.496728515625,
        "scores_per_experiment": [
          {
            "accuracy": 0.5029296875,
            "f1": 0.4870084495390147,
            "f1_weighted": 0.5040453390370898
          },
          {
            "accuracy": 0.517578125,
            "f1": 0.4999465370178628,
            "f1_weighted": 0.519511679637281
          },
          {
            "accuracy": 0.494140625,
            "f1": 0.48429217942266123,
            "f1_weighted": 0.4975898006756435
          },
          {
            "accuracy": 0.4462890625,
            "f1": 0.4331016699682809,
            "f1_weighted": 0.4389347364780526
          },
          {
            "accuracy": 0.4287109375,
            "f1": 0.4360252875205058,
            "f1_weighted": 0.43015372477077707
          },
          {
            "accuracy": 0.48681640625,
            "f1": 0.47997344759384586,
            "f1_weighted": 0.49481322538767986
          },
          {
            "accuracy": 0.5107421875,
            "f1": 0.4917255038798579,
            "f1_weighted": 0.5078158424826282
          },
          {
            "accuracy": 0.501953125,
            "f1": 0.4893467356050872,
            "f1_weighted": 0.50898746498206
          },
          {
            "accuracy": 0.53564453125,
            "f1": 0.5119804772983239,
            "f1_weighted": 0.5396566038065471
          },
          {
            "accuracy": 0.54248046875,
            "f1": 0.5371204663823618,
            "f1_weighted": 0.5535626645060342
          }
        ]
      }
    ]
  },
  "task_name": "SinhalaNewsClassification"
}