{
  "dataset_revision": "f8650438298df086750ff4973661bb58a201a5ee",
  "evaluation_time": 7468.545123338699,
  "kg_co2_emissions": null,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.9150197628458498,
        "f1": 0.8899868247694336,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.8899868247694336,
        "precision": 0.8781291172595521,
        "recall": 0.9150197628458498
      },
      {
        "accuracy": 0.8231225296442688,
        "f1": 0.7742447769621682,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ],
        "main_score": 0.7742447769621682,
        "precision": 0.753220873329569,
        "recall": 0.8231225296442688
      },
      {
        "accuracy": 0.7223320158102767,
        "f1": 0.6630536733797604,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ],
        "main_score": 0.6630536733797604,
        "precision": 0.6392210144927536,
        "recall": 0.7223320158102767
      },
      {
        "accuracy": 0.6452569169960475,
        "f1": 0.5621870882740448,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ],
        "main_score": 0.5621870882740448,
        "precision": 0.5291854884246188,
        "recall": 0.6452569169960475
      },
      {
        "accuracy": 0.9970355731225297,
        "f1": 0.9960474308300395,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.9960474308300395,
        "precision": 0.9955533596837944,
        "recall": 0.9970355731225297
      },
      {
        "accuracy": 0.9644268774703557,
        "f1": 0.9532279314888011,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ],
        "main_score": 0.9532279314888011,
        "precision": 0.947957839262187,
        "recall": 0.9644268774703557
      },
      {
        "accuracy": 0.7203557312252964,
        "f1": 0.6637022397891963,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ],
        "main_score": 0.6637022397891963,
        "precision": 0.6396950271950271,
        "recall": 0.7203557312252964
      },
      {
        "accuracy": 0.6590909090909091,
        "f1": 0.5834258736432649,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ],
        "main_score": 0.5834258736432649,
        "precision": 0.5529561923583662,
        "recall": 0.6590909090909091
      },
      {
        "accuracy": 0.6096837944664032,
        "f1": 0.5325475249388293,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.5325475249388293,
        "precision": 0.5026232825145869,
        "recall": 0.6096837944664032
      },
      {
        "accuracy": 0.5800395256916996,
        "f1": 0.49845511866262854,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ],
        "main_score": 0.49845511866262854,
        "precision": 0.4667639124160863,
        "recall": 0.5800395256916996
      },
      {
        "accuracy": 0.8715415019762845,
        "f1": 0.8373353096179184,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.8373353096179184,
        "precision": 0.8221437982307547,
        "recall": 0.8715415019762845
      },
      {
        "accuracy": 0.8023715415019763,
        "f1": 0.7486166007905137,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ],
        "main_score": 0.7486166007905137,
        "precision": 0.7254446640316207,
        "recall": 0.8023715415019763
      },
      {
        "accuracy": 0.35375494071146246,
        "f1": 0.29304220921809854,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.29304220921809854,
        "precision": 0.2741547172827689,
        "recall": 0.35375494071146246
      },
      {
        "accuracy": 0.35375494071146246,
        "f1": 0.278372834894574,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ],
        "main_score": 0.278372834894574,
        "precision": 0.25436298001515395,
        "recall": 0.35375494071146246
      },
      {
        "accuracy": 0.5632411067193676,
        "f1": 0.4926470378545478,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.4926470378545478,
        "precision": 0.46659648816269367,
        "recall": 0.5632411067193676
      },
      {
        "accuracy": 0.4733201581027668,
        "f1": 0.39018525940462695,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ],
        "main_score": 0.39018525940462695,
        "precision": 0.36184715591830213,
        "recall": 0.4733201581027668
      },
      {
        "accuracy": 0.9664031620553359,
        "f1": 0.9556982872200264,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.9556982872200264,
        "precision": 0.950592885375494,
        "recall": 0.9664031620553359
      },
      {
        "accuracy": 0.8952569169960475,
        "f1": 0.8655467720685112,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ],
        "main_score": 0.8655467720685112,
        "precision": 0.8522727272727273,
        "recall": 0.8952569169960475
      },
      {
        "accuracy": 0.6363636363636364,
        "f1": 0.5730911600476818,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ],
        "main_score": 0.5730911600476818,
        "precision": 0.5492604617604617,
        "recall": 0.6363636363636364
      },
      {
        "accuracy": 0.5770750988142292,
        "f1": 0.4884873580525754,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ],
        "main_score": 0.4884873580525754,
        "precision": 0.45433370976849236,
        "recall": 0.5770750988142292
      },
      {
        "accuracy": 0.9239130434782609,
        "f1": 0.903743961352657,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ],
        "main_score": 0.903743961352657,
        "precision": 0.8948040184453229,
        "recall": 0.9239130434782609
      },
      {
        "accuracy": 0.8448616600790514,
        "f1": 0.801482213438735,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ],
        "main_score": 0.801482213438735,
        "precision": 0.7824769433465086,
        "recall": 0.8448616600790514
      },
      {
        "accuracy": 0.8754940711462451,
        "f1": 0.8448451910408432,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ],
        "main_score": 0.8448451910408432,
        "precision": 0.8315715740122855,
        "recall": 0.8754940711462451
      },
      {
        "accuracy": 0.8033596837944664,
        "f1": 0.7507465963987704,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ],
        "main_score": 0.7507465963987704,
        "precision": 0.7282855731225296,
        "recall": 0.8033596837944664
      },
      {
        "accuracy": 0.3132411067193676,
        "f1": 0.26711703426130307,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ],
        "main_score": 0.26711703426130307,
        "precision": 0.2539317732666146,
        "recall": 0.3132411067193676
      },
      {
        "accuracy": 0.33300395256917,
        "f1": 0.2525520592714269,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ],
        "main_score": 0.2525520592714269,
        "precision": 0.22805936652280526,
        "recall": 0.33300395256917
      },
      {
        "accuracy": 0.5889328063241107,
        "f1": 0.5280071348439017,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ],
        "main_score": 0.5280071348439017,
        "precision": 0.5060406677043796,
        "recall": 0.5889328063241107
      },
      {
        "accuracy": 0.5523715415019763,
        "f1": 0.46569185645272604,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ],
        "main_score": 0.46569185645272604,
        "precision": 0.4341191417278374,
        "recall": 0.5523715415019763
      },
      {
        "accuracy": 0.6492094861660079,
        "f1": 0.5884475463439658,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ],
        "main_score": 0.5884475463439658,
        "precision": 0.5652771895978418,
        "recall": 0.6492094861660079
      },
      {
        "accuracy": 0.5711462450592886,
        "f1": 0.476661020139281,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ],
        "main_score": 0.476661020139281,
        "precision": 0.44026679841897226,
        "recall": 0.5711462450592886
      },
      {
        "accuracy": 0.8517786561264822,
        "f1": 0.8145939205721815,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ],
        "main_score": 0.8145939205721815,
        "precision": 0.7987358052575443,
        "recall": 0.8517786561264822
      },
      {
        "accuracy": 0.7737154150197628,
        "f1": 0.7158973273103708,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ],
        "main_score": 0.7158973273103708,
        "precision": 0.6920219273480144,
        "recall": 0.7737154150197628
      },
      {
        "accuracy": 0.9772727272727273,
        "f1": 0.9708498023715415,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.9708498023715415,
        "precision": 0.9677206851119894,
        "recall": 0.9772727272727273
      },
      {
        "accuracy": 0.9229249011857708,
        "f1": 0.9004281949934124,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ],
        "main_score": 0.9004281949934124,
        "precision": 0.8900691699604744,
        "recall": 0.9229249011857708
      },
      {
        "accuracy": 0.983201581027668,
        "f1": 0.9780961791831356,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ],
        "main_score": 0.9780961791831356,
        "precision": 0.9757905138339921,
        "recall": 0.983201581027668
      },
      {
        "accuracy": 0.9367588932806324,
        "f1": 0.9167325428194993,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ],
        "main_score": 0.9167325428194993,
        "precision": 0.9071969696969696,
        "recall": 0.9367588932806324
      },
      {
        "accuracy": 0.05434782608695652,
        "f1": 0.04461134860482227,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ],
        "main_score": 0.04461134860482227,
        "precision": 0.042325522434899565,
        "recall": 0.05434782608695652
      },
      {
        "accuracy": 0.07114624505928854,
        "f1": 0.02932836944063132,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ],
        "main_score": 0.02932836944063132,
        "precision": 0.022538280813388527,
        "recall": 0.07114624505928854
      },
      {
        "accuracy": 0.13043478260869565,
        "f1": 0.10291909585675536,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ],
        "main_score": 0.10291909585675536,
        "precision": 0.09620758822701969,
        "recall": 0.13043478260869565
      },
      {
        "accuracy": 0.17193675889328064,
        "f1": 0.11497961660133248,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ],
        "main_score": 0.11497961660133248,
        "precision": 0.09942426924932234,
        "recall": 0.17193675889328064
      },
      {
        "accuracy": 0.9624505928853755,
        "f1": 0.9512516469038208,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ],
        "main_score": 0.9512516469038208,
        "precision": 0.9459815546772069,
        "recall": 0.9624505928853755
      },
      {
        "accuracy": 0.8972332015810277,
        "f1": 0.8663702239789196,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ],
        "main_score": 0.8663702239789196,
        "precision": 0.8522727272727273,
        "recall": 0.8972332015810277
      },
      {
        "accuracy": 0.6541501976284585,
        "f1": 0.5943480048717203,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ],
        "main_score": 0.5943480048717203,
        "precision": 0.5718336313445008,
        "recall": 0.6541501976284585
      },
      {
        "accuracy": 0.599802371541502,
        "f1": 0.514486479703871,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ],
        "main_score": 0.514486479703871,
        "precision": 0.48206168831168833,
        "recall": 0.599802371541502
      },
      {
        "accuracy": 0.9486166007905138,
        "f1": 0.9344861660079051,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ],
        "main_score": 0.9344861660079051,
        "precision": 0.92850790513834,
        "recall": 0.9486166007905138
      },
      {
        "accuracy": 0.8952569169960475,
        "f1": 0.8644927536231883,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ],
        "main_score": 0.8644927536231883,
        "precision": 0.8507081686429512,
        "recall": 0.8952569169960475
      },
      {
        "accuracy": 0.9397233201581028,
        "f1": 0.9214097496706193,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ],
        "main_score": 0.9214097496706193,
        "precision": 0.912878787878788,
        "recall": 0.9397233201581028
      },
      {
        "accuracy": 0.8695652173913043,
        "f1": 0.8310935441370223,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ],
        "main_score": 0.8310935441370223,
        "precision": 0.8137187088274045,
        "recall": 0.8695652173913043
      },
      {
        "accuracy": 0.9377470355731226,
        "f1": 0.9215894119056174,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ],
        "main_score": 0.9215894119056174,
        "precision": 0.9144927536231884,
        "recall": 0.9377470355731226
      },
      {
        "accuracy": 0.8774703557312253,
        "f1": 0.8412549407114625,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ],
        "main_score": 0.8412549407114625,
        "precision": 0.8247459062676454,
        "recall": 0.8774703557312253
      },
      {
        "accuracy": 0.17193675889328064,
        "f1": 0.13625420328076687,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ],
        "main_score": 0.13625420328076687,
        "precision": 0.12776532871180393,
        "recall": 0.17193675889328064
      },
      {
        "accuracy": 0.20454545454545456,
        "f1": 0.13295764930528572,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ],
        "main_score": 0.13295764930528572,
        "precision": 0.1135605953664254,
        "recall": 0.20454545454545456
      },
      {
        "accuracy": 0.9644268774703557,
        "f1": 0.953129117259552,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ],
        "main_score": 0.953129117259552,
        "precision": 0.9477108036890646,
        "recall": 0.9644268774703557
      },
      {
        "accuracy": 0.8992094861660079,
        "f1": 0.8678194993412385,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ],
        "main_score": 0.8678194993412385,
        "precision": 0.8528491436100131,
        "recall": 0.8992094861660079
      },
      {
        "accuracy": 0.9575098814229249,
        "f1": 0.9451251646903821,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ],
        "main_score": 0.9451251646903821,
        "precision": 0.9395586297760211,
        "recall": 0.9575098814229249
      },
      {
        "accuracy": 0.9041501976284585,
        "f1": 0.8746376811594203,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ],
        "main_score": 0.8746376811594203,
        "precision": 0.8610013175230565,
        "recall": 0.9041501976284585
      },
      {
        "accuracy": 0.0266798418972332,
        "f1": 0.020022228486282708,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ],
        "main_score": 0.020022228486282708,
        "precision": 0.019152098339212473,
        "recall": 0.0266798418972332
      },
      {
        "accuracy": 0.03260869565217391,
        "f1": 0.007688932321701037,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ],
        "main_score": 0.007688932321701037,
        "precision": 0.005451709794800294,
        "recall": 0.03260869565217391
      }
    ],
    "validation": [
      {
        "accuracy": 0.9157472417251755,
        "f1": 0.891909060514878,
        "hf_subset": "ben-eng",
        "languages": [
          "ben-Beng",
          "eng-Latn"
        ],
        "main_score": 0.891909060514878,
        "precision": 0.8808926780341023,
        "recall": 0.9157472417251755
      },
      {
        "accuracy": 0.8254764292878636,
        "f1": 0.7758656923150402,
        "hf_subset": "eng-ben",
        "languages": [
          "eng-Latn",
          "ben-Beng"
        ],
        "main_score": 0.7758656923150402,
        "precision": 0.753961885656971,
        "recall": 0.8254764292878636
      },
      {
        "accuracy": 0.7352056168505516,
        "f1": 0.676286000859722,
        "hf_subset": "guj-eng",
        "languages": [
          "guj-Gujr",
          "eng-Latn"
        ],
        "main_score": 0.676286000859722,
        "precision": 0.6516661094394294,
        "recall": 0.7352056168505516
      },
      {
        "accuracy": 0.6710130391173521,
        "f1": 0.5923525265551343,
        "hf_subset": "eng-guj",
        "languages": [
          "eng-Latn",
          "guj-Gujr"
        ],
        "main_score": 0.5923525265551343,
        "precision": 0.5605089076754072,
        "recall": 0.6710130391173521
      },
      {
        "accuracy": 0.9969909729187563,
        "f1": 0.9961551320628552,
        "hf_subset": "hin-eng",
        "languages": [
          "hin-Deva",
          "eng-Latn"
        ],
        "main_score": 0.9961551320628552,
        "precision": 0.9958207957204949,
        "recall": 0.9969909729187563
      },
      {
        "accuracy": 0.9699097291875627,
        "f1": 0.9605483116014711,
        "hf_subset": "eng-hin",
        "languages": [
          "eng-Latn",
          "hin-Deva"
        ],
        "main_score": 0.9605483116014711,
        "precision": 0.9562019391507857,
        "recall": 0.9699097291875627
      },
      {
        "accuracy": 0.6990972918756269,
        "f1": 0.6319920077693397,
        "hf_subset": "kan-eng",
        "languages": [
          "kan-Knda",
          "eng-Latn"
        ],
        "main_score": 0.6319920077693397,
        "precision": 0.6039463628982183,
        "recall": 0.6990972918756269
      },
      {
        "accuracy": 0.6529588766298897,
        "f1": 0.578518962370517,
        "hf_subset": "eng-kan",
        "languages": [
          "eng-Latn",
          "kan-Knda"
        ],
        "main_score": 0.578518962370517,
        "precision": 0.5494674499689546,
        "recall": 0.6529588766298897
      },
      {
        "accuracy": 0.6228686058174524,
        "f1": 0.5612327825968748,
        "hf_subset": "mal-eng",
        "languages": [
          "mal-Mlym",
          "eng-Latn"
        ],
        "main_score": 0.5612327825968748,
        "precision": 0.5375758588898006,
        "recall": 0.6228686058174524
      },
      {
        "accuracy": 0.5847542627883651,
        "f1": 0.5007785260543536,
        "hf_subset": "eng-mal",
        "languages": [
          "eng-Latn",
          "mal-Mlym"
        ],
        "main_score": 0.5007785260543536,
        "precision": 0.4684856155768894,
        "recall": 0.5847542627883651
      },
      {
        "accuracy": 0.876629889669007,
        "f1": 0.8421598127716483,
        "hf_subset": "mar-eng",
        "languages": [
          "mar-Deva",
          "eng-Latn"
        ],
        "main_score": 0.8421598127716483,
        "precision": 0.8263457037780008,
        "recall": 0.876629889669007
      },
      {
        "accuracy": 0.7953861584754263,
        "f1": 0.7417307477989523,
        "hf_subset": "eng-mar",
        "languages": [
          "eng-Latn",
          "mar-Deva"
        ],
        "main_score": 0.7417307477989523,
        "precision": 0.7184565601566605,
        "recall": 0.7953861584754263
      },
      {
        "accuracy": 0.39418254764292876,
        "f1": 0.33329495089775935,
        "hf_subset": "tam-eng",
        "languages": [
          "tam-Taml",
          "eng-Latn"
        ],
        "main_score": 0.33329495089775935,
        "precision": 0.31266099886962473,
        "recall": 0.39418254764292876
      },
      {
        "accuracy": 0.36810431293881646,
        "f1": 0.28924406697726657,
        "hf_subset": "eng-tam",
        "languages": [
          "eng-Latn",
          "tam-Taml"
        ],
        "main_score": 0.28924406697726657,
        "precision": 0.26364089771812943,
        "recall": 0.36810431293881646
      },
      {
        "accuracy": 0.5977933801404213,
        "f1": 0.5238999985086069,
        "hf_subset": "tel-eng",
        "languages": [
          "tel-Telu",
          "eng-Latn"
        ],
        "main_score": 0.5238999985086069,
        "precision": 0.49540786862752767,
        "recall": 0.5977933801404213
      },
      {
        "accuracy": 0.49147442326980945,
        "f1": 0.401190078171021,
        "hf_subset": "eng-tel",
        "languages": [
          "eng-Latn",
          "tel-Telu"
        ],
        "main_score": 0.401190078171021,
        "precision": 0.3704597920746366,
        "recall": 0.49147442326980945
      },
      {
        "accuracy": 0.9689067201604814,
        "f1": 0.9587094617184888,
        "hf_subset": "urd-eng",
        "languages": [
          "urd-Arab",
          "eng-Latn"
        ],
        "main_score": 0.9587094617184888,
        "precision": 0.9536944165830826,
        "recall": 0.9689067201604814
      },
      {
        "accuracy": 0.9047141424272819,
        "f1": 0.8746238716148446,
        "hf_subset": "eng-urd",
        "languages": [
          "eng-Latn",
          "urd-Arab"
        ],
        "main_score": 0.8746238716148446,
        "precision": 0.8602474088933467,
        "recall": 0.9047141424272819
      },
      {
        "accuracy": 0.6720160481444333,
        "f1": 0.6077008804190348,
        "hf_subset": "asm-eng",
        "languages": [
          "asm-Beng",
          "eng-Latn"
        ],
        "main_score": 0.6077008804190348,
        "precision": 0.5812692980903494,
        "recall": 0.6720160481444333
      },
      {
        "accuracy": 0.6068204613841525,
        "f1": 0.5276535957077582,
        "hf_subset": "eng-asm",
        "languages": [
          "eng-Latn",
          "asm-Beng"
        ],
        "main_score": 0.5276535957077582,
        "precision": 0.497805320724077,
        "recall": 0.6068204613841525
      },
      {
        "accuracy": 0.9398194583751254,
        "f1": 0.9205951186894015,
        "hf_subset": "bho-eng",
        "languages": [
          "bho-Deva",
          "eng-Latn"
        ],
        "main_score": 0.9205951186894015,
        "precision": 0.9110665329321297,
        "recall": 0.9398194583751254
      },
      {
        "accuracy": 0.8385155466399198,
        "f1": 0.7916941300090747,
        "hf_subset": "eng-bho",
        "languages": [
          "eng-Latn",
          "bho-Deva"
        ],
        "main_score": 0.7916941300090747,
        "precision": 0.7705616850551655,
        "recall": 0.8385155466399198
      },
      {
        "accuracy": 0.8916750250752257,
        "f1": 0.8590437980608493,
        "hf_subset": "nep-eng",
        "languages": [
          "nep-Deva",
          "eng-Latn"
        ],
        "main_score": 0.8590437980608493,
        "precision": 0.8438649281176864,
        "recall": 0.8916750250752257
      },
      {
        "accuracy": 0.7923771313941825,
        "f1": 0.7355924917609974,
        "hf_subset": "eng-nep",
        "languages": [
          "eng-Latn",
          "nep-Deva"
        ],
        "main_score": 0.7355924917609974,
        "precision": 0.7110665329321297,
        "recall": 0.7923771313941825
      },
      {
        "accuracy": 0.32196589769307926,
        "f1": 0.2638627245592665,
        "hf_subset": "ory-eng",
        "languages": [
          "ory-Orya",
          "eng-Latn"
        ],
        "main_score": 0.2638627245592665,
        "precision": 0.2458155166006662,
        "recall": 0.32196589769307926
      },
      {
        "accuracy": 0.3410230692076229,
        "f1": 0.25587816841578126,
        "hf_subset": "eng-ory",
        "languages": [
          "eng-Latn",
          "ory-Orya"
        ],
        "main_score": 0.25587816841578126,
        "precision": 0.2286717044239612,
        "recall": 0.3410230692076229
      },
      {
        "accuracy": 0.6429287863590772,
        "f1": 0.5847734196377953,
        "hf_subset": "pan-eng",
        "languages": [
          "pan-Guru",
          "eng-Latn"
        ],
        "main_score": 0.5847734196377953,
        "precision": 0.5636761655818827,
        "recall": 0.6429287863590772
      },
      {
        "accuracy": 0.5877632898696088,
        "f1": 0.4994415787043671,
        "hf_subset": "eng-pan",
        "languages": [
          "eng-Latn",
          "pan-Guru"
        ],
        "main_score": 0.4994415787043671,
        "precision": 0.46567321010650997,
        "recall": 0.5877632898696088
      },
      {
        "accuracy": 0.6409227683049148,
        "f1": 0.5783748588761454,
        "hf_subset": "pus-eng",
        "languages": [
          "pus-Arab",
          "eng-Latn"
        ],
        "main_score": 0.5783748588761454,
        "precision": 0.5542900707895693,
        "recall": 0.6409227683049148
      },
      {
        "accuracy": 0.5656970912738215,
        "f1": 0.47017560618363025,
        "hf_subset": "eng-pus",
        "languages": [
          "eng-Latn",
          "pus-Arab"
        ],
        "main_score": 0.47017560618363025,
        "precision": 0.43360916081578066,
        "recall": 0.5656970912738215
      },
      {
        "accuracy": 0.843530591775326,
        "f1": 0.804618617758036,
        "hf_subset": "san-eng",
        "languages": [
          "san-Deva",
          "eng-Latn"
        ],
        "main_score": 0.804618617758036,
        "precision": 0.7885489802741559,
        "recall": 0.843530591775326
      },
      {
        "accuracy": 0.7402206619859579,
        "f1": 0.6742274442374743,
        "hf_subset": "eng-san",
        "languages": [
          "eng-Latn",
          "san-Deva"
        ],
        "main_score": 0.6742274442374743,
        "precision": 0.6463557338682716,
        "recall": 0.7402206619859579
      },
      {
        "accuracy": 0.9759277833500501,
        "f1": 0.9689735874289535,
        "hf_subset": "awa-eng",
        "languages": [
          "awa-Deva",
          "eng-Latn"
        ],
        "main_score": 0.9689735874289535,
        "precision": 0.9656469408224674,
        "recall": 0.9759277833500501
      },
      {
        "accuracy": 0.9287863590772317,
        "f1": 0.9076228686058175,
        "hf_subset": "eng-awa",
        "languages": [
          "eng-Latn",
          "awa-Deva"
        ],
        "main_score": 0.9076228686058175,
        "precision": 0.8977766633233033,
        "recall": 0.9287863590772317
      },
      {
        "accuracy": 0.966900702106319,
        "f1": 0.9565362754931461,
        "hf_subset": "bgc-eng",
        "languages": [
          "bgc-Deva",
          "eng-Latn"
        ],
        "main_score": 0.9565362754931461,
        "precision": 0.9515212303577398,
        "recall": 0.966900702106319
      },
      {
        "accuracy": 0.9167502507522568,
        "f1": 0.8912403878301571,
        "hf_subset": "eng-bgc",
        "languages": [
          "eng-Latn",
          "bgc-Deva"
        ],
        "main_score": 0.8912403878301571,
        "precision": 0.8794215981277165,
        "recall": 0.9167502507522568
      },
      {
        "accuracy": 0.06118355065195587,
        "f1": 0.050726412508617764,
        "hf_subset": "bod-eng",
        "languages": [
          "bod-Tibt",
          "eng-Latn"
        ],
        "main_score": 0.050726412508617764,
        "precision": 0.047915669927311985,
        "recall": 0.06118355065195587
      },
      {
        "accuracy": 0.08625877632898696,
        "f1": 0.039923389683675384,
        "hf_subset": "eng-bod",
        "languages": [
          "eng-Latn",
          "bod-Tibt"
        ],
        "main_score": 0.039923389683675384,
        "precision": 0.03138413246852556,
        "recall": 0.08625877632898696
      },
      {
        "accuracy": 0.18756268806419257,
        "f1": 0.14574351385342643,
        "hf_subset": "boy-eng",
        "languages": [
          "boy-Deva",
          "eng-Latn"
        ],
        "main_score": 0.14574351385342643,
        "precision": 0.1344536246988469,
        "recall": 0.18756268806419257
      },
      {
        "accuracy": 0.1995987963891675,
        "f1": 0.13434013952282312,
        "hf_subset": "eng-boy",
        "languages": [
          "eng-Latn",
          "boy-Deva"
        ],
        "main_score": 0.13434013952282312,
        "precision": 0.11617509359551276,
        "recall": 0.1995987963891675
      },
      {
        "accuracy": 0.9528585757271816,
        "f1": 0.9378134403209629,
        "hf_subset": "gbm-eng",
        "languages": [
          "gbm-Deva",
          "eng-Latn"
        ],
        "main_score": 0.9378134403209629,
        "precision": 0.9304580407890337,
        "recall": 0.9528585757271816
      },
      {
        "accuracy": 0.892678034102307,
        "f1": 0.8603142761618188,
        "hf_subset": "eng-gbm",
        "languages": [
          "eng-Latn",
          "gbm-Deva"
        ],
        "main_score": 0.8603142761618188,
        "precision": 0.8454864593781345,
        "recall": 0.892678034102307
      },
      {
        "accuracy": 0.6609829488465396,
        "f1": 0.5999530337042874,
        "hf_subset": "gom-eng",
        "languages": [
          "gom-Deva",
          "eng-Latn"
        ],
        "main_score": 0.5999530337042874,
        "precision": 0.5760627119453599,
        "recall": 0.6609829488465396
      },
      {
        "accuracy": 0.5807422266800402,
        "f1": 0.4979996265853839,
        "hf_subset": "eng-gom",
        "languages": [
          "eng-Latn",
          "gom-Deva"
        ],
        "main_score": 0.4979996265853839,
        "precision": 0.46637200980230065,
        "recall": 0.5807422266800402
      },
      {
        "accuracy": 0.9368104312938816,
        "f1": 0.9173186225342695,
        "hf_subset": "hne-eng",
        "languages": [
          "hne-Deva",
          "eng-Latn"
        ],
        "main_score": 0.9173186225342695,
        "precision": 0.9081410899364761,
        "recall": 0.9368104312938816
      },
      {
        "accuracy": 0.872617853560682,
        "f1": 0.8339017051153459,
        "hf_subset": "eng-hne",
        "languages": [
          "eng-Latn",
          "hne-Deva"
        ],
        "main_score": 0.8339017051153459,
        "precision": 0.8158976930792377,
        "recall": 0.872617853560682
      },
      {
        "accuracy": 0.9488465396188566,
        "f1": 0.9335339351387496,
        "hf_subset": "raj-eng",
        "languages": [
          "raj-Deva",
          "eng-Latn"
        ],
        "main_score": 0.9335339351387496,
        "precision": 0.9263624205951186,
        "recall": 0.9488465396188566
      },
      {
        "accuracy": 0.8645937813440321,
        "f1": 0.8239050484787696,
        "hf_subset": "eng-raj",
        "languages": [
          "eng-Latn",
          "raj-Deva"
        ],
        "main_score": 0.8239050484787696,
        "precision": 0.8054162487462387,
        "recall": 0.8645937813440321
      },
      {
        "accuracy": 0.9518555667001003,
        "f1": 0.9372116349047142,
        "hf_subset": "mai-eng",
        "languages": [
          "mai-Deva",
          "eng-Latn"
        ],
        "main_score": 0.9372116349047142,
        "precision": 0.9302072885322635,
        "recall": 0.9518555667001003
      },
      {
        "accuracy": 0.8665997993981945,
        "f1": 0.827415580073554,
        "hf_subset": "eng-mai",
        "languages": [
          "eng-Latn",
          "mai-Deva"
        ],
        "main_score": 0.827415580073554,
        "precision": 0.80962888665998,
        "recall": 0.8665997993981945
      },
      {
        "accuracy": 0.18355065195586762,
        "f1": 0.1472042938232442,
        "hf_subset": "mni-eng",
        "languages": [
          "mni-Mtei",
          "eng-Latn"
        ],
        "main_score": 0.1472042938232442,
        "precision": 0.1382256827415555,
        "recall": 0.18355065195586762
      },
      {
        "accuracy": 0.20361083249749248,
        "f1": 0.1323121824124833,
        "hf_subset": "eng-mni",
        "languages": [
          "eng-Latn",
          "mni-Mtei"
        ],
        "main_score": 0.1323121824124833,
        "precision": 0.11403455009272462,
        "recall": 0.20361083249749248
      },
      {
        "accuracy": 0.9638916750250752,
        "f1": 0.9534269475091942,
        "hf_subset": "mup-eng",
        "languages": [
          "mup-Deva",
          "eng-Latn"
        ],
        "main_score": 0.9534269475091942,
        "precision": 0.9484286191909059,
        "recall": 0.9638916750250752
      },
      {
        "accuracy": 0.8936810431293881,
        "f1": 0.8630558341691742,
        "hf_subset": "eng-mup",
        "languages": [
          "eng-Latn",
          "mup-Deva"
        ],
        "main_score": 0.8630558341691742,
        "precision": 0.8490805750585089,
        "recall": 0.8936810431293881
      },
      {
        "accuracy": 0.9689067201604814,
        "f1": 0.9597793380140421,
        "hf_subset": "mwr-eng",
        "languages": [
          "mwr-Deva",
          "eng-Latn"
        ],
        "main_score": 0.9597793380140421,
        "precision": 0.9554496823804748,
        "recall": 0.9689067201604814
      },
      {
        "accuracy": 0.8896690070210632,
        "f1": 0.8555332664660649,
        "hf_subset": "eng-mwr",
        "languages": [
          "eng-Latn",
          "mwr-Deva"
        ],
        "main_score": 0.8555332664660649,
        "precision": 0.8396857238381812,
        "recall": 0.8896690070210632
      },
      {
        "accuracy": 0.014042126379137413,
        "f1": 0.01166595023164732,
        "hf_subset": "sat-eng",
        "languages": [
          "sat-Olck",
          "eng-Latn"
        ],
        "main_score": 0.01166595023164732,
        "precision": 0.011200849936569288,
        "recall": 0.014042126379137413
      },
      {
        "accuracy": 0.028084252758274825,
        "f1": 0.006057038635407349,
        "hf_subset": "eng-sat",
        "languages": [
          "eng-Latn",
          "sat-Olck"
        ],
        "main_score": 0.006057038635407349,
        "precision": 0.004160220660212228,
        "recall": 0.028084252758274825
      }
    ]
  },
  "task_name": "IndicGenBenchFloresBitextMining"
}