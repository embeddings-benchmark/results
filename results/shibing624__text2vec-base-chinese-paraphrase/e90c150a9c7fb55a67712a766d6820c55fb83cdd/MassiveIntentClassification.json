{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.350959,
        "f1": 0.329253,
        "f1_weighted": 0.361253,
        "scores_per_experiment": [
          {
            "accuracy": 0.368913,
            "f1": 0.336468,
            "f1_weighted": 0.380772
          },
          {
            "accuracy": 0.352681,
            "f1": 0.32266,
            "f1_weighted": 0.360867
          },
          {
            "accuracy": 0.328087,
            "f1": 0.311631,
            "f1_weighted": 0.336883
          },
          {
            "accuracy": 0.35514,
            "f1": 0.326735,
            "f1_weighted": 0.367581
          },
          {
            "accuracy": 0.372848,
            "f1": 0.348446,
            "f1_weighted": 0.388766
          },
          {
            "accuracy": 0.352189,
            "f1": 0.334791,
            "f1_weighted": 0.367587
          },
          {
            "accuracy": 0.350713,
            "f1": 0.328762,
            "f1_weighted": 0.35376
          },
          {
            "accuracy": 0.342843,
            "f1": 0.320264,
            "f1_weighted": 0.356212
          },
          {
            "accuracy": 0.339892,
            "f1": 0.330378,
            "f1_weighted": 0.342666
          },
          {
            "accuracy": 0.346286,
            "f1": 0.332391,
            "f1_weighted": 0.357435
          }
        ],
        "main_score": 0.350959,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.354035,
        "f1": 0.33177,
        "f1_weighted": 0.364906,
        "scores_per_experiment": [
          {
            "accuracy": 0.376597,
            "f1": 0.34861,
            "f1_weighted": 0.386169
          },
          {
            "accuracy": 0.356086,
            "f1": 0.32248,
            "f1_weighted": 0.365749
          },
          {
            "accuracy": 0.338937,
            "f1": 0.316826,
            "f1_weighted": 0.346053
          },
          {
            "accuracy": 0.358776,
            "f1": 0.325729,
            "f1_weighted": 0.373706
          },
          {
            "accuracy": 0.366846,
            "f1": 0.347762,
            "f1_weighted": 0.379033
          },
          {
            "accuracy": 0.364492,
            "f1": 0.346112,
            "f1_weighted": 0.380633
          },
          {
            "accuracy": 0.344317,
            "f1": 0.327236,
            "f1_weighted": 0.350535
          },
          {
            "accuracy": 0.345662,
            "f1": 0.329027,
            "f1_weighted": 0.359255
          },
          {
            "accuracy": 0.340619,
            "f1": 0.324679,
            "f1_weighted": 0.352054
          },
          {
            "accuracy": 0.348016,
            "f1": 0.329239,
            "f1_weighted": 0.355869
          }
        ],
        "main_score": 0.354035,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 32.925318479537964,
  "kg_co2_emissions": null
}