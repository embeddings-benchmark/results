{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "task_name": "MassiveScenarioClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.35878,
        "f1": 0.348964,
        "f1_weighted": 0.365157,
        "scores_per_experiment": [
          {
            "accuracy": 0.395967,
            "f1": 0.385223,
            "f1_weighted": 0.397139
          },
          {
            "accuracy": 0.368913,
            "f1": 0.358681,
            "f1_weighted": 0.373065
          },
          {
            "accuracy": 0.374324,
            "f1": 0.358336,
            "f1_weighted": 0.382262
          },
          {
            "accuracy": 0.324643,
            "f1": 0.311029,
            "f1_weighted": 0.340132
          },
          {
            "accuracy": 0.354648,
            "f1": 0.348055,
            "f1_weighted": 0.367608
          },
          {
            "accuracy": 0.352681,
            "f1": 0.344095,
            "f1_weighted": 0.356676
          },
          {
            "accuracy": 0.36301,
            "f1": 0.356774,
            "f1_weighted": 0.367447
          },
          {
            "accuracy": 0.367437,
            "f1": 0.367647,
            "f1_weighted": 0.370076
          },
          {
            "accuracy": 0.3394,
            "f1": 0.326595,
            "f1_weighted": 0.341594
          },
          {
            "accuracy": 0.346778,
            "f1": 0.333204,
            "f1_weighted": 0.355573
          }
        ],
        "main_score": 0.35878,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.373974,
        "f1": 0.353829,
        "f1_weighted": 0.380519,
        "scores_per_experiment": [
          {
            "accuracy": 0.410222,
            "f1": 0.38919,
            "f1_weighted": 0.412968
          },
          {
            "accuracy": 0.381641,
            "f1": 0.366611,
            "f1_weighted": 0.3871
          },
          {
            "accuracy": 0.389711,
            "f1": 0.363357,
            "f1_weighted": 0.397071
          },
          {
            "accuracy": 0.366174,
            "f1": 0.332029,
            "f1_weighted": 0.381362
          },
          {
            "accuracy": 0.361802,
            "f1": 0.343187,
            "f1_weighted": 0.373566
          },
          {
            "accuracy": 0.375925,
            "f1": 0.357024,
            "f1_weighted": 0.375088
          },
          {
            "accuracy": 0.363147,
            "f1": 0.345132,
            "f1_weighted": 0.369924
          },
          {
            "accuracy": 0.386012,
            "f1": 0.375715,
            "f1_weighted": 0.390075
          },
          {
            "accuracy": 0.350034,
            "f1": 0.332261,
            "f1_weighted": 0.355221
          },
          {
            "accuracy": 0.355077,
            "f1": 0.333778,
            "f1_weighted": 0.36282
          }
        ],
        "main_score": 0.373974,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 14.134803533554077,
  "kg_co2_emissions": null
}