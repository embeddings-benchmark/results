{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.25.8",
  "scores": {
    "validation": [
      {
        "accuracy": 0.601771,
        "f1": 0.564996,
        "f1_weighted": 0.607702,
        "scores_per_experiment": [
          {
            "accuracy": 0.600098,
            "f1": 0.561911,
            "f1_weighted": 0.606242
          },
          {
            "accuracy": 0.613871,
            "f1": 0.560279,
            "f1_weighted": 0.621992
          },
          {
            "accuracy": 0.624201,
            "f1": 0.582134,
            "f1_weighted": 0.627721
          },
          {
            "accuracy": 0.614855,
            "f1": 0.562763,
            "f1_weighted": 0.618605
          },
          {
            "accuracy": 0.593212,
            "f1": 0.571446,
            "f1_weighted": 0.599454
          },
          {
            "accuracy": 0.599115,
            "f1": 0.558911,
            "f1_weighted": 0.606101
          },
          {
            "accuracy": 0.616331,
            "f1": 0.584888,
            "f1_weighted": 0.621064
          },
          {
            "accuracy": 0.586818,
            "f1": 0.544896,
            "f1_weighted": 0.590512
          },
          {
            "accuracy": 0.578947,
            "f1": 0.564935,
            "f1_weighted": 0.584257
          },
          {
            "accuracy": 0.590261,
            "f1": 0.5578,
            "f1_weighted": 0.601074
          }
        ],
        "main_score": 0.601771,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.60074,
        "f1": 0.563031,
        "f1_weighted": 0.607534,
        "scores_per_experiment": [
          {
            "accuracy": 0.608944,
            "f1": 0.564417,
            "f1_weighted": 0.617414
          },
          {
            "accuracy": 0.600874,
            "f1": 0.55863,
            "f1_weighted": 0.60917
          },
          {
            "accuracy": 0.594822,
            "f1": 0.556485,
            "f1_weighted": 0.600783
          },
          {
            "accuracy": 0.616678,
            "f1": 0.562651,
            "f1_weighted": 0.621844
          },
          {
            "accuracy": 0.593477,
            "f1": 0.567215,
            "f1_weighted": 0.597803
          },
          {
            "accuracy": 0.598857,
            "f1": 0.563555,
            "f1_weighted": 0.606075
          },
          {
            "accuracy": 0.62542,
            "f1": 0.58385,
            "f1_weighted": 0.631128
          },
          {
            "accuracy": 0.596167,
            "f1": 0.560951,
            "f1_weighted": 0.600307
          },
          {
            "accuracy": 0.580363,
            "f1": 0.55011,
            "f1_weighted": 0.591456
          },
          {
            "accuracy": 0.591796,
            "f1": 0.562441,
            "f1_weighted": 0.59936
          }
        ],
        "main_score": 0.60074,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 15.019257068634033,
  "kg_co2_emissions": null
}