{
  "dataset_revision": "fba4f2cfe2592641056f7a274c9aa8453b27a4a8",
  "evaluation_time": 29.521478414535522,
  "kg_co2_emissions": 0.0016570794122366698,
  "mteb_version": "1.18.0",
  "scores": {
    "train": [
      {
        "accuracy": 0.404,
        "f1": 0.3483002699055331,
        "hf_subset": "eng-ace",
        "languages": [
          "eng-Latn",
          "ace-Latn"
        ],
        "main_score": 0.3483002699055331,
        "precision": 0.3315131996658312,
        "recall": 0.404
      },
      {
        "accuracy": 0.416,
        "f1": 0.35525682979920264,
        "hf_subset": "eng-ban",
        "languages": [
          "eng-Latn",
          "ban-Latn"
        ],
        "main_score": 0.35525682979920264,
        "precision": 0.3347451369192748,
        "recall": 0.416
      },
      {
        "accuracy": 0.412,
        "f1": 0.35278598786834076,
        "hf_subset": "eng-bbc",
        "languages": [
          "eng-Latn",
          "bbc-Latn"
        ],
        "main_score": 0.35278598786834076,
        "precision": 0.334604238202458,
        "recall": 0.412
      },
      {
        "accuracy": 0.434,
        "f1": 0.38205228105228106,
        "hf_subset": "eng-bjn",
        "languages": [
          "eng-Latn",
          "bjn-Latn"
        ],
        "main_score": 0.38205228105228106,
        "precision": 0.3642278220804536,
        "recall": 0.434
      },
      {
        "accuracy": 0.364,
        "f1": 0.3106174603174603,
        "hf_subset": "eng-bug",
        "languages": [
          "eng-Latn",
          "bug-Latn"
        ],
        "main_score": 0.3106174603174603,
        "precision": 0.29479885263787703,
        "recall": 0.364
      },
      {
        "accuracy": 0.52,
        "f1": 0.4723456185508817,
        "hf_subset": "eng-ind",
        "languages": [
          "eng-Latn",
          "ind-Latn"
        ],
        "main_score": 0.4723456185508817,
        "precision": 0.45786875522138676,
        "recall": 0.52
      },
      {
        "accuracy": 0.442,
        "f1": 0.3913276556776557,
        "hf_subset": "eng-jav",
        "languages": [
          "eng-Latn",
          "jav-Latn"
        ],
        "main_score": 0.3913276556776557,
        "precision": 0.37578637140637133,
        "recall": 0.442
      },
      {
        "accuracy": 0.368,
        "f1": 0.3105005564005564,
        "hf_subset": "eng-mad",
        "languages": [
          "eng-Latn",
          "mad-Latn"
        ],
        "main_score": 0.3105005564005564,
        "precision": 0.2940592703592703,
        "recall": 0.368
      },
      {
        "accuracy": 0.47,
        "f1": 0.41738274798678027,
        "hf_subset": "eng-min",
        "languages": [
          "eng-Latn",
          "min-Latn"
        ],
        "main_score": 0.41738274798678027,
        "precision": 0.4022634300805778,
        "recall": 0.47
      },
      {
        "accuracy": 0.384,
        "f1": 0.33107431653314007,
        "hf_subset": "eng-nij",
        "languages": [
          "eng-Latn",
          "nij-Latn"
        ],
        "main_score": 0.33107431653314007,
        "precision": 0.3132988095238095,
        "recall": 0.384
      },
      {
        "accuracy": 0.432,
        "f1": 0.3740946301925025,
        "hf_subset": "eng-sun",
        "languages": [
          "eng-Latn",
          "sun-Latn"
        ],
        "main_score": 0.3740946301925025,
        "precision": 0.3543625258799172,
        "recall": 0.432
      }
    ]
  },
  "task_name": "NusaXBitextMining"
}