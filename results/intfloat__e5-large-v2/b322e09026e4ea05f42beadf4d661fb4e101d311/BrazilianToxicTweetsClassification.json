{
  "dataset_revision": "f333c1fcfa3ab43f008a327c8bd0140441354d34",
  "evaluation_time": 8.955863952636719,
  "kg_co2_emissions": 0.0004025222244581154,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.16923828125,
        "f1": 0.17614356649426482,
        "hf_subset": "default",
        "languages": [
          "por-Latn"
        ],
        "lrap": 0.7986918131510425,
        "main_score": 0.16923828125,
        "scores_per_experiment": [
          {
            "accuracy": 0.1259765625,
            "f1": 0.1782161430994761,
            "lrap": 0.819959852430556
          },
          {
            "accuracy": 0.14453125,
            "f1": 0.1803600161481914,
            "lrap": 0.8122965494791676
          },
          {
            "accuracy": 0.1591796875,
            "f1": 0.1645750553462934,
            "lrap": 0.8151584201388893
          },
          {
            "accuracy": 0.1669921875,
            "f1": 0.1592218411802255,
            "lrap": 0.8154839409722229
          },
          {
            "accuracy": 0.232421875,
            "f1": 0.16071810328920852,
            "lrap": 0.7821994357638896
          },
          {
            "accuracy": 0.1728515625,
            "f1": 0.1807361337569113,
            "lrap": 0.7953423394097237
          },
          {
            "accuracy": 0.212890625,
            "f1": 0.16085611952771364,
            "lrap": 0.7665269639756955
          },
          {
            "accuracy": 0.09326171875,
            "f1": 0.20466692189275912,
            "lrap": 0.8039279513888886
          },
          {
            "accuracy": 0.14697265625,
            "f1": 0.1763246626762012,
            "lrap": 0.7863091362847237
          },
          {
            "accuracy": 0.2373046875,
            "f1": 0.19576066802566813,
            "lrap": 0.7897135416666676
          }
        ]
      }
    ]
  },
  "task_name": "BrazilianToxicTweetsClassification"
}