{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "evaluation_time": 31.379591941833496,
  "kg_co2_emissions": 0.0012635922697794475,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.31240234375,
        "f1": 0.3046228034191589,
        "f1_weighted": 0.3046251716741865,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.31240234375,
        "scores_per_experiment": [
          {
            "accuracy": 0.2919921875,
            "f1": 0.2929437337050603,
            "f1_weighted": 0.2929232157535425
          },
          {
            "accuracy": 0.31787109375,
            "f1": 0.3059943258490045,
            "f1_weighted": 0.3059934588588261
          },
          {
            "accuracy": 0.33447265625,
            "f1": 0.32312595994400695,
            "f1_weighted": 0.3231358340369692
          },
          {
            "accuracy": 0.29150390625,
            "f1": 0.2908394406128717,
            "f1_weighted": 0.29081875201175045
          },
          {
            "accuracy": 0.3115234375,
            "f1": 0.3023687618698482,
            "f1_weighted": 0.30232199606028
          },
          {
            "accuracy": 0.314453125,
            "f1": 0.29881545745640764,
            "f1_weighted": 0.2988249571973803
          },
          {
            "accuracy": 0.27880859375,
            "f1": 0.2800063579632237,
            "f1_weighted": 0.28001992221051647
          },
          {
            "accuracy": 0.34326171875,
            "f1": 0.3319084923372088,
            "f1_weighted": 0.3319392490696618
          },
          {
            "accuracy": 0.333984375,
            "f1": 0.32652096682120035,
            "f1_weighted": 0.32653451189840715
          },
          {
            "accuracy": 0.30615234375,
            "f1": 0.29370453763275706,
            "f1_weighted": 0.2937398196445315
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.315380859375,
        "f1": 0.3067015007397434,
        "f1_weighted": 0.3066918994880982,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ],
        "main_score": 0.315380859375,
        "scores_per_experiment": [
          {
            "accuracy": 0.30615234375,
            "f1": 0.30578487426596984,
            "f1_weighted": 0.30575717184147494
          },
          {
            "accuracy": 0.31201171875,
            "f1": 0.29941392662657573,
            "f1_weighted": 0.29940538547203044
          },
          {
            "accuracy": 0.34814453125,
            "f1": 0.3372131036590588,
            "f1_weighted": 0.3371878992436308
          },
          {
            "accuracy": 0.32568359375,
            "f1": 0.3245081395337421,
            "f1_weighted": 0.3244902898720365
          },
          {
            "accuracy": 0.28369140625,
            "f1": 0.2759175799590163,
            "f1_weighted": 0.2758755377289465
          },
          {
            "accuracy": 0.3193359375,
            "f1": 0.30444355664645056,
            "f1_weighted": 0.3044315224568801
          },
          {
            "accuracy": 0.263671875,
            "f1": 0.26383708354629115,
            "f1_weighted": 0.2638483256512256
          },
          {
            "accuracy": 0.3330078125,
            "f1": 0.3164058803520368,
            "f1_weighted": 0.316410687854293
          },
          {
            "accuracy": 0.33935546875,
            "f1": 0.3319452500196201,
            "f1_weighted": 0.3319478191241241
          },
          {
            "accuracy": 0.32275390625,
            "f1": 0.3075456127886723,
            "f1_weighted": 0.3075643556363404
          }
        ]
      }
    ]
  },
  "task_name": "FilipinoShopeeReviewsClassification"
}