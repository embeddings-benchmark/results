{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "evaluation_time": 757.7728910446167,
  "kg_co2_emissions": 0.04398919798105713,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.08876953125,
        "f1": 0.07023501551455065,
        "f1_weighted": 0.07811008866555849,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ],
        "main_score": 0.08876953125,
        "scores_per_experiment": [
          {
            "accuracy": 0.07666015625,
            "f1": 0.05781446584844714,
            "f1_weighted": 0.06160452253191824
          },
          {
            "accuracy": 0.10009765625,
            "f1": 0.07718356473940263,
            "f1_weighted": 0.08923857240469764
          },
          {
            "accuracy": 0.0927734375,
            "f1": 0.07955026087937353,
            "f1_weighted": 0.08265359473620146
          },
          {
            "accuracy": 0.0947265625,
            "f1": 0.07163636434614844,
            "f1_weighted": 0.08119130328090504
          },
          {
            "accuracy": 0.08544921875,
            "f1": 0.06890671907399665,
            "f1_weighted": 0.08114929359102914
          },
          {
            "accuracy": 0.087890625,
            "f1": 0.06966684630372717,
            "f1_weighted": 0.08335203119724924
          },
          {
            "accuracy": 0.09521484375,
            "f1": 0.07010490279981912,
            "f1_weighted": 0.08218706153326721
          },
          {
            "accuracy": 0.08154296875,
            "f1": 0.0655192394709619,
            "f1_weighted": 0.06703157127911558
          },
          {
            "accuracy": 0.087890625,
            "f1": 0.0716045337970666,
            "f1_weighted": 0.07588269724478261
          },
          {
            "accuracy": 0.08544921875,
            "f1": 0.07036325788656339,
            "f1_weighted": 0.07681023885641874
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.0888671875,
        "f1": 0.06301162086453102,
        "f1_weighted": 0.08131261199372307,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ],
        "main_score": 0.0888671875,
        "scores_per_experiment": [
          {
            "accuracy": 0.08203125,
            "f1": 0.05801389735089068,
            "f1_weighted": 0.07200849095152156
          },
          {
            "accuracy": 0.083984375,
            "f1": 0.05923697452110704,
            "f1_weighted": 0.08141359505056348
          },
          {
            "accuracy": 0.08984375,
            "f1": 0.06582049657624302,
            "f1_weighted": 0.08109818455631404
          },
          {
            "accuracy": 0.08837890625,
            "f1": 0.05932542741423655,
            "f1_weighted": 0.0757843270848954
          },
          {
            "accuracy": 0.08203125,
            "f1": 0.055987319828862736,
            "f1_weighted": 0.0831324310230884
          },
          {
            "accuracy": 0.0849609375,
            "f1": 0.06370375845950575,
            "f1_weighted": 0.07996561453634957
          },
          {
            "accuracy": 0.1025390625,
            "f1": 0.07008636342126763,
            "f1_weighted": 0.09179386233693954
          },
          {
            "accuracy": 0.09326171875,
            "f1": 0.07036984551878075,
            "f1_weighted": 0.08131556552858538
          },
          {
            "accuracy": 0.087890625,
            "f1": 0.06311454568099398,
            "f1_weighted": 0.07825942493116483
          },
          {
            "accuracy": 0.09375,
            "f1": 0.06445757987342196,
            "f1_weighted": 0.08835462393780866
          }
        ]
      }
    ]
  },
  "task_name": "GreekLegalCodeClassification"
}