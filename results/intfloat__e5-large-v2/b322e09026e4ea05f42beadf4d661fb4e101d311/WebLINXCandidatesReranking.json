{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 10459.147360801697,
  "kg_co2_emissions": 0.7660642200695246,
  "mteb_version": "1.18.0",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07124944266095952,
        "map": 0.08667630706331533,
        "mrr": 0.07124944266095952,
        "nAUC_map_diff1": 0.20944003038127582,
        "nAUC_map_max": -0.03972849059858729,
        "nAUC_map_std": 0.32019985949381236,
        "nAUC_mrr_diff1": 0.19919343780675514,
        "nAUC_mrr_max": -0.04767729870424358,
        "nAUC_mrr_std": 0.28967989876840766
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07804496493471269,
        "map": 0.09761353274862095,
        "mrr": 0.07804496493471269,
        "nAUC_map_diff1": 0.14945565223970753,
        "nAUC_map_max": -0.008955434034966075,
        "nAUC_map_std": 0.13811565174615648,
        "nAUC_mrr_diff1": 0.1567897780213804,
        "nAUC_mrr_max": -0.008571803975286743,
        "nAUC_mrr_std": 0.13654859400035088
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07632210742433275,
        "map": 0.09206164785719159,
        "mrr": 0.07632210742433275,
        "nAUC_map_diff1": 0.1611504346942403,
        "nAUC_map_max": 0.056714647300829445,
        "nAUC_map_std": 0.19513718772120203,
        "nAUC_mrr_diff1": 0.17108469202865106,
        "nAUC_mrr_max": 0.07951070590580198,
        "nAUC_mrr_std": 0.17883034616282192
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.09127620785321804,
        "map": 0.10885642182543473,
        "mrr": 0.09127620785321804,
        "nAUC_map_diff1": 0.15812471307056755,
        "nAUC_map_max": -0.0170461113560516,
        "nAUC_map_std": 0.14479623103267444,
        "nAUC_mrr_diff1": 0.16745458344925798,
        "nAUC_mrr_max": 0.0019383866573081387,
        "nAUC_mrr_std": 0.12366844241235604
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07140168726523688,
        "map": 0.08752607152909334,
        "mrr": 0.07140168726523688,
        "nAUC_map_diff1": 0.1479492732116801,
        "nAUC_map_max": -0.0805421568991152,
        "nAUC_map_std": 0.13298654980921196,
        "nAUC_mrr_diff1": 0.1528133566853148,
        "nAUC_mrr_max": -0.08451547612210313,
        "nAUC_mrr_std": 0.11128813181096019
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.10537559630565012,
        "map": 0.1219605794715258,
        "mrr": 0.10537559630565012,
        "nAUC_map_diff1": 0.24381402257643991,
        "nAUC_map_max": -0.049662606164054615,
        "nAUC_map_std": 0.04506284367144245,
        "nAUC_mrr_diff1": 0.24892190068688394,
        "nAUC_mrr_max": -0.048935879186931604,
        "nAUC_mrr_std": 0.03091092931623817
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}