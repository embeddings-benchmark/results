{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "task_name": "GreekLegalCodeClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.061572,
        "f1": 0.034826,
        "f1_weighted": 0.050537,
        "scores_per_experiment": [
          {
            "accuracy": 0.066895,
            "f1": 0.036693,
            "f1_weighted": 0.058192
          },
          {
            "accuracy": 0.063477,
            "f1": 0.036796,
            "f1_weighted": 0.053685
          },
          {
            "accuracy": 0.057129,
            "f1": 0.034789,
            "f1_weighted": 0.046783
          },
          {
            "accuracy": 0.057129,
            "f1": 0.03244,
            "f1_weighted": 0.046529
          },
          {
            "accuracy": 0.050781,
            "f1": 0.030606,
            "f1_weighted": 0.036968
          },
          {
            "accuracy": 0.0625,
            "f1": 0.034705,
            "f1_weighted": 0.055207
          },
          {
            "accuracy": 0.07666,
            "f1": 0.03936,
            "f1_weighted": 0.066013
          },
          {
            "accuracy": 0.060547,
            "f1": 0.032769,
            "f1_weighted": 0.044441
          },
          {
            "accuracy": 0.059082,
            "f1": 0.032903,
            "f1_weighted": 0.042422
          },
          {
            "accuracy": 0.061523,
            "f1": 0.037199,
            "f1_weighted": 0.055132
          }
        ],
        "main_score": 0.061572,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.062305,
        "f1": 0.036522,
        "f1_weighted": 0.04867,
        "scores_per_experiment": [
          {
            "accuracy": 0.064941,
            "f1": 0.037941,
            "f1_weighted": 0.054703
          },
          {
            "accuracy": 0.067383,
            "f1": 0.038867,
            "f1_weighted": 0.05367
          },
          {
            "accuracy": 0.057129,
            "f1": 0.033263,
            "f1_weighted": 0.043115
          },
          {
            "accuracy": 0.057617,
            "f1": 0.033767,
            "f1_weighted": 0.040414
          },
          {
            "accuracy": 0.053223,
            "f1": 0.029923,
            "f1_weighted": 0.043593
          },
          {
            "accuracy": 0.068848,
            "f1": 0.038598,
            "f1_weighted": 0.055441
          },
          {
            "accuracy": 0.07373,
            "f1": 0.040903,
            "f1_weighted": 0.057725
          },
          {
            "accuracy": 0.061523,
            "f1": 0.037834,
            "f1_weighted": 0.047066
          },
          {
            "accuracy": 0.057129,
            "f1": 0.034905,
            "f1_weighted": 0.042019
          },
          {
            "accuracy": 0.061523,
            "f1": 0.039218,
            "f1_weighted": 0.04895
          }
        ],
        "main_score": 0.062305,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ]
  },
  "evaluation_time": 564.7711887359619,
  "kg_co2_emissions": 0.02447042130702763
}