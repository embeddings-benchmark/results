{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "task_name": "MassiveScenarioClassification",
  "mteb_version": "1.25.3",
  "scores": {
    "validation": [
      {
        "accuracy": 0.896508,
        "f1": 0.883696,
        "f1_weighted": 0.890252,
        "scores_per_experiment": [
          {
            "accuracy": 0.893753,
            "f1": 0.883138,
            "f1_weighted": 0.889295
          },
          {
            "accuracy": 0.898672,
            "f1": 0.88643,
            "f1_weighted": 0.893225
          },
          {
            "accuracy": 0.895721,
            "f1": 0.881854,
            "f1_weighted": 0.890303
          },
          {
            "accuracy": 0.899164,
            "f1": 0.885182,
            "f1_weighted": 0.890794
          },
          {
            "accuracy": 0.899164,
            "f1": 0.887483,
            "f1_weighted": 0.893659
          },
          {
            "accuracy": 0.893753,
            "f1": 0.881392,
            "f1_weighted": 0.888899
          },
          {
            "accuracy": 0.894737,
            "f1": 0.88214,
            "f1_weighted": 0.886321
          },
          {
            "accuracy": 0.891786,
            "f1": 0.87898,
            "f1_weighted": 0.884983
          },
          {
            "accuracy": 0.896704,
            "f1": 0.881491,
            "f1_weighted": 0.889029
          },
          {
            "accuracy": 0.901623,
            "f1": 0.88887,
            "f1_weighted": 0.896017
          }
        ],
        "main_score": 0.896508,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.89072,
        "f1": 0.877765,
        "f1_weighted": 0.885683,
        "scores_per_experiment": [
          {
            "accuracy": 0.893073,
            "f1": 0.883337,
            "f1_weighted": 0.890066
          },
          {
            "accuracy": 0.891392,
            "f1": 0.876778,
            "f1_weighted": 0.88677
          },
          {
            "accuracy": 0.888366,
            "f1": 0.874609,
            "f1_weighted": 0.884432
          },
          {
            "accuracy": 0.887357,
            "f1": 0.872346,
            "f1_weighted": 0.87948
          },
          {
            "accuracy": 0.892065,
            "f1": 0.879051,
            "f1_weighted": 0.887312
          },
          {
            "accuracy": 0.886012,
            "f1": 0.872675,
            "f1_weighted": 0.881412
          },
          {
            "accuracy": 0.887357,
            "f1": 0.87492,
            "f1_weighted": 0.880863
          },
          {
            "accuracy": 0.894418,
            "f1": 0.881052,
            "f1_weighted": 0.889155
          },
          {
            "accuracy": 0.890047,
            "f1": 0.876287,
            "f1_weighted": 0.884384
          },
          {
            "accuracy": 0.897108,
            "f1": 0.886591,
            "f1_weighted": 0.892955
          }
        ],
        "main_score": 0.89072,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 27.755131483078003,
  "kg_co2_emissions": null
}