{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.25.3",
  "scores": {
    "validation": [
      {
        "accuracy": 0.808756,
        "f1": 0.762207,
        "f1_weighted": 0.793052,
        "scores_per_experiment": [
          {
            "accuracy": 0.822922,
            "f1": 0.779757,
            "f1_weighted": 0.8093
          },
          {
            "accuracy": 0.826857,
            "f1": 0.776974,
            "f1_weighted": 0.812734
          },
          {
            "accuracy": 0.813576,
            "f1": 0.762923,
            "f1_weighted": 0.795054
          },
          {
            "accuracy": 0.817511,
            "f1": 0.754898,
            "f1_weighted": 0.802247
          },
          {
            "accuracy": 0.812592,
            "f1": 0.755714,
            "f1_weighted": 0.794686
          },
          {
            "accuracy": 0.786522,
            "f1": 0.743925,
            "f1_weighted": 0.771325
          },
          {
            "accuracy": 0.809149,
            "f1": 0.755523,
            "f1_weighted": 0.790131
          },
          {
            "accuracy": 0.797344,
            "f1": 0.758294,
            "f1_weighted": 0.777991
          },
          {
            "accuracy": 0.78603,
            "f1": 0.7528,
            "f1_weighted": 0.772829
          },
          {
            "accuracy": 0.815052,
            "f1": 0.781262,
            "f1_weighted": 0.804225
          }
        ],
        "main_score": 0.808756,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.804237,
        "f1": 0.763864,
        "f1_weighted": 0.78952,
        "scores_per_experiment": [
          {
            "accuracy": 0.816073,
            "f1": 0.768521,
            "f1_weighted": 0.803727
          },
          {
            "accuracy": 0.821116,
            "f1": 0.77483,
            "f1_weighted": 0.807893
          },
          {
            "accuracy": 0.802623,
            "f1": 0.752739,
            "f1_weighted": 0.787066
          },
          {
            "accuracy": 0.809348,
            "f1": 0.756839,
            "f1_weighted": 0.794449
          },
          {
            "accuracy": 0.798588,
            "f1": 0.750408,
            "f1_weighted": 0.780029
          },
          {
            "accuracy": 0.789173,
            "f1": 0.768591,
            "f1_weighted": 0.77634
          },
          {
            "accuracy": 0.81002,
            "f1": 0.774438,
            "f1_weighted": 0.79344
          },
          {
            "accuracy": 0.800269,
            "f1": 0.762326,
            "f1_weighted": 0.783855
          },
          {
            "accuracy": 0.783457,
            "f1": 0.755056,
            "f1_weighted": 0.767117
          },
          {
            "accuracy": 0.811701,
            "f1": 0.774894,
            "f1_weighted": 0.801284
          }
        ],
        "main_score": 0.804237,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 61.05408716201782,
  "kg_co2_emissions": null
}