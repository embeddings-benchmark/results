{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "task_name": "InappropriatenessClassification",
  "mteb_version": "1.38.57",
  "scores": {
    "test": [
      {
        "accuracy": 0.86123,
        "f1": 0.860931,
        "f1_weighted": 0.860931,
        "ap": 0.820522,
        "ap_weighted": 0.820522,
        "scores_per_experiment": [
          {
            "accuracy": 0.860352,
            "f1": 0.860281,
            "f1_weighted": 0.860281,
            "ap": 0.816137,
            "ap_weighted": 0.816137
          },
          {
            "accuracy": 0.858398,
            "f1": 0.857927,
            "f1_weighted": 0.857927,
            "ap": 0.824378,
            "ap_weighted": 0.824378
          },
          {
            "accuracy": 0.865723,
            "f1": 0.865552,
            "f1_weighted": 0.865552,
            "ap": 0.826881,
            "ap_weighted": 0.826881
          },
          {
            "accuracy": 0.839844,
            "f1": 0.838909,
            "f1_weighted": 0.838909,
            "ap": 0.806173,
            "ap_weighted": 0.806173
          },
          {
            "accuracy": 0.847656,
            "f1": 0.846835,
            "f1_weighted": 0.846835,
            "ap": 0.815436,
            "ap_weighted": 0.815436
          },
          {
            "accuracy": 0.863281,
            "f1": 0.863121,
            "f1_weighted": 0.863121,
            "ap": 0.823297,
            "ap_weighted": 0.823297
          },
          {
            "accuracy": 0.870117,
            "f1": 0.869974,
            "f1_weighted": 0.869974,
            "ap": 0.831789,
            "ap_weighted": 0.831789
          },
          {
            "accuracy": 0.866211,
            "f1": 0.866193,
            "f1_weighted": 0.866193,
            "ap": 0.820435,
            "ap_weighted": 0.820435
          },
          {
            "accuracy": 0.872559,
            "f1": 0.872396,
            "f1_weighted": 0.872396,
            "ap": 0.815843,
            "ap_weighted": 0.815843
          },
          {
            "accuracy": 0.868164,
            "f1": 0.868119,
            "f1_weighted": 0.868119,
            "ap": 0.824851,
            "ap_weighted": 0.824851
          }
        ],
        "main_score": 0.86123,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 12.443005323410034,
  "kg_co2_emissions": null
}