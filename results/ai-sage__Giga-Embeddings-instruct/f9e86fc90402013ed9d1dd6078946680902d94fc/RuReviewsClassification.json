{
  "dataset_revision": "f6d2c31f4dc6b88f468552750bfec05b4b41b05a",
  "task_name": "RuReviewsClassification",
  "mteb_version": "1.38.54",
  "scores": {
    "test": [
      {
        "accuracy": 0.76709,
        "f1": 0.759548,
        "f1_weighted": 0.759542,
        "scores_per_experiment": [
          {
            "accuracy": 0.765625,
            "f1": 0.757007,
            "f1_weighted": 0.757001
          },
          {
            "accuracy": 0.762695,
            "f1": 0.754686,
            "f1_weighted": 0.754676
          },
          {
            "accuracy": 0.741211,
            "f1": 0.725824,
            "f1_weighted": 0.725804
          },
          {
            "accuracy": 0.776855,
            "f1": 0.772858,
            "f1_weighted": 0.772858
          },
          {
            "accuracy": 0.788574,
            "f1": 0.788205,
            "f1_weighted": 0.788213
          },
          {
            "accuracy": 0.779297,
            "f1": 0.776057,
            "f1_weighted": 0.776056
          },
          {
            "accuracy": 0.770996,
            "f1": 0.763346,
            "f1_weighted": 0.76334
          },
          {
            "accuracy": 0.769043,
            "f1": 0.763357,
            "f1_weighted": 0.763354
          },
          {
            "accuracy": 0.738281,
            "f1": 0.717874,
            "f1_weighted": 0.717847
          },
          {
            "accuracy": 0.77832,
            "f1": 0.776268,
            "f1_weighted": 0.776273
          }
        ],
        "main_score": 0.76709,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 15.815458297729492,
  "kg_co2_emissions": null
}