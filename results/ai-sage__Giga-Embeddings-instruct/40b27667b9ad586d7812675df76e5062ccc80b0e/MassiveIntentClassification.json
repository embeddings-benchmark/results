{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.38.30",
  "scores": {
    "validation": [
      {
        "accuracy": 0.831628,
        "f1": 0.785758,
        "f1_weighted": 0.818019,
        "scores_per_experiment": [
          {
            "accuracy": 0.842105,
            "f1": 0.801315,
            "f1_weighted": 0.831736
          },
          {
            "accuracy": 0.837186,
            "f1": 0.792376,
            "f1_weighted": 0.824227
          },
          {
            "accuracy": 0.831776,
            "f1": 0.779501,
            "f1_weighted": 0.816086
          },
          {
            "accuracy": 0.835219,
            "f1": 0.791036,
            "f1_weighted": 0.818805
          },
          {
            "accuracy": 0.825381,
            "f1": 0.770207,
            "f1_weighted": 0.812505
          },
          {
            "accuracy": 0.822922,
            "f1": 0.776561,
            "f1_weighted": 0.81061
          },
          {
            "accuracy": 0.828824,
            "f1": 0.782511,
            "f1_weighted": 0.811719
          },
          {
            "accuracy": 0.827841,
            "f1": 0.787006,
            "f1_weighted": 0.812213
          },
          {
            "accuracy": 0.829316,
            "f1": 0.782779,
            "f1_weighted": 0.816503
          },
          {
            "accuracy": 0.835711,
            "f1": 0.794289,
            "f1_weighted": 0.825785
          }
        ],
        "main_score": 0.831628,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.824815,
        "f1": 0.783837,
        "f1_weighted": 0.813288,
        "scores_per_experiment": [
          {
            "accuracy": 0.832549,
            "f1": 0.789991,
            "f1_weighted": 0.823843
          },
          {
            "accuracy": 0.82885,
            "f1": 0.789375,
            "f1_weighted": 0.81615
          },
          {
            "accuracy": 0.818763,
            "f1": 0.773978,
            "f1_weighted": 0.80501
          },
          {
            "accuracy": 0.828514,
            "f1": 0.781904,
            "f1_weighted": 0.816053
          },
          {
            "accuracy": 0.813719,
            "f1": 0.770622,
            "f1_weighted": 0.800835
          },
          {
            "accuracy": 0.817418,
            "f1": 0.78849,
            "f1_weighted": 0.809229
          },
          {
            "accuracy": 0.821453,
            "f1": 0.786911,
            "f1_weighted": 0.806647
          },
          {
            "accuracy": 0.827505,
            "f1": 0.787324,
            "f1_weighted": 0.814677
          },
          {
            "accuracy": 0.827169,
            "f1": 0.78226,
            "f1_weighted": 0.815902
          },
          {
            "accuracy": 0.832213,
            "f1": 0.787516,
            "f1_weighted": 0.824537
          }
        ],
        "main_score": 0.824815,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 56.327454566955566,
  "kg_co2_emissions": null
}