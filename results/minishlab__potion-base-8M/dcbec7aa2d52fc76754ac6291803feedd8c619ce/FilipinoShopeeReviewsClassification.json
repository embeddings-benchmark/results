{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.262158,
        "f1": 0.256812,
        "f1_weighted": 0.256794,
        "scores_per_experiment": [
          {
            "accuracy": 0.237305,
            "f1": 0.236937,
            "f1_weighted": 0.236931
          },
          {
            "accuracy": 0.234863,
            "f1": 0.221544,
            "f1_weighted": 0.221487
          },
          {
            "accuracy": 0.236816,
            "f1": 0.235109,
            "f1_weighted": 0.235111
          },
          {
            "accuracy": 0.275879,
            "f1": 0.274674,
            "f1_weighted": 0.274643
          },
          {
            "accuracy": 0.258789,
            "f1": 0.251655,
            "f1_weighted": 0.251669
          },
          {
            "accuracy": 0.26709,
            "f1": 0.255466,
            "f1_weighted": 0.255439
          },
          {
            "accuracy": 0.256348,
            "f1": 0.253813,
            "f1_weighted": 0.253766
          },
          {
            "accuracy": 0.29541,
            "f1": 0.286266,
            "f1_weighted": 0.286262
          },
          {
            "accuracy": 0.287109,
            "f1": 0.282402,
            "f1_weighted": 0.282383
          },
          {
            "accuracy": 0.271973,
            "f1": 0.270258,
            "f1_weighted": 0.270252
          }
        ],
        "main_score": 0.262158,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.267725,
        "f1": 0.261822,
        "f1_weighted": 0.261805,
        "scores_per_experiment": [
          {
            "accuracy": 0.245117,
            "f1": 0.244383,
            "f1_weighted": 0.244365
          },
          {
            "accuracy": 0.249512,
            "f1": 0.234983,
            "f1_weighted": 0.234926
          },
          {
            "accuracy": 0.253418,
            "f1": 0.252069,
            "f1_weighted": 0.252066
          },
          {
            "accuracy": 0.26709,
            "f1": 0.26763,
            "f1_weighted": 0.267604
          },
          {
            "accuracy": 0.289062,
            "f1": 0.28166,
            "f1_weighted": 0.281648
          },
          {
            "accuracy": 0.259277,
            "f1": 0.243801,
            "f1_weighted": 0.243783
          },
          {
            "accuracy": 0.269043,
            "f1": 0.267592,
            "f1_weighted": 0.267566
          },
          {
            "accuracy": 0.275391,
            "f1": 0.267402,
            "f1_weighted": 0.267423
          },
          {
            "accuracy": 0.294922,
            "f1": 0.288041,
            "f1_weighted": 0.288021
          },
          {
            "accuracy": 0.274414,
            "f1": 0.270654,
            "f1_weighted": 0.270648
          }
        ],
        "main_score": 0.267725,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 13.532833337783813,
  "kg_co2_emissions": 0.0003043790877004021
}