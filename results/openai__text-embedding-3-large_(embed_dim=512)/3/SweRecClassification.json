{
  "dataset_revision": "b07c6ce548f6a7ac8d546e1bbe197a0086409190",
  "task_name": "SweRecClassification",
  "mteb_version": "2.3.3",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.811035,
            "f1": 0.718581,
            "f1_weighted": 0.814913,
            "precision": 0.71578,
            "precision_weighted": 0.820896,
            "recall": 0.723782,
            "recall_weighted": 0.811035,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.802734,
            "f1": 0.733322,
            "f1_weighted": 0.81995,
            "precision": 0.733704,
            "precision_weighted": 0.848422,
            "recall": 0.755785,
            "recall_weighted": 0.802734,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.780273,
            "f1": 0.708769,
            "f1_weighted": 0.798592,
            "precision": 0.716321,
            "precision_weighted": 0.841457,
            "recall": 0.735434,
            "recall_weighted": 0.780273,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.808105,
            "f1": 0.724183,
            "f1_weighted": 0.819189,
            "precision": 0.721787,
            "precision_weighted": 0.834326,
            "recall": 0.734989,
            "recall_weighted": 0.808105,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.793457,
            "f1": 0.685258,
            "f1_weighted": 0.792067,
            "precision": 0.685691,
            "precision_weighted": 0.791261,
            "recall": 0.68536,
            "recall_weighted": 0.793457,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.817383,
            "f1": 0.713283,
            "f1_weighted": 0.816337,
            "precision": 0.714045,
            "precision_weighted": 0.815352,
            "recall": 0.71263,
            "recall_weighted": 0.817383,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.823242,
            "f1": 0.736511,
            "f1_weighted": 0.829961,
            "precision": 0.732576,
            "precision_weighted": 0.83852,
            "recall": 0.744224,
            "recall_weighted": 0.823242,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.760254,
            "f1": 0.709332,
            "f1_weighted": 0.790734,
            "precision": 0.728145,
            "precision_weighted": 0.855326,
            "recall": 0.752819,
            "recall_weighted": 0.760254,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.742676,
            "f1": 0.669634,
            "f1_weighted": 0.769966,
            "precision": 0.685527,
            "precision_weighted": 0.820053,
            "recall": 0.689294,
            "recall_weighted": 0.742676,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.843262,
            "f1": 0.7493,
            "f1_weighted": 0.841841,
            "precision": 0.75144,
            "precision_weighted": 0.841941,
            "recall": 0.748543,
            "recall_weighted": 0.843262,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.798242,
        "f1": 0.714817,
        "f1_weighted": 0.809355,
        "precision": 0.718502,
        "precision_weighted": 0.830755,
        "recall": 0.728286,
        "recall_weighted": 0.798242,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.798242,
        "hf_subset": "default",
        "languages": [
          "swe-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 50.82265567779541,
  "kg_co2_emissions": null
}