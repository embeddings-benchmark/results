{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 12786.770101547241,
  "kg_co2_emissions": 1.013193832252355,
  "mteb_version": "1.18.0",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.05581750490458356,
        "map": 0.07217211453323402,
        "mrr": 0.05581750490458356,
        "nAUC_map_diff1": 0.0857747125534718,
        "nAUC_map_max": -0.12815691448126681,
        "nAUC_map_std": 0.24611961167726562,
        "nAUC_mrr_diff1": 0.07185322422519143,
        "nAUC_mrr_max": -0.13817307216926544,
        "nAUC_mrr_std": 0.20026863474462175
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08921677838480117,
        "map": 0.1079127802566856,
        "mrr": 0.08921677838480117,
        "nAUC_map_diff1": 0.06966191576051685,
        "nAUC_map_max": -0.18550199473399825,
        "nAUC_map_std": -0.012921327213678516,
        "nAUC_mrr_diff1": 0.07195409432416053,
        "nAUC_mrr_max": -0.18919389550071597,
        "nAUC_mrr_std": -0.021250403252258195
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.0704492019339029,
        "map": 0.08993281433850726,
        "mrr": 0.0704492019339029,
        "nAUC_map_diff1": 0.13664241760163937,
        "nAUC_map_max": 0.012053880991015236,
        "nAUC_map_std": 0.0927748837309077,
        "nAUC_mrr_diff1": 0.1367431171095104,
        "nAUC_mrr_max": 0.01668378524739301,
        "nAUC_mrr_std": 0.0745365249859515
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08974957606044809,
        "map": 0.10776657661581814,
        "mrr": 0.08974957606044809,
        "nAUC_map_diff1": 0.13603947027025146,
        "nAUC_map_max": -0.007943372488213982,
        "nAUC_map_std": 0.12396010597840511,
        "nAUC_mrr_diff1": 0.14114006857349265,
        "nAUC_mrr_max": -0.0043239216492330125,
        "nAUC_mrr_std": 0.1087292836207442
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07154292378529019,
        "map": 0.08847815269565239,
        "mrr": 0.07154292378529019,
        "nAUC_map_diff1": 0.1145780391538513,
        "nAUC_map_max": -0.12922871041825562,
        "nAUC_map_std": 0.06954817897706866,
        "nAUC_mrr_diff1": 0.11209157543307424,
        "nAUC_mrr_max": -0.12360448225445282,
        "nAUC_mrr_std": 0.05337443654258513
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.0979527347705672,
        "map": 0.11582448794559327,
        "mrr": 0.0979527347705672,
        "nAUC_map_diff1": 0.14754495786986133,
        "nAUC_map_max": -0.10042472966616117,
        "nAUC_map_std": -0.035146091049081105,
        "nAUC_mrr_diff1": 0.16122924550255607,
        "nAUC_mrr_max": -0.09100447238420677,
        "nAUC_mrr_std": -0.040286726643677
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}