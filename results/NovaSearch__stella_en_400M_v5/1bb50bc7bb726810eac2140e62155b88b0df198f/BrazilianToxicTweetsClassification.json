{
  "dataset_revision": "f333c1fcfa3ab43f008a327c8bd0140441354d34",
  "evaluation_time": 10.619741916656494,
  "kg_co2_emissions": 0.00047728925552658296,
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.232421875,
        "f1": 0.12784398139906417,
        "hf_subset": "default",
        "languages": [
          "por-Latn"
        ],
        "lrap": 0.7865030924479175,
        "main_score": 0.232421875,
        "scores_per_experiment": [
          {
            "accuracy": 0.1748046875,
            "f1": 0.15014791136958264,
            "lrap": 0.8451334635416667
          },
          {
            "accuracy": 0.1240234375,
            "f1": 0.15443054228611675,
            "lrap": 0.8145345052083341
          },
          {
            "accuracy": 0.17138671875,
            "f1": 0.14341298458390297,
            "lrap": 0.812893337673612
          },
          {
            "accuracy": 0.3203125,
            "f1": 0.10274411559415962,
            "lrap": 0.7950032552083346
          },
          {
            "accuracy": 0.33642578125,
            "f1": 0.1434393089936998,
            "lrap": 0.7699381510416676
          },
          {
            "accuracy": 0.2294921875,
            "f1": 0.12680313455309355,
            "lrap": 0.8349202473958344
          },
          {
            "accuracy": 0.13720703125,
            "f1": 0.09912295096724355,
            "lrap": 0.7510308159722233
          },
          {
            "accuracy": 0.21728515625,
            "f1": 0.1407634296751476,
            "lrap": 0.7464192708333341
          },
          {
            "accuracy": 0.33837890625,
            "f1": 0.10374602667437556,
            "lrap": 0.6959228515624993
          },
          {
            "accuracy": 0.27490234375,
            "f1": 0.11382940929331957,
            "lrap": 0.7992350260416683
          }
        ]
      }
    ]
  },
  "task_name": "BrazilianToxicTweetsClassification"
}