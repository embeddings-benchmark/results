{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "evaluation_time": 26.26379108428955,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.52470703125,
        "ap": 0.5139557556695517,
        "ap_weighted": 0.5139557556695517,
        "f1": 0.5210637908725082,
        "f1_weighted": 0.5210637908725082,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.52470703125,
        "scores_per_experiment": [
          {
            "accuracy": 0.55322265625,
            "ap": 0.5298378406562848,
            "ap_weighted": 0.5298378406562848,
            "f1": 0.5515520574803664,
            "f1_weighted": 0.5515520574803664
          },
          {
            "accuracy": 0.5498046875,
            "ap": 0.5269507623487903,
            "ap_weighted": 0.5269507623487903,
            "f1": 0.5447405372876387,
            "f1_weighted": 0.5447405372876387
          },
          {
            "accuracy": 0.5087890625,
            "ap": 0.5044759114583334,
            "ap_weighted": 0.5044759114583334,
            "f1": 0.5084721824601584,
            "f1_weighted": 0.5084721824601584
          },
          {
            "accuracy": 0.537109375,
            "ap": 0.5203173828125001,
            "ap_weighted": 0.5203173828125001,
            "f1": 0.5315048183839881,
            "f1_weighted": 0.5315048183839881
          },
          {
            "accuracy": 0.4794921875,
            "ap": 0.49020620492788464,
            "ap_weighted": 0.49020620492788464,
            "f1": 0.4785293892838034,
            "f1_weighted": 0.4785293892838034
          },
          {
            "accuracy": 0.55419921875,
            "ap": 0.5295235067485898,
            "ap_weighted": 0.5295235067485898,
            "f1": 0.5491374331931187,
            "f1_weighted": 0.5491374331931187
          },
          {
            "accuracy": 0.55029296875,
            "ap": 0.5272747406532456,
            "ap_weighted": 0.5272747406532456,
            "f1": 0.5462634004120706,
            "f1_weighted": 0.5462634004120706
          },
          {
            "accuracy": 0.5380859375,
            "ap": 0.5202088647959184,
            "ap_weighted": 0.5202088647959184,
            "f1": 0.5310987646074209,
            "f1_weighted": 0.5310987646074209
          },
          {
            "accuracy": 0.45703125,
            "ap": 0.4808848292606516,
            "ap_weighted": 0.4808848292606516,
            "f1": 0.4503377559719137,
            "f1_weighted": 0.4503377559719137
          },
          {
            "accuracy": 0.51904296875,
            "ap": 0.5098775130333173,
            "ap_weighted": 0.5098775130333173,
            "f1": 0.5190015696446041,
            "f1_weighted": 0.5190015696446041
          }
        ]
      }
    ]
  },
  "task_name": "InappropriatenessClassification"
}