{
  "dataset_revision": "2fe05ee6b5832cda29f2ef7aaad7b7fe6a3609eb",
  "task_name": "HeadlineClassification",
  "mteb_version": "2.3.2",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.826172,
            "f1": 0.825232,
            "f1_weighted": 0.825204,
            "precision": 0.834047,
            "precision_weighted": 0.834042,
            "recall": 0.826223,
            "recall_weighted": 0.826172,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.842285,
            "f1": 0.84124,
            "f1_weighted": 0.841222,
            "precision": 0.846537,
            "precision_weighted": 0.846508,
            "recall": 0.842294,
            "recall_weighted": 0.842285,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.841797,
            "f1": 0.841794,
            "f1_weighted": 0.841774,
            "precision": 0.84907,
            "precision_weighted": 0.849068,
            "recall": 0.841842,
            "recall_weighted": 0.841797,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.825684,
            "f1": 0.824068,
            "f1_weighted": 0.824031,
            "precision": 0.83654,
            "precision_weighted": 0.836543,
            "recall": 0.825756,
            "recall_weighted": 0.825684,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.830078,
            "f1": 0.828501,
            "f1_weighted": 0.828468,
            "precision": 0.840796,
            "precision_weighted": 0.840806,
            "recall": 0.830148,
            "recall_weighted": 0.830078,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.826172,
            "f1": 0.825915,
            "f1_weighted": 0.825888,
            "precision": 0.835106,
            "precision_weighted": 0.835111,
            "recall": 0.826233,
            "recall_weighted": 0.826172,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.83252,
            "f1": 0.831811,
            "f1_weighted": 0.83179,
            "precision": 0.837661,
            "precision_weighted": 0.837657,
            "recall": 0.832554,
            "recall_weighted": 0.83252,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.846191,
            "f1": 0.845113,
            "f1_weighted": 0.845101,
            "precision": 0.851262,
            "precision_weighted": 0.851215,
            "recall": 0.846174,
            "recall_weighted": 0.846191,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.836914,
            "f1": 0.836549,
            "f1_weighted": 0.836541,
            "precision": 0.839551,
            "precision_weighted": 0.839527,
            "recall": 0.836909,
            "recall_weighted": 0.836914,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.847168,
            "f1": 0.846195,
            "f1_weighted": 0.846176,
            "precision": 0.849747,
            "precision_weighted": 0.849731,
            "recall": 0.847193,
            "recall_weighted": 0.847168,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.835498,
        "f1": 0.834642,
        "f1_weighted": 0.83462,
        "precision": 0.842032,
        "precision_weighted": 0.842021,
        "recall": 0.835533,
        "recall_weighted": 0.835498,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.835498,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 70.81275701522827,
  "kg_co2_emissions": null
}
