{
    "dataset_revision": "9080400076fbadbb4c4dcb136ff4eddc40b42553",
    "task_name": "Tatoeba",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "sqi-eng",
                "languages": [
                    "sqi-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9739999999999999,
                "f1": 0.9675,
                "precision": 0.9645,
                "recall": 0.9739999999999999,
                "main_score": 0.9675
            },
            {
                "hf_subset": "fry-eng",
                "languages": [
                    "fry-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8554913294797689,
                "f1": 0.8246628131021194,
                "precision": 0.8111753371868979,
                "recall": 0.8554913294797689,
                "main_score": 0.8246628131021194
            },
            {
                "hf_subset": "kur-eng",
                "languages": [
                    "kur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8121951219512195,
                "f1": 0.7733333333333334,
                "precision": 0.7554878048780488,
                "recall": 0.8121951219512195,
                "main_score": 0.7733333333333334
            },
            {
                "hf_subset": "tur-eng",
                "languages": [
                    "tur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.986,
                "f1": 0.9826666666666665,
                "precision": 0.981,
                "recall": 0.986,
                "main_score": 0.9826666666666665
            },
            {
                "hf_subset": "deu-eng",
                "languages": [
                    "deu-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.995,
                "f1": 0.9933333333333333,
                "precision": 0.9925,
                "recall": 0.995,
                "main_score": 0.9933333333333333
            },
            {
                "hf_subset": "nld-eng",
                "languages": [
                    "nld-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.978,
                "f1": 0.972,
                "precision": 0.9689999999999999,
                "recall": 0.978,
                "main_score": 0.972
            },
            {
                "hf_subset": "ron-eng",
                "languages": [
                    "ron-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.978,
                "f1": 0.9718333333333333,
                "precision": 0.9688333333333332,
                "recall": 0.978,
                "main_score": 0.9718333333333333
            },
            {
                "hf_subset": "ang-eng",
                "languages": [
                    "ang-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7761194029850746,
                "f1": 0.7281094527363183,
                "precision": 0.7083333333333333,
                "recall": 0.7761194029850746,
                "main_score": 0.7281094527363183
            },
            {
                "hf_subset": "ido-eng",
                "languages": [
                    "ido-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.937,
                "f1": 0.9191666666666667,
                "precision": 0.9108333333333334,
                "recall": 0.937,
                "main_score": 0.9191666666666667
            },
            {
                "hf_subset": "jav-eng",
                "languages": [
                    "jav-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8829268292682927,
                "f1": 0.8527642276422764,
                "precision": 0.8401277584204414,
                "recall": 0.8829268292682927,
                "main_score": 0.8527642276422764
            },
            {
                "hf_subset": "isl-eng",
                "languages": [
                    "isl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.961,
                "f1": 0.95,
                "precision": 0.9446666666666669,
                "recall": 0.961,
                "main_score": 0.95
            },
            {
                "hf_subset": "slv-eng",
                "languages": [
                    "slv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.93681652490887,
                "f1": 0.9190765492102065,
                "precision": 0.9105913325232887,
                "recall": 0.93681652490887,
                "main_score": 0.9190765492102065
            },
            {
                "hf_subset": "cym-eng",
                "languages": [
                    "cym-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9217391304347826,
                "f1": 0.8997101449275361,
                "precision": 0.8896811594202899,
                "recall": 0.9217391304347826,
                "main_score": 0.8997101449275361
            },
            {
                "hf_subset": "kaz-eng",
                "languages": [
                    "kaz-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.9043478260869565,
                "f1": 0.8772173913043478,
                "precision": 0.8642028985507245,
                "recall": 0.9043478260869565,
                "main_score": 0.8772173913043478
            },
            {
                "hf_subset": "est-eng",
                "languages": [
                    "est-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.904,
                "f1": 0.8803,
                "precision": 0.8695,
                "recall": 0.904,
                "main_score": 0.8803
            },
            {
                "hf_subset": "heb-eng",
                "languages": [
                    "heb-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.934,
                "f1": 0.9145666666666666,
                "precision": 0.9052500000000001,
                "recall": 0.934,
                "main_score": 0.9145666666666666
            },
            {
                "hf_subset": "gla-eng",
                "languages": [
                    "gla-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8190591073582629,
                "f1": 0.7832557872364869,
                "precision": 0.7678260286824823,
                "recall": 0.8190591073582629,
                "main_score": 0.7832557872364869
            },
            {
                "hf_subset": "mar-eng",
                "languages": [
                    "mar-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.943,
                "f1": 0.9258333333333333,
                "precision": 0.9173333333333332,
                "recall": 0.943,
                "main_score": 0.9258333333333333
            },
            {
                "hf_subset": "lat-eng",
                "languages": [
                    "lat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.791,
                "f1": 0.7450500000000001,
                "precision": 0.7258928571428571,
                "recall": 0.791,
                "main_score": 0.7450500000000001
            },
            {
                "hf_subset": "bel-eng",
                "languages": [
                    "bel-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.966,
                "f1": 0.9555,
                "precision": 0.9505,
                "recall": 0.966,
                "main_score": 0.9555
            },
            {
                "hf_subset": "pms-eng",
                "languages": [
                    "pms-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.820952380952381,
                "f1": 0.7798458049886621,
                "precision": 0.7619682539682541,
                "recall": 0.820952380952381,
                "main_score": 0.7798458049886621
            },
            {
                "hf_subset": "gle-eng",
                "languages": [
                    "gle-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.879,
                "f1": 0.8499190476190477,
                "precision": 0.8365,
                "recall": 0.879,
                "main_score": 0.8499190476190477
            },
            {
                "hf_subset": "pes-eng",
                "languages": [
                    "pes-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.9570000000000001,
                "f1": 0.9456666666666667,
                "precision": 0.9401666666666667,
                "recall": 0.9570000000000001,
                "main_score": 0.9456666666666667
            },
            {
                "hf_subset": "nob-eng",
                "languages": [
                    "nob-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.986,
                "f1": 0.982,
                "precision": 0.98,
                "recall": 0.986,
                "main_score": 0.982
            },
            {
                "hf_subset": "bul-eng",
                "languages": [
                    "bul-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.956,
                "f1": 0.9438333333333334,
                "precision": 0.9378333333333334,
                "recall": 0.956,
                "main_score": 0.9438333333333334
            },
            {
                "hf_subset": "cbk-eng",
                "languages": [
                    "cbk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8740000000000001,
                "f1": 0.8410380952380951,
                "precision": 0.8267,
                "recall": 0.8740000000000001,
                "main_score": 0.8410380952380951
            },
            {
                "hf_subset": "hun-eng",
                "languages": [
                    "hun-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.955,
                "f1": 0.9433333333333335,
                "precision": 0.9378333333333333,
                "recall": 0.955,
                "main_score": 0.9433333333333335
            },
            {
                "hf_subset": "uig-eng",
                "languages": [
                    "uig-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.894,
                "f1": 0.8682000000000001,
                "precision": 0.8564500000000002,
                "recall": 0.894,
                "main_score": 0.8682000000000001
            },
            {
                "hf_subset": "rus-eng",
                "languages": [
                    "rus-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.951,
                "f1": 0.9356666666666668,
                "precision": 0.9281666666666666,
                "recall": 0.951,
                "main_score": 0.9356666666666668
            },
            {
                "hf_subset": "spa-eng",
                "languages": [
                    "spa-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9890000000000001,
                "f1": 0.986,
                "precision": 0.9845,
                "recall": 0.9890000000000001,
                "main_score": 0.986
            },
            {
                "hf_subset": "hye-eng",
                "languages": [
                    "hye-Armn",
                    "eng-Latn"
                ],
                "accuracy": 0.950134770889488,
                "f1": 0.9351752021563342,
                "precision": 0.9282794249775381,
                "recall": 0.950134770889488,
                "main_score": 0.9351752021563342
            },
            {
                "hf_subset": "tel-eng",
                "languages": [
                    "tel-Telu",
                    "eng-Latn"
                ],
                "accuracy": 0.9700854700854701,
                "f1": 0.9608262108262108,
                "precision": 0.9565527065527067,
                "recall": 0.9700854700854701,
                "main_score": 0.9608262108262108
            },
            {
                "hf_subset": "afr-eng",
                "languages": [
                    "afr-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.965,
                "f1": 0.954,
                "precision": 0.9488333333333333,
                "recall": 0.965,
                "main_score": 0.954
            },
            {
                "hf_subset": "mon-eng",
                "languages": [
                    "mon-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.9659090909090909,
                "f1": 0.9549242424242425,
                "precision": 0.9496212121212121,
                "recall": 0.9659090909090909,
                "main_score": 0.9549242424242425
            },
            {
                "hf_subset": "arz-eng",
                "languages": [
                    "arz-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.8490566037735848,
                "f1": 0.8185883997204751,
                "precision": 0.8054507337526204,
                "recall": 0.8490566037735848,
                "main_score": 0.8185883997204751
            },
            {
                "hf_subset": "hrv-eng",
                "languages": [
                    "hrv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.975,
                "f1": 0.9675,
                "precision": 0.9638333333333332,
                "recall": 0.975,
                "main_score": 0.9675
            },
            {
                "hf_subset": "nov-eng",
                "languages": [
                    "nov-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.867704280155642,
                "f1": 0.8299610894941636,
                "precision": 0.8132295719844358,
                "recall": 0.867704280155642,
                "main_score": 0.8299610894941636
            },
            {
                "hf_subset": "gsw-eng",
                "languages": [
                    "gsw-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.6752136752136753,
                "f1": 0.6189662189662191,
                "precision": 0.5968660968660969,
                "recall": 0.6752136752136753,
                "main_score": 0.6189662189662191
            },
            {
                "hf_subset": "nds-eng",
                "languages": [
                    "nds-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.892,
                "f1": 0.8632,
                "precision": 0.85015,
                "recall": 0.892,
                "main_score": 0.8632
            },
            {
                "hf_subset": "ukr-eng",
                "languages": [
                    "ukr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.96,
                "f1": 0.9478333333333333,
                "precision": 0.9418333333333334,
                "recall": 0.96,
                "main_score": 0.9478333333333333
            },
            {
                "hf_subset": "uzb-eng",
                "languages": [
                    "uzb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8387850467289719,
                "f1": 0.8054517133956385,
                "precision": 0.79154984423676,
                "recall": 0.8387850467289719,
                "main_score": 0.8054517133956385
            },
            {
                "hf_subset": "lit-eng",
                "languages": [
                    "lit-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.936,
                "f1": 0.9201333333333334,
                "precision": 0.9128333333333333,
                "recall": 0.936,
                "main_score": 0.9201333333333334
            },
            {
                "hf_subset": "ina-eng",
                "languages": [
                    "ina-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.971,
                "f1": 0.9626666666666667,
                "precision": 0.9585000000000001,
                "recall": 0.971,
                "main_score": 0.9626666666666667
            },
            {
                "hf_subset": "lfn-eng",
                "languages": [
                    "lfn-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.843,
                "f1": 0.8067833333333333,
                "precision": 0.7903928571428571,
                "recall": 0.843,
                "main_score": 0.8067833333333333
            },
            {
                "hf_subset": "zsm-eng",
                "languages": [
                    "zsm-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.973,
                "f1": 0.9648333333333332,
                "precision": 0.9608333333333331,
                "recall": 0.973,
                "main_score": 0.9648333333333332
            },
            {
                "hf_subset": "ita-eng",
                "languages": [
                    "ita-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9570000000000001,
                "f1": 0.9466666666666668,
                "precision": 0.9416666666666668,
                "recall": 0.9570000000000001,
                "main_score": 0.9466666666666668
            },
            {
                "hf_subset": "cmn-eng",
                "languages": [
                    "cmn-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.972,
                "f1": 0.9636666666666668,
                "precision": 0.9596666666666668,
                "recall": 0.972,
                "main_score": 0.9636666666666668
            },
            {
                "hf_subset": "lvs-eng",
                "languages": [
                    "lvs-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.943,
                "f1": 0.9280666666666667,
                "precision": 0.9212833333333333,
                "recall": 0.943,
                "main_score": 0.9280666666666667
            },
            {
                "hf_subset": "glg-eng",
                "languages": [
                    "glg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.97,
                "f1": 0.9622333333333334,
                "precision": 0.95875,
                "recall": 0.97,
                "main_score": 0.9622333333333334
            },
            {
                "hf_subset": "ceb-eng",
                "languages": [
                    "ceb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7433333333333333,
                "f1": 0.7078174603174603,
                "precision": 0.6928333333333332,
                "recall": 0.7433333333333333,
                "main_score": 0.7078174603174603
            },
            {
                "hf_subset": "bre-eng",
                "languages": [
                    "bre-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.376,
                "f1": 0.3293834895209036,
                "precision": 0.312811038961039,
                "recall": 0.376,
                "main_score": 0.3293834895209036
            },
            {
                "hf_subset": "ben-eng",
                "languages": [
                    "ben-Beng",
                    "eng-Latn"
                ],
                "accuracy": 0.915,
                "f1": 0.8913333333333333,
                "precision": 0.8803333333333333,
                "recall": 0.915,
                "main_score": 0.8913333333333333
            },
            {
                "hf_subset": "swg-eng",
                "languages": [
                    "swg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8214285714285714,
                "f1": 0.7767857142857143,
                "precision": 0.7559523809523809,
                "recall": 0.8214285714285714,
                "main_score": 0.7767857142857143
            },
            {
                "hf_subset": "arq-eng",
                "languages": [
                    "arq-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.690450054884742,
                "f1": 0.6307040928336207,
                "precision": 0.6058992781824835,
                "recall": 0.690450054884742,
                "main_score": 0.6307040928336207
            },
            {
                "hf_subset": "kab-eng",
                "languages": [
                    "kab-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.631,
                "f1": 0.5784833333333333,
                "precision": 0.5569500000000001,
                "recall": 0.631,
                "main_score": 0.5784833333333333
            },
            {
                "hf_subset": "fra-eng",
                "languages": [
                    "fra-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.961,
                "f1": 0.9501666666666666,
                "precision": 0.945,
                "recall": 0.961,
                "main_score": 0.9501666666666666
            },
            {
                "hf_subset": "por-eng",
                "languages": [
                    "por-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.959,
                "f1": 0.9490666666666666,
                "precision": 0.9442499999999999,
                "recall": 0.959,
                "main_score": 0.9490666666666666
            },
            {
                "hf_subset": "tat-eng",
                "languages": [
                    "tat-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.8759999999999999,
                "f1": 0.8461333333333333,
                "precision": 0.8327,
                "recall": 0.8759999999999999,
                "main_score": 0.8461333333333333
            },
            {
                "hf_subset": "oci-eng",
                "languages": [
                    "oci-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.764,
                "f1": 0.7190746031746031,
                "precision": 0.7007027777777778,
                "recall": 0.764,
                "main_score": 0.7190746031746031
            },
            {
                "hf_subset": "pol-eng",
                "languages": [
                    "pol-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9789999999999999,
                "f1": 0.9726666666666667,
                "precision": 0.9695,
                "recall": 0.9789999999999999,
                "main_score": 0.9726666666666667
            },
            {
                "hf_subset": "war-eng",
                "languages": [
                    "war-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7879999999999999,
                "f1": 0.7439555555555555,
                "precision": 0.7259416666666667,
                "recall": 0.7879999999999999,
                "main_score": 0.7439555555555555
            },
            {
                "hf_subset": "aze-eng",
                "languages": [
                    "aze-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9519999999999998,
                "f1": 0.9379,
                "precision": 0.93125,
                "recall": 0.9519999999999998,
                "main_score": 0.9379
            },
            {
                "hf_subset": "vie-eng",
                "languages": [
                    "vie-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.978,
                "f1": 0.971,
                "precision": 0.9675,
                "recall": 0.978,
                "main_score": 0.971
            },
            {
                "hf_subset": "nno-eng",
                "languages": [
                    "nno-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.956,
                "f1": 0.9425666666666666,
                "precision": 0.9364166666666668,
                "recall": 0.956,
                "main_score": 0.9425666666666666
            },
            {
                "hf_subset": "cha-eng",
                "languages": [
                    "cha-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.5693430656934306,
                "f1": 0.5146159193604448,
                "precision": 0.49374348279457764,
                "recall": 0.5693430656934306,
                "main_score": 0.5146159193604448
            },
            {
                "hf_subset": "mhr-eng",
                "languages": [
                    "mhr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.20200000000000004,
                "f1": 0.1691799284049284,
                "precision": 0.1579185515873016,
                "recall": 0.20200000000000004,
                "main_score": 0.1691799284049284
            },
            {
                "hf_subset": "dan-eng",
                "languages": [
                    "dan-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9620000000000001,
                "f1": 0.953,
                "precision": 0.9484999999999999,
                "recall": 0.9620000000000001,
                "main_score": 0.953
            },
            {
                "hf_subset": "ell-eng",
                "languages": [
                    "ell-Grek",
                    "eng-Latn"
                ],
                "accuracy": 0.963,
                "f1": 0.9511666666666667,
                "precision": 0.9453333333333334,
                "recall": 0.963,
                "main_score": 0.9511666666666667
            },
            {
                "hf_subset": "amh-eng",
                "languages": [
                    "amh-Ethi",
                    "eng-Latn"
                ],
                "accuracy": 0.8988095238095238,
                "f1": 0.8714285714285714,
                "precision": 0.8596230158730161,
                "recall": 0.8988095238095238,
                "main_score": 0.8714285714285714
            },
            {
                "hf_subset": "pam-eng",
                "languages": [
                    "pam-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.241,
                "f1": 0.19630969083349784,
                "precision": 0.18275094905094907,
                "recall": 0.241,
                "main_score": 0.19630969083349784
            },
            {
                "hf_subset": "hsb-eng",
                "languages": [
                    "hsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8343685300207039,
                "f1": 0.7945183870649708,
                "precision": 0.777432712215321,
                "recall": 0.8343685300207039,
                "main_score": 0.7945183870649708
            },
            {
                "hf_subset": "srp-eng",
                "languages": [
                    "srp-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.958,
                "f1": 0.9453333333333334,
                "precision": 0.9391666666666666,
                "recall": 0.958,
                "main_score": 0.9453333333333334
            },
            {
                "hf_subset": "epo-eng",
                "languages": [
                    "epo-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.988,
                "f1": 0.9848333333333332,
                "precision": 0.9833333333333334,
                "recall": 0.988,
                "main_score": 0.9848333333333332
            },
            {
                "hf_subset": "kzj-eng",
                "languages": [
                    "kzj-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.175,
                "f1": 0.14979285714285714,
                "precision": 0.1423235060690943,
                "recall": 0.175,
                "main_score": 0.14979285714285714
            },
            {
                "hf_subset": "awa-eng",
                "languages": [
                    "awa-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.9393939393939393,
                "f1": 0.91991341991342,
                "precision": 0.9105339105339105,
                "recall": 0.9393939393939393,
                "main_score": 0.91991341991342
            },
            {
                "hf_subset": "fao-eng",
                "languages": [
                    "fao-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8931297709923665,
                "f1": 0.8676844783715012,
                "precision": 0.8563613231552164,
                "recall": 0.8931297709923665,
                "main_score": 0.8676844783715012
            },
            {
                "hf_subset": "mal-eng",
                "languages": [
                    "mal-Mlym",
                    "eng-Latn"
                ],
                "accuracy": 0.9912663755458514,
                "f1": 0.9893255701115965,
                "precision": 0.9883551673944687,
                "recall": 0.9912663755458514,
                "main_score": 0.9893255701115965
            },
            {
                "hf_subset": "ile-eng",
                "languages": [
                    "ile-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.92,
                "f1": 0.8977999999999998,
                "precision": 0.8878333333333334,
                "recall": 0.92,
                "main_score": 0.8977999999999998
            },
            {
                "hf_subset": "bos-eng",
                "languages": [
                    "bos-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9689265536723164,
                "f1": 0.9585687382297553,
                "precision": 0.9533898305084746,
                "recall": 0.9689265536723164,
                "main_score": 0.9585687382297553
            },
            {
                "hf_subset": "cor-eng",
                "languages": [
                    "cor-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.146,
                "f1": 0.11820611790170615,
                "precision": 0.11022616224355355,
                "recall": 0.146,
                "main_score": 0.11820611790170615
            },
            {
                "hf_subset": "cat-eng",
                "languages": [
                    "cat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.959,
                "f1": 0.9493333333333334,
                "precision": 0.9448666666666666,
                "recall": 0.959,
                "main_score": 0.9493333333333334
            },
            {
                "hf_subset": "eus-eng",
                "languages": [
                    "eus-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8759999999999999,
                "f1": 0.8472333333333334,
                "precision": 0.8344166666666666,
                "recall": 0.8759999999999999,
                "main_score": 0.8472333333333334
            },
            {
                "hf_subset": "yue-eng",
                "languages": [
                    "yue-Hant",
                    "eng-Latn"
                ],
                "accuracy": 0.948,
                "f1": 0.9347333333333333,
                "precision": 0.92875,
                "recall": 0.948,
                "main_score": 0.9347333333333333
            },
            {
                "hf_subset": "swe-eng",
                "languages": [
                    "swe-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.966,
                "f1": 0.9571666666666665,
                "precision": 0.9528333333333334,
                "recall": 0.966,
                "main_score": 0.9571666666666665
            },
            {
                "hf_subset": "dtp-eng",
                "languages": [
                    "dtp-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.17800000000000002,
                "f1": 0.14511074040901628,
                "precision": 0.13503791000666002,
                "recall": 0.17800000000000002,
                "main_score": 0.14511074040901628
            },
            {
                "hf_subset": "kat-eng",
                "languages": [
                    "kat-Geor",
                    "eng-Latn"
                ],
                "accuracy": 0.9410187667560321,
                "f1": 0.9246648793565683,
                "precision": 0.9171134941912423,
                "recall": 0.9410187667560321,
                "main_score": 0.9246648793565683
            },
            {
                "hf_subset": "jpn-eng",
                "languages": [
                    "jpn-Jpan",
                    "eng-Latn"
                ],
                "accuracy": 0.97,
                "f1": 0.9611666666666666,
                "precision": 0.9568333333333334,
                "recall": 0.97,
                "main_score": 0.9611666666666666
            },
            {
                "hf_subset": "csb-eng",
                "languages": [
                    "csb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7272727272727273,
                "f1": 0.6658949745906267,
                "precision": 0.6386693017127799,
                "recall": 0.7272727272727273,
                "main_score": 0.6658949745906267
            },
            {
                "hf_subset": "xho-eng",
                "languages": [
                    "xho-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9014084507042255,
                "f1": 0.8826291079812206,
                "precision": 0.8732394366197183,
                "recall": 0.9014084507042255,
                "main_score": 0.8826291079812206
            },
            {
                "hf_subset": "orv-eng",
                "languages": [
                    "orv-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.6467065868263472,
                "f1": 0.582876627696987,
                "precision": 0.5579255774165953,
                "recall": 0.6467065868263472,
                "main_score": 0.582876627696987
            },
            {
                "hf_subset": "ind-eng",
                "languages": [
                    "ind-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.956,
                "f1": 0.9441666666666667,
                "precision": 0.9384999999999999,
                "recall": 0.956,
                "main_score": 0.9441666666666667
            },
            {
                "hf_subset": "tuk-eng",
                "languages": [
                    "tuk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.5517241379310345,
                "f1": 0.4963992493549144,
                "precision": 0.47714051137696456,
                "recall": 0.5517241379310345,
                "main_score": 0.4963992493549144
            },
            {
                "hf_subset": "max-eng",
                "languages": [
                    "max-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.7746478873239436,
                "f1": 0.734417616811983,
                "precision": 0.7191607981220658,
                "recall": 0.7746478873239436,
                "main_score": 0.734417616811983
            },
            {
                "hf_subset": "swh-eng",
                "languages": [
                    "swh-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8461538461538461,
                "f1": 0.8091452991452993,
                "precision": 0.7933760683760682,
                "recall": 0.8461538461538461,
                "main_score": 0.8091452991452993
            },
            {
                "hf_subset": "hin-eng",
                "languages": [
                    "hin-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.982,
                "f1": 0.976,
                "precision": 0.973,
                "recall": 0.982,
                "main_score": 0.976
            },
            {
                "hf_subset": "dsb-eng",
                "languages": [
                    "dsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.755741127348643,
                "f1": 0.7200417536534445,
                "precision": 0.7053467872883321,
                "recall": 0.755741127348643,
                "main_score": 0.7200417536534445
            },
            {
                "hf_subset": "ber-eng",
                "languages": [
                    "ber-Tfng",
                    "eng-Latn"
                ],
                "accuracy": 0.622,
                "f1": 0.5557746031746031,
                "precision": 0.5298583333333333,
                "recall": 0.622,
                "main_score": 0.5557746031746031
            },
            {
                "hf_subset": "tam-eng",
                "languages": [
                    "tam-Taml",
                    "eng-Latn"
                ],
                "accuracy": 0.9218241042345277,
                "f1": 0.906468124709167,
                "precision": 0.8995656894679697,
                "recall": 0.9218241042345277,
                "main_score": 0.906468124709167
            },
            {
                "hf_subset": "slk-eng",
                "languages": [
                    "slk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.961,
                "f1": 0.9513333333333333,
                "precision": 0.9466666666666668,
                "recall": 0.961,
                "main_score": 0.9513333333333333
            },
            {
                "hf_subset": "tgl-eng",
                "languages": [
                    "tgl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.968,
                "f1": 0.9585000000000001,
                "precision": 0.954,
                "recall": 0.968,
                "main_score": 0.9585000000000001
            },
            {
                "hf_subset": "ast-eng",
                "languages": [
                    "ast-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9212598425196851,
                "f1": 0.8976377952755905,
                "precision": 0.8871391076115485,
                "recall": 0.9212598425196851,
                "main_score": 0.8976377952755905
            },
            {
                "hf_subset": "mkd-eng",
                "languages": [
                    "mkd-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.941,
                "f1": 0.9248999999999999,
                "precision": 0.9172499999999999,
                "recall": 0.941,
                "main_score": 0.9248999999999999
            },
            {
                "hf_subset": "khm-eng",
                "languages": [
                    "khm-Khmr",
                    "eng-Latn"
                ],
                "accuracy": 0.775623268698061,
                "f1": 0.7327364463791057,
                "precision": 0.7151947852086358,
                "recall": 0.775623268698061,
                "main_score": 0.7327364463791057
            },
            {
                "hf_subset": "ces-eng",
                "languages": [
                    "ces-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9739999999999999,
                "f1": 0.9656666666666667,
                "precision": 0.9616666666666667,
                "recall": 0.9739999999999999,
                "main_score": 0.9656666666666667
            },
            {
                "hf_subset": "tzl-eng",
                "languages": [
                    "tzl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.6634615384615384,
                "f1": 0.6109203296703296,
                "precision": 0.5927197802197802,
                "recall": 0.6634615384615384,
                "main_score": 0.6109203296703296
            },
            {
                "hf_subset": "urd-eng",
                "languages": [
                    "urd-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.949,
                "f1": 0.9341190476190476,
                "precision": 0.927,
                "recall": 0.949,
                "main_score": 0.9341190476190476
            },
            {
                "hf_subset": "ara-eng",
                "languages": [
                    "ara-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.931,
                "f1": 0.911,
                "precision": 0.9013333333333332,
                "recall": 0.931,
                "main_score": 0.911
            },
            {
                "hf_subset": "kor-eng",
                "languages": [
                    "kor-Hang",
                    "eng-Latn"
                ],
                "accuracy": 0.937,
                "f1": 0.9197333333333334,
                "precision": 0.9114166666666667,
                "recall": 0.937,
                "main_score": 0.9197333333333334
            },
            {
                "hf_subset": "yid-eng",
                "languages": [
                    "yid-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.9221698113207547,
                "f1": 0.903796046720575,
                "precision": 0.8956367924528302,
                "recall": 0.9221698113207547,
                "main_score": 0.903796046720575
            },
            {
                "hf_subset": "fin-eng",
                "languages": [
                    "fin-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.976,
                "f1": 0.9691666666666667,
                "precision": 0.966,
                "recall": 0.976,
                "main_score": 0.9691666666666667
            },
            {
                "hf_subset": "tha-eng",
                "languages": [
                    "tha-Thai",
                    "eng-Latn"
                ],
                "accuracy": 0.9744525547445255,
                "f1": 0.9671532846715328,
                "precision": 0.9635036496350365,
                "recall": 0.9744525547445255,
                "main_score": 0.9671532846715328
            },
            {
                "hf_subset": "wuu-eng",
                "languages": [
                    "wuu-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.941,
                "f1": 0.9234000000000002,
                "precision": 0.9149166666666667,
                "recall": 0.941,
                "main_score": 0.9234000000000002
            }
        ]
    }
}