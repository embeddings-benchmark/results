{
  "dataset_revision": "2269ed7d95d8abaab829f1592b4b2047372e9f81",
  "task_name": "DutchColaClassification",
  "mteb_version": "2.1.8",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.52375,
            "f1": 0.521957,
            "f1_weighted": 0.521957,
            "precision": 0.524112,
            "precision_weighted": 0.524112,
            "recall": 0.52375,
            "recall_weighted": 0.52375,
            "ap": 0.512378,
            "ap_weighted": 0.512378
          },
          {
            "accuracy": 0.5425,
            "f1": 0.542174,
            "f1_weighted": 0.542174,
            "precision": 0.542621,
            "precision_weighted": 0.542621,
            "recall": 0.5425,
            "recall_weighted": 0.5425,
            "ap": 0.522965,
            "ap_weighted": 0.522965
          },
          {
            "accuracy": 0.531667,
            "f1": 0.529975,
            "f1_weighted": 0.529975,
            "precision": 0.532129,
            "precision_weighted": 0.532129,
            "recall": 0.531667,
            "recall_weighted": 0.531667,
            "ap": 0.516729,
            "ap_weighted": 0.516729
          },
          {
            "accuracy": 0.56,
            "f1": 0.557621,
            "f1_weighted": 0.557621,
            "precision": 0.561319,
            "precision_weighted": 0.561319,
            "recall": 0.56,
            "recall_weighted": 0.56,
            "ap": 0.53314,
            "ap_weighted": 0.53314
          },
          {
            "accuracy": 0.55875,
            "f1": 0.558722,
            "f1_weighted": 0.558722,
            "precision": 0.558765,
            "precision_weighted": 0.558765,
            "recall": 0.55875,
            "recall_weighted": 0.55875,
            "ap": 0.532773,
            "ap_weighted": 0.532773
          },
          {
            "accuracy": 0.548333,
            "f1": 0.548051,
            "f1_weighted": 0.548051,
            "precision": 0.548454,
            "precision_weighted": 0.548454,
            "recall": 0.548333,
            "recall_weighted": 0.548333,
            "ap": 0.526392,
            "ap_weighted": 0.526392
          },
          {
            "accuracy": 0.566667,
            "f1": 0.566377,
            "f1_weighted": 0.566377,
            "precision": 0.566845,
            "precision_weighted": 0.566845,
            "recall": 0.566667,
            "recall_weighted": 0.566667,
            "ap": 0.53802,
            "ap_weighted": 0.53802
          },
          {
            "accuracy": 0.53875,
            "f1": 0.535557,
            "f1_weighted": 0.535557,
            "precision": 0.539846,
            "precision_weighted": 0.539846,
            "recall": 0.53875,
            "recall_weighted": 0.53875,
            "ap": 0.520663,
            "ap_weighted": 0.520663
          },
          {
            "accuracy": 0.53375,
            "f1": 0.527455,
            "f1_weighted": 0.527455,
            "precision": 0.53565,
            "precision_weighted": 0.53565,
            "recall": 0.53375,
            "recall_weighted": 0.53375,
            "ap": 0.5178,
            "ap_weighted": 0.5178
          },
          {
            "accuracy": 0.555417,
            "f1": 0.555148,
            "f1_weighted": 0.555148,
            "precision": 0.555551,
            "precision_weighted": 0.555551,
            "recall": 0.555417,
            "recall_weighted": 0.555417,
            "ap": 0.530635,
            "ap_weighted": 0.530635
          }
        ],
        "accuracy": 0.545958,
        "f1": 0.544304,
        "f1_weighted": 0.544304,
        "precision": 0.546529,
        "precision_weighted": 0.546529,
        "recall": 0.545958,
        "recall_weighted": 0.545958,
        "ap": 0.525149,
        "ap_weighted": 0.525149,
        "main_score": 0.544304,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 38.97218465805054,
  "kg_co2_emissions": null
}