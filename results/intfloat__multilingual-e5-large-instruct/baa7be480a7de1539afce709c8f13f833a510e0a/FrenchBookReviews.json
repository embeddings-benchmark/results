{
  "dataset_revision": "534725e03fec6f560dbe8166e8ae3825314a6290",
  "evaluation_time": 812.8804879188538,
  "kg_co2_emissions": 0.016866192317205795,
  "mteb_version": "1.12.41",
  "scores": {
    "train": [
      {
        "accuracy": 0.472314453125,
        "f1": 0.3943863681841481,
        "f1_weighted": 0.50304351693718,
        "hf_subset": "default",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.472314453125,
        "scores_per_experiment": [
          {
            "accuracy": 0.40673828125,
            "f1": 0.35100726807549293,
            "f1_weighted": 0.4617960086251717
          },
          {
            "accuracy": 0.62060546875,
            "f1": 0.4605327419281391,
            "f1_weighted": 0.6151799862168215
          },
          {
            "accuracy": 0.4755859375,
            "f1": 0.4084867653513376,
            "f1_weighted": 0.5076594319223398
          },
          {
            "accuracy": 0.4970703125,
            "f1": 0.4198601804981936,
            "f1_weighted": 0.5342311803485146
          },
          {
            "accuracy": 0.5146484375,
            "f1": 0.3684025597965442,
            "f1_weighted": 0.5547721379792918
          },
          {
            "accuracy": 0.45361328125,
            "f1": 0.4211572404051,
            "f1_weighted": 0.48770758550631865
          },
          {
            "accuracy": 0.5234375,
            "f1": 0.39796110873867163,
            "f1_weighted": 0.543894656620781
          },
          {
            "accuracy": 0.4521484375,
            "f1": 0.39591254405036075,
            "f1_weighted": 0.4916893516066101
          },
          {
            "accuracy": 0.46240234375,
            "f1": 0.4159386664834821,
            "f1_weighted": 0.4962423758320857
          },
          {
            "accuracy": 0.31689453125,
            "f1": 0.3046046065141587,
            "f1_weighted": 0.3372624547138648
          }
        ]
      }
    ]
  },
  "task_name": "FrenchBookReviews"
}