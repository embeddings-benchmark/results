{
  "dataset_revision": "12ca3b695563788fead87a982ad1a068284413f4",
  "evaluation_time": 751.5998628139496,
  "kg_co2_emissions": 0.015721700757009807,
  "mteb_version": "1.12.41",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.70654296875,
          "accuracy_threshold": 0.786007285118103,
          "ap": 0.8048676059983706,
          "f1": 0.7749911691981634,
          "f1_threshold": 0.7687939405441284,
          "precision": 0.6813664596273292,
          "recall": 0.8984438984438985
        },
        "dot": {
          "accuracy": 0.59716796875,
          "accuracy_threshold": 259.0301513671875,
          "ap": 0.5710612386677695,
          "f1": 0.7474747474747475,
          "f1_threshold": 259.0301513671875,
          "precision": 0.5967741935483871,
          "recall": 1.0
        },
        "euclidean": {
          "accuracy": 0.7080078125,
          "accuracy_threshold": 13.61213493347168,
          "ap": 0.8210343061876728,
          "f1": 0.7707808564231738,
          "f1_threshold": 14.283919334411621,
          "precision": 0.6874197689345315,
          "recall": 0.8771498771498771
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.70849609375,
        "manhattan": {
          "accuracy": 0.70849609375,
          "accuracy_threshold": 344.73577880859375,
          "ap": 0.8212971142034774,
          "f1": 0.7691232972406566,
          "f1_threshold": 367.11328125,
          "precision": 0.6705237515225335,
          "recall": 0.9017199017199017
        },
        "max": {
          "accuracy": 0.70849609375,
          "ap": 0.8212971142034774,
          "f1": 0.7749911691981634
        },
        "similarity": {
          "accuracy": 0.70654296875,
          "accuracy_threshold": 0.786007285118103,
          "ap": 0.8048676059983706,
          "f1": 0.7749911691981634,
          "f1_threshold": 0.7687939405441284,
          "precision": 0.6813664596273292,
          "recall": 0.8984438984438985
        }
      }
    ]
  },
  "task_name": "LegalBenchPC"
}