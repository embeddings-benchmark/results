{
  "dataset_revision": "b48bc27d383cfca5b6a47135a52390fa5f66b253",
  "task_name": "AmazonCounterfactualVNClassification",
  "mteb_version": "1.38.42",
  "scores": {
    "test": [
      {
        "accuracy": 0.561803,
        "f1": 0.510123,
        "f1_weighted": 0.6082,
        "ap": 0.223567,
        "ap_weighted": 0.223567,
        "scores_per_experiment": [
          {
            "accuracy": 0.519313,
            "f1": 0.48756,
            "f1_weighted": 0.568585,
            "ap": 0.223415,
            "ap_weighted": 0.223415
          },
          {
            "accuracy": 0.555794,
            "f1": 0.520154,
            "f1_weighted": 0.60322,
            "ap": 0.242533,
            "ap_weighted": 0.242533
          },
          {
            "accuracy": 0.650215,
            "f1": 0.577793,
            "f1_weighted": 0.688864,
            "ap": 0.253677,
            "ap_weighted": 0.253677
          },
          {
            "accuracy": 0.463519,
            "f1": 0.440764,
            "f1_weighted": 0.512419,
            "ap": 0.204033,
            "ap_weighted": 0.204033
          },
          {
            "accuracy": 0.476395,
            "f1": 0.445498,
            "f1_weighted": 0.528639,
            "ap": 0.198372,
            "ap_weighted": 0.198372
          },
          {
            "accuracy": 0.527897,
            "f1": 0.486712,
            "f1_weighted": 0.579066,
            "ap": 0.213594,
            "ap_weighted": 0.213594
          },
          {
            "accuracy": 0.618026,
            "f1": 0.552773,
            "f1_weighted": 0.661283,
            "ap": 0.239689,
            "ap_weighted": 0.239689
          },
          {
            "accuracy": 0.641631,
            "f1": 0.555635,
            "f1_weighted": 0.679804,
            "ap": 0.229205,
            "ap_weighted": 0.229205
          },
          {
            "accuracy": 0.583691,
            "f1": 0.5215,
            "f1_weighted": 0.631075,
            "ap": 0.220166,
            "ap_weighted": 0.220166
          },
          {
            "accuracy": 0.581545,
            "f1": 0.512842,
            "f1_weighted": 0.629048,
            "ap": 0.210981,
            "ap_weighted": 0.210981
          }
        ],
        "main_score": 0.561803,
        "hf_subset": "default",
        "languages": [
          "vie-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 1.676079511642456,
  "kg_co2_emissions": null
}