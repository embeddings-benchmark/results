{
    "dataset_revision": "ed9a4403ed4adbfaf4aab56d5b2709e9f6c3ba33",
    "task_name": "NTREXBitextMining",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "arb_Arab-hun_Latn",
                "languages": [
                    "arb-Arab",
                    "hun-Latn"
                ],
                "accuracy": 0.8567851777666501,
                "f1": 0.8192049979731502,
                "main_score": 0.8192049979731502,
                "precision": 0.8021115005842097,
                "recall": 0.8567851777666501
            },
            {
                "hf_subset": "ben_Beng-hun_Latn",
                "languages": [
                    "ben-Beng",
                    "hun-Latn"
                ],
                "accuracy": 0.4456685027541312,
                "f1": 0.3907033025889276,
                "main_score": 0.3907033025889276,
                "precision": 0.3707348327291399,
                "recall": 0.4456685027541312
            },
            {
                "hf_subset": "deu_Latn-hun_Latn",
                "languages": [
                    "deu-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.9344016024036054,
                "f1": 0.9161909530963112,
                "main_score": 0.9161909530963112,
                "precision": 0.9075279586045736,
                "recall": 0.9344016024036054
            },
            {
                "hf_subset": "ell_Grek-hun_Latn",
                "languages": [
                    "ell-Grek",
                    "hun-Latn"
                ],
                "accuracy": 0.9143715573360041,
                "f1": 0.8902615828504661,
                "main_score": 0.8902615828504661,
                "precision": 0.8790435653480221,
                "recall": 0.9143715573360041
            },
            {
                "hf_subset": "eng_Latn-hun_Latn",
                "languages": [
                    "eng-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.9444166249374061,
                "f1": 0.9280921382073111,
                "main_score": 0.9280921382073111,
                "precision": 0.920422300116842,
                "recall": 0.9444166249374061
            },
            {
                "hf_subset": "fas_Arab-hun_Latn",
                "languages": [
                    "fas-Arab",
                    "hun-Latn"
                ],
                "accuracy": 0.8953430145217827,
                "f1": 0.8672270310227245,
                "main_score": 0.8672270310227245,
                "precision": 0.8542814221331997,
                "recall": 0.8953430145217827
            },
            {
                "hf_subset": "fin_Latn-hun_Latn",
                "languages": [
                    "fin-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.9098647971957938,
                "f1": 0.8844600233683859,
                "main_score": 0.8844600233683859,
                "precision": 0.8725755299616089,
                "recall": 0.9098647971957938
            },
            {
                "hf_subset": "fra_Latn-hun_Latn",
                "languages": [
                    "fra-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.9228843264897347,
                "f1": 0.9012518778167251,
                "main_score": 0.9012518778167251,
                "precision": 0.8912535469871473,
                "recall": 0.9228843264897347
            },
            {
                "hf_subset": "heb_Hebr-hun_Latn",
                "languages": [
                    "heb-Hebr",
                    "hun-Latn"
                ],
                "accuracy": 0.8733099649474211,
                "f1": 0.8388582874311468,
                "main_score": 0.8388582874311468,
                "precision": 0.8231263562009681,
                "recall": 0.8733099649474211
            },
            {
                "hf_subset": "hin_Deva-hun_Latn",
                "languages": [
                    "hin-Deva",
                    "hun-Latn"
                ],
                "accuracy": 0.8652979469203805,
                "f1": 0.8308240137984755,
                "main_score": 0.8308240137984755,
                "precision": 0.8151352028042064,
                "recall": 0.8652979469203805
            },
            {
                "hf_subset": "hun_Latn-arb_Arab",
                "languages": [
                    "hun-Latn",
                    "arb-Arab"
                ],
                "accuracy": 0.8673009514271406,
                "f1": 0.8312397167179342,
                "main_score": 0.8312397167179342,
                "precision": 0.8147805040894676,
                "recall": 0.8673009514271406
            },
            {
                "hf_subset": "hun_Latn-ben_Beng",
                "languages": [
                    "hun-Latn",
                    "ben-Beng"
                ],
                "accuracy": 0.41161742613920876,
                "f1": 0.32730255195202623,
                "main_score": 0.32730255195202623,
                "precision": 0.29859172986363774,
                "recall": 0.41161742613920876
            },
            {
                "hf_subset": "hun_Latn-deu_Latn",
                "languages": [
                    "hun-Latn",
                    "deu-Latn"
                ],
                "accuracy": 0.9339008512769154,
                "f1": 0.9154565181104989,
                "main_score": 0.9154565181104989,
                "precision": 0.9066099148723085,
                "recall": 0.9339008512769154
            },
            {
                "hf_subset": "hun_Latn-ell_Grek",
                "languages": [
                    "hun-Latn",
                    "ell-Grek"
                ],
                "accuracy": 0.9203805708562843,
                "f1": 0.8981305291270238,
                "main_score": 0.8981305291270238,
                "precision": 0.8878317476214321,
                "recall": 0.9203805708562843
            },
            {
                "hf_subset": "hun_Latn-eng_Latn",
                "languages": [
                    "hun-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9474211316975463,
                "f1": 0.9323985978968453,
                "main_score": 0.9323985978968453,
                "precision": 0.9251377065598397,
                "recall": 0.9474211316975463
            },
            {
                "hf_subset": "hun_Latn-fas_Arab",
                "languages": [
                    "hun-Latn",
                    "fas-Arab"
                ],
                "accuracy": 0.8853279919879821,
                "f1": 0.8549240527457853,
                "main_score": 0.8549240527457853,
                "precision": 0.8410413238905978,
                "recall": 0.8853279919879821
            },
            {
                "hf_subset": "hun_Latn-fin_Latn",
                "languages": [
                    "hun-Latn",
                    "fin-Latn"
                ],
                "accuracy": 0.9023535302954432,
                "f1": 0.8753296611584043,
                "main_score": 0.8753296611584043,
                "precision": 0.8626690035052579,
                "recall": 0.9023535302954432
            },
            {
                "hf_subset": "hun_Latn-fra_Latn",
                "languages": [
                    "hun-Latn",
                    "fra-Latn"
                ],
                "accuracy": 0.9263895843765648,
                "f1": 0.9047070605908862,
                "main_score": 0.9047070605908862,
                "precision": 0.89421632448673,
                "recall": 0.9263895843765648
            },
            {
                "hf_subset": "hun_Latn-heb_Hebr",
                "languages": [
                    "hun-Latn",
                    "heb-Hebr"
                ],
                "accuracy": 0.8662994491737607,
                "f1": 0.8319388173168845,
                "main_score": 0.8319388173168845,
                "precision": 0.8165832081455516,
                "recall": 0.8662994491737607
            },
            {
                "hf_subset": "hun_Latn-hin_Deva",
                "languages": [
                    "hun-Latn",
                    "hin-Deva"
                ],
                "accuracy": 0.8397596394591887,
                "f1": 0.7985502062617736,
                "main_score": 0.7985502062617736,
                "precision": 0.7801758192844823,
                "recall": 0.8397596394591887
            },
            {
                "hf_subset": "hun_Latn-ind_Latn",
                "languages": [
                    "hun-Latn",
                    "ind-Latn"
                ],
                "accuracy": 0.926890335503255,
                "f1": 0.9064596895343014,
                "main_score": 0.9064596895343014,
                "precision": 0.8968869971624103,
                "recall": 0.926890335503255
            },
            {
                "hf_subset": "hun_Latn-jpn_Jpan",
                "languages": [
                    "hun-Latn",
                    "jpn-Jpan"
                ],
                "accuracy": 0.85778668002003,
                "f1": 0.8219829744616924,
                "main_score": 0.8219829744616924,
                "precision": 0.8062426973794026,
                "recall": 0.85778668002003
            },
            {
                "hf_subset": "hun_Latn-kor_Hang",
                "languages": [
                    "hun-Latn",
                    "kor-Hang"
                ],
                "accuracy": 0.8417626439659489,
                "f1": 0.8026746468909713,
                "main_score": 0.8026746468909713,
                "precision": 0.785646097351155,
                "recall": 0.8417626439659489
            },
            {
                "hf_subset": "hun_Latn-lav_Latn",
                "languages": [
                    "hun-Latn",
                    "lav-Latn"
                ],
                "accuracy": 0.901352028042063,
                "f1": 0.8730262059756302,
                "main_score": 0.8730262059756302,
                "precision": 0.8598731430479052,
                "recall": 0.901352028042063
            },
            {
                "hf_subset": "hun_Latn-lit_Latn",
                "languages": [
                    "hun-Latn",
                    "lit-Latn"
                ],
                "accuracy": 0.8958437656484726,
                "f1": 0.868252378567852,
                "main_score": 0.868252378567852,
                "precision": 0.8554581872809215,
                "recall": 0.8958437656484726
            },
            {
                "hf_subset": "hun_Latn-nld_Latn",
                "languages": [
                    "hun-Latn",
                    "nld-Latn"
                ],
                "accuracy": 0.9303955933900852,
                "f1": 0.9103989317309297,
                "main_score": 0.9103989317309297,
                "precision": 0.9008930061759305,
                "recall": 0.9303955933900852
            },
            {
                "hf_subset": "hun_Latn-pol_Latn",
                "languages": [
                    "hun-Latn",
                    "pol-Latn"
                ],
                "accuracy": 0.9158738107160741,
                "f1": 0.8928225671841095,
                "main_score": 0.8928225671841095,
                "precision": 0.8818227341011518,
                "recall": 0.9158738107160741
            },
            {
                "hf_subset": "hun_Latn-por_Latn",
                "languages": [
                    "hun-Latn",
                    "por-Latn"
                ],
                "accuracy": 0.9359038557836754,
                "f1": 0.9171256885327992,
                "main_score": 0.9171256885327992,
                "precision": 0.9080287097312635,
                "recall": 0.9359038557836754
            },
            {
                "hf_subset": "hun_Latn-rus_Cyrl",
                "languages": [
                    "hun-Latn",
                    "rus-Cyrl"
                ],
                "accuracy": 0.9133700550826239,
                "f1": 0.8888916708395926,
                "main_score": 0.8888916708395926,
                "precision": 0.8775961561389704,
                "recall": 0.9133700550826239
            },
            {
                "hf_subset": "hun_Latn-spa_Latn",
                "languages": [
                    "hun-Latn",
                    "spa-Latn"
                ],
                "accuracy": 0.9369053580370555,
                "f1": 0.9194959105324653,
                "main_score": 0.9194959105324653,
                "precision": 0.9112418627941914,
                "recall": 0.9369053580370555
            },
            {
                "hf_subset": "hun_Latn-swa_Latn",
                "languages": [
                    "hun-Latn",
                    "swa-Latn"
                ],
                "accuracy": 0.35803705558337506,
                "f1": 0.2779832969518814,
                "main_score": 0.2779832969518814,
                "precision": 0.25370895920971037,
                "recall": 0.35803705558337506
            },
            {
                "hf_subset": "hun_Latn-swe_Latn",
                "languages": [
                    "hun-Latn",
                    "swe-Latn"
                ],
                "accuracy": 0.9359038557836754,
                "f1": 0.9166249374061092,
                "main_score": 0.9166249374061092,
                "precision": 0.9074445000834586,
                "recall": 0.9359038557836754
            },
            {
                "hf_subset": "hun_Latn-tam_Taml",
                "languages": [
                    "hun-Latn",
                    "tam-Taml"
                ],
                "accuracy": 0.27391086629944916,
                "f1": 0.19094552675413096,
                "main_score": 0.19094552675413096,
                "precision": 0.1688288208814635,
                "recall": 0.27391086629944916
            },
            {
                "hf_subset": "hun_Latn-tur_Latn",
                "languages": [
                    "hun-Latn",
                    "tur-Latn"
                ],
                "accuracy": 0.9148723084626941,
                "f1": 0.8911700884660323,
                "main_score": 0.8911700884660323,
                "precision": 0.8799031881155066,
                "recall": 0.9148723084626941
            },
            {
                "hf_subset": "hun_Latn-vie_Latn",
                "languages": [
                    "hun-Latn",
                    "vie-Latn"
                ],
                "accuracy": 0.9113670505758636,
                "f1": 0.8866967117342681,
                "main_score": 0.8866967117342681,
                "precision": 0.8749374061091637,
                "recall": 0.9113670505758636
            },
            {
                "hf_subset": "hun_Latn-zho_Hant",
                "languages": [
                    "hun-Latn",
                    "zho-Hant"
                ],
                "accuracy": 0.8933400100150224,
                "f1": 0.8655745523046474,
                "main_score": 0.8655745523046474,
                "precision": 0.8529794692038056,
                "recall": 0.8933400100150224
            },
            {
                "hf_subset": "hun_Latn-zul_Latn",
                "languages": [
                    "hun-Latn",
                    "zul-Latn"
                ],
                "accuracy": 0.16675012518778168,
                "f1": 0.1121636405139599,
                "main_score": 0.1121636405139599,
                "precision": 0.09903070059112946,
                "recall": 0.16675012518778168
            },
            {
                "hf_subset": "ind_Latn-hun_Latn",
                "languages": [
                    "ind-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.9293940911367051,
                "f1": 0.9096478050408946,
                "main_score": 0.9096478050408946,
                "precision": 0.9003922550492406,
                "recall": 0.9293940911367051
            },
            {
                "hf_subset": "jpn_Jpan-hun_Latn",
                "languages": [
                    "jpn-Jpan",
                    "hun-Latn"
                ],
                "accuracy": 0.8828242363545317,
                "f1": 0.8511433817392756,
                "main_score": 0.8511433817392756,
                "precision": 0.8367551326990486,
                "recall": 0.8828242363545317
            },
            {
                "hf_subset": "kor_Hang-hun_Latn",
                "languages": [
                    "kor-Hang",
                    "hun-Latn"
                ],
                "accuracy": 0.85778668002003,
                "f1": 0.8183608746453013,
                "main_score": 0.8183608746453013,
                "precision": 0.8002336838591221,
                "recall": 0.85778668002003
            },
            {
                "hf_subset": "lav_Latn-hun_Latn",
                "languages": [
                    "lav-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.9173760640961443,
                "f1": 0.8942914371557336,
                "main_score": 0.8942914371557336,
                "precision": 0.8832832582206642,
                "recall": 0.9173760640961443
            },
            {
                "hf_subset": "lit_Latn-hun_Latn",
                "languages": [
                    "lit-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.9178768152228343,
                "f1": 0.8950926389584376,
                "main_score": 0.8950926389584376,
                "precision": 0.8839926556501418,
                "recall": 0.9178768152228343
            },
            {
                "hf_subset": "nld_Latn-hun_Latn",
                "languages": [
                    "nld-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.9349023535302955,
                "f1": 0.916190953096311,
                "main_score": 0.916190953096311,
                "precision": 0.9072775830412286,
                "recall": 0.9349023535302955
            },
            {
                "hf_subset": "pol_Latn-hun_Latn",
                "languages": [
                    "pol-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.9128693039559339,
                "f1": 0.8899515940577533,
                "main_score": 0.8899515940577533,
                "precision": 0.879293940911367,
                "recall": 0.9128693039559339
            },
            {
                "hf_subset": "por_Latn-hun_Latn",
                "languages": [
                    "por-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.9303955933900852,
                "f1": 0.9108496077449508,
                "main_score": 0.9108496077449508,
                "precision": 0.9017860123518612,
                "recall": 0.9303955933900852
            },
            {
                "hf_subset": "rus_Cyrl-hun_Latn",
                "languages": [
                    "rus-Cyrl",
                    "hun-Latn"
                ],
                "accuracy": 0.9098647971957938,
                "f1": 0.8843932565514937,
                "main_score": 0.8843932565514937,
                "precision": 0.872475379736271,
                "recall": 0.9098647971957938
            },
            {
                "hf_subset": "spa_Latn-hun_Latn",
                "languages": [
                    "spa-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.9323985978968453,
                "f1": 0.913386746786847,
                "main_score": 0.913386746786847,
                "precision": 0.9043148055416457,
                "recall": 0.9323985978968453
            },
            {
                "hf_subset": "swa_Latn-hun_Latn",
                "languages": [
                    "swa-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.35953930896344516,
                "f1": 0.3061225793903419,
                "main_score": 0.3061225793903419,
                "precision": 0.28995078568906946,
                "recall": 0.35953930896344516
            },
            {
                "hf_subset": "swe_Latn-hun_Latn",
                "languages": [
                    "swe-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.9364046069103655,
                "f1": 0.9186613253213153,
                "main_score": 0.9186613253213153,
                "precision": 0.9104072775830413,
                "recall": 0.9364046069103655
            },
            {
                "hf_subset": "tam_Taml-hun_Latn",
                "languages": [
                    "tam-Taml",
                    "hun-Latn"
                ],
                "accuracy": 0.2904356534802203,
                "f1": 0.25164093122029807,
                "main_score": 0.25164093122029807,
                "precision": 0.23849573878565544,
                "recall": 0.2904356534802203
            },
            {
                "hf_subset": "tur_Latn-hun_Latn",
                "languages": [
                    "tur-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.9083625438157236,
                "f1": 0.8836087464530128,
                "main_score": 0.8836087464530128,
                "precision": 0.8719829744616925,
                "recall": 0.9083625438157236
            },
            {
                "hf_subset": "vie_Latn-hun_Latn",
                "languages": [
                    "vie-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.9068602904356535,
                "f1": 0.8810882991153397,
                "main_score": 0.8810882991153397,
                "precision": 0.8690118511099983,
                "recall": 0.9068602904356535
            },
            {
                "hf_subset": "zho_Hant-hun_Latn",
                "languages": [
                    "zho-Hant",
                    "hun-Latn"
                ],
                "accuracy": 0.901352028042063,
                "f1": 0.874603572024704,
                "main_score": 0.874603572024704,
                "precision": 0.8619810668383527,
                "recall": 0.901352028042063
            },
            {
                "hf_subset": "zul_Latn-hun_Latn",
                "languages": [
                    "zul-Latn",
                    "hun-Latn"
                ],
                "accuracy": 0.171256885327992,
                "f1": 0.13692538409811572,
                "main_score": 0.13692538409811572,
                "precision": 0.12811084017018845,
                "recall": 0.171256885327992
            }
        ]
    }
}