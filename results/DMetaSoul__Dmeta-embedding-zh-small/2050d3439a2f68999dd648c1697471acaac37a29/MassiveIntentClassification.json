{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.29.10",
  "scores": {
    "validation": [
      {
        "accuracy": 0.345204,
        "f1": 0.306497,
        "f1_weighted": 0.330536,
        "scores_per_experiment": [
          {
            "accuracy": 0.348254,
            "f1": 0.307948,
            "f1_weighted": 0.329845
          },
          {
            "accuracy": 0.335957,
            "f1": 0.296757,
            "f1_weighted": 0.327023
          },
          {
            "accuracy": 0.3212,
            "f1": 0.280409,
            "f1_weighted": 0.303991
          },
          {
            "accuracy": 0.358091,
            "f1": 0.313084,
            "f1_weighted": 0.341283
          },
          {
            "accuracy": 0.391048,
            "f1": 0.330387,
            "f1_weighted": 0.37768
          },
          {
            "accuracy": 0.342351,
            "f1": 0.30008,
            "f1_weighted": 0.333714
          },
          {
            "accuracy": 0.32366,
            "f1": 0.299903,
            "f1_weighted": 0.304057
          },
          {
            "accuracy": 0.347762,
            "f1": 0.308596,
            "f1_weighted": 0.331907
          },
          {
            "accuracy": 0.344811,
            "f1": 0.322202,
            "f1_weighted": 0.33355
          },
          {
            "accuracy": 0.338908,
            "f1": 0.305609,
            "f1_weighted": 0.322312
          }
        ],
        "main_score": 0.345204,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.360457,
        "f1": 0.322571,
        "f1_weighted": 0.339201,
        "scores_per_experiment": [
          {
            "accuracy": 0.383322,
            "f1": 0.342196,
            "f1_weighted": 0.368308
          },
          {
            "accuracy": 0.354069,
            "f1": 0.309386,
            "f1_weighted": 0.335541
          },
          {
            "accuracy": 0.33961,
            "f1": 0.302399,
            "f1_weighted": 0.308823
          },
          {
            "accuracy": 0.362811,
            "f1": 0.319615,
            "f1_weighted": 0.334232
          },
          {
            "accuracy": 0.389711,
            "f1": 0.334247,
            "f1_weighted": 0.367717
          },
          {
            "accuracy": 0.34768,
            "f1": 0.324039,
            "f1_weighted": 0.331642
          },
          {
            "accuracy": 0.339274,
            "f1": 0.316726,
            "f1_weighted": 0.31117
          },
          {
            "accuracy": 0.374243,
            "f1": 0.328008,
            "f1_weighted": 0.355679
          },
          {
            "accuracy": 0.362811,
            "f1": 0.330044,
            "f1_weighted": 0.342452
          },
          {
            "accuracy": 0.351042,
            "f1": 0.319048,
            "f1_weighted": 0.336443
          }
        ],
        "main_score": 0.360457,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 20.805570363998413,
  "kg_co2_emissions": null
}