{
  "dataset_revision": "b07c6ce548f6a7ac8d546e1bbe197a0086409190",
  "task_name": "SweRecClassification",
  "mteb_version": "2.1.17",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.852539,
            "f1": 0.74635,
            "f1_weighted": 0.846851,
            "precision": 0.755083,
            "precision_weighted": 0.842679,
            "recall": 0.741087,
            "recall_weighted": 0.852539,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.811035,
            "f1": 0.737502,
            "f1_weighted": 0.826022,
            "precision": 0.735693,
            "precision_weighted": 0.851219,
            "recall": 0.757926,
            "recall_weighted": 0.811035,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.827148,
            "f1": 0.734479,
            "f1_weighted": 0.831793,
            "precision": 0.732636,
            "precision_weighted": 0.841815,
            "recall": 0.741482,
            "recall_weighted": 0.827148,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.836914,
            "f1": 0.736966,
            "f1_weighted": 0.836799,
            "precision": 0.737062,
            "precision_weighted": 0.836685,
            "recall": 0.736872,
            "recall_weighted": 0.836914,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.819824,
            "f1": 0.711168,
            "f1_weighted": 0.821027,
            "precision": 0.710353,
            "precision_weighted": 0.822395,
            "recall": 0.712157,
            "recall_weighted": 0.819824,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.837891,
            "f1": 0.725462,
            "f1_weighted": 0.831862,
            "precision": 0.732818,
            "precision_weighted": 0.827223,
            "recall": 0.721265,
            "recall_weighted": 0.837891,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.837891,
            "f1": 0.744172,
            "f1_weighted": 0.838166,
            "precision": 0.745227,
            "precision_weighted": 0.839891,
            "recall": 0.744254,
            "recall_weighted": 0.837891,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.839844,
            "f1": 0.769278,
            "f1_weighted": 0.850925,
            "precision": 0.763355,
            "precision_weighted": 0.868319,
            "recall": 0.78855,
            "recall_weighted": 0.839844,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.76709,
            "f1": 0.684158,
            "f1_weighted": 0.785047,
            "precision": 0.694352,
            "precision_weighted": 0.826216,
            "recall": 0.702519,
            "recall_weighted": 0.76709,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.850098,
            "f1": 0.751346,
            "f1_weighted": 0.84716,
            "precision": 0.755591,
            "precision_weighted": 0.84464,
            "recall": 0.748029,
            "recall_weighted": 0.850098,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.828027,
        "f1": 0.734088,
        "f1_weighted": 0.831565,
        "precision": 0.736217,
        "precision_weighted": 0.840108,
        "recall": 0.739414,
        "recall_weighted": 0.828027,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.828027,
        "hf_subset": "default",
        "languages": [
          "swe-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 39.04622793197632,
  "kg_co2_emissions": null
}