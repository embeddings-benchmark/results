{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "task_name": "InappropriatenessClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.522363,
        "f1": 0.520899,
        "f1_weighted": 0.520899,
        "ap": 0.51205,
        "ap_weighted": 0.51205,
        "scores_per_experiment": [
          {
            "accuracy": 0.556641,
            "f1": 0.556558,
            "f1_weighted": 0.556558,
            "ap": 0.531443,
            "ap_weighted": 0.531443
          },
          {
            "accuracy": 0.532715,
            "f1": 0.527409,
            "f1_weighted": 0.527409,
            "ap": 0.517241,
            "ap_weighted": 0.517241
          },
          {
            "accuracy": 0.540039,
            "f1": 0.539644,
            "f1_weighted": 0.539644,
            "ap": 0.521722,
            "ap_weighted": 0.521722
          },
          {
            "accuracy": 0.505371,
            "f1": 0.505153,
            "f1_weighted": 0.505153,
            "ap": 0.502713,
            "ap_weighted": 0.502713
          },
          {
            "accuracy": 0.532715,
            "f1": 0.531338,
            "f1_weighted": 0.531338,
            "ap": 0.517558,
            "ap_weighted": 0.517558
          },
          {
            "accuracy": 0.531738,
            "f1": 0.531448,
            "f1_weighted": 0.531448,
            "ap": 0.516929,
            "ap_weighted": 0.516929
          },
          {
            "accuracy": 0.500977,
            "f1": 0.495571,
            "f1_weighted": 0.495571,
            "ap": 0.500489,
            "ap_weighted": 0.500489
          },
          {
            "accuracy": 0.490723,
            "f1": 0.490702,
            "f1_weighted": 0.490702,
            "ap": 0.495449,
            "ap_weighted": 0.495449
          },
          {
            "accuracy": 0.509766,
            "f1": 0.508731,
            "f1_weighted": 0.508731,
            "ap": 0.504988,
            "ap_weighted": 0.504988
          },
          {
            "accuracy": 0.522949,
            "f1": 0.522438,
            "f1_weighted": 0.522438,
            "ap": 0.511969,
            "ap_weighted": 0.511969
          }
        ],
        "main_score": 0.522363,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 716.3607952594757,
  "kg_co2_emissions": null
}