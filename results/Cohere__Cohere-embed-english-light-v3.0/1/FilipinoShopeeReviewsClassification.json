{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.31416,
        "f1": 0.304054,
        "f1_weighted": 0.304045,
        "scores_per_experiment": [
          {
            "accuracy": 0.33252,
            "f1": 0.325581,
            "f1_weighted": 0.325564
          },
          {
            "accuracy": 0.291504,
            "f1": 0.28109,
            "f1_weighted": 0.281108
          },
          {
            "accuracy": 0.316895,
            "f1": 0.303234,
            "f1_weighted": 0.303236
          },
          {
            "accuracy": 0.336914,
            "f1": 0.326826,
            "f1_weighted": 0.326815
          },
          {
            "accuracy": 0.294922,
            "f1": 0.282929,
            "f1_weighted": 0.282912
          },
          {
            "accuracy": 0.312988,
            "f1": 0.303766,
            "f1_weighted": 0.303747
          },
          {
            "accuracy": 0.267578,
            "f1": 0.264596,
            "f1_weighted": 0.26455
          },
          {
            "accuracy": 0.339355,
            "f1": 0.328174,
            "f1_weighted": 0.328186
          },
          {
            "accuracy": 0.337402,
            "f1": 0.327233,
            "f1_weighted": 0.327209
          },
          {
            "accuracy": 0.311523,
            "f1": 0.29711,
            "f1_weighted": 0.297121
          }
        ],
        "main_score": 0.31416,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.316162,
        "f1": 0.305872,
        "f1_weighted": 0.305869,
        "scores_per_experiment": [
          {
            "accuracy": 0.315918,
            "f1": 0.308126,
            "f1_weighted": 0.308098
          },
          {
            "accuracy": 0.304688,
            "f1": 0.294389,
            "f1_weighted": 0.294405
          },
          {
            "accuracy": 0.327637,
            "f1": 0.313417,
            "f1_weighted": 0.31342
          },
          {
            "accuracy": 0.318848,
            "f1": 0.310089,
            "f1_weighted": 0.310083
          },
          {
            "accuracy": 0.316895,
            "f1": 0.304178,
            "f1_weighted": 0.304167
          },
          {
            "accuracy": 0.306152,
            "f1": 0.292014,
            "f1_weighted": 0.291988
          },
          {
            "accuracy": 0.269531,
            "f1": 0.269013,
            "f1_weighted": 0.268989
          },
          {
            "accuracy": 0.350586,
            "f1": 0.334827,
            "f1_weighted": 0.334851
          },
          {
            "accuracy": 0.334961,
            "f1": 0.325022,
            "f1_weighted": 0.325029
          },
          {
            "accuracy": 0.316406,
            "f1": 0.307641,
            "f1_weighted": 0.307661
          }
        ],
        "main_score": 0.316162,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 157.30241656303406,
  "kg_co2_emissions": null
}