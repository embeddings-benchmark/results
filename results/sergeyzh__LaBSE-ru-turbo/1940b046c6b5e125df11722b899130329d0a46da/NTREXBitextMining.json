{
  "dataset_revision": "ed9a4403ed4adbfaf4aab56d5b2709e9f6c3ba33",
  "evaluation_time": 80.0495195388794,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test": [
      {
        "accuracy": 0.06509764646970456,
        "f1": 0.04880756202406125,
        "hf_subset": "arb_Arab-rus_Cyrl",
        "languages": [
          "arb-Arab",
          "rus-Cyrl"
        ],
        "main_score": 0.04880756202406125,
        "precision": 0.04443315384313043,
        "recall": 0.06509764646970456
      },
      {
        "accuracy": 0.8407611417125689,
        "f1": 0.8067246832982392,
        "hf_subset": "bel_Cyrl-rus_Cyrl",
        "languages": [
          "bel-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.8067246832982392,
        "precision": 0.7928289259285755,
        "recall": 0.8407611417125689
      },
      {
        "accuracy": 0.04406609914872309,
        "f1": 0.03200583539750235,
        "hf_subset": "ben_Beng-rus_Cyrl",
        "languages": [
          "ben-Beng",
          "rus-Cyrl"
        ],
        "main_score": 0.03200583539750235,
        "precision": 0.02928169556766814,
        "recall": 0.04406609914872309
      },
      {
        "accuracy": 0.5948923385077617,
        "f1": 0.5494027353507088,
        "hf_subset": "bos_Latn-rus_Cyrl",
        "languages": [
          "bos-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.5494027353507088,
        "precision": 0.5350027748191425,
        "recall": 0.5948923385077617
      },
      {
        "accuracy": 0.9459188783174762,
        "f1": 0.9311999134232484,
        "hf_subset": "bul_Cyrl-rus_Cyrl",
        "languages": [
          "bul-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.9311999134232484,
        "precision": 0.9246953763979302,
        "recall": 0.9459188783174762
      },
      {
        "accuracy": 0.5818728092138208,
        "f1": 0.5338965487515986,
        "hf_subset": "ces_Latn-rus_Cyrl",
        "languages": [
          "ces-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.5338965487515986,
        "precision": 0.5176792671609898,
        "recall": 0.5818728092138208
      },
      {
        "accuracy": 0.7080620931397096,
        "f1": 0.6657826128771475,
        "hf_subset": "deu_Latn-rus_Cyrl",
        "languages": [
          "deu-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.6657826128771475,
        "precision": 0.6508054856129935,
        "recall": 0.7080620931397096
      },
      {
        "accuracy": 0.14071106659989985,
        "f1": 0.11328410374269476,
        "hf_subset": "ell_Grek-rus_Cyrl",
        "languages": [
          "ell-Grek",
          "rus-Cyrl"
        ],
        "main_score": 0.11328410374269476,
        "precision": 0.1049933844547164,
        "recall": 0.14071106659989985
      },
      {
        "accuracy": 0.9889834752128193,
        "f1": 0.985478217325989,
        "hf_subset": "eng_Latn-rus_Cyrl",
        "languages": [
          "eng-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.985478217325989,
        "precision": 0.9838090469036889,
        "recall": 0.9889834752128193
      },
      {
        "accuracy": 0.16675012518778168,
        "f1": 0.14594663676795921,
        "hf_subset": "fas_Arab-rus_Cyrl",
        "languages": [
          "fas-Arab",
          "rus-Cyrl"
        ],
        "main_score": 0.14594663676795921,
        "precision": 0.13958699801839514,
        "recall": 0.16675012518778168
      },
      {
        "accuracy": 0.4496745117676515,
        "f1": 0.4032040114608448,
        "hf_subset": "fin_Latn-rus_Cyrl",
        "languages": [
          "fin-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.4032040114608448,
        "precision": 0.38897691534756734,
        "recall": 0.4496745117676515
      },
      {
        "accuracy": 0.8372558838257386,
        "f1": 0.801232231369829,
        "hf_subset": "fra_Latn-rus_Cyrl",
        "languages": [
          "fra-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.801232231369829,
        "precision": 0.7869491827929485,
        "recall": 0.8372558838257386
      },
      {
        "accuracy": 0.10866299449173761,
        "f1": 0.08442292827544921,
        "hf_subset": "heb_Hebr-rus_Cyrl",
        "languages": [
          "heb-Hebr",
          "rus-Cyrl"
        ],
        "main_score": 0.08442292827544921,
        "precision": 0.07803833679190662,
        "recall": 0.10866299449173761
      },
      {
        "accuracy": 0.09514271407110667,
        "f1": 0.072895686673908,
        "hf_subset": "hin_Deva-rus_Cyrl",
        "languages": [
          "hin-Deva",
          "rus-Cyrl"
        ],
        "main_score": 0.072895686673908,
        "precision": 0.06724897827340359,
        "recall": 0.09514271407110667
      },
      {
        "accuracy": 0.5878818227341012,
        "f1": 0.542103595569139,
        "hf_subset": "hrv_Latn-rus_Cyrl",
        "languages": [
          "hrv-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.542103595569139,
        "precision": 0.5278617355359898,
        "recall": 0.5878818227341012
      },
      {
        "accuracy": 0.4101151727591387,
        "f1": 0.36279575750378756,
        "hf_subset": "hun_Latn-rus_Cyrl",
        "languages": [
          "hun-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.36279575750378756,
        "precision": 0.3484854642082763,
        "recall": 0.4101151727591387
      },
      {
        "accuracy": 0.4246369554331497,
        "f1": 0.3773113391337017,
        "hf_subset": "ind_Latn-rus_Cyrl",
        "languages": [
          "ind-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.3773113391337017,
        "precision": 0.36257733461770353,
        "recall": 0.4246369554331497
      },
      {
        "accuracy": 0.05107661492238358,
        "f1": 0.03760645074434319,
        "hf_subset": "jpn_Jpan-rus_Cyrl",
        "languages": [
          "jpn-Jpan",
          "rus-Cyrl"
        ],
        "main_score": 0.03760645074434319,
        "precision": 0.03413473652579605,
        "recall": 0.05107661492238358
      },
      {
        "accuracy": 0.09864797195793691,
        "f1": 0.08800453936149924,
        "hf_subset": "kor_Hang-rus_Cyrl",
        "languages": [
          "kor-Hang",
          "rus-Cyrl"
        ],
        "main_score": 0.08800453936149924,
        "precision": 0.08482217245326998,
        "recall": 0.09864797195793691
      },
      {
        "accuracy": 0.39959939909864794,
        "f1": 0.3555417630690983,
        "hf_subset": "lit_Latn-rus_Cyrl",
        "languages": [
          "lit-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.3555417630690983,
        "precision": 0.34307119091259697,
        "recall": 0.39959939909864794
      },
      {
        "accuracy": 0.9083625438157236,
        "f1": 0.8867373641106822,
        "hf_subset": "mkd_Cyrl-rus_Cyrl",
        "languages": [
          "mkd-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.8867373641106822,
        "precision": 0.8772122469418414,
        "recall": 0.9083625438157236
      },
      {
        "accuracy": 0.6634952428642964,
        "f1": 0.6214956852654588,
        "hf_subset": "nld_Latn-rus_Cyrl",
        "languages": [
          "nld-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.6214956852654588,
        "precision": 0.6069089305245047,
        "recall": 0.6634952428642964
      },
      {
        "accuracy": 0.5187781672508763,
        "f1": 0.4684380759007405,
        "hf_subset": "pol_Latn-rus_Cyrl",
        "languages": [
          "pol-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.4684380759007405,
        "precision": 0.4526550519986278,
        "recall": 0.5187781672508763
      },
      {
        "accuracy": 0.7265898848272409,
        "f1": 0.6801916869442072,
        "hf_subset": "por_Latn-rus_Cyrl",
        "languages": [
          "por-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.6801916869442072,
        "precision": 0.6641653783954696,
        "recall": 0.7265898848272409
      },
      {
        "accuracy": 0.10866299449173761,
        "f1": 0.07949908162723732,
        "hf_subset": "rus_Cyrl-arb_Arab",
        "languages": [
          "rus-Cyrl",
          "arb-Arab"
        ],
        "main_score": 0.07949908162723732,
        "precision": 0.07341199421837027,
        "recall": 0.10866299449173761
      },
      {
        "accuracy": 0.8612919379068603,
        "f1": 0.824514549602181,
        "hf_subset": "rus_Cyrl-bel_Cyrl",
        "languages": [
          "rus-Cyrl",
          "bel-Cyrl"
        ],
        "main_score": 0.824514549602181,
        "precision": 0.8081330328826574,
        "recall": 0.8612919379068603
      },
      {
        "accuracy": 0.07260891337005508,
        "f1": 0.04594056511054593,
        "hf_subset": "rus_Cyrl-ben_Beng",
        "languages": [
          "rus-Cyrl",
          "ben-Beng"
        ],
        "main_score": 0.04594056511054593,
        "precision": 0.039606351012165496,
        "recall": 0.07260891337005508
      },
      {
        "accuracy": 0.7095643465197796,
        "f1": 0.6513492460913592,
        "hf_subset": "rus_Cyrl-bos_Latn",
        "languages": [
          "rus-Cyrl",
          "bos-Latn"
        ],
        "main_score": 0.6513492460913592,
        "precision": 0.6283437060352433,
        "recall": 0.7095643465197796
      },
      {
        "accuracy": 0.9534301452178268,
        "f1": 0.9389083625438157,
        "hf_subset": "rus_Cyrl-bul_Cyrl",
        "languages": [
          "rus-Cyrl",
          "bul-Cyrl"
        ],
        "main_score": 0.9389083625438157,
        "precision": 0.9319813052912703,
        "recall": 0.9534301452178268
      },
      {
        "accuracy": 0.7020530796194292,
        "f1": 0.6454073968095,
        "hf_subset": "rus_Cyrl-ces_Latn",
        "languages": [
          "rus-Cyrl",
          "ces-Latn"
        ],
        "main_score": 0.6454073968095,
        "precision": 0.6233175159564744,
        "recall": 0.7020530796194292
      },
      {
        "accuracy": 0.7936905358037055,
        "f1": 0.7430932112454396,
        "hf_subset": "rus_Cyrl-deu_Latn",
        "languages": [
          "rus-Cyrl",
          "deu-Latn"
        ],
        "main_score": 0.7430932112454396,
        "precision": 0.7217004077544888,
        "recall": 0.7936905358037055
      },
      {
        "accuracy": 0.18627941912869303,
        "f1": 0.12991244624904133,
        "hf_subset": "rus_Cyrl-ell_Grek",
        "languages": [
          "rus-Cyrl",
          "ell-Grek"
        ],
        "main_score": 0.12991244624904133,
        "precision": 0.11639775984418817,
        "recall": 0.18627941912869303
      },
      {
        "accuracy": 0.9879819729594391,
        "f1": 0.9840594224670338,
        "hf_subset": "rus_Cyrl-eng_Latn",
        "languages": [
          "rus-Cyrl",
          "eng-Latn"
        ],
        "main_score": 0.9840594224670338,
        "precision": 0.9821398764813887,
        "recall": 0.9879819729594391
      },
      {
        "accuracy": 0.21482223335002504,
        "f1": 0.14683366295497755,
        "hf_subset": "rus_Cyrl-fas_Arab",
        "languages": [
          "rus-Cyrl",
          "fas-Arab"
        ],
        "main_score": 0.14683366295497755,
        "precision": 0.13017372818657788,
        "recall": 0.21482223335002504
      },
      {
        "accuracy": 0.57135703555333,
        "f1": 0.5048852138236215,
        "hf_subset": "rus_Cyrl-fin_Latn",
        "languages": [
          "rus-Cyrl",
          "fin-Latn"
        ],
        "main_score": 0.5048852138236215,
        "precision": 0.481152253410641,
        "recall": 0.57135703555333
      },
      {
        "accuracy": 0.8693039559339009,
        "f1": 0.8344278322245272,
        "hf_subset": "rus_Cyrl-fra_Latn",
        "languages": [
          "rus-Cyrl",
          "fra-Latn"
        ],
        "main_score": 0.8344278322245272,
        "precision": 0.8188282423635452,
        "recall": 0.8693039559339009
      },
      {
        "accuracy": 0.1517275913870806,
        "f1": 0.1048430019294976,
        "hf_subset": "rus_Cyrl-heb_Hebr",
        "languages": [
          "rus-Cyrl",
          "heb-Hebr"
        ],
        "main_score": 0.1048430019294976,
        "precision": 0.09526529695425623,
        "recall": 0.1517275913870806
      },
      {
        "accuracy": 0.1357035553329995,
        "f1": 0.09464590953613519,
        "hf_subset": "rus_Cyrl-hin_Deva",
        "languages": [
          "rus-Cyrl",
          "hin-Deva"
        ],
        "main_score": 0.09464590953613519,
        "precision": 0.08564597115487124,
        "recall": 0.1357035553329995
      },
      {
        "accuracy": 0.7040560841261893,
        "f1": 0.6431739962035405,
        "hf_subset": "rus_Cyrl-hrv_Latn",
        "languages": [
          "rus-Cyrl",
          "hrv-Latn"
        ],
        "main_score": 0.6431739962035405,
        "precision": 0.6198138477557606,
        "recall": 0.7040560841261893
      },
      {
        "accuracy": 0.5533299949924887,
        "f1": 0.48702382578196296,
        "hf_subset": "rus_Cyrl-hun_Latn",
        "languages": [
          "rus-Cyrl",
          "hun-Latn"
        ],
        "main_score": 0.48702382578196296,
        "precision": 0.4631310421620887,
        "recall": 0.5533299949924887
      },
      {
        "accuracy": 0.5037556334501753,
        "f1": 0.4233186903234814,
        "hf_subset": "rus_Cyrl-ind_Latn",
        "languages": [
          "rus-Cyrl",
          "ind-Latn"
        ],
        "main_score": 0.4233186903234814,
        "precision": 0.39709252037118836,
        "recall": 0.5037556334501753
      },
      {
        "accuracy": 0.0786179268903355,
        "f1": 0.046268356932756646,
        "hf_subset": "rus_Cyrl-jpn_Jpan",
        "languages": [
          "rus-Cyrl",
          "jpn-Jpan"
        ],
        "main_score": 0.046268356932756646,
        "precision": 0.040014051004519484,
        "recall": 0.0786179268903355
      },
      {
        "accuracy": 0.1397095643465198,
        "f1": 0.077650317712593,
        "hf_subset": "rus_Cyrl-kor_Hang",
        "languages": [
          "rus-Cyrl",
          "kor-Hang"
        ],
        "main_score": 0.077650317712593,
        "precision": 0.0643752438767403,
        "recall": 0.1397095643465198
      },
      {
        "accuracy": 0.542313470205308,
        "f1": 0.4707925441275466,
        "hf_subset": "rus_Cyrl-lit_Latn",
        "languages": [
          "rus-Cyrl",
          "lit-Latn"
        ],
        "main_score": 0.4707925441275466,
        "precision": 0.44534520735351396,
        "recall": 0.542313470205308
      },
      {
        "accuracy": 0.9298948422633951,
        "f1": 0.9098481054915707,
        "hf_subset": "rus_Cyrl-mkd_Cyrl",
        "languages": [
          "rus-Cyrl",
          "mkd-Cyrl"
        ],
        "main_score": 0.9098481054915707,
        "precision": 0.900350525788683,
        "recall": 0.9298948422633951
      },
      {
        "accuracy": 0.757636454682023,
        "f1": 0.7043410714917976,
        "hf_subset": "rus_Cyrl-nld_Latn",
        "languages": [
          "rus-Cyrl",
          "nld-Latn"
        ],
        "main_score": 0.7043410714917976,
        "precision": 0.6830801758192844,
        "recall": 0.757636454682023
      },
      {
        "accuracy": 0.6459689534301453,
        "f1": 0.582310345304837,
        "hf_subset": "rus_Cyrl-pol_Latn",
        "languages": [
          "rus-Cyrl",
          "pol-Latn"
        ],
        "main_score": 0.582310345304837,
        "precision": 0.5580869319852795,
        "recall": 0.6459689534301453
      },
      {
        "accuracy": 0.7951927891837757,
        "f1": 0.7500846507857024,
        "hf_subset": "rus_Cyrl-por_Latn",
        "languages": [
          "rus-Cyrl",
          "por-Latn"
        ],
        "main_score": 0.7500846507857024,
        "precision": 0.7308852167139598,
        "recall": 0.7951927891837757
      },
      {
        "accuracy": 0.7030545818728092,
        "f1": 0.6459470952460438,
        "hf_subset": "rus_Cyrl-slk_Latn",
        "languages": [
          "rus-Cyrl",
          "slk-Latn"
        ],
        "main_score": 0.6459470952460438,
        "precision": 0.6238902003799349,
        "recall": 0.7030545818728092
      },
      {
        "accuracy": 0.6730095142714071,
        "f1": 0.6090814793618999,
        "hf_subset": "rus_Cyrl-slv_Latn",
        "languages": [
          "rus-Cyrl",
          "slv-Latn"
        ],
        "main_score": 0.6090814793618999,
        "precision": 0.5845504548308755,
        "recall": 0.6730095142714071
      },
      {
        "accuracy": 0.8462694041061593,
        "f1": 0.8070097570598322,
        "hf_subset": "rus_Cyrl-spa_Latn",
        "languages": [
          "rus-Cyrl",
          "spa-Latn"
        ],
        "main_score": 0.8070097570598322,
        "precision": 0.7902039567287439,
        "recall": 0.8462694041061593
      },
      {
        "accuracy": 0.8217325988983475,
        "f1": 0.7775258125283163,
        "hf_subset": "rus_Cyrl-srp_Cyrl",
        "languages": [
          "rus-Cyrl",
          "srp-Cyrl"
        ],
        "main_score": 0.7775258125283163,
        "precision": 0.7582039726256051,
        "recall": 0.8217325988983475
      },
      {
        "accuracy": 0.7025538307461192,
        "f1": 0.6432524146970817,
        "hf_subset": "rus_Cyrl-srp_Latn",
        "languages": [
          "rus-Cyrl",
          "srp-Latn"
        ],
        "main_score": 0.6432524146970817,
        "precision": 0.6204707855433944,
        "recall": 0.7025538307461192
      },
      {
        "accuracy": 0.4101151727591387,
        "f1": 0.32719321552726827,
        "hf_subset": "rus_Cyrl-swa_Latn",
        "languages": [
          "rus-Cyrl",
          "swa-Latn"
        ],
        "main_score": 0.32719321552726827,
        "precision": 0.302612403380099,
        "recall": 0.4101151727591387
      },
      {
        "accuracy": 0.7836755132699048,
        "f1": 0.7359710200220967,
        "hf_subset": "rus_Cyrl-swe_Latn",
        "languages": [
          "rus-Cyrl",
          "swe-Latn"
        ],
        "main_score": 0.7359710200220967,
        "precision": 0.7164399071134174,
        "recall": 0.7836755132699048
      },
      {
        "accuracy": 0.1402103154732098,
        "f1": 0.09658469154847996,
        "hf_subset": "rus_Cyrl-tam_Taml",
        "languages": [
          "rus-Cyrl",
          "tam-Taml"
        ],
        "main_score": 0.09658469154847996,
        "precision": 0.08759865954495558,
        "recall": 0.1402103154732098
      },
      {
        "accuracy": 0.5107661492238358,
        "f1": 0.44609894639939707,
        "hf_subset": "rus_Cyrl-tur_Latn",
        "languages": [
          "rus-Cyrl",
          "tur-Latn"
        ],
        "main_score": 0.44609894639939707,
        "precision": 0.4232217373679567,
        "recall": 0.5107661492238358
      },
      {
        "accuracy": 0.9694541812719079,
        "f1": 0.9604740443999332,
        "hf_subset": "rus_Cyrl-ukr_Cyrl",
        "languages": [
          "rus-Cyrl",
          "ukr-Cyrl"
        ],
        "main_score": 0.9604740443999332,
        "precision": 0.9562260056751795,
        "recall": 0.9694541812719079
      },
      {
        "accuracy": 0.5312969454181272,
        "f1": 0.473493216875783,
        "hf_subset": "rus_Cyrl-vie_Latn",
        "languages": [
          "rus-Cyrl",
          "vie-Latn"
        ],
        "main_score": 0.473493216875783,
        "precision": 0.4524815180924344,
        "recall": 0.5312969454181272
      },
      {
        "accuracy": 0.313970956434652,
        "f1": 0.24260927422223683,
        "hf_subset": "rus_Cyrl-zho_Hant",
        "languages": [
          "rus-Cyrl",
          "zho-Hant"
        ],
        "main_score": 0.24260927422223683,
        "precision": 0.22350452075330857,
        "recall": 0.313970956434652
      },
      {
        "accuracy": 0.3985978968452679,
        "f1": 0.3203881231331921,
        "hf_subset": "rus_Cyrl-zul_Latn",
        "languages": [
          "rus-Cyrl",
          "zul-Latn"
        ],
        "main_score": 0.3203881231331921,
        "precision": 0.29770266682015883,
        "recall": 0.3985978968452679
      },
      {
        "accuracy": 0.557836755132699,
        "f1": 0.5135922533581,
        "hf_subset": "slk_Latn-rus_Cyrl",
        "languages": [
          "slk-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.5135922533581,
        "precision": 0.5000050690007637,
        "recall": 0.557836755132699
      },
      {
        "accuracy": 0.5508262393590385,
        "f1": 0.5060256255275988,
        "hf_subset": "slv_Latn-rus_Cyrl",
        "languages": [
          "slv-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.5060256255275988,
        "precision": 0.49215013435870536,
        "recall": 0.5508262393590385
      },
      {
        "accuracy": 0.7911867801702553,
        "f1": 0.7511286087750785,
        "hf_subset": "spa_Latn-rus_Cyrl",
        "languages": [
          "spa-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.7511286087750785,
        "precision": 0.7364055140682038,
        "recall": 0.7911867801702553
      },
      {
        "accuracy": 0.7721582373560341,
        "f1": 0.7387494828318855,
        "hf_subset": "srp_Cyrl-rus_Cyrl",
        "languages": [
          "srp-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.7387494828318855,
        "precision": 0.7269864464071216,
        "recall": 0.7721582373560341
      },
      {
        "accuracy": 0.5528292438657987,
        "f1": 0.5063399861526777,
        "hf_subset": "srp_Latn-rus_Cyrl",
        "languages": [
          "srp-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.5063399861526777,
        "precision": 0.49137847789701605,
        "recall": 0.5528292438657987
      },
      {
        "accuracy": 0.2944416624937406,
        "f1": 0.2592275535416291,
        "hf_subset": "swa_Latn-rus_Cyrl",
        "languages": [
          "swa-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.2592275535416291,
        "precision": 0.24911257044958537,
        "recall": 0.2944416624937406
      },
      {
        "accuracy": 0.6905358037055583,
        "f1": 0.6430506458930674,
        "hf_subset": "swe_Latn-rus_Cyrl",
        "languages": [
          "swe-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.6430506458930674,
        "precision": 0.6264585106702423,
        "recall": 0.6905358037055583
      },
      {
        "accuracy": 0.10315473209814723,
        "f1": 0.08031950987608973,
        "hf_subset": "tam_Taml-rus_Cyrl",
        "languages": [
          "tam-Taml",
          "rus-Cyrl"
        ],
        "main_score": 0.08031950987608973,
        "precision": 0.07413612985933278,
        "recall": 0.10315473209814723
      },
      {
        "accuracy": 0.39509263895843766,
        "f1": 0.3454484828621361,
        "hf_subset": "tur_Latn-rus_Cyrl",
        "languages": [
          "tur-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.3454484828621361,
        "precision": 0.33124363251443417,
        "recall": 0.39509263895843766
      },
      {
        "accuracy": 0.9619429143715573,
        "f1": 0.9517943582039725,
        "hf_subset": "ukr_Cyrl-rus_Cyrl",
        "languages": [
          "ukr-Cyrl",
          "rus-Cyrl"
        ],
        "main_score": 0.9517943582039725,
        "precision": 0.9472124853947589,
        "recall": 0.9619429143715573
      },
      {
        "accuracy": 0.41111667501251875,
        "f1": 0.36551373726009684,
        "hf_subset": "vie_Latn-rus_Cyrl",
        "languages": [
          "vie-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.36551373726009684,
        "precision": 0.3512578997218753,
        "recall": 0.41111667501251875
      },
      {
        "accuracy": 0.27240861291937907,
        "f1": 0.24006993217863198,
        "hf_subset": "zho_Hant-rus_Cyrl",
        "languages": [
          "zho-Hant",
          "rus-Cyrl"
        ],
        "main_score": 0.24006993217863198,
        "precision": 0.23001666514980884,
        "recall": 0.27240861291937907
      },
      {
        "accuracy": 0.3194792188282424,
        "f1": 0.2851122013269222,
        "hf_subset": "zul_Latn-rus_Cyrl",
        "languages": [
          "zul-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.2851122013269222,
        "precision": 0.27456465796638774,
        "recall": 0.3194792188282424
      }
    ]
  },
  "task_name": "NTREXBitextMining"
}