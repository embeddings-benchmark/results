{
  "dataset_revision": "09698e0180d87dc247ca447d3a1248b931ac0cdb",
  "evaluation_time": 6.922283887863159,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.7091575091575092,
        "cosine_accuracy_threshold": 0.7832814455032349,
        "cosine_ap": 0.7709500397739182,
        "cosine_f1": 0.7359364659166115,
        "cosine_f1_threshold": 0.7531269788742065,
        "cosine_precision": 0.6706875753920386,
        "cosine_recall": 0.8152492668621701,
        "dot_accuracy": 0.7091575091575092,
        "dot_accuracy_threshold": 0.7832813262939453,
        "dot_ap": 0.7709500397739182,
        "dot_f1": 0.7359364659166115,
        "dot_f1_threshold": 0.7531269788742065,
        "dot_precision": 0.6706875753920386,
        "dot_recall": 0.8152492668621701,
        "euclidean_accuracy": 0.7091575091575092,
        "euclidean_accuracy_threshold": 0.6583592891693115,
        "euclidean_ap": 0.7709500397739182,
        "euclidean_f1": 0.7359364659166115,
        "euclidean_f1_threshold": 0.7026705741882324,
        "euclidean_precision": 0.6706875753920386,
        "euclidean_recall": 0.8152492668621701,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.7709500397739182,
        "manhattan_accuracy": 0.7040293040293041,
        "manhattan_accuracy_threshold": 15.276962280273438,
        "manhattan_ap": 0.7682522549831512,
        "manhattan_f1": 0.7317397078353253,
        "manhattan_f1_threshold": 15.276962280273438,
        "manhattan_precision": 0.6686893203883495,
        "manhattan_recall": 0.8079178885630498,
        "max_ap": 0.7709500397739182,
        "max_f1": 0.7359364659166115,
        "max_precision": 0.6706875753920386,
        "max_recall": 0.8152492668621701,
        "similarity_accuracy": 0.7091575091575092,
        "similarity_accuracy_threshold": 0.7832814455032349,
        "similarity_ap": 0.7709500397739182,
        "similarity_f1": 0.7359364659166115,
        "similarity_f1_threshold": 0.7531269788742065,
        "similarity_precision": 0.6706875753920386,
        "similarity_recall": 0.8152492668621701
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.7201465201465201,
        "cosine_accuracy_threshold": 0.7796648740768433,
        "cosine_ap": 0.7832647958326411,
        "cosine_f1": 0.7314578005115089,
        "cosine_f1_threshold": 0.7443016767501831,
        "cosine_precision": 0.6485260770975056,
        "cosine_recall": 0.8387096774193549,
        "dot_accuracy": 0.7201465201465201,
        "dot_accuracy_threshold": 0.7796648740768433,
        "dot_ap": 0.7832647958326411,
        "dot_f1": 0.7314578005115089,
        "dot_f1_threshold": 0.7443017363548279,
        "dot_precision": 0.6485260770975056,
        "dot_recall": 0.8387096774193549,
        "euclidean_accuracy": 0.7201465201465201,
        "euclidean_accuracy_threshold": 0.6638299822807312,
        "euclidean_ap": 0.7832647958326411,
        "euclidean_f1": 0.7314578005115089,
        "euclidean_f1_threshold": 0.7151200175285339,
        "euclidean_precision": 0.6485260770975056,
        "euclidean_recall": 0.8387096774193549,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.7836372866830557,
        "manhattan_accuracy": 0.7201465201465201,
        "manhattan_accuracy_threshold": 14.288792610168457,
        "manhattan_ap": 0.7836372866830557,
        "manhattan_f1": 0.7294716740929345,
        "manhattan_f1_threshold": 15.687447547912598,
        "manhattan_precision": 0.6445444319460067,
        "manhattan_recall": 0.8401759530791789,
        "max_ap": 0.7836372866830557,
        "max_f1": 0.7314578005115089,
        "max_precision": 0.6485260770975056,
        "max_recall": 0.8401759530791789,
        "similarity_accuracy": 0.7201465201465201,
        "similarity_accuracy_threshold": 0.7796648740768433,
        "similarity_ap": 0.7832647958326411,
        "similarity_f1": 0.7314578005115089,
        "similarity_f1_threshold": 0.7443017363548279,
        "similarity_precision": 0.6485260770975056,
        "similarity_recall": 0.8387096774193549
      }
    ]
  },
  "task_name": "XNLI"
}