{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 17.33305311203003,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test": [
      {
        "accuracy": 0.6607935440484197,
        "f1": 0.6343081703341417,
        "f1_weighted": 0.6497766872014012,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6607935440484197,
        "scores_per_experiment": [
          {
            "accuracy": 0.6731674512441157,
            "f1": 0.6531105757875425,
            "f1_weighted": 0.6656177498284838
          },
          {
            "accuracy": 0.6933422999327505,
            "f1": 0.6610965130974625,
            "f1_weighted": 0.6839461739346869
          },
          {
            "accuracy": 0.6519838601210491,
            "f1": 0.6262324637074906,
            "f1_weighted": 0.6441711580522536
          },
          {
            "accuracy": 0.6893073301950235,
            "f1": 0.6556615322302324,
            "f1_weighted": 0.6843116926549452
          },
          {
            "accuracy": 0.65635507733692,
            "f1": 0.6102243550232248,
            "f1_weighted": 0.6428766126558386
          },
          {
            "accuracy": 0.624747814391392,
            "f1": 0.6192461211166198,
            "f1_weighted": 0.6143729465420229
          },
          {
            "accuracy": 0.6566913248150639,
            "f1": 0.6282394930195696,
            "f1_weighted": 0.6416958047945244
          },
          {
            "accuracy": 0.6513113651647613,
            "f1": 0.6265743767461933,
            "f1_weighted": 0.6402046172162404
          },
          {
            "accuracy": 0.6297915265635508,
            "f1": 0.6136278404008212,
            "f1_weighted": 0.6091639102677674
          },
          {
            "accuracy": 0.6812373907195696,
            "f1": 0.6490684322122594,
            "f1_weighted": 0.6714062060672489
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.653910477127398,
        "f1": 0.6116235219832036,
        "f1_weighted": 0.643272418661857,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.653910477127398,
        "scores_per_experiment": [
          {
            "accuracy": 0.661583866207575,
            "f1": 0.6233512175942642,
            "f1_weighted": 0.6610832046895213
          },
          {
            "accuracy": 0.6935563207083129,
            "f1": 0.6428552473432402,
            "f1_weighted": 0.6838733414281566
          },
          {
            "accuracy": 0.6625676340383669,
            "f1": 0.6204448648785962,
            "f1_weighted": 0.6552077546861638
          },
          {
            "accuracy": 0.6650270536153468,
            "f1": 0.6139539502401764,
            "f1_weighted": 0.6589632349141006
          },
          {
            "accuracy": 0.6645351696999509,
            "f1": 0.6095626653160383,
            "f1_weighted": 0.652814979543798
          },
          {
            "accuracy": 0.6119035907525824,
            "f1": 0.5925954577233485,
            "f1_weighted": 0.5995760400278768
          },
          {
            "accuracy": 0.6384653221839646,
            "f1": 0.5921245565487646,
            "f1_weighted": 0.6192342432272874
          },
          {
            "accuracy": 0.6296114117068372,
            "f1": 0.5938226933432713,
            "f1_weighted": 0.6180514598678591
          },
          {
            "accuracy": 0.6335464830300049,
            "f1": 0.6017283644065152,
            "f1_weighted": 0.6148525168247683
          },
          {
            "accuracy": 0.6783079193310378,
            "f1": 0.6257962024378204,
            "f1_weighted": 0.6690674114090381
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}