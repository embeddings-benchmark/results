{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.28877,
        "f1": 0.280074,
        "f1_weighted": 0.28006,
        "scores_per_experiment": [
          {
            "accuracy": 0.295898,
            "f1": 0.296709,
            "f1_weighted": 0.296712
          },
          {
            "accuracy": 0.277832,
            "f1": 0.265841,
            "f1_weighted": 0.265798
          },
          {
            "accuracy": 0.255859,
            "f1": 0.25169,
            "f1_weighted": 0.251664
          },
          {
            "accuracy": 0.322266,
            "f1": 0.319098,
            "f1_weighted": 0.319108
          },
          {
            "accuracy": 0.267578,
            "f1": 0.266681,
            "f1_weighted": 0.266676
          },
          {
            "accuracy": 0.296387,
            "f1": 0.278205,
            "f1_weighted": 0.2782
          },
          {
            "accuracy": 0.248535,
            "f1": 0.244917,
            "f1_weighted": 0.244879
          },
          {
            "accuracy": 0.333008,
            "f1": 0.318996,
            "f1_weighted": 0.31897
          },
          {
            "accuracy": 0.303711,
            "f1": 0.292839,
            "f1_weighted": 0.292819
          },
          {
            "accuracy": 0.286621,
            "f1": 0.265769,
            "f1_weighted": 0.265775
          }
        ],
        "main_score": 0.28877,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.292383,
        "f1": 0.283607,
        "f1_weighted": 0.283603,
        "scores_per_experiment": [
          {
            "accuracy": 0.289551,
            "f1": 0.291744,
            "f1_weighted": 0.291751
          },
          {
            "accuracy": 0.290039,
            "f1": 0.277683,
            "f1_weighted": 0.277644
          },
          {
            "accuracy": 0.289551,
            "f1": 0.286756,
            "f1_weighted": 0.286753
          },
          {
            "accuracy": 0.285156,
            "f1": 0.283782,
            "f1_weighted": 0.283795
          },
          {
            "accuracy": 0.290527,
            "f1": 0.2881,
            "f1_weighted": 0.288102
          },
          {
            "accuracy": 0.277832,
            "f1": 0.255035,
            "f1_weighted": 0.25503
          },
          {
            "accuracy": 0.283691,
            "f1": 0.280904,
            "f1_weighted": 0.280897
          },
          {
            "accuracy": 0.318848,
            "f1": 0.302015,
            "f1_weighted": 0.302017
          },
          {
            "accuracy": 0.311035,
            "f1": 0.302452,
            "f1_weighted": 0.302429
          },
          {
            "accuracy": 0.287598,
            "f1": 0.267595,
            "f1_weighted": 0.267608
          }
        ],
        "main_score": 0.292383,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 15.97752594947815,
  "kg_co2_emissions": 0.0005978486585662588
}