{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 11.238596439361572,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test": [
      {
        "accuracy": 0.7112642905178211,
        "f1": 0.7035797017070132,
        "f1_weighted": 0.7052516832938883,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.7112642905178211,
        "scores_per_experiment": [
          {
            "accuracy": 0.7215870880968392,
            "f1": 0.7215058667446401,
            "f1_weighted": 0.7154585354728603
          },
          {
            "accuracy": 0.7199058507061197,
            "f1": 0.7149693379160601,
            "f1_weighted": 0.7112620631338245
          },
          {
            "accuracy": 0.7259583053127101,
            "f1": 0.7125060617936306,
            "f1_weighted": 0.7223394833491779
          },
          {
            "accuracy": 0.7286482851378615,
            "f1": 0.7206542370334886,
            "f1_weighted": 0.7270715576919436
          },
          {
            "accuracy": 0.703093476798924,
            "f1": 0.682373094300659,
            "f1_weighted": 0.6918924358195725
          },
          {
            "accuracy": 0.6809011432414257,
            "f1": 0.6717742770650729,
            "f1_weighted": 0.667860594856785
          },
          {
            "accuracy": 0.6936785474108944,
            "f1": 0.6844879928343284,
            "f1_weighted": 0.6873843576387859
          },
          {
            "accuracy": 0.7000672494956288,
            "f1": 0.69946210328831,
            "f1_weighted": 0.6981859021802034
          },
          {
            "accuracy": 0.7215870880968392,
            "f1": 0.7181786666009672,
            "f1_weighted": 0.7215390354758083
          },
          {
            "accuracy": 0.7172158708809684,
            "f1": 0.7098853794929766,
            "f1_weighted": 0.7095228673199212
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.7071323167732415,
        "f1": 0.6971408164740018,
        "f1_weighted": 0.7031860412120622,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.7071323167732415,
        "scores_per_experiment": [
          {
            "accuracy": 0.7270044269552386,
            "f1": 0.7170541187860603,
            "f1_weighted": 0.7255240823113983
          },
          {
            "accuracy": 0.7161829808165273,
            "f1": 0.7109051789058611,
            "f1_weighted": 0.7119058635135944
          },
          {
            "accuracy": 0.7274963108706345,
            "f1": 0.7154362795713805,
            "f1_weighted": 0.7276221630620682
          },
          {
            "accuracy": 0.706837186424004,
            "f1": 0.7004567650603355,
            "f1_weighted": 0.7019055641408652
          },
          {
            "accuracy": 0.7142154451549434,
            "f1": 0.6928719693765745,
            "f1_weighted": 0.7069056441161415
          },
          {
            "accuracy": 0.6660108214461387,
            "f1": 0.6568322135552939,
            "f1_weighted": 0.6554137228984807
          },
          {
            "accuracy": 0.690113133300541,
            "f1": 0.6762670199218028,
            "f1_weighted": 0.6873424032089852
          },
          {
            "accuracy": 0.6969995081160846,
            "f1": 0.6912939409064918,
            "f1_weighted": 0.6943830899318398
          },
          {
            "accuracy": 0.7166748647319232,
            "f1": 0.7106340402973855,
            "f1_weighted": 0.7168944965565803
          },
          {
            "accuracy": 0.7097884899163798,
            "f1": 0.6996566383588322,
            "f1_weighted": 0.7039633823806679
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}