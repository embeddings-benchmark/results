{
  "dataset_revision": "65cdf4a4565f09b1747cd8fb37d18cd9aa1f6dd9",
  "task_name": "CommonLanguageGenderDetection",
  "mteb_version": "2.4.2",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.4545,
            "f1": 0.293492,
            "f1_weighted": 0.563933,
            "precision": 0.379271,
            "precision_weighted": 0.780967,
            "recall": 0.302524,
            "recall_weighted": 0.4545,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.376,
            "f1": 0.242608,
            "f1_weighted": 0.494453,
            "precision": 0.343899,
            "precision_weighted": 0.804586,
            "recall": 0.268518,
            "recall_weighted": 0.376,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.4015,
            "f1": 0.22561,
            "f1_weighted": 0.513844,
            "precision": 0.35368,
            "precision_weighted": 0.780533,
            "recall": 0.485912,
            "recall_weighted": 0.4015,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.462,
            "f1": 0.313642,
            "f1_weighted": 0.566931,
            "precision": 0.375217,
            "precision_weighted": 0.820944,
            "recall": 0.47265,
            "recall_weighted": 0.462,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.4245,
            "f1": 0.282965,
            "f1_weighted": 0.512861,
            "precision": 0.328583,
            "precision_weighted": 0.710168,
            "recall": 0.43941,
            "recall_weighted": 0.4245,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.393,
            "f1": 0.257406,
            "f1_weighted": 0.510303,
            "precision": 0.344239,
            "precision_weighted": 0.802147,
            "recall": 0.518666,
            "recall_weighted": 0.393,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.346,
            "f1": 0.225249,
            "f1_weighted": 0.461469,
            "precision": 0.321173,
            "precision_weighted": 0.785724,
            "recall": 0.492281,
            "recall_weighted": 0.346,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.343,
            "f1": 0.237706,
            "f1_weighted": 0.439418,
            "precision": 0.299464,
            "precision_weighted": 0.676098,
            "recall": 0.494573,
            "recall_weighted": 0.343,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.385,
            "f1": 0.262525,
            "f1_weighted": 0.493643,
            "precision": 0.344034,
            "precision_weighted": 0.77996,
            "recall": 0.547779,
            "recall_weighted": 0.385,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.427,
            "f1": 0.286177,
            "f1_weighted": 0.543754,
            "precision": 0.370825,
            "precision_weighted": 0.802282,
            "recall": 0.295926,
            "recall_weighted": 0.427,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.40125,
        "f1": 0.262738,
        "f1_weighted": 0.510061,
        "precision": 0.346039,
        "precision_weighted": 0.774341,
        "recall": 0.431824,
        "recall_weighted": 0.40125,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.40125,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 451.8738503456116,
  "kg_co2_emissions": null
}