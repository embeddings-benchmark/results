{
  "dataset_revision": "1cd02f1579dab39fedc95de8cc15fd620557a9f2",
  "task_name": "IconclassClassification",
  "mteb_version": "2.1.14",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.460396,
            "f1": 0.450627,
            "f1_weighted": 0.451728,
            "precision": 0.470668,
            "precision_weighted": 0.471375,
            "recall": 0.459376,
            "recall_weighted": 0.460396,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.420792,
            "f1": 0.402621,
            "f1_weighted": 0.404523,
            "precision": 0.412953,
            "precision_weighted": 0.413834,
            "recall": 0.418094,
            "recall_weighted": 0.420792,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.430693,
            "f1": 0.410505,
            "f1_weighted": 0.41182,
            "precision": 0.406872,
            "precision_weighted": 0.408011,
            "recall": 0.429293,
            "recall_weighted": 0.430693,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.445545,
            "f1": 0.422305,
            "f1_weighted": 0.424174,
            "precision": 0.428649,
            "precision_weighted": 0.429457,
            "recall": 0.442688,
            "recall_weighted": 0.445545,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.420792,
            "f1": 0.407516,
            "f1_weighted": 0.408873,
            "precision": 0.408334,
            "precision_weighted": 0.409181,
            "recall": 0.418972,
            "recall_weighted": 0.420792,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.425743,
            "f1": 0.414216,
            "f1_weighted": 0.41585,
            "precision": 0.456484,
            "precision_weighted": 0.456866,
            "recall": 0.423803,
            "recall_weighted": 0.425743,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.470297,
            "f1": 0.460758,
            "f1_weighted": 0.462109,
            "precision": 0.479501,
            "precision_weighted": 0.480064,
            "recall": 0.468599,
            "recall_weighted": 0.470297,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.440594,
            "f1": 0.418108,
            "f1_weighted": 0.4204,
            "precision": 0.422215,
            "precision_weighted": 0.423944,
            "recall": 0.437637,
            "recall_weighted": 0.440594,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.460396,
            "f1": 0.452418,
            "f1_weighted": 0.453914,
            "precision": 0.458839,
            "precision_weighted": 0.45995,
            "recall": 0.458498,
            "recall_weighted": 0.460396,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.435644,
            "f1": 0.420252,
            "f1_weighted": 0.422161,
            "precision": 0.420482,
            "precision_weighted": 0.421775,
            "recall": 0.433245,
            "recall_weighted": 0.435644,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.441089,
        "f1": 0.425933,
        "f1_weighted": 0.427555,
        "precision": 0.4365,
        "precision_weighted": 0.437446,
        "recall": 0.439021,
        "recall_weighted": 0.441089,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.425933,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 4.043517827987671,
  "kg_co2_emissions": null
}