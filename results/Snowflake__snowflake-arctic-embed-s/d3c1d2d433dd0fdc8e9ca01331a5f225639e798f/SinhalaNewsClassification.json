{
  "dataset_revision": "7fb2f514ea683c5282dfec0a9672ece8de90ac50",
  "task_name": "SinhalaNewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "train": [
      {
        "accuracy": 0.401904,
        "f1": 0.394886,
        "f1_weighted": 0.399868,
        "scores_per_experiment": [
          {
            "accuracy": 0.37207,
            "f1": 0.365857,
            "f1_weighted": 0.368701
          },
          {
            "accuracy": 0.38623,
            "f1": 0.382377,
            "f1_weighted": 0.38748
          },
          {
            "accuracy": 0.405273,
            "f1": 0.412892,
            "f1_weighted": 0.407826
          },
          {
            "accuracy": 0.380371,
            "f1": 0.364193,
            "f1_weighted": 0.378334
          },
          {
            "accuracy": 0.405762,
            "f1": 0.414871,
            "f1_weighted": 0.410982
          },
          {
            "accuracy": 0.416016,
            "f1": 0.400346,
            "f1_weighted": 0.407799
          },
          {
            "accuracy": 0.422852,
            "f1": 0.422488,
            "f1_weighted": 0.425829
          },
          {
            "accuracy": 0.408691,
            "f1": 0.372158,
            "f1_weighted": 0.379006
          },
          {
            "accuracy": 0.425293,
            "f1": 0.416148,
            "f1_weighted": 0.429147
          },
          {
            "accuracy": 0.396484,
            "f1": 0.397532,
            "f1_weighted": 0.403575
          }
        ],
        "main_score": 0.401904,
        "hf_subset": "default",
        "languages": [
          "sin-Sinh"
        ]
      }
    ]
  },
  "evaluation_time": 7.683415174484253,
  "kg_co2_emissions": 0.0002313726445409686
}