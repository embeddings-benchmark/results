{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 0.7337256220578345,
                "f1": 0.7256456170538992,
                "main_score": 0.7337256220578345
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 0.47205783456624073,
                "f1": 0.45905999859074437,
                "main_score": 0.47205783456624073
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 0.6983523873570949,
                "f1": 0.6943553987525273,
                "main_score": 0.6983523873570949
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 0.6700403496973774,
                "f1": 0.6597477215779144,
                "main_score": 0.6700403496973774
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 0.6804976462676531,
                "f1": 0.6724581993778398,
                "main_score": 0.6804976462676531
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 0.6188298587760592,
                "f1": 0.599952931999888,
                "main_score": 0.6188298587760592
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 0.7675857431069266,
                "f1": 0.7652031675299841,
                "main_score": 0.7675857431069266
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 0.7903496973772697,
                "f1": 0.7925548063175344,
                "main_score": 0.7903496973772697
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 0.7296570275722931,
                "f1": 0.7219110435289121,
                "main_score": 0.7296570275722931
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 0.8238735709482178,
                "f1": 0.8234495627619784,
                "main_score": 0.8238735709482178
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 0.7883994620040352,
                "f1": 0.7891526355393667,
                "main_score": 0.7883994620040352
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 0.767350369872226,
                "f1": 0.7591943734492701,
                "main_score": 0.767350369872226
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 0.7121721587088097,
                "f1": 0.7082973286243263,
                "main_score": 0.7121721587088097
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 0.7859784801613988,
                "f1": 0.7847383161087422,
                "main_score": 0.7859784801613988
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 0.6964021519838601,
                "f1": 0.6845118053027653,
                "main_score": 0.6964021519838601
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 0.7351042367182246,
                "f1": 0.7290013022879003,
                "main_score": 0.7351042367182246
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 0.7405514458641559,
                "f1": 0.7345871761713292,
                "main_score": 0.7405514458641559
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 0.5954606590450571,
                "f1": 0.5772711794953869,
                "main_score": 0.5954606590450571
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 0.7740753194351042,
                "f1": 0.7681574555065209,
                "main_score": 0.7740753194351042
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 0.6658372562205783,
                "f1": 0.6526548687097581,
                "main_score": 0.6658372562205783
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 0.7839273705447208,
                "f1": 0.7835929565948371,
                "main_score": 0.7839273705447208
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 0.7962004034969739,
                "f1": 0.7978673754501855,
                "main_score": 0.7962004034969739
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 0.6429051782111634,
                "f1": 0.6312502587609454,
                "main_score": 0.6429051782111634
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 0.5751849361129792,
                "f1": 0.5632320906403241,
                "main_score": 0.5751849361129792
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 0.5241761936785474,
                "f1": 0.49113762010098305,
                "main_score": 0.5241761936785474
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 0.5854741089441828,
                "f1": 0.5687580674198118,
                "main_score": 0.5854741089441828
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 0.7889038332212508,
                "f1": 0.7909210140529848,
                "main_score": 0.7889038332212508
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 0.6350369872225958,
                "f1": 0.6145718858568352,
                "main_score": 0.6350369872225958
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 0.5402824478816408,
                "f1": 0.5273273898138651,
                "main_score": 0.5402824478816408
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 0.5423671822461331,
                "f1": 0.5268808037254529,
                "main_score": 0.5423671822461331
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 0.7553127101546739,
                "f1": 0.7459368478550698,
                "main_score": 0.7553127101546739
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 0.5219233355749832,
                "f1": 0.5018302290152229,
                "main_score": 0.5219233355749832
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 0.756960322797579,
                "f1": 0.7525331182714856,
                "main_score": 0.756960322797579
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 0.7847679892400808,
                "f1": 0.7824044732352423,
                "main_score": 0.7847679892400808
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 0.7736718224613316,
                "f1": 0.7727144529853889,
                "main_score": 0.7736718224613316
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 0.7796234028244788,
                "f1": 0.7821282127011372,
                "main_score": 0.7796234028244788
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 0.7319435104236718,
                "f1": 0.731963711292812,
                "main_score": 0.7319435104236718
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 0.8052118359112306,
                "f1": 0.8041799643902879,
                "main_score": 0.8052118359112306
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 0.7365837256220577,
                "f1": 0.7307156989634904,
                "main_score": 0.7365837256220577
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 0.6402824478816409,
                "f1": 0.6297239902771367,
                "main_score": 0.6402824478816409
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 0.7887020847343645,
                "f1": 0.78224240866849,
                "main_score": 0.7887020847343645
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 0.646570275722932,
                "f1": 0.6327487181141255,
                "main_score": 0.646570275722932
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 0.5776059179556152,
                "f1": 0.5673711528075771,
                "main_score": 0.5776059179556152
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 0.5726967047747142,
                "f1": 0.5574735330863165,
                "main_score": 0.5726967047747142
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 0.7246133154001345,
                "f1": 0.719644168952811,
                "main_score": 0.7246133154001345
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 0.7370880968392737,
                "f1": 0.7361543141070883,
                "main_score": 0.7370880968392737
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 0.750437121721587,
                "f1": 0.7483359868879922,
                "main_score": 0.750437121721587
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 0.6705110961667787,
                "f1": 0.6625869819274315,
                "main_score": 0.6705110961667787
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 0.7552118359112306,
                "f1": 0.7592098546052303,
                "main_score": 0.7552118359112306
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 0.7992938802958978,
                "f1": 0.7979833572573796,
                "main_score": 0.7992938802958978
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 0.7686617350369872,
                "f1": 0.7742645654909517,
                "main_score": 0.7686617350369872
            }
        ]
    }
}