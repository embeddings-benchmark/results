{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 0.6405850706119705,
                "f1": 0.6220100273658395,
                "main_score": 0.6405850706119705
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 0.6324142568930733,
                "f1": 0.6204502352209821,
                "main_score": 0.6324142568930733
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 0.636852723604573,
                "f1": 0.6331574455740329,
                "main_score": 0.636852723604573
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 0.6085743106926698,
                "f1": 0.5910691798650564,
                "main_score": 0.6085743106926698
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 0.671654337592468,
                "f1": 0.6566986920813582,
                "main_score": 0.671654337592468
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 0.5651983860121049,
                "f1": 0.5473278620356588,
                "main_score": 0.5651983860121049
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 0.6776395427034297,
                "f1": 0.663447645997219,
                "main_score": 0.6776395427034297
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 0.6747814391392065,
                "f1": 0.6632841368787448,
                "main_score": 0.6747814391392065
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 0.7022864828513786,
                "f1": 0.6902774052818218,
                "main_score": 0.7022864828513786
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 0.6904841963685273,
                "f1": 0.6770789401248665,
                "main_score": 0.6904841963685273
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 0.6908204438466712,
                "f1": 0.6839277940460933,
                "main_score": 0.6908204438466712
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 0.7210154673839946,
                "f1": 0.7077371942882149,
                "main_score": 0.7210154673839946
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 0.6716207128446537,
                "f1": 0.6623118203772119,
                "main_score": 0.6716207128446537
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 0.6301950235373234,
                "f1": 0.6210550089531865,
                "main_score": 0.6301950235373234
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 0.6882985877605918,
                "f1": 0.6748944494334491,
                "main_score": 0.6882985877605918
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 0.6689643577673168,
                "f1": 0.6545745898521055,
                "main_score": 0.6689643577673168
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 0.6932750504371217,
                "f1": 0.6819665323990438,
                "main_score": 0.6932750504371217
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 0.6582380632145259,
                "f1": 0.6460872984606973,
                "main_score": 0.6582380632145259
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 0.6898117014122395,
                "f1": 0.676669714702764,
                "main_score": 0.6898117014122395
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 0.6313718897108271,
                "f1": 0.6158358081191463,
                "main_score": 0.6313718897108271
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 0.700437121721587,
                "f1": 0.6906747206775308,
                "main_score": 0.700437121721587
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 0.7067585743106926,
                "f1": 0.7008618915891508,
                "main_score": 0.7067585743106926
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 0.5978816408876934,
                "f1": 0.5791398932676417,
                "main_score": 0.5978816408876934
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 0.6103227975790182,
                "f1": 0.6004443225848671,
                "main_score": 0.6103227975790182
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 0.4905178211163415,
                "f1": 0.4543458193158155,
                "main_score": 0.4905178211163415
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 0.6378278412911902,
                "f1": 0.6210619762588153,
                "main_score": 0.6378278412911902
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 0.6959986550100874,
                "f1": 0.6894355682848476,
                "main_score": 0.6959986550100874
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 0.5997310020174847,
                "f1": 0.5909912773329623,
                "main_score": 0.5997310020174847
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 0.6920309347679893,
                "f1": 0.6790665916607239,
                "main_score": 0.6920309347679893
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 0.6272024209818428,
                "f1": 0.6077165334831407,
                "main_score": 0.6272024209818428
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 0.6787155346334902,
                "f1": 0.6579060324466791,
                "main_score": 0.6787155346334902
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 0.6497646267652992,
                "f1": 0.6389390215791396,
                "main_score": 0.6497646267652992
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 0.6581371889710828,
                "f1": 0.6439323436519936,
                "main_score": 0.6581371889710828
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 0.6979825151311365,
                "f1": 0.6853789900442244,
                "main_score": 0.6979825151311365
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 0.6998991257565568,
                "f1": 0.6893867074879778,
                "main_score": 0.6998991257565568
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 0.6750168123739071,
                "f1": 0.6674576449039721,
                "main_score": 0.6750168123739071
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 0.6752521856086078,
                "f1": 0.6683370797374445,
                "main_score": 0.6752521856086078
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 0.6796234028244786,
                "f1": 0.6758983110064196,
                "main_score": 0.6796234028244786
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 0.6956624075319435,
                "f1": 0.6835270162147211,
                "main_score": 0.6956624075319435
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 0.6848352387357095,
                "f1": 0.6666973143886907,
                "main_score": 0.6848352387357095
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 0.6792535305985206,
                "f1": 0.6652058462942483,
                "main_score": 0.6792535305985206
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 0.6318426361802287,
                "f1": 0.6171153164960602,
                "main_score": 0.6318426361802287
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 0.6484532616005381,
                "f1": 0.6386320943911235,
                "main_score": 0.6484532616005381
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 0.6539340954942838,
                "f1": 0.6385484524633183,
                "main_score": 0.6539340954942838
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 0.679892400806994,
                "f1": 0.6657022479007356,
                "main_score": 0.679892400806994
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 0.6339946200403497,
                "f1": 0.6162381473991175,
                "main_score": 0.6339946200403497
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 0.6577336919973099,
                "f1": 0.6558317907780943,
                "main_score": 0.6577336919973099
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 0.6580699394754539,
                "f1": 0.6447027323557235,
                "main_score": 0.6580699394754539
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 0.6651647612642904,
                "f1": 0.6566061210324213,
                "main_score": 0.6651647612642904
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 0.6888365837256221,
                "f1": 0.6756956454874091,
                "main_score": 0.6888365837256221
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 0.632985877605918,
                "f1": 0.6276318771484755,
                "main_score": 0.632985877605918
            }
        ]
    }
}