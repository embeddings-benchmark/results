{
    "dataset_revision": "9080400076fbadbb4c4dcb136ff4eddc40b42553",
    "task_name": "Tatoeba",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "sqi-eng",
                "languages": [
                    "sqi-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9890000000000001,
                "f1": 0.9855000000000002,
                "precision": 0.9838333333333334,
                "recall": 0.9890000000000001,
                "main_score": 0.9855000000000002
            },
            {
                "hf_subset": "fry-eng",
                "languages": [
                    "fry-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.653179190751445,
                "f1": 0.5944582071749702,
                "precision": 0.5749678869621065,
                "recall": 0.653179190751445,
                "main_score": 0.5944582071749702
            },
            {
                "hf_subset": "kur-eng",
                "languages": [
                    "kur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.3853658536585366,
                "f1": 0.34217555952803785,
                "precision": 0.3296511296649355,
                "recall": 0.3853658536585366,
                "main_score": 0.34217555952803785
            },
            {
                "hf_subset": "tur-eng",
                "languages": [
                    "tur-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.987,
                "f1": 0.9826666666666665,
                "precision": 0.9804999999999999,
                "recall": 0.987,
                "main_score": 0.9826666666666665
            },
            {
                "hf_subset": "deu-eng",
                "languages": [
                    "deu-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.993,
                "f1": 0.9913333333333333,
                "precision": 0.9905000000000002,
                "recall": 0.993,
                "main_score": 0.9913333333333333
            },
            {
                "hf_subset": "nld-eng",
                "languages": [
                    "nld-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9789999999999999,
                "f1": 0.972,
                "precision": 0.9685000000000001,
                "recall": 0.9789999999999999,
                "main_score": 0.972
            },
            {
                "hf_subset": "ron-eng",
                "languages": [
                    "ron-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.982,
                "f1": 0.976,
                "precision": 0.973,
                "recall": 0.982,
                "main_score": 0.976
            },
            {
                "hf_subset": "ang-eng",
                "languages": [
                    "ang-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.5223880597014925,
                "f1": 0.46340992406389103,
                "precision": 0.44556384742951904,
                "recall": 0.5223880597014925,
                "main_score": 0.46340992406389103
            },
            {
                "hf_subset": "ido-eng",
                "languages": [
                    "ido-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.95,
                "f1": 0.9367000000000002,
                "precision": 0.9307500000000001,
                "recall": 0.95,
                "main_score": 0.9367000000000002
            },
            {
                "hf_subset": "jav-eng",
                "languages": [
                    "jav-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8829268292682927,
                "f1": 0.8576422764227642,
                "precision": 0.8484204413472706,
                "recall": 0.8829268292682927,
                "main_score": 0.8576422764227642
            },
            {
                "hf_subset": "isl-eng",
                "languages": [
                    "isl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.972,
                "f1": 0.9646666666666667,
                "precision": 0.961,
                "recall": 0.972,
                "main_score": 0.9646666666666667
            },
            {
                "hf_subset": "slv-eng",
                "languages": [
                    "slv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9684082624544349,
                "f1": 0.9599027946537061,
                "precision": 0.9556500607533416,
                "recall": 0.9684082624544349,
                "main_score": 0.9599027946537061
            },
            {
                "hf_subset": "cym-eng",
                "languages": [
                    "cym-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.933913043478261,
                "f1": 0.9130434782608695,
                "precision": 0.9028985507246376,
                "recall": 0.933913043478261,
                "main_score": 0.9130434782608695
            },
            {
                "hf_subset": "kaz-eng",
                "languages": [
                    "kaz-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.9060869565217391,
                "f1": 0.881159420289855,
                "precision": 0.869623188405797,
                "recall": 0.9060869565217391,
                "main_score": 0.881159420289855
            },
            {
                "hf_subset": "est-eng",
                "languages": [
                    "est-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.978,
                "f1": 0.9716666666666667,
                "precision": 0.9686666666666668,
                "recall": 0.978,
                "main_score": 0.9716666666666667
            },
            {
                "hf_subset": "heb-eng",
                "languages": [
                    "heb-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.94,
                "f1": 0.9234,
                "precision": 0.9154166666666668,
                "recall": 0.94,
                "main_score": 0.9234
            },
            {
                "hf_subset": "gla-eng",
                "languages": [
                    "gla-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8492159227985525,
                "f1": 0.808868975817106,
                "precision": 0.7911540008041817,
                "recall": 0.8492159227985525,
                "main_score": 0.808868975817106
            },
            {
                "hf_subset": "mar-eng",
                "languages": [
                    "mar-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.949,
                "f1": 0.9335,
                "precision": 0.9258333333333334,
                "recall": 0.949,
                "main_score": 0.9335
            },
            {
                "hf_subset": "lat-eng",
                "languages": [
                    "lat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.433,
                "f1": 0.3664473116255726,
                "precision": 0.34640177524573806,
                "recall": 0.433,
                "main_score": 0.3664473116255726
            },
            {
                "hf_subset": "bel-eng",
                "languages": [
                    "bel-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.9670000000000001,
                "f1": 0.9568333333333332,
                "precision": 0.9519999999999998,
                "recall": 0.9670000000000001,
                "main_score": 0.9568333333333332
            },
            {
                "hf_subset": "pms-eng",
                "languages": [
                    "pms-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7047619047619048,
                "f1": 0.6663032734461306,
                "precision": 0.6546459191863879,
                "recall": 0.7047619047619048,
                "main_score": 0.6663032734461306
            },
            {
                "hf_subset": "gle-eng",
                "languages": [
                    "gle-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.935,
                "f1": 0.9163,
                "precision": 0.9075,
                "recall": 0.935,
                "main_score": 0.9163
            },
            {
                "hf_subset": "pes-eng",
                "languages": [
                    "pes-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.955,
                "f1": 0.9436666666666667,
                "precision": 0.9383333333333332,
                "recall": 0.955,
                "main_score": 0.9436666666666667
            },
            {
                "hf_subset": "nob-eng",
                "languages": [
                    "nob-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.993,
                "f1": 0.9906666666666666,
                "precision": 0.9895,
                "recall": 0.993,
                "main_score": 0.9906666666666666
            },
            {
                "hf_subset": "bul-eng",
                "languages": [
                    "bul-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.958,
                "f1": 0.9451666666666667,
                "precision": 0.9388333333333334,
                "recall": 0.958,
                "main_score": 0.9451666666666667
            },
            {
                "hf_subset": "cbk-eng",
                "languages": [
                    "cbk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.84,
                "f1": 0.8046675324675325,
                "precision": 0.7895999999999997,
                "recall": 0.84,
                "main_score": 0.8046675324675325
            },
            {
                "hf_subset": "hun-eng",
                "languages": [
                    "hun-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.977,
                "f1": 0.9693333333333333,
                "precision": 0.9655,
                "recall": 0.977,
                "main_score": 0.9693333333333333
            },
            {
                "hf_subset": "uig-eng",
                "languages": [
                    "uig-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.921,
                "f1": 0.9007333333333334,
                "precision": 0.8916166666666667,
                "recall": 0.921,
                "main_score": 0.9007333333333334
            },
            {
                "hf_subset": "rus-eng",
                "languages": [
                    "rus-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.956,
                "f1": 0.9434999999999999,
                "precision": 0.9375,
                "recall": 0.956,
                "main_score": 0.9434999999999999
            },
            {
                "hf_subset": "spa-eng",
                "languages": [
                    "spa-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9890000000000001,
                "f1": 0.9853333333333335,
                "precision": 0.9835,
                "recall": 0.9890000000000001,
                "main_score": 0.9853333333333335
            },
            {
                "hf_subset": "hye-eng",
                "languages": [
                    "hye-Armn",
                    "eng-Latn"
                ],
                "accuracy": 0.9622641509433962,
                "f1": 0.9514824797843666,
                "precision": 0.9460916442048517,
                "recall": 0.9622641509433962,
                "main_score": 0.9514824797843666
            },
            {
                "hf_subset": "tel-eng",
                "languages": [
                    "tel-Telu",
                    "eng-Latn"
                ],
                "accuracy": 0.9358974358974359,
                "f1": 0.9159544159544158,
                "precision": 0.9066951566951567,
                "recall": 0.9358974358974359,
                "main_score": 0.9159544159544158
            },
            {
                "hf_subset": "afr-eng",
                "languages": [
                    "afr-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.981,
                "f1": 0.9746666666666668,
                "precision": 0.9715,
                "recall": 0.981,
                "main_score": 0.9746666666666668
            },
            {
                "hf_subset": "mon-eng",
                "languages": [
                    "mon-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.9340909090909091,
                "f1": 0.9159090909090909,
                "precision": 0.9071969696969697,
                "recall": 0.9340909090909091,
                "main_score": 0.9159090909090909
            },
            {
                "hf_subset": "arz-eng",
                "languages": [
                    "arz-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.8951781970649894,
                "f1": 0.8676150544075072,
                "precision": 0.8555206149545772,
                "recall": 0.8951781970649894,
                "main_score": 0.8676150544075072
            },
            {
                "hf_subset": "hrv-eng",
                "languages": [
                    "hrv-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.982,
                "f1": 0.9765,
                "precision": 0.9738333333333332,
                "recall": 0.982,
                "main_score": 0.9765
            },
            {
                "hf_subset": "nov-eng",
                "languages": [
                    "nov-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7587548638132295,
                "f1": 0.7124698906800073,
                "precision": 0.6966572338167668,
                "recall": 0.7587548638132295,
                "main_score": 0.7124698906800073
            },
            {
                "hf_subset": "gsw-eng",
                "languages": [
                    "gsw-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.6153846153846154,
                "f1": 0.5483234714003944,
                "precision": 0.5206552706552707,
                "recall": 0.6153846153846154,
                "main_score": 0.5483234714003944
            },
            {
                "hf_subset": "nds-eng",
                "languages": [
                    "nds-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.592,
                "f1": 0.5418321123321123,
                "precision": 0.5248751719986241,
                "recall": 0.592,
                "main_score": 0.5418321123321123
            },
            {
                "hf_subset": "ukr-eng",
                "languages": [
                    "ukr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.956,
                "f1": 0.943,
                "precision": 0.9365000000000001,
                "recall": 0.956,
                "main_score": 0.943
            },
            {
                "hf_subset": "uzb-eng",
                "languages": [
                    "uzb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8785046728971964,
                "f1": 0.8525700934579439,
                "precision": 0.8409267912772586,
                "recall": 0.8785046728971964,
                "main_score": 0.8525700934579439
            },
            {
                "hf_subset": "lit-eng",
                "languages": [
                    "lit-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.98,
                "f1": 0.9743333333333333,
                "precision": 0.9715,
                "recall": 0.98,
                "main_score": 0.9743333333333333
            },
            {
                "hf_subset": "ina-eng",
                "languages": [
                    "ina-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9079999999999999,
                "f1": 0.8866055555555554,
                "precision": 0.8781845238095238,
                "recall": 0.9079999999999999,
                "main_score": 0.8866055555555554
            },
            {
                "hf_subset": "lfn-eng",
                "languages": [
                    "lfn-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.706,
                "f1": 0.65538895353013,
                "precision": 0.6369531394330308,
                "recall": 0.706,
                "main_score": 0.65538895353013
            },
            {
                "hf_subset": "zsm-eng",
                "languages": [
                    "zsm-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9689999999999999,
                "f1": 0.9606666666666668,
                "precision": 0.9568333333333334,
                "recall": 0.9689999999999999,
                "main_score": 0.9606666666666668
            },
            {
                "hf_subset": "ita-eng",
                "languages": [
                    "ita-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.968,
                "f1": 0.9595,
                "precision": 0.9555,
                "recall": 0.968,
                "main_score": 0.9595
            },
            {
                "hf_subset": "cmn-eng",
                "languages": [
                    "cmn-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.9519999999999998,
                "f1": 0.938,
                "precision": 0.9313333333333333,
                "recall": 0.9519999999999998,
                "main_score": 0.938
            },
            {
                "hf_subset": "lvs-eng",
                "languages": [
                    "lvs-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.965,
                "f1": 0.9545,
                "precision": 0.9493333333333334,
                "recall": 0.965,
                "main_score": 0.9545
            },
            {
                "hf_subset": "glg-eng",
                "languages": [
                    "glg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9789999999999999,
                "f1": 0.9728333333333332,
                "precision": 0.9698333333333333,
                "recall": 0.9789999999999999,
                "main_score": 0.9728333333333332
            },
            {
                "hf_subset": "ceb-eng",
                "languages": [
                    "ceb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7816666666666666,
                "f1": 0.7467336721249764,
                "precision": 0.7326035353535354,
                "recall": 0.7816666666666666,
                "main_score": 0.7467336721249764
            },
            {
                "hf_subset": "bre-eng",
                "languages": [
                    "bre-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.11200000000000002,
                "f1": 0.0848123815073815,
                "precision": 0.07843657708032709,
                "recall": 0.11200000000000002,
                "main_score": 0.0848123815073815
            },
            {
                "hf_subset": "ben-eng",
                "languages": [
                    "ben-Beng",
                    "eng-Latn"
                ],
                "accuracy": 0.9129999999999999,
                "f1": 0.8902333333333332,
                "precision": 0.87975,
                "recall": 0.9129999999999999,
                "main_score": 0.8902333333333332
            },
            {
                "hf_subset": "swg-eng",
                "languages": [
                    "swg-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.7232142857142857,
                "f1": 0.6769209956709956,
                "precision": 0.6619047619047619,
                "recall": 0.7232142857142857,
                "main_score": 0.6769209956709956
            },
            {
                "hf_subset": "arq-eng",
                "languages": [
                    "arq-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.7969264544456641,
                "f1": 0.7540693115885212,
                "precision": 0.7367544822539336,
                "recall": 0.7969264544456641,
                "main_score": 0.7540693115885212
            },
            {
                "hf_subset": "kab-eng",
                "languages": [
                    "kab-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.868,
                "f1": 0.8365666666666667,
                "precision": 0.8224833333333333,
                "recall": 0.868,
                "main_score": 0.8365666666666667
            },
            {
                "hf_subset": "fra-eng",
                "languages": [
                    "fra-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.964,
                "f1": 0.9536666666666666,
                "precision": 0.9486666666666665,
                "recall": 0.964,
                "main_score": 0.9536666666666666
            },
            {
                "hf_subset": "por-eng",
                "languages": [
                    "por-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.963,
                "f1": 0.9549,
                "precision": 0.9510833333333334,
                "recall": 0.963,
                "main_score": 0.9549
            },
            {
                "hf_subset": "tat-eng",
                "languages": [
                    "tat-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.8960000000000001,
                "f1": 0.8704746031746032,
                "precision": 0.8589583333333333,
                "recall": 0.8960000000000001,
                "main_score": 0.8704746031746032
            },
            {
                "hf_subset": "oci-eng",
                "languages": [
                    "oci-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8690000000000001,
                "f1": 0.8457088023088022,
                "precision": 0.836475,
                "recall": 0.8690000000000001,
                "main_score": 0.8457088023088022
            },
            {
                "hf_subset": "pol-eng",
                "languages": [
                    "pol-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.982,
                "f1": 0.977,
                "precision": 0.9746666666666668,
                "recall": 0.982,
                "main_score": 0.977
            },
            {
                "hf_subset": "war-eng",
                "languages": [
                    "war-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8539999999999999,
                "f1": 0.8283333333333333,
                "precision": 0.8180137426900587,
                "recall": 0.8539999999999999,
                "main_score": 0.8283333333333333
            },
            {
                "hf_subset": "aze-eng",
                "languages": [
                    "aze-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.914,
                "f1": 0.8911999999999999,
                "precision": 0.8812777777777778,
                "recall": 0.914,
                "main_score": 0.8911999999999999
            },
            {
                "hf_subset": "vie-eng",
                "languages": [
                    "vie-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.978,
                "f1": 0.9716666666666669,
                "precision": 0.9685000000000001,
                "recall": 0.978,
                "main_score": 0.9716666666666669
            },
            {
                "hf_subset": "nno-eng",
                "languages": [
                    "nno-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9789999999999999,
                "f1": 0.9730666666666665,
                "precision": 0.97025,
                "recall": 0.9789999999999999,
                "main_score": 0.9730666666666665
            },
            {
                "hf_subset": "cha-eng",
                "languages": [
                    "cha-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.27007299270072993,
                "f1": 0.2511489591781562,
                "precision": 0.24602283361407448,
                "recall": 0.27007299270072993,
                "main_score": 0.2511489591781562
            },
            {
                "hf_subset": "mhr-eng",
                "languages": [
                    "mhr-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.141,
                "f1": 0.11869284007509813,
                "precision": 0.11199695454818405,
                "recall": 0.141,
                "main_score": 0.11869284007509813
            },
            {
                "hf_subset": "dan-eng",
                "languages": [
                    "dan-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.977,
                "f1": 0.9709,
                "precision": 0.9680833333333332,
                "recall": 0.977,
                "main_score": 0.9709
            },
            {
                "hf_subset": "ell-eng",
                "languages": [
                    "ell-Grek",
                    "eng-Latn"
                ],
                "accuracy": 0.965,
                "f1": 0.9547333333333333,
                "precision": 0.94975,
                "recall": 0.965,
                "main_score": 0.9547333333333333
            },
            {
                "hf_subset": "amh-eng",
                "languages": [
                    "amh-Ethi",
                    "eng-Latn"
                ],
                "accuracy": 0.9345238095238095,
                "f1": 0.9166666666666665,
                "precision": 0.9077380952380952,
                "recall": 0.9345238095238095,
                "main_score": 0.9166666666666665
            },
            {
                "hf_subset": "pam-eng",
                "languages": [
                    "pam-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.11899999999999998,
                "f1": 0.10303261315113037,
                "precision": 0.09902986584515606,
                "recall": 0.11899999999999998,
                "main_score": 0.10303261315113037
            },
            {
                "hf_subset": "hsb-eng",
                "languages": [
                    "hsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.8157349896480333,
                "f1": 0.7786519438693352,
                "precision": 0.7635595081247254,
                "recall": 0.8157349896480333,
                "main_score": 0.7786519438693352
            },
            {
                "hf_subset": "srp-eng",
                "languages": [
                    "srp-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.961,
                "f1": 0.9486666666666668,
                "precision": 0.9425,
                "recall": 0.961,
                "main_score": 0.9486666666666668
            },
            {
                "hf_subset": "epo-eng",
                "languages": [
                    "epo-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.988,
                "f1": 0.9846666666666667,
                "precision": 0.983,
                "recall": 0.988,
                "main_score": 0.9846666666666667
            },
            {
                "hf_subset": "kzj-eng",
                "languages": [
                    "kzj-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.107,
                "f1": 0.08621683883854935,
                "precision": 0.08188292731521031,
                "recall": 0.107,
                "main_score": 0.08621683883854935
            },
            {
                "hf_subset": "awa-eng",
                "languages": [
                    "awa-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.9047619047619048,
                "f1": 0.878581735724593,
                "precision": 0.8672438672438674,
                "recall": 0.9047619047619048,
                "main_score": 0.878581735724593
            },
            {
                "hf_subset": "fao-eng",
                "languages": [
                    "fao-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.950381679389313,
                "f1": 0.9360050890585242,
                "precision": 0.92970737913486,
                "recall": 0.950381679389313,
                "main_score": 0.9360050890585242
            },
            {
                "hf_subset": "mal-eng",
                "languages": [
                    "mal-Mlym",
                    "eng-Latn"
                ],
                "accuracy": 0.982532751091703,
                "f1": 0.9767103347889374,
                "precision": 0.9737991266375546,
                "recall": 0.982532751091703,
                "main_score": 0.9767103347889374
            },
            {
                "hf_subset": "ile-eng",
                "languages": [
                    "ile-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.846,
                "f1": 0.8099904761904763,
                "precision": 0.7954634920634919,
                "recall": 0.846,
                "main_score": 0.8099904761904763
            },
            {
                "hf_subset": "bos-eng",
                "languages": [
                    "bos-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9689265536723164,
                "f1": 0.9590395480225989,
                "precision": 0.954331450094162,
                "recall": 0.9689265536723164,
                "main_score": 0.9590395480225989
            },
            {
                "hf_subset": "cor-eng",
                "languages": [
                    "cor-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.126,
                "f1": 0.09981918087824627,
                "precision": 0.09326319147606549,
                "recall": 0.126,
                "main_score": 0.09981918087824627
            },
            {
                "hf_subset": "cat-eng",
                "languages": [
                    "cat-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9739999999999999,
                "f1": 0.9665,
                "precision": 0.9628333333333333,
                "recall": 0.9739999999999999,
                "main_score": 0.9665
            },
            {
                "hf_subset": "eus-eng",
                "languages": [
                    "eus-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.965,
                "f1": 0.9538333333333333,
                "precision": 0.9483333333333333,
                "recall": 0.965,
                "main_score": 0.9538333333333333
            },
            {
                "hf_subset": "yue-eng",
                "languages": [
                    "yue-Hant",
                    "eng-Latn"
                ],
                "accuracy": 0.9079999999999999,
                "f1": 0.8843666666666665,
                "precision": 0.87395,
                "recall": 0.9079999999999999,
                "main_score": 0.8843666666666665
            },
            {
                "hf_subset": "swe-eng",
                "languages": [
                    "swe-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.977,
                "f1": 0.9703333333333333,
                "precision": 0.9671666666666667,
                "recall": 0.977,
                "main_score": 0.9703333333333333
            },
            {
                "hf_subset": "dtp-eng",
                "languages": [
                    "dtp-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.094,
                "f1": 0.0794688910522006,
                "precision": 0.07665059865752875,
                "recall": 0.094,
                "main_score": 0.0794688910522006
            },
            {
                "hf_subset": "kat-eng",
                "languages": [
                    "kat-Geor",
                    "eng-Latn"
                ],
                "accuracy": 0.9504021447721179,
                "f1": 0.9368632707774799,
                "precision": 0.9308534405719392,
                "recall": 0.9504021447721179,
                "main_score": 0.9368632707774799
            },
            {
                "hf_subset": "jpn-eng",
                "languages": [
                    "jpn-Jpan",
                    "eng-Latn"
                ],
                "accuracy": 0.959,
                "f1": 0.9466666666666668,
                "precision": 0.9408333333333334,
                "recall": 0.959,
                "main_score": 0.9466666666666668
            },
            {
                "hf_subset": "csb-eng",
                "languages": [
                    "csb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.826086956521739,
                "f1": 0.7798418972332015,
                "precision": 0.7596837944664031,
                "recall": 0.826086956521739,
                "main_score": 0.7798418972332015
            },
            {
                "hf_subset": "xho-eng",
                "languages": [
                    "xho-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9577464788732394,
                "f1": 0.948356807511737,
                "precision": 0.9436619718309859,
                "recall": 0.9577464788732394,
                "main_score": 0.948356807511737
            },
            {
                "hf_subset": "orv-eng",
                "languages": [
                    "orv-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.5317365269461077,
                "f1": 0.47070430567436555,
                "precision": 0.4516136324183078,
                "recall": 0.5317365269461077,
                "main_score": 0.47070430567436555
            },
            {
                "hf_subset": "ind-eng",
                "languages": [
                    "ind-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.955,
                "f1": 0.945,
                "precision": 0.9403333333333334,
                "recall": 0.955,
                "main_score": 0.945
            },
            {
                "hf_subset": "tuk-eng",
                "languages": [
                    "tuk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9359605911330049,
                "f1": 0.9182266009852217,
                "precision": 0.9109195402298852,
                "recall": 0.9359605911330049,
                "main_score": 0.9182266009852217
            },
            {
                "hf_subset": "max-eng",
                "languages": [
                    "max-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.7640845070422535,
                "f1": 0.7273082942097027,
                "precision": 0.7146686939820742,
                "recall": 0.7640845070422535,
                "main_score": 0.7273082942097027
            },
            {
                "hf_subset": "swh-eng",
                "languages": [
                    "swh-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9358974358974359,
                "f1": 0.9198290598290597,
                "precision": 0.913119658119658,
                "recall": 0.9358974358974359,
                "main_score": 0.9198290598290597
            },
            {
                "hf_subset": "hin-eng",
                "languages": [
                    "hin-Deva",
                    "eng-Latn"
                ],
                "accuracy": 0.978,
                "f1": 0.9706666666666668,
                "precision": 0.9670000000000001,
                "recall": 0.978,
                "main_score": 0.9706666666666668
            },
            {
                "hf_subset": "dsb-eng",
                "languages": [
                    "dsb-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.6889352818371609,
                "f1": 0.6447860652453555,
                "precision": 0.6287865191859258,
                "recall": 0.6889352818371609,
                "main_score": 0.6447860652453555
            },
            {
                "hf_subset": "ber-eng",
                "languages": [
                    "ber-Tfng",
                    "eng-Latn"
                ],
                "accuracy": 0.338,
                "f1": 0.2929077434411237,
                "precision": 0.28066016735704646,
                "recall": 0.338,
                "main_score": 0.2929077434411237
            },
            {
                "hf_subset": "tam-eng",
                "languages": [
                    "tam-Taml",
                    "eng-Latn"
                ],
                "accuracy": 0.9022801302931596,
                "f1": 0.8807817589576546,
                "precision": 0.8717155266015201,
                "recall": 0.9022801302931596,
                "main_score": 0.8807817589576546
            },
            {
                "hf_subset": "slk-eng",
                "languages": [
                    "slk-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.982,
                "f1": 0.9763333333333334,
                "precision": 0.9736666666666668,
                "recall": 0.982,
                "main_score": 0.9763333333333334
            },
            {
                "hf_subset": "tgl-eng",
                "languages": [
                    "tgl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.977,
                "f1": 0.9695,
                "precision": 0.9658333333333331,
                "recall": 0.977,
                "main_score": 0.9695
            },
            {
                "hf_subset": "ast-eng",
                "languages": [
                    "ast-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.9291338582677166,
                "f1": 0.9081364829396327,
                "precision": 0.8989501312335958,
                "recall": 0.9291338582677166,
                "main_score": 0.9081364829396327
            },
            {
                "hf_subset": "mkd-eng",
                "languages": [
                    "mkd-Cyrl",
                    "eng-Latn"
                ],
                "accuracy": 0.9689999999999999,
                "f1": 0.9598333333333332,
                "precision": 0.9556666666666668,
                "recall": 0.9689999999999999,
                "main_score": 0.9598333333333332
            },
            {
                "hf_subset": "khm-eng",
                "languages": [
                    "khm-Khmr",
                    "eng-Latn"
                ],
                "accuracy": 0.7451523545706371,
                "f1": 0.7020346919931407,
                "precision": 0.6863895657888951,
                "recall": 0.7451523545706371,
                "main_score": 0.7020346919931407
            },
            {
                "hf_subset": "ces-eng",
                "languages": [
                    "ces-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.976,
                "f1": 0.9688333333333332,
                "precision": 0.9653333333333333,
                "recall": 0.976,
                "main_score": 0.9688333333333332
            },
            {
                "hf_subset": "tzl-eng",
                "languages": [
                    "tzl-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.4615384615384615,
                "f1": 0.3947885447885448,
                "precision": 0.37301528599605527,
                "recall": 0.4615384615384615,
                "main_score": 0.3947885447885448
            },
            {
                "hf_subset": "urd-eng",
                "languages": [
                    "urd-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.9469999999999998,
                "f1": 0.9316666666666668,
                "precision": 0.9241666666666667,
                "recall": 0.9469999999999998,
                "main_score": 0.9316666666666668
            },
            {
                "hf_subset": "ara-eng",
                "languages": [
                    "ara-Arab",
                    "eng-Latn"
                ],
                "accuracy": 0.9519999999999998,
                "f1": 0.9383333333333332,
                "precision": 0.9316666666666668,
                "recall": 0.9519999999999998,
                "main_score": 0.9383333333333332
            },
            {
                "hf_subset": "kor-eng",
                "languages": [
                    "kor-Hang",
                    "eng-Latn"
                ],
                "accuracy": 0.92,
                "f1": 0.8998666666666666,
                "precision": 0.8909166666666667,
                "recall": 0.92,
                "main_score": 0.8998666666666666
            },
            {
                "hf_subset": "yid-eng",
                "languages": [
                    "yid-Hebr",
                    "eng-Latn"
                ],
                "accuracy": 0.9551886792452831,
                "f1": 0.9430031446540881,
                "precision": 0.9375,
                "recall": 0.9551886792452831,
                "main_score": 0.9430031446540881
            },
            {
                "hf_subset": "fin-eng",
                "languages": [
                    "fin-Latn",
                    "eng-Latn"
                ],
                "accuracy": 0.982,
                "f1": 0.9783333333333333,
                "precision": 0.9765,
                "recall": 0.982,
                "main_score": 0.9783333333333333
            },
            {
                "hf_subset": "tha-eng",
                "languages": [
                    "tha-Thai",
                    "eng-Latn"
                ],
                "accuracy": 0.968978102189781,
                "f1": 0.9604622871046228,
                "precision": 0.9562043795620438,
                "recall": 0.968978102189781,
                "main_score": 0.9604622871046228
            },
            {
                "hf_subset": "wuu-eng",
                "languages": [
                    "wuu-Hans",
                    "eng-Latn"
                ],
                "accuracy": 0.851,
                "f1": 0.8178564213564213,
                "precision": 0.8046416666666667,
                "recall": 0.851,
                "main_score": 0.8178564213564213
            }
        ]
    }
}