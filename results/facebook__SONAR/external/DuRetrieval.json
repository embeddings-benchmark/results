{
    "dataset_revision": "a1a333e290fe30b10f3f56498e3a0d911a693ced",
    "task_name": "DuRetrieval",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "dev": [
            {
                "hf_subset": "default",
                "languages": [
                    "cmn-Hans"
                ],
                "map_at_1": 0.0060799999999999995,
                "map_at_10": 0.00882,
                "map_at_100": 0.00962,
                "map_at_1000": 0.010280000000000001,
                "map_at_3": 0.00749,
                "map_at_5": 0.00824,
                "mrr_at_1": 0.020500000000000004,
                "mrr_at_10": 0.02796,
                "mrr_at_100": 0.029830000000000002,
                "mrr_at_1000": 0.030899999999999997,
                "mrr_at_3": 0.02483,
                "mrr_at_5": 0.02661,
                "ndcg_at_1": 0.020500000000000004,
                "ndcg_at_10": 0.01435,
                "ndcg_at_100": 0.01991,
                "ndcg_at_1000": 0.04961,
                "ndcg_at_3": 0.01428,
                "ndcg_at_5": 0.01369,
                "precision_at_1": 0.020500000000000004,
                "precision_at_10": 0.005349999999999999,
                "precision_at_100": 0.00127,
                "precision_at_1000": 0.00086,
                "precision_at_3": 0.0105,
                "precision_at_5": 0.0084,
                "recall_at_1": 0.0060799999999999995,
                "recall_at_10": 0.0154,
                "recall_at_100": 0.03507,
                "recall_at_1000": 0.20531,
                "recall_at_3": 0.00901,
                "recall_at_5": 0.01168,
                "main_score": 0.01435
            }
        ]
    }
}