{
  "dataset_revision": "d604517c81ca91fe16a244d1248fc021f9ecee7a",
  "task_name": "TweetSentimentExtractionClassification",
  "mteb_version": "1.38.39",
  "scores": {
    "test": [
      {
        "accuracy": 0.606706,
        "f1": 0.610837,
        "f1_weighted": 0.601973,
        "scores_per_experiment": [
          {
            "accuracy": 0.629315,
            "f1": 0.631079,
            "f1_weighted": 0.620669
          },
          {
            "accuracy": 0.612054,
            "f1": 0.615418,
            "f1_weighted": 0.60543
          },
          {
            "accuracy": 0.61828,
            "f1": 0.624031,
            "f1_weighted": 0.616206
          },
          {
            "accuracy": 0.605829,
            "f1": 0.610587,
            "f1_weighted": 0.601035
          },
          {
            "accuracy": 0.613186,
            "f1": 0.617764,
            "f1_weighted": 0.611493
          },
          {
            "accuracy": 0.634692,
            "f1": 0.640833,
            "f1_weighted": 0.633434
          },
          {
            "accuracy": 0.574986,
            "f1": 0.575932,
            "f1_weighted": 0.567347
          },
          {
            "accuracy": 0.6279,
            "f1": 0.631883,
            "f1_weighted": 0.622587
          },
          {
            "accuracy": 0.556027,
            "f1": 0.56032,
            "f1_weighted": 0.550024
          },
          {
            "accuracy": 0.594793,
            "f1": 0.600524,
            "f1_weighted": 0.591511
          }
        ],
        "main_score": 0.606706,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 4.384728193283081,
  "kg_co2_emissions": null
}