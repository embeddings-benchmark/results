{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.797595,
        "f1": 0.665985,
        "f1_weighted": 0.82234,
        "ap": 0.277768,
        "ap_weighted": 0.277768,
        "scores_per_experiment": [
          {
            "accuracy": 0.766323,
            "f1": 0.629105,
            "f1_weighted": 0.798108,
            "ap": 0.2334,
            "ap_weighted": 0.2334
          },
          {
            "accuracy": 0.844502,
            "f1": 0.696389,
            "f1_weighted": 0.85525,
            "ap": 0.293802,
            "ap_weighted": 0.293802
          },
          {
            "accuracy": 0.81701,
            "f1": 0.703095,
            "f1_weighted": 0.840868,
            "ap": 0.331623,
            "ap_weighted": 0.331623
          },
          {
            "accuracy": 0.852234,
            "f1": 0.729536,
            "f1_weighted": 0.866006,
            "ap": 0.352673,
            "ap_weighted": 0.352673
          },
          {
            "accuracy": 0.78866,
            "f1": 0.621148,
            "f1_weighted": 0.809869,
            "ap": 0.209094,
            "ap_weighted": 0.209094
          },
          {
            "accuracy": 0.808419,
            "f1": 0.679055,
            "f1_weighted": 0.831701,
            "ap": 0.289089,
            "ap_weighted": 0.289089
          },
          {
            "accuracy": 0.831615,
            "f1": 0.671996,
            "f1_weighted": 0.84341,
            "ap": 0.260707,
            "ap_weighted": 0.260707
          },
          {
            "accuracy": 0.706186,
            "f1": 0.610908,
            "f1_weighted": 0.755148,
            "ap": 0.254201,
            "ap_weighted": 0.254201
          },
          {
            "accuracy": 0.792096,
            "f1": 0.673251,
            "f1_weighted": 0.820876,
            "ap": 0.293196,
            "ap_weighted": 0.293196
          },
          {
            "accuracy": 0.7689,
            "f1": 0.645366,
            "f1_weighted": 0.802167,
            "ap": 0.259896,
            "ap_weighted": 0.259896
          }
        ],
        "main_score": 0.797595,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 7.837534189224243,
  "kg_co2_emissions": 0.0002576628473183431
}