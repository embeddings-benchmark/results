{
  "dataset_revision": "2269ed7d95d8abaab829f1592b4b2047372e9f81",
  "task_name": "DutchColaClassification",
  "mteb_version": "2.1.14",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.542083,
            "f1": 0.541303,
            "f1_weighted": 0.541303,
            "precision": 0.542372,
            "precision_weighted": 0.542372,
            "recall": 0.542083,
            "recall_weighted": 0.542083,
            "ap": 0.522678,
            "ap_weighted": 0.522678
          },
          {
            "accuracy": 0.512917,
            "f1": 0.512886,
            "f1_weighted": 0.512886,
            "precision": 0.51292,
            "precision_weighted": 0.51292,
            "recall": 0.512917,
            "recall_weighted": 0.512917,
            "ap": 0.506623,
            "ap_weighted": 0.506623
          },
          {
            "accuracy": 0.542083,
            "f1": 0.540955,
            "f1_weighted": 0.540955,
            "precision": 0.542501,
            "precision_weighted": 0.542501,
            "recall": 0.542083,
            "recall_weighted": 0.542083,
            "ap": 0.523008,
            "ap_weighted": 0.523008
          },
          {
            "accuracy": 0.547917,
            "f1": 0.547696,
            "f1_weighted": 0.547696,
            "precision": 0.54801,
            "precision_weighted": 0.54801,
            "recall": 0.547917,
            "recall_weighted": 0.547917,
            "ap": 0.52636,
            "ap_weighted": 0.52636
          },
          {
            "accuracy": 0.54375,
            "f1": 0.541082,
            "f1_weighted": 0.541082,
            "precision": 0.544792,
            "precision_weighted": 0.544792,
            "recall": 0.54375,
            "recall_weighted": 0.54375,
            "ap": 0.524133,
            "ap_weighted": 0.524133
          },
          {
            "accuracy": 0.520417,
            "f1": 0.52038,
            "f1_weighted": 0.52038,
            "precision": 0.520423,
            "precision_weighted": 0.520423,
            "recall": 0.520417,
            "recall_weighted": 0.520417,
            "ap": 0.510618,
            "ap_weighted": 0.510618
          },
          {
            "accuracy": 0.515417,
            "f1": 0.515325,
            "f1_weighted": 0.515325,
            "precision": 0.515428,
            "precision_weighted": 0.515428,
            "recall": 0.515417,
            "recall_weighted": 0.515417,
            "ap": 0.50794,
            "ap_weighted": 0.50794
          },
          {
            "accuracy": 0.53125,
            "f1": 0.528858,
            "f1_weighted": 0.528858,
            "precision": 0.531898,
            "precision_weighted": 0.531898,
            "recall": 0.53125,
            "recall_weighted": 0.53125,
            "ap": 0.516764,
            "ap_weighted": 0.516764
          },
          {
            "accuracy": 0.541667,
            "f1": 0.541585,
            "f1_weighted": 0.541585,
            "precision": 0.541696,
            "precision_weighted": 0.541696,
            "recall": 0.541667,
            "recall_weighted": 0.541667,
            "ap": 0.522524,
            "ap_weighted": 0.522524
          },
          {
            "accuracy": 0.544583,
            "f1": 0.544114,
            "f1_weighted": 0.544114,
            "precision": 0.544768,
            "precision_weighted": 0.544768,
            "recall": 0.544583,
            "recall_weighted": 0.544583,
            "ap": 0.524416,
            "ap_weighted": 0.524416
          }
        ],
        "accuracy": 0.534208,
        "f1": 0.533418,
        "f1_weighted": 0.533418,
        "precision": 0.534481,
        "precision_weighted": 0.534481,
        "recall": 0.534208,
        "recall_weighted": 0.534208,
        "ap": 0.518506,
        "ap_weighted": 0.518506,
        "main_score": 0.533418,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 42.22158741950989,
  "kg_co2_emissions": null
}