{
  "dataset_revision": "d743dcd5abb03d5ab357757a0e83522fc6696fcd",
  "task_name": "DanishPoliticalCommentsClassification",
  "mteb_version": "2.1.11",
  "scores": {
    "train": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.345962,
            "f1": 0.329268,
            "f1_weighted": 0.365355,
            "precision": 0.334559,
            "precision_weighted": 0.457916,
            "recall": 0.407602,
            "recall_weighted": 0.345962,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.394949,
            "f1": 0.364408,
            "f1_weighted": 0.408564,
            "precision": 0.356901,
            "precision_weighted": 0.472507,
            "recall": 0.443824,
            "recall_weighted": 0.394949,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.376492,
            "f1": 0.340288,
            "f1_weighted": 0.39729,
            "precision": 0.342967,
            "precision_weighted": 0.478213,
            "recall": 0.421549,
            "recall_weighted": 0.376492,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.456009,
            "f1": 0.393187,
            "f1_weighted": 0.464155,
            "precision": 0.377347,
            "precision_weighted": 0.490837,
            "recall": 0.456108,
            "recall_weighted": 0.456009,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.407577,
            "f1": 0.35434,
            "f1_weighted": 0.422292,
            "precision": 0.348942,
            "precision_weighted": 0.475457,
            "recall": 0.440091,
            "recall_weighted": 0.407577,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.337219,
            "f1": 0.307693,
            "f1_weighted": 0.346486,
            "precision": 0.307951,
            "precision_weighted": 0.421049,
            "recall": 0.41403,
            "recall_weighted": 0.337219,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.356508,
            "f1": 0.341683,
            "f1_weighted": 0.363747,
            "precision": 0.342488,
            "precision_weighted": 0.459742,
            "recall": 0.436771,
            "recall_weighted": 0.356508,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.408548,
            "f1": 0.351943,
            "f1_weighted": 0.428243,
            "precision": 0.353761,
            "precision_weighted": 0.487972,
            "recall": 0.426167,
            "recall_weighted": 0.408548,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.387871,
            "f1": 0.351781,
            "f1_weighted": 0.39433,
            "precision": 0.347921,
            "precision_weighted": 0.462685,
            "recall": 0.451491,
            "recall_weighted": 0.387871,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.388426,
            "f1": 0.36259,
            "f1_weighted": 0.40476,
            "precision": 0.365921,
            "precision_weighted": 0.482182,
            "recall": 0.438514,
            "recall_weighted": 0.388426,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.385956,
        "f1": 0.349718,
        "f1_weighted": 0.399522,
        "precision": 0.347876,
        "precision_weighted": 0.468856,
        "recall": 0.433615,
        "recall_weighted": 0.385956,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.385956,
        "hf_subset": "default",
        "languages": [
          "dan-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 283.14326906204224,
  "kg_co2_emissions": null
}