{
  "dataset_revision": "edfaf9da55d3dd50d43143d90c1ac476895ae6de",
  "task_name": "ToxicConversationsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.693896,
        "f1": 0.529949,
        "f1_weighted": 0.760031,
        "ap": 0.131714,
        "ap_weighted": 0.131714,
        "scores_per_experiment": [
          {
            "accuracy": 0.705078,
            "f1": 0.542854,
            "f1_weighted": 0.772095,
            "ap": 0.140678,
            "ap_weighted": 0.140678
          },
          {
            "accuracy": 0.748535,
            "f1": 0.556821,
            "f1_weighted": 0.802192,
            "ap": 0.132743,
            "ap_weighted": 0.132743
          },
          {
            "accuracy": 0.802734,
            "f1": 0.59774,
            "f1_weighted": 0.839471,
            "ap": 0.155228,
            "ap_weighted": 0.155228
          },
          {
            "accuracy": 0.772461,
            "f1": 0.585225,
            "f1_weighted": 0.819814,
            "ap": 0.157216,
            "ap_weighted": 0.157216
          },
          {
            "accuracy": 0.510742,
            "f1": 0.410651,
            "f1_weighted": 0.615103,
            "ap": 0.090216,
            "ap_weighted": 0.090216
          },
          {
            "accuracy": 0.558105,
            "f1": 0.455508,
            "f1_weighted": 0.654471,
            "ap": 0.116951,
            "ap_weighted": 0.116951
          },
          {
            "accuracy": 0.770508,
            "f1": 0.556877,
            "f1_weighted": 0.815878,
            "ap": 0.121954,
            "ap_weighted": 0.121954
          },
          {
            "accuracy": 0.641602,
            "f1": 0.50565,
            "f1_weighted": 0.723881,
            "ap": 0.129977,
            "ap_weighted": 0.129977
          },
          {
            "accuracy": 0.701172,
            "f1": 0.537633,
            "f1_weighted": 0.769112,
            "ap": 0.135669,
            "ap_weighted": 0.135669
          },
          {
            "accuracy": 0.728027,
            "f1": 0.550526,
            "f1_weighted": 0.788298,
            "ap": 0.136513,
            "ap_weighted": 0.136513
          }
        ],
        "main_score": 0.693896,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 326.1966540813446,
  "kg_co2_emissions": null
}