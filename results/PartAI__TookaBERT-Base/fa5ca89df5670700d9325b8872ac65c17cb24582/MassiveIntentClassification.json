{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.25.8",
  "scores": {
    "validation": [
      {
        "accuracy": 0.606542,
        "f1": 0.565855,
        "f1_weighted": 0.613699,
        "scores_per_experiment": [
          {
            "accuracy": 0.605017,
            "f1": 0.566951,
            "f1_weighted": 0.612278
          },
          {
            "accuracy": 0.632563,
            "f1": 0.580594,
            "f1_weighted": 0.642775
          },
          {
            "accuracy": 0.604525,
            "f1": 0.55377,
            "f1_weighted": 0.612769
          },
          {
            "accuracy": 0.618298,
            "f1": 0.575787,
            "f1_weighted": 0.624411
          },
          {
            "accuracy": 0.596163,
            "f1": 0.555494,
            "f1_weighted": 0.600505
          },
          {
            "accuracy": 0.616331,
            "f1": 0.571779,
            "f1_weighted": 0.623492
          },
          {
            "accuracy": 0.613871,
            "f1": 0.579601,
            "f1_weighted": 0.622653
          },
          {
            "accuracy": 0.583374,
            "f1": 0.543787,
            "f1_weighted": 0.592892
          },
          {
            "accuracy": 0.601082,
            "f1": 0.56656,
            "f1_weighted": 0.605
          },
          {
            "accuracy": 0.594196,
            "f1": 0.564225,
            "f1_weighted": 0.600217
          }
        ],
        "main_score": 0.606542,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.599765,
        "f1": 0.555393,
        "f1_weighted": 0.604962,
        "scores_per_experiment": [
          {
            "accuracy": 0.61197,
            "f1": 0.558219,
            "f1_weighted": 0.618646
          },
          {
            "accuracy": 0.613652,
            "f1": 0.558601,
            "f1_weighted": 0.621527
          },
          {
            "accuracy": 0.607263,
            "f1": 0.557962,
            "f1_weighted": 0.611363
          },
          {
            "accuracy": 0.608608,
            "f1": 0.564659,
            "f1_weighted": 0.613507
          },
          {
            "accuracy": 0.578346,
            "f1": 0.541525,
            "f1_weighted": 0.580844
          },
          {
            "accuracy": 0.598857,
            "f1": 0.555404,
            "f1_weighted": 0.605311
          },
          {
            "accuracy": 0.597512,
            "f1": 0.562528,
            "f1_weighted": 0.603383
          },
          {
            "accuracy": 0.588097,
            "f1": 0.550356,
            "f1_weighted": 0.591515
          },
          {
            "accuracy": 0.601547,
            "f1": 0.559286,
            "f1_weighted": 0.606894
          },
          {
            "accuracy": 0.591796,
            "f1": 0.545391,
            "f1_weighted": 0.596633
          }
        ],
        "main_score": 0.599765,
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 13.98459506034851,
  "kg_co2_emissions": null
}