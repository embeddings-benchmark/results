{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "task_name": "GreekLegalCodeClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.020605,
        "f1": 0.010125,
        "f1_weighted": 0.010442,
        "scores_per_experiment": [
          {
            "accuracy": 0.02002,
            "f1": 0.009709,
            "f1_weighted": 0.009084
          },
          {
            "accuracy": 0.023438,
            "f1": 0.012657,
            "f1_weighted": 0.01531
          },
          {
            "accuracy": 0.017578,
            "f1": 0.008426,
            "f1_weighted": 0.007352
          },
          {
            "accuracy": 0.020508,
            "f1": 0.008322,
            "f1_weighted": 0.009817
          },
          {
            "accuracy": 0.01416,
            "f1": 0.00707,
            "f1_weighted": 0.007961
          },
          {
            "accuracy": 0.022461,
            "f1": 0.013293,
            "f1_weighted": 0.012243
          },
          {
            "accuracy": 0.024414,
            "f1": 0.012653,
            "f1_weighted": 0.012329
          },
          {
            "accuracy": 0.020508,
            "f1": 0.007881,
            "f1_weighted": 0.009135
          },
          {
            "accuracy": 0.023926,
            "f1": 0.009464,
            "f1_weighted": 0.010459
          },
          {
            "accuracy": 0.019043,
            "f1": 0.011772,
            "f1_weighted": 0.010732
          }
        ],
        "main_score": 0.020605,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.020801,
        "f1": 0.007022,
        "f1_weighted": 0.008587,
        "scores_per_experiment": [
          {
            "accuracy": 0.017578,
            "f1": 0.002976,
            "f1_weighted": 0.005097
          },
          {
            "accuracy": 0.024414,
            "f1": 0.01004,
            "f1_weighted": 0.013377
          },
          {
            "accuracy": 0.016602,
            "f1": 0.005555,
            "f1_weighted": 0.006272
          },
          {
            "accuracy": 0.015625,
            "f1": 0.00821,
            "f1_weighted": 0.007089
          },
          {
            "accuracy": 0.018066,
            "f1": 0.006311,
            "f1_weighted": 0.008504
          },
          {
            "accuracy": 0.02002,
            "f1": 0.005604,
            "f1_weighted": 0.006734
          },
          {
            "accuracy": 0.024414,
            "f1": 0.005736,
            "f1_weighted": 0.007143
          },
          {
            "accuracy": 0.022461,
            "f1": 0.00764,
            "f1_weighted": 0.008874
          },
          {
            "accuracy": 0.026367,
            "f1": 0.008136,
            "f1_weighted": 0.011079
          },
          {
            "accuracy": 0.022461,
            "f1": 0.010015,
            "f1_weighted": 0.011701
          }
        ],
        "main_score": 0.020801,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ]
  },
  "evaluation_time": 171.31711292266846,
  "kg_co2_emissions": 0.006056843596983566
}