{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.807646,
        "f1": 0.674499,
        "f1_weighted": 0.829766,
        "ap": 0.284709,
        "ap_weighted": 0.284709,
        "scores_per_experiment": [
          {
            "accuracy": 0.832474,
            "f1": 0.687376,
            "f1_weighted": 0.846929,
            "ap": 0.285566,
            "ap_weighted": 0.285566
          },
          {
            "accuracy": 0.821306,
            "f1": 0.675669,
            "f1_weighted": 0.838484,
            "ap": 0.27269,
            "ap_weighted": 0.27269
          },
          {
            "accuracy": 0.829038,
            "f1": 0.706004,
            "f1_weighted": 0.848481,
            "ap": 0.324775,
            "ap_weighted": 0.324775
          },
          {
            "accuracy": 0.839347,
            "f1": 0.709021,
            "f1_weighted": 0.854906,
            "ap": 0.321257,
            "ap_weighted": 0.321257
          },
          {
            "accuracy": 0.75,
            "f1": 0.605157,
            "f1_weighted": 0.78431,
            "ap": 0.20767,
            "ap_weighted": 0.20767
          },
          {
            "accuracy": 0.831615,
            "f1": 0.702992,
            "f1_weighted": 0.849414,
            "ap": 0.315921,
            "ap_weighted": 0.315921
          },
          {
            "accuracy": 0.866838,
            "f1": 0.702607,
            "f1_weighted": 0.868168,
            "ap": 0.295645,
            "ap_weighted": 0.295645
          },
          {
            "accuracy": 0.703608,
            "f1": 0.597811,
            "f1_weighted": 0.752342,
            "ap": 0.230521,
            "ap_weighted": 0.230521
          },
          {
            "accuracy": 0.80756,
            "f1": 0.680525,
            "f1_weighted": 0.831444,
            "ap": 0.292802,
            "ap_weighted": 0.292802
          },
          {
            "accuracy": 0.794674,
            "f1": 0.677826,
            "f1_weighted": 0.823177,
            "ap": 0.300244,
            "ap_weighted": 0.300244
          }
        ],
        "main_score": 0.807646,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 7.159691572189331,
  "kg_co2_emissions": 0.00020313237614991167
}