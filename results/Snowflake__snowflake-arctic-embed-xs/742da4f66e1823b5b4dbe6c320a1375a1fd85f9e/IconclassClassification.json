{
  "dataset_revision": "1cd02f1579dab39fedc95de8cc15fd620557a9f2",
  "task_name": "IconclassClassification",
  "mteb_version": "2.1.17",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.455446,
            "f1": 0.449032,
            "f1_weighted": 0.449497,
            "precision": 0.454741,
            "precision_weighted": 0.455695,
            "recall": 0.455424,
            "recall_weighted": 0.455446,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.455446,
            "f1": 0.43628,
            "f1_weighted": 0.437409,
            "precision": 0.437569,
            "precision_weighted": 0.438554,
            "recall": 0.454326,
            "recall_weighted": 0.455446,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.430693,
            "f1": 0.418636,
            "f1_weighted": 0.418818,
            "precision": 0.424022,
            "precision_weighted": 0.42495,
            "recall": 0.43105,
            "recall_weighted": 0.430693,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.450495,
            "f1": 0.442458,
            "f1_weighted": 0.442732,
            "precision": 0.444373,
            "precision_weighted": 0.444328,
            "recall": 0.449934,
            "recall_weighted": 0.450495,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.425743,
            "f1": 0.412956,
            "f1_weighted": 0.413457,
            "precision": 0.415226,
            "precision_weighted": 0.41591,
            "recall": 0.42556,
            "recall_weighted": 0.425743,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.445545,
            "f1": 0.426581,
            "f1_weighted": 0.427226,
            "precision": 0.427957,
            "precision_weighted": 0.428322,
            "recall": 0.444664,
            "recall_weighted": 0.445545,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.430693,
            "f1": 0.421939,
            "f1_weighted": 0.423256,
            "precision": 0.421007,
            "precision_weighted": 0.422682,
            "recall": 0.429732,
            "recall_weighted": 0.430693,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.490099,
            "f1": 0.472821,
            "f1_weighted": 0.474009,
            "precision": 0.476063,
            "precision_weighted": 0.47751,
            "recall": 0.48924,
            "recall_weighted": 0.490099,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.445545,
            "f1": 0.436522,
            "f1_weighted": 0.437253,
            "precision": 0.44429,
            "precision_weighted": 0.444535,
            "recall": 0.444444,
            "recall_weighted": 0.445545,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.455446,
            "f1": 0.442852,
            "f1_weighted": 0.443834,
            "precision": 0.445339,
            "precision_weighted": 0.445957,
            "recall": 0.454326,
            "recall_weighted": 0.455446,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.448515,
        "f1": 0.436008,
        "f1_weighted": 0.436749,
        "precision": 0.439059,
        "precision_weighted": 0.439844,
        "recall": 0.44787,
        "recall_weighted": 0.448515,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.436008,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 3.1620535850524902,
  "kg_co2_emissions": null
}