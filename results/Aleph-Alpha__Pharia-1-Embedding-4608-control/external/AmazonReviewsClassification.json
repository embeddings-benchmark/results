{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 137.32827019691467,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.21",
  "scores": {
    "test": [
      {
        "accuracy": 0.48647999999999997,
        "f1": 0.4800683627357392,
        "f1_weighted": 0.4800683627357393,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.48647999999999997,
        "scores_per_experiment": [
          {
            "accuracy": 0.4886,
            "f1": 0.47456029804810873,
            "f1_weighted": 0.4745602980481088
          },
          {
            "accuracy": 0.483,
            "f1": 0.48029883184237204,
            "f1_weighted": 0.48029883184237204
          },
          {
            "accuracy": 0.4684,
            "f1": 0.4673114615538269,
            "f1_weighted": 0.467311461553827
          },
          {
            "accuracy": 0.4994,
            "f1": 0.498984646595302,
            "f1_weighted": 0.498984646595302
          },
          {
            "accuracy": 0.4896,
            "f1": 0.4764479574502323,
            "f1_weighted": 0.4764479574502322
          },
          {
            "accuracy": 0.4882,
            "f1": 0.4906955301600049,
            "f1_weighted": 0.49069553016000483
          },
          {
            "accuracy": 0.4876,
            "f1": 0.4824123447607535,
            "f1_weighted": 0.4824123447607536
          },
          {
            "accuracy": 0.5,
            "f1": 0.49141866189686906,
            "f1_weighted": 0.491418661896869
          },
          {
            "accuracy": 0.474,
            "f1": 0.46830091897020426,
            "f1_weighted": 0.4683009189702043
          },
          {
            "accuracy": 0.486,
            "f1": 0.47025297607971905,
            "f1_weighted": 0.47025297607971905
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}