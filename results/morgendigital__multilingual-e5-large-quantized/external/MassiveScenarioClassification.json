{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 0.6873907195696032,
                "f1": 0.6698484521791418,
                "main_score": 0.6873907195696032
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 0.6058843308675185,
                "f1": 0.5895591723092005,
                "main_score": 0.6058843308675185
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 0.6622730329522528,
                "f1": 0.6608944997121149,
                "main_score": 0.6622730329522528
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 0.6648285137861465,
                "f1": 0.6521963176785157,
                "main_score": 0.6648285137861465
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 0.6774714189643578,
                "f1": 0.6682121927454121,
                "main_score": 0.6774714189643578
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 0.5909213180901143,
                "f1": 0.5670735546356339,
                "main_score": 0.5909213180901143
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 0.7505716207128448,
                "f1": 0.748413712365364,
                "main_score": 0.7505716207128448
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 0.7469737726967047,
                "f1": 0.747664341963,
                "main_score": 0.7469737726967047
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 0.7390383322125084,
                "f1": 0.7359201554448324,
                "main_score": 0.7390383322125084
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 0.7751176866173504,
                "f1": 0.7746104434577759,
                "main_score": 0.7751176866173504
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 0.7431069266980497,
                "f1": 0.7461048660675635,
                "main_score": 0.7431069266980497
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 0.7295225285810356,
                "f1": 0.7233160006574627,
                "main_score": 0.7295225285810356
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 0.7312373907195696,
                "f1": 0.7320921012557481,
                "main_score": 0.7312373907195696
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 0.7386684599865501,
                "f1": 0.7382348774610832,
                "main_score": 0.7386684599865501
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 0.7140215198386012,
                "f1": 0.7111945183971858,
                "main_score": 0.7140215198386012
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 0.7212844653665098,
                "f1": 0.7134450495911766,
                "main_score": 0.7212844653665098
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 0.7452252858103566,
                "f1": 0.7398878711342999,
                "main_score": 0.7452252858103566
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 0.6493611297915265,
                "f1": 0.6372320046765338,
                "main_score": 0.6493611297915265
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 0.7411903160726295,
                "f1": 0.7382138439467095,
                "main_score": 0.7411903160726295
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 0.6715198386012105,
                "f1": 0.6602172193802167,
                "main_score": 0.6715198386012105
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 0.7432414256893073,
                "f1": 0.7430943421170574,
                "main_score": 0.7432414256893073
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 0.7746805648957633,
                "f1": 0.776280840929821,
                "main_score": 0.7746805648957633
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 0.6331876260928043,
                "f1": 0.6209428406607508,
                "main_score": 0.6331876260928043
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 0.5834902488231338,
                "f1": 0.5712893860987984,
                "main_score": 0.5834902488231338
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 0.5088433086751849,
                "f1": 0.482272350802058,
                "main_score": 0.5088433086751849
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 0.664425016812374,
                "f1": 0.6461463095996173,
                "main_score": 0.664425016812374
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 0.7504707464694015,
                "f1": 0.7505099199098999,
                "main_score": 0.7504707464694015
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 0.7050437121721587,
                "f1": 0.6983397721096314,
                "main_score": 0.7050437121721587
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 0.6994283792871553,
                "f1": 0.688704663703913,
                "main_score": 0.6994283792871553
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 0.6479488903833222,
                "f1": 0.6361542406334544,
                "main_score": 0.6479488903833222
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 0.6988231338264963,
                "f1": 0.6857892302593237,
                "main_score": 0.6988231338264963
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 0.6324815063887022,
                "f1": 0.6106680605338809,
                "main_score": 0.6324815063887022
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 0.7484196368527236,
                "f1": 0.7452566464968763,
                "main_score": 0.7484196368527236
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 0.748285137861466,
                "f1": 0.748853197608802,
                "main_score": 0.748285137861466
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 0.7413248150638868,
                "f1": 0.7439820409991791,
                "main_score": 0.7413248150638868
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 0.7349024882313383,
                "f1": 0.7382153848368573,
                "main_score": 0.7349024882313383
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 0.7172158708809684,
                "f1": 0.718504943318054,
                "main_score": 0.7172158708809684
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 0.75137861466039,
                "f1": 0.7537628348188466,
                "main_score": 0.75137861466039
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 0.7186953597848016,
                "f1": 0.7187537624521662,
                "main_score": 0.7186953597848016
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 0.7027572293207801,
                "f1": 0.6880017302344231,
                "main_score": 0.7027572293207801
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 0.760995292535306,
                "f1": 0.7607992707688408,
                "main_score": 0.760995292535306
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 0.6314055144586416,
                "f1": 0.6173855010331415,
                "main_score": 0.6314055144586416
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 0.6627774041694687,
                "f1": 0.6483664868894539,
                "main_score": 0.6627774041694687
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 0.6669468728984533,
                "f1": 0.6476239666920869,
                "main_score": 0.6669468728984533
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 0.7344653665097511,
                "f1": 0.7314646052013873,
                "main_score": 0.7344653665097511
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 0.6771351714862139,
                "f1": 0.6667212180163382,
                "main_score": 0.6771351714862139
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 0.739946200403497,
                "f1": 0.7387348793725524,
                "main_score": 0.739946200403497
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 0.6815400134498992,
                "f1": 0.6709433241421094,
                "main_score": 0.6815400134498992
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 0.7311365164761264,
                "f1": 0.7359502539433752,
                "main_score": 0.7311365164761264
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 0.7682582380632145,
                "f1": 0.7689992945316313,
                "main_score": 0.7682582380632145
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 0.7181237390719569,
                "f1": 0.7236499770986264,
                "main_score": 0.7181237390719569
            }
        ]
    }
}