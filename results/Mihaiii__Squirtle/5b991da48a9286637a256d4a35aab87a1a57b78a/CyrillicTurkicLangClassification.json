{
  "dataset_revision": "e42d330f33d65b7b72dfd408883daf1661f06f18",
  "task_name": "CyrillicTurkicLangClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.615088,
        "f1": 0.589269,
        "f1_weighted": 0.589432,
        "scores_per_experiment": [
          {
            "accuracy": 0.608398,
            "f1": 0.588296,
            "f1_weighted": 0.588441
          },
          {
            "accuracy": 0.601562,
            "f1": 0.559361,
            "f1_weighted": 0.559558
          },
          {
            "accuracy": 0.643066,
            "f1": 0.622994,
            "f1_weighted": 0.623085
          },
          {
            "accuracy": 0.647949,
            "f1": 0.631836,
            "f1_weighted": 0.632012
          },
          {
            "accuracy": 0.639648,
            "f1": 0.621897,
            "f1_weighted": 0.622103
          },
          {
            "accuracy": 0.567383,
            "f1": 0.541468,
            "f1_weighted": 0.54173
          },
          {
            "accuracy": 0.556152,
            "f1": 0.518904,
            "f1_weighted": 0.518911
          },
          {
            "accuracy": 0.633301,
            "f1": 0.611251,
            "f1_weighted": 0.611428
          },
          {
            "accuracy": 0.641113,
            "f1": 0.623148,
            "f1_weighted": 0.623315
          },
          {
            "accuracy": 0.612305,
            "f1": 0.573531,
            "f1_weighted": 0.573737
          }
        ],
        "main_score": 0.615088,
        "hf_subset": "default",
        "languages": [
          "bak-Cyrl",
          "chv-Cyrl",
          "tat-Cyrl",
          "kir-Cyrl",
          "rus-Cyrl",
          "kaz-Cyrl",
          "tyv-Cyrl",
          "krc-Cyrl",
          "sah-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 7.810680150985718,
  "kg_co2_emissions": 0.0002265705729051572
}