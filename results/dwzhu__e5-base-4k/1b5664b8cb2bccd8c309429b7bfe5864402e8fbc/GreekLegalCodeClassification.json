{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "task_name": "GreekLegalCodeClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.073682,
        "f1": 0.055299,
        "f1_weighted": 0.077912,
        "scores_per_experiment": [
          {
            "accuracy": 0.075195,
            "f1": 0.059162,
            "f1_weighted": 0.075875
          },
          {
            "accuracy": 0.072266,
            "f1": 0.054343,
            "f1_weighted": 0.071786
          },
          {
            "accuracy": 0.066895,
            "f1": 0.056452,
            "f1_weighted": 0.072132
          },
          {
            "accuracy": 0.074707,
            "f1": 0.052709,
            "f1_weighted": 0.08319
          },
          {
            "accuracy": 0.071777,
            "f1": 0.051821,
            "f1_weighted": 0.075654
          },
          {
            "accuracy": 0.07959,
            "f1": 0.054683,
            "f1_weighted": 0.082968
          },
          {
            "accuracy": 0.077148,
            "f1": 0.053866,
            "f1_weighted": 0.079928
          },
          {
            "accuracy": 0.06543,
            "f1": 0.049355,
            "f1_weighted": 0.0715
          },
          {
            "accuracy": 0.074219,
            "f1": 0.055332,
            "f1_weighted": 0.079554
          },
          {
            "accuracy": 0.07959,
            "f1": 0.065263,
            "f1_weighted": 0.086536
          }
        ],
        "main_score": 0.073682,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.070801,
        "f1": 0.058492,
        "f1_weighted": 0.070682,
        "scores_per_experiment": [
          {
            "accuracy": 0.074707,
            "f1": 0.063878,
            "f1_weighted": 0.073726
          },
          {
            "accuracy": 0.077637,
            "f1": 0.070212,
            "f1_weighted": 0.079572
          },
          {
            "accuracy": 0.070312,
            "f1": 0.061071,
            "f1_weighted": 0.069748
          },
          {
            "accuracy": 0.067871,
            "f1": 0.05933,
            "f1_weighted": 0.0659
          },
          {
            "accuracy": 0.067383,
            "f1": 0.051244,
            "f1_weighted": 0.067038
          },
          {
            "accuracy": 0.06543,
            "f1": 0.054883,
            "f1_weighted": 0.067365
          },
          {
            "accuracy": 0.061523,
            "f1": 0.048001,
            "f1_weighted": 0.064078
          },
          {
            "accuracy": 0.080078,
            "f1": 0.065634,
            "f1_weighted": 0.080314
          },
          {
            "accuracy": 0.067383,
            "f1": 0.053504,
            "f1_weighted": 0.065208
          },
          {
            "accuracy": 0.075684,
            "f1": 0.057162,
            "f1_weighted": 0.07387
          }
        ],
        "main_score": 0.070801,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ]
  },
  "evaluation_time": 2072.1986355781555,
  "kg_co2_emissions": 0.10371328324913814
}