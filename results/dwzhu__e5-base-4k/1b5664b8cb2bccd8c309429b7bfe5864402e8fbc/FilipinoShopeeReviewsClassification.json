{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.284814,
        "f1": 0.28038,
        "f1_weighted": 0.280371,
        "scores_per_experiment": [
          {
            "accuracy": 0.295898,
            "f1": 0.297873,
            "f1_weighted": 0.297878
          },
          {
            "accuracy": 0.269531,
            "f1": 0.265474,
            "f1_weighted": 0.265447
          },
          {
            "accuracy": 0.285156,
            "f1": 0.283486,
            "f1_weighted": 0.283459
          },
          {
            "accuracy": 0.32373,
            "f1": 0.323714,
            "f1_weighted": 0.323735
          },
          {
            "accuracy": 0.272461,
            "f1": 0.263399,
            "f1_weighted": 0.263377
          },
          {
            "accuracy": 0.269043,
            "f1": 0.266032,
            "f1_weighted": 0.266038
          },
          {
            "accuracy": 0.220703,
            "f1": 0.220102,
            "f1_weighted": 0.220071
          },
          {
            "accuracy": 0.320801,
            "f1": 0.31629,
            "f1_weighted": 0.316316
          },
          {
            "accuracy": 0.307129,
            "f1": 0.294008,
            "f1_weighted": 0.293954
          },
          {
            "accuracy": 0.283691,
            "f1": 0.273425,
            "f1_weighted": 0.273432
          }
        ],
        "main_score": 0.284814,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.290332,
        "f1": 0.286415,
        "f1_weighted": 0.28641,
        "scores_per_experiment": [
          {
            "accuracy": 0.291016,
            "f1": 0.292664,
            "f1_weighted": 0.292665
          },
          {
            "accuracy": 0.283203,
            "f1": 0.280257,
            "f1_weighted": 0.28023
          },
          {
            "accuracy": 0.304199,
            "f1": 0.303273,
            "f1_weighted": 0.303255
          },
          {
            "accuracy": 0.300293,
            "f1": 0.302161,
            "f1_weighted": 0.302193
          },
          {
            "accuracy": 0.289062,
            "f1": 0.27992,
            "f1_weighted": 0.27989
          },
          {
            "accuracy": 0.264648,
            "f1": 0.259192,
            "f1_weighted": 0.259201
          },
          {
            "accuracy": 0.238281,
            "f1": 0.237365,
            "f1_weighted": 0.237355
          },
          {
            "accuracy": 0.32373,
            "f1": 0.321136,
            "f1_weighted": 0.321163
          },
          {
            "accuracy": 0.319336,
            "f1": 0.304615,
            "f1_weighted": 0.304565
          },
          {
            "accuracy": 0.289551,
            "f1": 0.283565,
            "f1_weighted": 0.283584
          }
        ],
        "main_score": 0.290332,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 16.564026594161987,
  "kg_co2_emissions": 0.0005904087680501951
}