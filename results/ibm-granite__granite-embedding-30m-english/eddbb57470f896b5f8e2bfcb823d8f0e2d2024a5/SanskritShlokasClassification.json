{
  "dataset_revision": "5a79d6472db143690c7ce6e974995d3610eee7f0",
  "task_name": "SanskritShlokasClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "train": [
      {
        "accuracy": 0.699217,
        "f1": 0.693239,
        "f1_weighted": 0.694098,
        "scores_per_experiment": [
          {
            "accuracy": 0.684073,
            "f1": 0.68161,
            "f1_weighted": 0.683418
          },
          {
            "accuracy": 0.712794,
            "f1": 0.710083,
            "f1_weighted": 0.706562
          },
          {
            "accuracy": 0.684073,
            "f1": 0.657333,
            "f1_weighted": 0.663393
          },
          {
            "accuracy": 0.733681,
            "f1": 0.733969,
            "f1_weighted": 0.734088
          },
          {
            "accuracy": 0.736292,
            "f1": 0.733421,
            "f1_weighted": 0.730718
          },
          {
            "accuracy": 0.686684,
            "f1": 0.683716,
            "f1_weighted": 0.6856
          },
          {
            "accuracy": 0.684073,
            "f1": 0.66187,
            "f1_weighted": 0.667605
          },
          {
            "accuracy": 0.673629,
            "f1": 0.673596,
            "f1_weighted": 0.673504
          },
          {
            "accuracy": 0.70235,
            "f1": 0.702835,
            "f1_weighted": 0.701332
          },
          {
            "accuracy": 0.694517,
            "f1": 0.693963,
            "f1_weighted": 0.694756
          }
        ],
        "main_score": 0.699217,
        "hf_subset": "default",
        "languages": [
          "san-Deva"
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.68125,
        "f1": 0.687576,
        "f1_weighted": 0.677721,
        "scores_per_experiment": [
          {
            "accuracy": 0.677083,
            "f1": 0.684514,
            "f1_weighted": 0.677819
          },
          {
            "accuracy": 0.677083,
            "f1": 0.689049,
            "f1_weighted": 0.669687
          },
          {
            "accuracy": 0.71875,
            "f1": 0.710444,
            "f1_weighted": 0.710242
          },
          {
            "accuracy": 0.677083,
            "f1": 0.689049,
            "f1_weighted": 0.679259
          },
          {
            "accuracy": 0.666667,
            "f1": 0.673989,
            "f1_weighted": 0.655253
          },
          {
            "accuracy": 0.697917,
            "f1": 0.711443,
            "f1_weighted": 0.697917
          },
          {
            "accuracy": 0.666667,
            "f1": 0.669136,
            "f1_weighted": 0.66466
          },
          {
            "accuracy": 0.677083,
            "f1": 0.674471,
            "f1_weighted": 0.671132
          },
          {
            "accuracy": 0.6875,
            "f1": 0.700893,
            "f1_weighted": 0.68457
          },
          {
            "accuracy": 0.666667,
            "f1": 0.672772,
            "f1_weighted": 0.666667
          }
        ],
        "main_score": 0.68125,
        "hf_subset": "default",
        "languages": [
          "san-Deva"
        ]
      }
    ]
  },
  "evaluation_time": 12.09647536277771,
  "kg_co2_emissions": 0.0003468409709355175
}