{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "task_name": "GreekLegalCodeClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.019482,
        "f1": 0.010415,
        "f1_weighted": 0.012003,
        "scores_per_experiment": [
          {
            "accuracy": 0.015137,
            "f1": 0.007645,
            "f1_weighted": 0.009683
          },
          {
            "accuracy": 0.022461,
            "f1": 0.010251,
            "f1_weighted": 0.01219
          },
          {
            "accuracy": 0.019043,
            "f1": 0.00984,
            "f1_weighted": 0.011928
          },
          {
            "accuracy": 0.019043,
            "f1": 0.01264,
            "f1_weighted": 0.011925
          },
          {
            "accuracy": 0.023438,
            "f1": 0.01302,
            "f1_weighted": 0.013957
          },
          {
            "accuracy": 0.019531,
            "f1": 0.008292,
            "f1_weighted": 0.010597
          },
          {
            "accuracy": 0.02002,
            "f1": 0.008564,
            "f1_weighted": 0.013879
          },
          {
            "accuracy": 0.017578,
            "f1": 0.01227,
            "f1_weighted": 0.013058
          },
          {
            "accuracy": 0.018066,
            "f1": 0.008871,
            "f1_weighted": 0.010124
          },
          {
            "accuracy": 0.020508,
            "f1": 0.012755,
            "f1_weighted": 0.012691
          }
        ],
        "main_score": 0.019482,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.019385,
        "f1": 0.010795,
        "f1_weighted": 0.010069,
        "scores_per_experiment": [
          {
            "accuracy": 0.018066,
            "f1": 0.007896,
            "f1_weighted": 0.009044
          },
          {
            "accuracy": 0.020996,
            "f1": 0.011507,
            "f1_weighted": 0.010904
          },
          {
            "accuracy": 0.021484,
            "f1": 0.011313,
            "f1_weighted": 0.011359
          },
          {
            "accuracy": 0.018066,
            "f1": 0.010276,
            "f1_weighted": 0.010181
          },
          {
            "accuracy": 0.021973,
            "f1": 0.013287,
            "f1_weighted": 0.011808
          },
          {
            "accuracy": 0.022949,
            "f1": 0.014837,
            "f1_weighted": 0.014346
          },
          {
            "accuracy": 0.016602,
            "f1": 0.006785,
            "f1_weighted": 0.008822
          },
          {
            "accuracy": 0.023926,
            "f1": 0.017085,
            "f1_weighted": 0.013604
          },
          {
            "accuracy": 0.013184,
            "f1": 0.004905,
            "f1_weighted": 0.004138
          },
          {
            "accuracy": 0.016602,
            "f1": 0.010062,
            "f1_weighted": 0.006486
          }
        ],
        "main_score": 0.019385,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ]
  },
  "evaluation_time": 238.26173901557922,
  "kg_co2_emissions": 0.007904533334048051
}