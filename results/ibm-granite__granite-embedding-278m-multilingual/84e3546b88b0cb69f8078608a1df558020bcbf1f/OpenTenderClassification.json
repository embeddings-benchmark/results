{
  "dataset_revision": "9af5657575a669dc18c7f897a67287ff7d1a0c65",
  "task_name": "OpenTenderClassification",
  "mteb_version": "2.1.0",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.443478,
            "f1": 0.422718,
            "f1_weighted": 0.422744,
            "precision": 0.439064,
            "precision_weighted": 0.439087,
            "recall": 0.443477,
            "recall_weighted": 0.443478,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.431661,
            "f1": 0.412764,
            "f1_weighted": 0.41267,
            "precision": 0.432864,
            "precision_weighted": 0.432808,
            "recall": 0.431773,
            "recall_weighted": 0.431661,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.448161,
            "f1": 0.423664,
            "f1_weighted": 0.423672,
            "precision": 0.438514,
            "precision_weighted": 0.438558,
            "recall": 0.448189,
            "recall_weighted": 0.448161,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.442586,
            "f1": 0.42483,
            "f1_weighted": 0.424766,
            "precision": 0.451255,
            "precision_weighted": 0.451176,
            "recall": 0.442646,
            "recall_weighted": 0.442586,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.422965,
            "f1": 0.403441,
            "f1_weighted": 0.403434,
            "precision": 0.424933,
            "precision_weighted": 0.424916,
            "recall": 0.422928,
            "recall_weighted": 0.422965,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.438573,
            "f1": 0.420574,
            "f1_weighted": 0.420472,
            "precision": 0.432796,
            "precision_weighted": 0.432717,
            "recall": 0.438674,
            "recall_weighted": 0.438573,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.438796,
            "f1": 0.419378,
            "f1_weighted": 0.419352,
            "precision": 0.43387,
            "precision_weighted": 0.433823,
            "recall": 0.438789,
            "recall_weighted": 0.438796,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.422074,
            "f1": 0.399328,
            "f1_weighted": 0.399326,
            "precision": 0.420211,
            "precision_weighted": 0.420128,
            "recall": 0.422024,
            "recall_weighted": 0.422074,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.416722,
            "f1": 0.392459,
            "f1_weighted": 0.392355,
            "precision": 0.415266,
            "precision_weighted": 0.415083,
            "recall": 0.416752,
            "recall_weighted": 0.416722,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.422965,
            "f1": 0.397553,
            "f1_weighted": 0.397478,
            "precision": 0.404666,
            "precision_weighted": 0.404604,
            "recall": 0.423027,
            "recall_weighted": 0.422965,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.432798,
        "f1": 0.411671,
        "f1_weighted": 0.411627,
        "precision": 0.429344,
        "precision_weighted": 0.42929,
        "recall": 0.432828,
        "recall_weighted": 0.432798,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.411671,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 42.863245725631714,
  "kg_co2_emissions": null
}