{
  "dataset_revision": "5a79d6472db143690c7ce6e974995d3610eee7f0",
  "task_name": "SanskritShlokasClassification",
  "mteb_version": "1.38.30",
  "scores": {
    "train": [
      {
        "accuracy": 0.806527,
        "f1": 0.805588,
        "f1_weighted": 0.806036,
        "scores_per_experiment": [
          {
            "accuracy": 0.804178,
            "f1": 0.804779,
            "f1_weighted": 0.803759
          },
          {
            "accuracy": 0.81201,
            "f1": 0.811362,
            "f1_weighted": 0.811872
          },
          {
            "accuracy": 0.73107,
            "f1": 0.731275,
            "f1_weighted": 0.731477
          },
          {
            "accuracy": 0.793734,
            "f1": 0.79336,
            "f1_weighted": 0.793895
          },
          {
            "accuracy": 0.825065,
            "f1": 0.8249,
            "f1_weighted": 0.823347
          },
          {
            "accuracy": 0.830287,
            "f1": 0.831028,
            "f1_weighted": 0.830776
          },
          {
            "accuracy": 0.840731,
            "f1": 0.840266,
            "f1_weighted": 0.840778
          },
          {
            "accuracy": 0.798956,
            "f1": 0.797466,
            "f1_weighted": 0.79882
          },
          {
            "accuracy": 0.825065,
            "f1": 0.824052,
            "f1_weighted": 0.825784
          },
          {
            "accuracy": 0.804178,
            "f1": 0.797398,
            "f1_weighted": 0.799851
          }
        ],
        "main_score": 0.806527,
        "hf_subset": "default",
        "languages": [
          "san-Deva"
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.797917,
        "f1": 0.803869,
        "f1_weighted": 0.798732,
        "scores_per_experiment": [
          {
            "accuracy": 0.791667,
            "f1": 0.801585,
            "f1_weighted": 0.793711
          },
          {
            "accuracy": 0.791667,
            "f1": 0.798799,
            "f1_weighted": 0.79298
          },
          {
            "accuracy": 0.739583,
            "f1": 0.745576,
            "f1_weighted": 0.740177
          },
          {
            "accuracy": 0.729167,
            "f1": 0.738438,
            "f1_weighted": 0.730875
          },
          {
            "accuracy": 0.833333,
            "f1": 0.841094,
            "f1_weighted": 0.835121
          },
          {
            "accuracy": 0.885417,
            "f1": 0.889662,
            "f1_weighted": 0.886189
          },
          {
            "accuracy": 0.8125,
            "f1": 0.819892,
            "f1_weighted": 0.81376
          },
          {
            "accuracy": 0.802083,
            "f1": 0.808211,
            "f1_weighted": 0.803157
          },
          {
            "accuracy": 0.8125,
            "f1": 0.80608,
            "f1_weighted": 0.80863
          },
          {
            "accuracy": 0.78125,
            "f1": 0.789355,
            "f1_weighted": 0.782724
          }
        ],
        "main_score": 0.797917,
        "hf_subset": "default",
        "languages": [
          "san-Deva"
        ]
      }
    ]
  },
  "evaluation_time": 52.777884006500244,
  "kg_co2_emissions": null
}