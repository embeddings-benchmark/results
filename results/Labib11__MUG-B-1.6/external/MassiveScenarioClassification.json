{
    "dataset_revision": "7d571f92784cd94a019292a1f45445077d0ef634",
    "task_name": "MassiveScenarioClassification",
    "evaluation_time": null,
    "mteb_version": null,
    "scores": {
        "test": [
            {
                "hf_subset": "af",
                "languages": [
                    "afr-Latn"
                ],
                "accuracy": 0.47797579018157355,
                "f1": 0.4507825042119332,
                "main_score": 0.47797579018157355
            },
            {
                "hf_subset": "am",
                "languages": [
                    "amh-Ethi"
                ],
                "accuracy": 0.07078009414929388,
                "f1": 0.040122456300041645,
                "main_score": 0.07078009414929388
            },
            {
                "hf_subset": "ar",
                "languages": [
                    "ara-Arab"
                ],
                "accuracy": 0.22831203765971753,
                "f1": 0.20131610050816554,
                "main_score": 0.22831203765971753
            },
            {
                "hf_subset": "az",
                "languages": [
                    "aze-Latn"
                ],
                "accuracy": 0.4495292535305985,
                "f1": 0.426865575762921,
                "main_score": 0.4495292535305985
            },
            {
                "hf_subset": "bn",
                "languages": [
                    "ben-Beng"
                ],
                "accuracy": 0.16593813046402153,
                "f1": 0.1408714450304429,
                "main_score": 0.16593813046402153
            },
            {
                "hf_subset": "cy",
                "languages": [
                    "cym-Latn"
                ],
                "accuracy": 0.3791862811028917,
                "f1": 0.3496840272791191,
                "main_score": 0.3791862811028917
            },
            {
                "hf_subset": "da",
                "languages": [
                    "dan-Latn"
                ],
                "accuracy": 0.5192333557498319,
                "f1": 0.49357147840776333,
                "main_score": 0.5192333557498319
            },
            {
                "hf_subset": "de",
                "languages": [
                    "deu-Latn"
                ],
                "accuracy": 0.5873570948217889,
                "f1": 0.5492084137819753,
                "main_score": 0.5873570948217889
            },
            {
                "hf_subset": "el",
                "languages": [
                    "ell-Grek"
                ],
                "accuracy": 0.4299596503026228,
                "f1": 0.38475125427530693,
                "main_score": 0.4299596503026228
            },
            {
                "hf_subset": "en",
                "languages": [
                    "eng-Latn"
                ],
                "accuracy": 0.7742098184263618,
                "f1": 0.7703413816048876,
                "main_score": 0.7742098184263618
            },
            {
                "hf_subset": "es",
                "languages": [
                    "spa-Latn"
                ],
                "accuracy": 0.5446536650975118,
                "f1": 0.5308520810835907,
                "main_score": 0.5446536650975118
            },
            {
                "hf_subset": "fa",
                "languages": [
                    "fas-Arab"
                ],
                "accuracy": 0.30578345662407524,
                "f1": 0.28822998245702636,
                "main_score": 0.30578345662407524
            },
            {
                "hf_subset": "fi",
                "languages": [
                    "fin-Latn"
                ],
                "accuracy": 0.43567585743106924,
                "f1": 0.39792166517143474,
                "main_score": 0.43567585743106924
            },
            {
                "hf_subset": "fr",
                "languages": [
                    "fra-Latn"
                ],
                "accuracy": 0.5698722259583053,
                "f1": 0.5531168113501439,
                "main_score": 0.5698722259583053
            },
            {
                "hf_subset": "he",
                "languages": [
                    "heb-Hebr"
                ],
                "accuracy": 0.2807666442501681,
                "f1": 0.24927348965627572,
                "main_score": 0.2807666442501681
            },
            {
                "hf_subset": "hi",
                "languages": [
                    "hin-Deva"
                ],
                "accuracy": 0.18096839273705445,
                "f1": 0.17386603595777103,
                "main_score": 0.18096839273705445
            },
            {
                "hf_subset": "hu",
                "languages": [
                    "hun-Latn"
                ],
                "accuracy": 0.41738399462004033,
                "f1": 0.3865545902563735,
                "main_score": 0.41738399462004033
            },
            {
                "hf_subset": "hy",
                "languages": [
                    "hye-Armn"
                ],
                "accuracy": 0.11536650975117688,
                "f1": 0.10898336694524854,
                "main_score": 0.11536650975117688
            },
            {
                "hf_subset": "id",
                "languages": [
                    "ind-Latn"
                ],
                "accuracy": 0.469502353732347,
                "f1": 0.44332561323528646,
                "main_score": 0.469502353732347
            },
            {
                "hf_subset": "is",
                "languages": [
                    "isl-Latn"
                ],
                "accuracy": 0.42777404169468725,
                "f1": 0.39378117766055354,
                "main_score": 0.42777404169468725
            },
            {
                "hf_subset": "it",
                "languages": [
                    "ita-Latn"
                ],
                "accuracy": 0.546469401479489,
                "f1": 0.5251202527485179,
                "main_score": 0.546469401479489
            },
            {
                "hf_subset": "ja",
                "languages": [
                    "jpn-Jpan"
                ],
                "accuracy": 0.3590114324142569,
                "f1": 0.34903312747126053,
                "main_score": 0.3590114324142569
            },
            {
                "hf_subset": "jv",
                "languages": [
                    "jav-Latn"
                ],
                "accuracy": 0.4251176866173504,
                "f1": 0.3941754184568568,
                "main_score": 0.4251176866173504
            },
            {
                "hf_subset": "ka",
                "languages": [
                    "kat-Geor"
                ],
                "accuracy": 0.13799596503026226,
                "f1": 0.11587556164962251,
                "main_score": 0.13799596503026226
            },
            {
                "hf_subset": "km",
                "languages": [
                    "khm-Khmr"
                ],
                "accuracy": 0.0944855413584398,
                "f1": 0.043071107707690695,
                "main_score": 0.0944855413584398
            },
            {
                "hf_subset": "kn",
                "languages": [
                    "kan-Knda"
                ],
                "accuracy": 0.08157363819771352,
                "f1": 0.055588908736809516,
                "main_score": 0.08157363819771352
            },
            {
                "hf_subset": "ko",
                "languages": [
                    "kor-Kore"
                ],
                "accuracy": 0.19909213180901145,
                "f1": 0.18964761241087985,
                "main_score": 0.19909213180901145
            },
            {
                "hf_subset": "lv",
                "languages": [
                    "lav-Latn"
                ],
                "accuracy": 0.4047747141896436,
                "f1": 0.3817159556642586,
                "main_score": 0.4047747141896436
            },
            {
                "hf_subset": "ml",
                "languages": [
                    "mal-Mlym"
                ],
                "accuracy": 0.06701412239408204,
                "f1": 0.03621974155647488,
                "main_score": 0.06701412239408204
            },
            {
                "hf_subset": "mn",
                "languages": [
                    "mon-Cyrl"
                ],
                "accuracy": 0.2855413584398117,
                "f1": 0.2658254892366275,
                "main_score": 0.2855413584398117
            },
            {
                "hf_subset": "ms",
                "languages": [
                    "msa-Latn"
                ],
                "accuracy": 0.46617350369872235,
                "f1": 0.41353974192674253,
                "main_score": 0.46617350369872235
            },
            {
                "hf_subset": "my",
                "languages": [
                    "mya-Mymr"
                ],
                "accuracy": 0.09976462676529926,
                "f1": 0.05900764382768462,
                "main_score": 0.09976462676529926
            },
            {
                "hf_subset": "nb",
                "languages": [
                    "nob-Latn"
                ],
                "accuracy": 0.5089441829186281,
                "f1": 0.47709294037710864,
                "main_score": 0.5089441829186281
            },
            {
                "hf_subset": "nl",
                "languages": [
                    "nld-Latn"
                ],
                "accuracy": 0.517619367854741,
                "f1": 0.4842797973062516,
                "main_score": 0.517619367854741
            },
            {
                "hf_subset": "pl",
                "languages": [
                    "pol-Latn"
                ],
                "accuracy": 0.46213853396099525,
                "f1": 0.437081546200347,
                "main_score": 0.46213853396099525
            },
            {
                "hf_subset": "pt",
                "languages": [
                    "por-Latn"
                ],
                "accuracy": 0.5559852051109617,
                "f1": 0.5419610878409633,
                "main_score": 0.5559852051109617
            },
            {
                "hf_subset": "ro",
                "languages": [
                    "ron-Latn"
                ],
                "accuracy": 0.5054135843981169,
                "f1": 0.4779393938467311,
                "main_score": 0.5054135843981169
            },
            {
                "hf_subset": "ru",
                "languages": [
                    "rus-Cyrl"
                ],
                "accuracy": 0.3773032952252858,
                "f1": 0.35964501497080414,
                "main_score": 0.3773032952252858
            },
            {
                "hf_subset": "sl",
                "languages": [
                    "slv-Latn"
                ],
                "accuracy": 0.41671149966375254,
                "f1": 0.4028283538885605,
                "main_score": 0.41671149966375254
            },
            {
                "hf_subset": "sq",
                "languages": [
                    "sqi-Latn"
                ],
                "accuracy": 0.47380632145258905,
                "f1": 0.44932640160071524,
                "main_score": 0.47380632145258905
            },
            {
                "hf_subset": "sv",
                "languages": [
                    "swe-Latn"
                ],
                "accuracy": 0.4928379287155347,
                "f1": 0.4625486396570196,
                "main_score": 0.4928379287155347
            },
            {
                "hf_subset": "sw",
                "languages": [
                    "swa-Latn"
                ],
                "accuracy": 0.44182918628110296,
                "f1": 0.4117519157172804,
                "main_score": 0.44182918628110296
            },
            {
                "hf_subset": "ta",
                "languages": [
                    "tam-Taml"
                ],
                "accuracy": 0.12599193006052453,
                "f1": 0.11129236666238378,
                "main_score": 0.12599193006052453
            },
            {
                "hf_subset": "te",
                "languages": [
                    "tel-Telu"
                ],
                "accuracy": 0.07017484868863484,
                "f1": 0.039665415549749075,
                "main_score": 0.07017484868863484
            },
            {
                "hf_subset": "th",
                "languages": [
                    "tha-Thai"
                ],
                "accuracy": 0.19788164088769336,
                "f1": 0.15783384761347583,
                "main_score": 0.19788164088769336
            },
            {
                "hf_subset": "tl",
                "languages": [
                    "tgl-Latn"
                ],
                "accuracy": 0.5035978480161398,
                "f1": 0.4730586047800275,
                "main_score": 0.5035978480161398
            },
            {
                "hf_subset": "tr",
                "languages": [
                    "tur-Latn"
                ],
                "accuracy": 0.45484196368527235,
                "f1": 0.4465101184252231,
                "main_score": 0.45484196368527235
            },
            {
                "hf_subset": "ur",
                "languages": [
                    "urd-Arab"
                ],
                "accuracy": 0.23681909885675856,
                "f1": 0.22247817138937523,
                "main_score": 0.23681909885675856
            },
            {
                "hf_subset": "vi",
                "languages": [
                    "vie-Latn"
                ],
                "accuracy": 0.4163080026899798,
                "f1": 0.39546896741744,
                "main_score": 0.4163080026899798
            },
            {
                "hf_subset": "zh-CN",
                "languages": [
                    "cmo-Hans"
                ],
                "accuracy": 0.30141223940820444,
                "f1": 0.2817783896007812,
                "main_score": 0.30141223940820444
            },
            {
                "hf_subset": "zh-TW",
                "languages": [
                    "cmo-Hant"
                ],
                "accuracy": 0.27515131136516474,
                "f1": 0.26514325837594654,
                "main_score": 0.27515131136516474
            }
        ]
    }
}