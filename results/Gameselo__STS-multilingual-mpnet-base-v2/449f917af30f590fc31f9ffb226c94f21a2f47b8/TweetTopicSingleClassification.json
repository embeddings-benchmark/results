{
  "dataset_revision": "87b7a0d1c402dbb481db649569c556d9aa27ac05",
  "task_name": "TweetTopicSingleClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test_2021": [
      {
        "accuracy": 0.527939,
        "f1": 0.405223,
        "f1_weighted": 0.566722,
        "scores_per_experiment": [
          {
            "accuracy": 0.503249,
            "f1": 0.405976,
            "f1_weighted": 0.547971
          },
          {
            "accuracy": 0.500295,
            "f1": 0.403844,
            "f1_weighted": 0.53121
          },
          {
            "accuracy": 0.545186,
            "f1": 0.41902,
            "f1_weighted": 0.570066
          },
          {
            "accuracy": 0.533963,
            "f1": 0.426091,
            "f1_weighted": 0.568519
          },
          {
            "accuracy": 0.567041,
            "f1": 0.412535,
            "f1_weighted": 0.59871
          },
          {
            "accuracy": 0.529238,
            "f1": 0.405256,
            "f1_weighted": 0.568617
          },
          {
            "accuracy": 0.482575,
            "f1": 0.363337,
            "f1_weighted": 0.533999
          },
          {
            "accuracy": 0.575901,
            "f1": 0.434637,
            "f1_weighted": 0.61364
          },
          {
            "accuracy": 0.490254,
            "f1": 0.364164,
            "f1_weighted": 0.531815
          },
          {
            "accuracy": 0.551683,
            "f1": 0.41737,
            "f1_weighted": 0.602676
          }
        ],
        "main_score": 0.527939,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 8.021530866622925,
  "kg_co2_emissions": 0.00027059605063328046
}