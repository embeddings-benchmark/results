{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "task_name": "GreekLegalCodeClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.317236,
        "f1": 0.250802,
        "f1_weighted": 0.320108,
        "scores_per_experiment": [
          {
            "accuracy": 0.311523,
            "f1": 0.239614,
            "f1_weighted": 0.317369
          },
          {
            "accuracy": 0.311035,
            "f1": 0.24993,
            "f1_weighted": 0.313774
          },
          {
            "accuracy": 0.316406,
            "f1": 0.252267,
            "f1_weighted": 0.318286
          },
          {
            "accuracy": 0.317871,
            "f1": 0.253948,
            "f1_weighted": 0.318472
          },
          {
            "accuracy": 0.305176,
            "f1": 0.236989,
            "f1_weighted": 0.306521
          },
          {
            "accuracy": 0.309082,
            "f1": 0.243927,
            "f1_weighted": 0.315026
          },
          {
            "accuracy": 0.32666,
            "f1": 0.257391,
            "f1_weighted": 0.33307
          },
          {
            "accuracy": 0.319336,
            "f1": 0.253584,
            "f1_weighted": 0.320032
          },
          {
            "accuracy": 0.325684,
            "f1": 0.257669,
            "f1_weighted": 0.326092
          },
          {
            "accuracy": 0.32959,
            "f1": 0.262702,
            "f1_weighted": 0.332442
          }
        ],
        "main_score": 0.317236,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.300879,
        "f1": 0.255951,
        "f1_weighted": 0.303774,
        "scores_per_experiment": [
          {
            "accuracy": 0.297363,
            "f1": 0.255032,
            "f1_weighted": 0.303175
          },
          {
            "accuracy": 0.304688,
            "f1": 0.262294,
            "f1_weighted": 0.310278
          },
          {
            "accuracy": 0.308105,
            "f1": 0.256246,
            "f1_weighted": 0.304629
          },
          {
            "accuracy": 0.294922,
            "f1": 0.24693,
            "f1_weighted": 0.29926
          },
          {
            "accuracy": 0.286621,
            "f1": 0.249776,
            "f1_weighted": 0.289743
          },
          {
            "accuracy": 0.292969,
            "f1": 0.247752,
            "f1_weighted": 0.297163
          },
          {
            "accuracy": 0.307129,
            "f1": 0.25545,
            "f1_weighted": 0.310628
          },
          {
            "accuracy": 0.305664,
            "f1": 0.272382,
            "f1_weighted": 0.304904
          },
          {
            "accuracy": 0.303711,
            "f1": 0.255999,
            "f1_weighted": 0.306266
          },
          {
            "accuracy": 0.307617,
            "f1": 0.257649,
            "f1_weighted": 0.311698
          }
        ],
        "main_score": 0.300879,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ]
  },
  "evaluation_time": 507.3104875087738,
  "kg_co2_emissions": 0.01653273699572811
}