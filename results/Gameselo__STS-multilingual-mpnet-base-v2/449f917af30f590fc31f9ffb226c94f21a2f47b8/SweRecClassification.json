{
  "dataset_revision": "b07c6ce548f6a7ac8d546e1bbe197a0086409190",
  "task_name": "SweRecClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.64668,
        "f1": 0.578946,
        "f1_weighted": 0.674608,
        "scores_per_experiment": [
          {
            "accuracy": 0.56543,
            "f1": 0.509551,
            "f1_weighted": 0.593198
          },
          {
            "accuracy": 0.647949,
            "f1": 0.59654,
            "f1_weighted": 0.690983
          },
          {
            "accuracy": 0.644043,
            "f1": 0.595075,
            "f1_weighted": 0.684568
          },
          {
            "accuracy": 0.665039,
            "f1": 0.58835,
            "f1_weighted": 0.689047
          },
          {
            "accuracy": 0.571289,
            "f1": 0.507503,
            "f1_weighted": 0.608784
          },
          {
            "accuracy": 0.650391,
            "f1": 0.581442,
            "f1_weighted": 0.673367
          },
          {
            "accuracy": 0.705566,
            "f1": 0.617438,
            "f1_weighted": 0.720281
          },
          {
            "accuracy": 0.672363,
            "f1": 0.606798,
            "f1_weighted": 0.699914
          },
          {
            "accuracy": 0.668457,
            "f1": 0.600881,
            "f1_weighted": 0.691355
          },
          {
            "accuracy": 0.67627,
            "f1": 0.58588,
            "f1_weighted": 0.694589
          }
        ],
        "main_score": 0.64668,
        "hf_subset": "default",
        "languages": [
          "swe-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 8.01111888885498,
  "kg_co2_emissions": 0.0002682775887461516
}