{
  "dataset_revision": "8f95949846bb9e33c6aaf730ccfdb8fe6bcfb7a9",
  "task_name": "MultiHateClassification",
  "mteb_version": "2.1.17",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.607,
            "f1": 0.538611,
            "f1_weighted": 0.610375,
            "precision": 0.538183,
            "precision_weighted": 0.614145,
            "recall": 0.539518,
            "recall_weighted": 0.607,
            "ap": 0.317428,
            "ap_weighted": 0.317428
          },
          {
            "accuracy": 0.468,
            "f1": 0.467583,
            "f1_weighted": 0.473605,
            "precision": 0.543573,
            "precision_weighted": 0.631307,
            "recall": 0.545766,
            "recall_weighted": 0.468,
            "ap": 0.318973,
            "ap_weighted": 0.318973
          },
          {
            "accuracy": 0.581,
            "f1": 0.545632,
            "f1_weighted": 0.596846,
            "precision": 0.551104,
            "precision_weighted": 0.629532,
            "recall": 0.559624,
            "recall_weighted": 0.581,
            "ap": 0.327883,
            "ap_weighted": 0.327883
          },
          {
            "accuracy": 0.515,
            "f1": 0.496637,
            "f1_weighted": 0.535478,
            "precision": 0.519453,
            "precision_weighted": 0.601234,
            "recall": 0.523237,
            "recall_weighted": 0.515,
            "ap": 0.308343,
            "ap_weighted": 0.308343
          },
          {
            "accuracy": 0.547,
            "f1": 0.522389,
            "f1_weighted": 0.56619,
            "precision": 0.536992,
            "precision_weighted": 0.617853,
            "recall": 0.544097,
            "recall_weighted": 0.547,
            "ap": 0.318855,
            "ap_weighted": 0.318855
          },
          {
            "accuracy": 0.503,
            "f1": 0.492919,
            "f1_weighted": 0.521804,
            "precision": 0.528062,
            "precision_weighted": 0.611053,
            "recall": 0.533036,
            "recall_weighted": 0.503,
            "ap": 0.312965,
            "ap_weighted": 0.312965
          },
          {
            "accuracy": 0.52,
            "f1": 0.511749,
            "f1_weighted": 0.537392,
            "precision": 0.550127,
            "precision_weighted": 0.634652,
            "recall": 0.558663,
            "recall_weighted": 0.52,
            "ap": 0.326078,
            "ap_weighted": 0.326078
          },
          {
            "accuracy": 0.527,
            "f1": 0.498345,
            "f1_weighted": 0.546783,
            "precision": 0.512118,
            "precision_weighted": 0.593364,
            "recall": 0.514403,
            "recall_weighted": 0.527,
            "ap": 0.304289,
            "ap_weighted": 0.304289
          },
          {
            "accuracy": 0.517,
            "f1": 0.506917,
            "f1_weighted": 0.535403,
            "precision": 0.541413,
            "precision_weighted": 0.624995,
            "recall": 0.548801,
            "recall_weighted": 0.517,
            "ap": 0.320921,
            "ap_weighted": 0.320921
          },
          {
            "accuracy": 0.595,
            "f1": 0.522788,
            "f1_weighted": 0.597785,
            "precision": 0.522615,
            "precision_weighted": 0.600806,
            "recall": 0.523246,
            "recall_weighted": 0.595,
            "ap": 0.30874,
            "ap_weighted": 0.30874
          }
        ],
        "accuracy": 0.538,
        "f1": 0.510357,
        "f1_weighted": 0.552166,
        "precision": 0.534364,
        "precision_weighted": 0.615894,
        "recall": 0.539039,
        "recall_weighted": 0.538,
        "ap": 0.316447,
        "ap_weighted": 0.316447,
        "main_score": 0.538,
        "hf_subset": "nld",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 13.92847990989685,
  "kg_co2_emissions": null
}