{
  "dataset_revision": "ef5c383d1b87eb8feccde3dfb7f95e42b1b050dd",
  "evaluation_time": 18.834572315216064,
  "kg_co2_emissions": 0.0007690375371536747,
  "mteb_version": "1.16.5",
  "scores": {
    "test": [
      {
        "cosine_pearson": 0.8245876444673796,
        "cosine_spearman": 0.8199575302745242,
        "euclidean_pearson": 0.7534413957522428,
        "euclidean_spearman": 0.7789134131075132,
        "hf_subset": "afr",
        "languages": [
          "afr-Latn"
        ],
        "main_score": 0.8199575302745242,
        "manhattan_pearson": 0.7529509958951071,
        "manhattan_spearman": 0.7775460654311366,
        "pearson": 0.8245876444673796,
        "spearman": 0.8199575302745242
      },
      {
        "cosine_pearson": 0.02590954473937655,
        "cosine_spearman": 0.009385651720933265,
        "euclidean_pearson": -0.028502354804838537,
        "euclidean_spearman": -0.0473053489016089,
        "hf_subset": "amh",
        "languages": [
          "amh-Ethi"
        ],
        "main_score": 0.009385651720933265,
        "manhattan_pearson": -0.03171401554238612,
        "manhattan_spearman": -0.050131545458745194,
        "pearson": 0.02590954473937655,
        "spearman": 0.009385651720933265
      },
      {
        "cosine_pearson": 0.6353424602410248,
        "cosine_spearman": 0.6408145202369881,
        "euclidean_pearson": 0.5415274498333803,
        "euclidean_spearman": 0.5347677233198044,
        "hf_subset": "arb",
        "languages": [
          "arb-Arab"
        ],
        "main_score": 0.6408145202369881,
        "manhattan_pearson": 0.5380681903524043,
        "manhattan_spearman": 0.5312897094721895,
        "pearson": 0.6353424602410248,
        "spearman": 0.6408145202369881
      },
      {
        "cosine_pearson": 0.5384141324629351,
        "cosine_spearman": 0.4984775949270699,
        "euclidean_pearson": 0.48510071411904515,
        "euclidean_spearman": 0.44647668359644976,
        "hf_subset": "arq",
        "languages": [
          "arq-Arab"
        ],
        "main_score": 0.4984775949270699,
        "manhattan_pearson": 0.48473227221255166,
        "manhattan_spearman": 0.44752026644518783,
        "pearson": 0.5384141324629351,
        "spearman": 0.4984775949270699
      },
      {
        "cosine_pearson": 0.5136524175060324,
        "cosine_spearman": 0.4968022679794799,
        "euclidean_pearson": 0.5654738725203229,
        "euclidean_spearman": 0.5561116482243247,
        "hf_subset": "ary",
        "languages": [
          "ary-Arab"
        ],
        "main_score": 0.4968022679794799,
        "manhattan_pearson": 0.5644777914181924,
        "manhattan_spearman": 0.5563555398480047,
        "pearson": 0.5136524175060324,
        "spearman": 0.4968022679794799
      },
      {
        "cosine_pearson": 0.8262341254074697,
        "cosine_spearman": 0.8227089077726746,
        "euclidean_pearson": 0.7921368785636271,
        "euclidean_spearman": 0.7910204084445799,
        "hf_subset": "eng",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8227089077726746,
        "manhattan_pearson": 0.7927233487352541,
        "manhattan_spearman": 0.7918177834170761,
        "pearson": 0.8262341254074697,
        "spearman": 0.8227089077726746
      },
      {
        "cosine_pearson": 0.4164747592846911,
        "cosine_spearman": 0.38680907242426404,
        "euclidean_pearson": 0.3393340978702597,
        "euclidean_spearman": 0.29897248679321514,
        "hf_subset": "hau",
        "languages": [
          "hau-Latn"
        ],
        "main_score": 0.38680907242426404,
        "manhattan_pearson": 0.33863392260786,
        "manhattan_spearman": 0.2970395964465908,
        "pearson": 0.4164747592846911,
        "spearman": 0.38680907242426404
      },
      {
        "cosine_pearson": 0.7142787861959452,
        "cosine_spearman": 0.7155812321595278,
        "euclidean_pearson": 0.6306012344547849,
        "euclidean_spearman": 0.6204528729296283,
        "hf_subset": "hin",
        "languages": [
          "hin-Deva"
        ],
        "main_score": 0.7155812321595278,
        "manhattan_pearson": 0.6269373918315729,
        "manhattan_spearman": 0.616192502390839,
        "pearson": 0.7142787861959452,
        "spearman": 0.7155812321595278
      },
      {
        "cosine_pearson": 0.4109994334271355,
        "cosine_spearman": 0.4070474493445136,
        "euclidean_pearson": 0.45007965886101325,
        "euclidean_spearman": 0.42696603850022186,
        "hf_subset": "ind",
        "languages": [
          "ind-Latn"
        ],
        "main_score": 0.4070474493445136,
        "manhattan_pearson": 0.44996611430932154,
        "manhattan_spearman": 0.4269687030956275,
        "pearson": 0.4109994334271355,
        "spearman": 0.4070474493445136
      },
      {
        "cosine_pearson": 0.46492359876087574,
        "cosine_spearman": 0.42119924122783503,
        "euclidean_pearson": 0.42922090920709954,
        "euclidean_spearman": 0.38305273538998186,
        "hf_subset": "kin",
        "languages": [
          "kin-Latn"
        ],
        "main_score": 0.42119924122783503,
        "manhattan_pearson": 0.43214449058782406,
        "manhattan_spearman": 0.3864849275882574,
        "pearson": 0.46492359876087574,
        "spearman": 0.42119924122783503
      },
      {
        "cosine_pearson": 0.7561371378517471,
        "cosine_spearman": 0.7399974123373684,
        "euclidean_pearson": 0.7124619221802482,
        "euclidean_spearman": 0.7091568249254154,
        "hf_subset": "mar",
        "languages": [
          "mar-Deva"
        ],
        "main_score": 0.7399974123373684,
        "manhattan_pearson": 0.7130234223857789,
        "manhattan_spearman": 0.7041556485883419,
        "pearson": 0.7561371378517471,
        "spearman": 0.7399974123373684
      },
      {
        "cosine_pearson": 0.8709378868323082,
        "cosine_spearman": 0.865039156851507,
        "euclidean_pearson": 0.8164728340255609,
        "euclidean_spearman": 0.8126294168867129,
        "hf_subset": "tel",
        "languages": [
          "tel-Telu"
        ],
        "main_score": 0.865039156851507,
        "manhattan_pearson": 0.8178479757544517,
        "manhattan_spearman": 0.812626667919877,
        "pearson": 0.8709378868323082,
        "spearman": 0.865039156851507
      }
    ]
  },
  "task_name": "SemRel24STS"
}