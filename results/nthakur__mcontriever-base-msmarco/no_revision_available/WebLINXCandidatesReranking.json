{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "evaluation_time": 4290.62239074707,
  "kg_co2_emissions": 0.2543693635202506,
  "mteb_version": "1.16.5",
  "scores": {
    "test_cat": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.056417313179953625,
        "map": 0.07307095633968878,
        "mrr": 0.056417313179953625,
        "nAUC_map_diff1": 0.17874170238928042,
        "nAUC_map_max": 0.0004000571942629103,
        "nAUC_map_std": 0.23374529075574227,
        "nAUC_mrr_diff1": 0.16835893497291168,
        "nAUC_mrr_max": -0.006479913362494033,
        "nAUC_mrr_std": 0.1909617858080491
      }
    ],
    "test_geo": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.07467969829645989,
        "map": 0.09183353313254045,
        "mrr": 0.07467969829645989,
        "nAUC_map_diff1": 0.1038684949750576,
        "nAUC_map_max": -0.10501098050339999,
        "nAUC_map_std": 0.055910899665491266,
        "nAUC_mrr_diff1": 0.10711998553647224,
        "nAUC_mrr_max": -0.10473456561083991,
        "nAUC_mrr_std": 0.04929369697815604
      }
    ],
    "test_iid": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08677506236616112,
        "map": 0.10276869202957449,
        "mrr": 0.08677506236616112,
        "nAUC_map_diff1": 0.13154941472594908,
        "nAUC_map_max": -0.008020472345207134,
        "nAUC_map_std": 0.1509676219407547,
        "nAUC_mrr_diff1": 0.12363188376057857,
        "nAUC_mrr_max": -0.003047965521911493,
        "nAUC_mrr_std": 0.14064507205635993
      }
    ],
    "test_vis": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.08260020253225236,
        "map": 0.10010741378728648,
        "mrr": 0.08260020253225236,
        "nAUC_map_diff1": 0.13033135722312394,
        "nAUC_map_max": -0.03199767373827098,
        "nAUC_map_std": 0.1275340231195058,
        "nAUC_mrr_diff1": 0.13086585930753805,
        "nAUC_mrr_max": -0.018727865251580426,
        "nAUC_mrr_std": 0.11809730703848292
      }
    ],
    "test_web": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.06845111878508825,
        "map": 0.08545809641565139,
        "mrr": 0.06845111878508825,
        "nAUC_map_diff1": 0.17711492944867935,
        "nAUC_map_max": -0.038749797349894415,
        "nAUC_map_std": 0.14331001477293925,
        "nAUC_mrr_diff1": 0.1847806883025646,
        "nAUC_mrr_max": -0.0326454833291513,
        "nAUC_mrr_std": 0.1330061966102307
      }
    ],
    "validation": [
      {
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.1085206129595061,
        "map": 0.12441993688581997,
        "mrr": 0.1085206129595061,
        "nAUC_map_diff1": 0.18505494782782597,
        "nAUC_map_max": -0.1194795420860939,
        "nAUC_map_std": -0.010815916258836801,
        "nAUC_mrr_diff1": 0.1855444641246071,
        "nAUC_mrr_max": -0.11963961664425067,
        "nAUC_mrr_std": -0.015941574121814808
      }
    ]
  },
  "task_name": "WebLINXCandidatesReranking"
}