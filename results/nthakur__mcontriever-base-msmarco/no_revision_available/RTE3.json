{
  "dataset_revision": "d94f96ca5a6798e20f5a77e566f7a288dc6138d7",
  "evaluation_time": 9.26380205154419,
  "kg_co2_emissions": 0.00031927002402028676,
  "mteb_version": "1.16.5",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.8482328482328483,
        "cosine_accuracy_threshold": 0.4221675395965576,
        "cosine_ap": 0.8514440506292833,
        "cosine_f1": 0.9178852643419573,
        "cosine_f1_threshold": 0.3923646807670593,
        "cosine_precision": 0.85,
        "cosine_recall": 0.9975550122249389,
        "dot_accuracy": 0.8482328482328483,
        "dot_accuracy_threshold": 1.7125178575515747,
        "dot_ap": 0.8313446883536673,
        "dot_f1": 0.9178852643419573,
        "dot_f1_threshold": 1.7125178575515747,
        "dot_precision": 0.85,
        "dot_recall": 0.9975550122249389,
        "euclidean_accuracy": 0.8482328482328483,
        "euclidean_accuracy_threshold": 2.437593460083008,
        "euclidean_ap": 0.8557849061460093,
        "euclidean_f1": 0.9178852643419573,
        "euclidean_f1_threshold": 2.437593460083008,
        "euclidean_precision": 0.85,
        "euclidean_recall": 0.9975550122249389,
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ],
        "main_score": 0.8557849061460093,
        "manhattan_accuracy": 0.8482328482328483,
        "manhattan_accuracy_threshold": 53.90217208862305,
        "manhattan_ap": 0.8553452841919678,
        "manhattan_f1": 0.9178852643419573,
        "manhattan_f1_threshold": 53.90217208862305,
        "manhattan_precision": 0.85,
        "manhattan_recall": 0.9975550122249389,
        "max_accuracy": 0.8482328482328483,
        "max_ap": 0.8557849061460093,
        "max_f1": 0.9178852643419573,
        "max_precision": 0.85,
        "max_recall": 0.9975550122249389,
        "similarity_accuracy": 0.8482328482328483,
        "similarity_accuracy_threshold": 0.4221675395965576,
        "similarity_ap": 0.8514440506292833,
        "similarity_f1": 0.9178852643419573,
        "similarity_f1_threshold": 0.3923646807670593,
        "similarity_precision": 0.85,
        "similarity_recall": 0.9975550122249389
      },
      {
        "cosine_accuracy": 0.8485477178423236,
        "cosine_accuracy_threshold": 0.3831043839454651,
        "cosine_ap": 0.8589670434259181,
        "cosine_f1": 0.9180695847362514,
        "cosine_f1_threshold": 0.3831043839454651,
        "cosine_precision": 0.8503118503118503,
        "cosine_recall": 0.9975609756097561,
        "dot_accuracy": 0.8485477178423236,
        "dot_accuracy_threshold": 1.7448391914367676,
        "dot_ap": 0.8297774096182917,
        "dot_f1": 0.9180695847362514,
        "dot_f1_threshold": 1.7448391914367676,
        "dot_precision": 0.8503118503118503,
        "dot_recall": 0.9975609756097561,
        "euclidean_accuracy": 0.8485477178423236,
        "euclidean_accuracy_threshold": 2.567338466644287,
        "euclidean_ap": 0.8660010528299444,
        "euclidean_f1": 0.9180695847362514,
        "euclidean_f1_threshold": 2.567338466644287,
        "euclidean_precision": 0.8503118503118503,
        "euclidean_recall": 0.9975609756097561,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8660010528299444,
        "manhattan_accuracy": 0.8485477178423236,
        "manhattan_accuracy_threshold": 57.278228759765625,
        "manhattan_ap": 0.8658627894779589,
        "manhattan_f1": 0.9180695847362514,
        "manhattan_f1_threshold": 57.278228759765625,
        "manhattan_precision": 0.8503118503118503,
        "manhattan_recall": 0.9975609756097561,
        "max_accuracy": 0.8485477178423236,
        "max_ap": 0.8660010528299444,
        "max_f1": 0.9180695847362514,
        "max_precision": 0.8503118503118503,
        "max_recall": 0.9975609756097561,
        "similarity_accuracy": 0.8485477178423236,
        "similarity_accuracy_threshold": 0.3831043839454651,
        "similarity_ap": 0.8589670434259181,
        "similarity_f1": 0.9180695847362514,
        "similarity_f1_threshold": 0.3831043839454651,
        "similarity_precision": 0.8503118503118503,
        "similarity_recall": 0.9975609756097561
      },
      {
        "cosine_accuracy": 0.8464730290456431,
        "cosine_accuracy_threshold": 0.4240805506706238,
        "cosine_ap": 0.8583202640528144,
        "cosine_f1": 0.9168539325842697,
        "cosine_f1_threshold": 0.4240805506706238,
        "cosine_precision": 0.8482328482328483,
        "cosine_recall": 0.9975550122249389,
        "dot_accuracy": 0.8464730290456431,
        "dot_accuracy_threshold": 1.665956974029541,
        "dot_ap": 0.8366049231636357,
        "dot_f1": 0.9168539325842697,
        "dot_f1_threshold": 1.665956974029541,
        "dot_precision": 0.8482328482328483,
        "dot_recall": 0.9975550122249389,
        "euclidean_accuracy": 0.8464730290456431,
        "euclidean_accuracy_threshold": 2.342529296875,
        "euclidean_ap": 0.8618178974138928,
        "euclidean_f1": 0.9168539325842697,
        "euclidean_f1_threshold": 2.342529296875,
        "euclidean_precision": 0.8482328482328483,
        "euclidean_recall": 0.9975550122249389,
        "hf_subset": "fr",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.8626013439485766,
        "manhattan_accuracy": 0.8464730290456431,
        "manhattan_accuracy_threshold": 51.88111114501953,
        "manhattan_ap": 0.8626013439485766,
        "manhattan_f1": 0.9168539325842697,
        "manhattan_f1_threshold": 51.88111114501953,
        "manhattan_precision": 0.8482328482328483,
        "manhattan_recall": 0.9975550122249389,
        "max_accuracy": 0.8464730290456431,
        "max_ap": 0.8626013439485766,
        "max_f1": 0.9168539325842697,
        "max_precision": 0.8482328482328483,
        "max_recall": 0.9975550122249389,
        "similarity_accuracy": 0.8464730290456431,
        "similarity_accuracy_threshold": 0.4240805506706238,
        "similarity_ap": 0.8583202640528144,
        "similarity_f1": 0.9168539325842697,
        "similarity_f1_threshold": 0.4240805506706238,
        "similarity_precision": 0.8482328482328483,
        "similarity_recall": 0.9975550122249389
      },
      {
        "cosine_accuracy": 0.8472803347280334,
        "cosine_accuracy_threshold": 0.423010915517807,
        "cosine_ap": 0.8546176510535884,
        "cosine_f1": 0.9173272933182333,
        "cosine_f1_threshold": 0.423010915517807,
        "cosine_precision": 0.8490566037735849,
        "cosine_recall": 0.9975369458128078,
        "dot_accuracy": 0.8472803347280334,
        "dot_accuracy_threshold": 1.828237771987915,
        "dot_ap": 0.8250235723315692,
        "dot_f1": 0.9173272933182333,
        "dot_f1_threshold": 1.828237771987915,
        "dot_precision": 0.8490566037735849,
        "dot_recall": 0.9975369458128078,
        "euclidean_accuracy": 0.8472803347280334,
        "euclidean_accuracy_threshold": 2.460519552230835,
        "euclidean_ap": 0.8625883938254609,
        "euclidean_f1": 0.9173272933182333,
        "euclidean_f1_threshold": 2.460519552230835,
        "euclidean_precision": 0.8490566037735849,
        "euclidean_recall": 0.9975369458128078,
        "hf_subset": "it",
        "languages": [
          "ita-Latn"
        ],
        "main_score": 0.8630523669283996,
        "manhattan_accuracy": 0.8472803347280334,
        "manhattan_accuracy_threshold": 53.175537109375,
        "manhattan_ap": 0.8630523669283996,
        "manhattan_f1": 0.9173272933182333,
        "manhattan_f1_threshold": 54.83467102050781,
        "manhattan_precision": 0.8490566037735849,
        "manhattan_recall": 0.9975369458128078,
        "max_accuracy": 0.8472803347280334,
        "max_ap": 0.8630523669283996,
        "max_f1": 0.9173272933182333,
        "max_precision": 0.8490566037735849,
        "max_recall": 0.9975369458128078,
        "similarity_accuracy": 0.8472803347280334,
        "similarity_accuracy_threshold": 0.423010915517807,
        "similarity_ap": 0.8546176510535884,
        "similarity_f1": 0.9173272933182333,
        "similarity_f1_threshold": 0.423010915517807,
        "similarity_precision": 0.8490566037735849,
        "similarity_recall": 0.9975369458128078
      }
    ]
  },
  "task_name": "RTE3"
}