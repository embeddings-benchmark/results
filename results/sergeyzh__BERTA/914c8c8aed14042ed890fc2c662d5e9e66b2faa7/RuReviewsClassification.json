{
  "dataset_revision": "f6d2c31f4dc6b88f468552750bfec05b4b41b05a",
  "task_name": "RuReviewsClassification",
  "mteb_version": "1.36.16",
  "scores": {
    "test": [
      {
        "accuracy": 0.722949,
        "f1": 0.716192,
        "f1_weighted": 0.716187,
        "scores_per_experiment": [
          {
            "accuracy": 0.725098,
            "f1": 0.722265,
            "f1_weighted": 0.722261
          },
          {
            "accuracy": 0.695312,
            "f1": 0.682509,
            "f1_weighted": 0.682496
          },
          {
            "accuracy": 0.709473,
            "f1": 0.696781,
            "f1_weighted": 0.696765
          },
          {
            "accuracy": 0.735352,
            "f1": 0.733723,
            "f1_weighted": 0.733724
          },
          {
            "accuracy": 0.743164,
            "f1": 0.744398,
            "f1_weighted": 0.744405
          },
          {
            "accuracy": 0.713867,
            "f1": 0.706354,
            "f1_weighted": 0.706351
          },
          {
            "accuracy": 0.734375,
            "f1": 0.728091,
            "f1_weighted": 0.728087
          },
          {
            "accuracy": 0.737793,
            "f1": 0.734114,
            "f1_weighted": 0.734118
          },
          {
            "accuracy": 0.710938,
            "f1": 0.694692,
            "f1_weighted": 0.694665
          },
          {
            "accuracy": 0.724121,
            "f1": 0.718993,
            "f1_weighted": 0.718998
          }
        ],
        "main_score": 0.722949,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 6.427979946136475,
  "kg_co2_emissions": null
}