{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "task_name": "MassiveScenarioClassification",
  "mteb_version": "1.36.16",
  "scores": {
    "validation": [
      {
        "accuracy": 0.84604,
        "f1": 0.836355,
        "f1_weighted": 0.843628,
        "scores_per_experiment": [
          {
            "accuracy": 0.845548,
            "f1": 0.838163,
            "f1_weighted": 0.844973
          },
          {
            "accuracy": 0.85391,
            "f1": 0.843265,
            "f1_weighted": 0.851264
          },
          {
            "accuracy": 0.857846,
            "f1": 0.848271,
            "f1_weighted": 0.853002
          },
          {
            "accuracy": 0.841121,
            "f1": 0.833726,
            "f1_weighted": 0.837569
          },
          {
            "accuracy": 0.846532,
            "f1": 0.837552,
            "f1_weighted": 0.84369
          },
          {
            "accuracy": 0.839154,
            "f1": 0.825871,
            "f1_weighted": 0.83579
          },
          {
            "accuracy": 0.826365,
            "f1": 0.812775,
            "f1_weighted": 0.823798
          },
          {
            "accuracy": 0.831284,
            "f1": 0.822564,
            "f1_weighted": 0.832204
          },
          {
            "accuracy": 0.863256,
            "f1": 0.8513,
            "f1_weighted": 0.859449
          },
          {
            "accuracy": 0.855386,
            "f1": 0.850065,
            "f1_weighted": 0.854544
          }
        ],
        "main_score": 0.84604,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.845024,
        "f1": 0.836857,
        "f1_weighted": 0.842567,
        "scores_per_experiment": [
          {
            "accuracy": 0.840955,
            "f1": 0.838206,
            "f1_weighted": 0.839704
          },
          {
            "accuracy": 0.851379,
            "f1": 0.839277,
            "f1_weighted": 0.847187
          },
          {
            "accuracy": 0.854405,
            "f1": 0.840472,
            "f1_weighted": 0.85029
          },
          {
            "accuracy": 0.840619,
            "f1": 0.832708,
            "f1_weighted": 0.837378
          },
          {
            "accuracy": 0.8423,
            "f1": 0.834852,
            "f1_weighted": 0.840059
          },
          {
            "accuracy": 0.84499,
            "f1": 0.83545,
            "f1_weighted": 0.841
          },
          {
            "accuracy": 0.825824,
            "f1": 0.81715,
            "f1_weighted": 0.82311
          },
          {
            "accuracy": 0.834902,
            "f1": 0.83237,
            "f1_weighted": 0.836539
          },
          {
            "accuracy": 0.85575,
            "f1": 0.846792,
            "f1_weighted": 0.851793
          },
          {
            "accuracy": 0.859112,
            "f1": 0.851289,
            "f1_weighted": 0.858612
          }
        ],
        "main_score": 0.845024,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 9.918192625045776,
  "kg_co2_emissions": null
}