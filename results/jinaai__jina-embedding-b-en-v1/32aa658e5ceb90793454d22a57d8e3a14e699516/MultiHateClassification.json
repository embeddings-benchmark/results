{
  "dataset_revision": "8f95949846bb9e33c6aaf730ccfdb8fe6bcfb7a9",
  "task_name": "MultiHateClassification",
  "mteb_version": "2.1.17",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.549,
            "f1": 0.524498,
            "f1_weighted": 0.568105,
            "precision": 0.538997,
            "precision_weighted": 0.619818,
            "recall": 0.546488,
            "recall_weighted": 0.549,
            "ap": 0.320123,
            "ap_weighted": 0.320123
          },
          {
            "accuracy": 0.387,
            "f1": 0.380682,
            "f1_weighted": 0.355412,
            "precision": 0.514874,
            "precision_weighted": 0.600124,
            "recall": 0.511248,
            "recall_weighted": 0.387,
            "ap": 0.302799,
            "ap_weighted": 0.302799
          },
          {
            "accuracy": 0.567,
            "f1": 0.535865,
            "f1_weighted": 0.584431,
            "precision": 0.544551,
            "precision_weighted": 0.624107,
            "recall": 0.552549,
            "recall_weighted": 0.567,
            "ap": 0.323648,
            "ap_weighted": 0.323648
          },
          {
            "accuracy": 0.572,
            "f1": 0.52385,
            "f1_weighted": 0.585022,
            "precision": 0.526566,
            "precision_weighted": 0.605684,
            "recall": 0.530039,
            "recall_weighted": 0.572,
            "ap": 0.311948,
            "ap_weighted": 0.311948
          },
          {
            "accuracy": 0.577,
            "f1": 0.516004,
            "f1_weighted": 0.585419,
            "precision": 0.516757,
            "precision_weighted": 0.596293,
            "recall": 0.51815,
            "recall_weighted": 0.577,
            "ap": 0.306152,
            "ap_weighted": 0.306152
          },
          {
            "accuracy": 0.548,
            "f1": 0.503297,
            "f1_weighted": 0.563497,
            "precision": 0.508744,
            "precision_weighted": 0.58966,
            "recall": 0.510048,
            "recall_weighted": 0.548,
            "ap": 0.302352,
            "ap_weighted": 0.302352
          },
          {
            "accuracy": 0.607,
            "f1": 0.544273,
            "f1_weighted": 0.612579,
            "precision": 0.543719,
            "precision_weighted": 0.619357,
            "recall": 0.546277,
            "recall_weighted": 0.607,
            "ap": 0.321163,
            "ap_weighted": 0.321163
          },
          {
            "accuracy": 0.405,
            "f1": 0.404985,
            "f1_weighted": 0.403783,
            "precision": 0.486276,
            "precision_weighted": 0.565588,
            "recall": 0.48641,
            "recall_weighted": 0.405,
            "ap": 0.292467,
            "ap_weighted": 0.292467
          },
          {
            "accuracy": 0.438,
            "f1": 0.437559,
            "f1_weighted": 0.431197,
            "precision": 0.539219,
            "precision_weighted": 0.628115,
            "recall": 0.536951,
            "recall_weighted": 0.438,
            "ap": 0.314559,
            "ap_weighted": 0.314559
          },
          {
            "accuracy": 0.475,
            "f1": 0.469589,
            "f1_weighted": 0.491232,
            "precision": 0.516476,
            "precision_weighted": 0.599429,
            "recall": 0.518887,
            "recall_weighted": 0.475,
            "ap": 0.306251,
            "ap_weighted": 0.306251
          }
        ],
        "accuracy": 0.5125,
        "f1": 0.48406,
        "f1_weighted": 0.518068,
        "precision": 0.523618,
        "precision_weighted": 0.604817,
        "recall": 0.525705,
        "recall_weighted": 0.5125,
        "ap": 0.310146,
        "ap_weighted": 0.310146,
        "main_score": 0.5125,
        "hf_subset": "nld",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 11.573872804641724,
  "kg_co2_emissions": null
}