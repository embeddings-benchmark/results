{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "task_name": "WebLINXCandidatesReranking",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "map": 0.138266,
        "mrr": 0.121305,
        "nAUC_map_max": -0.104041,
        "nAUC_map_std": 0.001428,
        "nAUC_map_diff1": 0.196527,
        "nAUC_mrr_max": -0.098265,
        "nAUC_mrr_std": -0.00526,
        "nAUC_mrr_diff1": 0.199776,
        "main_score": 0.121305,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_iid": [
      {
        "map": 0.112247,
        "mrr": 0.094911,
        "nAUC_map_max": 0.028738,
        "nAUC_map_std": 0.170649,
        "nAUC_map_diff1": 0.054616,
        "nAUC_mrr_max": 0.033289,
        "nAUC_mrr_std": 0.14568,
        "nAUC_mrr_diff1": 0.049371,
        "main_score": 0.094911,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_cat": [
      {
        "map": 0.102656,
        "mrr": 0.086794,
        "nAUC_map_max": 0.008645,
        "nAUC_map_std": 0.347788,
        "nAUC_map_diff1": 0.066576,
        "nAUC_mrr_max": 0.02709,
        "nAUC_mrr_std": 0.338237,
        "nAUC_mrr_diff1": 0.066497,
        "main_score": 0.086794,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_geo": [
      {
        "map": 0.117311,
        "mrr": 0.1,
        "nAUC_map_max": -0.046543,
        "nAUC_map_std": 0.040321,
        "nAUC_map_diff1": 0.041219,
        "nAUC_mrr_max": -0.051131,
        "nAUC_mrr_std": 0.023306,
        "nAUC_mrr_diff1": 0.044056,
        "main_score": 0.1,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_vis": [
      {
        "map": 0.140343,
        "mrr": 0.12274,
        "nAUC_map_max": -0.087342,
        "nAUC_map_std": 0.108544,
        "nAUC_map_diff1": 0.10707,
        "nAUC_mrr_max": -0.081544,
        "nAUC_mrr_std": 0.09692,
        "nAUC_mrr_diff1": 0.110142,
        "main_score": 0.12274,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_web": [
      {
        "map": 0.122291,
        "mrr": 0.106438,
        "nAUC_map_max": -0.005004,
        "nAUC_map_std": 0.142988,
        "nAUC_map_diff1": 0.172408,
        "nAUC_mrr_max": -0.002217,
        "nAUC_mrr_std": 0.138967,
        "nAUC_mrr_diff1": 0.173784,
        "main_score": 0.106438,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 2681.6536898612976,
  "kg_co2_emissions": 0.22194360261076396
}