{
  "dataset_revision": "b48bc27d383cfca5b6a47135a52390fa5f66b253",
  "task_name": "AmazonCounterfactualVNClassification",
  "mteb_version": "1.38.41",
  "scores": {
    "test": [
      {
        "accuracy": 0.620815,
        "f1": 0.559138,
        "f1_weighted": 0.662066,
        "ap": 0.251928,
        "ap_weighted": 0.251928,
        "scores_per_experiment": [
          {
            "accuracy": 0.637339,
            "f1": 0.577797,
            "f1_weighted": 0.678508,
            "ap": 0.264009,
            "ap_weighted": 0.264009
          },
          {
            "accuracy": 0.718884,
            "f1": 0.642862,
            "f1_weighted": 0.747525,
            "ap": 0.308786,
            "ap_weighted": 0.308786
          },
          {
            "accuracy": 0.643777,
            "f1": 0.589091,
            "f1_weighted": 0.684308,
            "ap": 0.278491,
            "ap_weighted": 0.278491
          },
          {
            "accuracy": 0.553648,
            "f1": 0.524163,
            "f1_weighted": 0.599401,
            "ap": 0.253839,
            "ap_weighted": 0.253839
          },
          {
            "accuracy": 0.570815,
            "f1": 0.515068,
            "f1_weighted": 0.619506,
            "ap": 0.220362,
            "ap_weighted": 0.220362
          },
          {
            "accuracy": 0.637339,
            "f1": 0.554445,
            "f1_weighted": 0.676518,
            "ap": 0.229987,
            "ap_weighted": 0.229987
          },
          {
            "accuracy": 0.658798,
            "f1": 0.591622,
            "f1_weighted": 0.696829,
            "ap": 0.269091,
            "ap_weighted": 0.269091
          },
          {
            "accuracy": 0.624464,
            "f1": 0.561151,
            "f1_weighted": 0.66703,
            "ap": 0.247456,
            "ap_weighted": 0.247456
          },
          {
            "accuracy": 0.551502,
            "f1": 0.519078,
            "f1_weighted": 0.598397,
            "ap": 0.245711,
            "ap_weighted": 0.245711
          },
          {
            "accuracy": 0.611588,
            "f1": 0.516107,
            "f1_weighted": 0.65264,
            "ap": 0.201552,
            "ap_weighted": 0.201552
          }
        ],
        "main_score": 0.620815,
        "hf_subset": "default",
        "languages": [
          "vie-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 1.2999205589294434,
  "kg_co2_emissions": null
}