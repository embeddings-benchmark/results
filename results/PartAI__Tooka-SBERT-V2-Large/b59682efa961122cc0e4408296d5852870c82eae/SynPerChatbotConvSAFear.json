{
  "dataset_revision": "3c22f7e6bf4e366c86d69293c9164bf9e9d80aac",
  "task_name": "SynPerChatbotConvSAFear",
  "mteb_version": "1.38.27",
  "scores": {
    "test": [
      {
        "accuracy": 0.832479,
        "f1": 0.829135,
        "f1_weighted": 0.834166,
        "ap": 0.852319,
        "ap_weighted": 0.852319,
        "scores_per_experiment": [
          {
            "accuracy": 0.854701,
            "f1": 0.850327,
            "f1_weighted": 0.855794,
            "ap": 0.862263,
            "ap_weighted": 0.862263
          },
          {
            "accuracy": 0.811966,
            "f1": 0.809615,
            "f1_weighted": 0.814135,
            "ap": 0.841408,
            "ap_weighted": 0.841408
          },
          {
            "accuracy": 0.846154,
            "f1": 0.841987,
            "f1_weighted": 0.84747,
            "ap": 0.856844,
            "ap_weighted": 0.856844
          },
          {
            "accuracy": 0.811966,
            "f1": 0.809615,
            "f1_weighted": 0.814135,
            "ap": 0.841408,
            "ap_weighted": 0.841408
          },
          {
            "accuracy": 0.837607,
            "f1": 0.834512,
            "f1_weighted": 0.839348,
            "ap": 0.857732,
            "ap_weighted": 0.857732
          },
          {
            "accuracy": 0.837607,
            "f1": 0.83367,
            "f1_weighted": 0.839138,
            "ap": 0.851428,
            "ap_weighted": 0.851428
          },
          {
            "accuracy": 0.863248,
            "f1": 0.860963,
            "f1_weighted": 0.864771,
            "ap": 0.887837,
            "ap_weighted": 0.887837
          },
          {
            "accuracy": 0.837607,
            "f1": 0.83588,
            "f1_weighted": 0.839477,
            "ap": 0.871332,
            "ap_weighted": 0.871332
          },
          {
            "accuracy": 0.820513,
            "f1": 0.81511,
            "f1_weighted": 0.821863,
            "ap": 0.828994,
            "ap_weighted": 0.828994
          },
          {
            "accuracy": 0.803419,
            "f1": 0.799672,
            "f1_weighted": 0.805526,
            "ap": 0.82394,
            "ap_weighted": 0.82394
          }
        ],
        "main_score": 0.832479,
        "hf_subset": "default",
        "languages": [
          "fas-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 37.65090250968933,
  "kg_co2_emissions": null
}