{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "task_name": "InappropriatenessClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.527637,
        "f1": 0.523933,
        "f1_weighted": 0.523933,
        "ap": 0.515054,
        "ap_weighted": 0.515054,
        "scores_per_experiment": [
          {
            "accuracy": 0.51123,
            "f1": 0.507297,
            "f1_weighted": 0.507297,
            "ap": 0.505769,
            "ap_weighted": 0.505769
          },
          {
            "accuracy": 0.555176,
            "f1": 0.555046,
            "f1_weighted": 0.555046,
            "ap": 0.530532,
            "ap_weighted": 0.530532
          },
          {
            "accuracy": 0.51709,
            "f1": 0.51709,
            "f1_weighted": 0.51709,
            "ap": 0.508837,
            "ap_weighted": 0.508837
          },
          {
            "accuracy": 0.519531,
            "f1": 0.511287,
            "f1_weighted": 0.511287,
            "ap": 0.510068,
            "ap_weighted": 0.510068
          },
          {
            "accuracy": 0.537598,
            "f1": 0.536332,
            "f1_weighted": 0.536332,
            "ap": 0.520079,
            "ap_weighted": 0.520079
          },
          {
            "accuracy": 0.523438,
            "f1": 0.507178,
            "f1_weighted": 0.507178,
            "ap": 0.512122,
            "ap_weighted": 0.512122
          },
          {
            "accuracy": 0.581543,
            "f1": 0.581219,
            "f1_weighted": 0.581219,
            "ap": 0.54707,
            "ap_weighted": 0.54707
          },
          {
            "accuracy": 0.523438,
            "f1": 0.52185,
            "f1_weighted": 0.52185,
            "ap": 0.512211,
            "ap_weighted": 0.512211
          },
          {
            "accuracy": 0.512207,
            "f1": 0.510498,
            "f1_weighted": 0.510498,
            "ap": 0.506272,
            "ap_weighted": 0.506272
          },
          {
            "accuracy": 0.495117,
            "f1": 0.491531,
            "f1_weighted": 0.491531,
            "ap": 0.497579,
            "ap_weighted": 0.497579
          }
        ],
        "main_score": 0.527637,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 8.62972116470337,
  "kg_co2_emissions": 0.0002631240051096507
}