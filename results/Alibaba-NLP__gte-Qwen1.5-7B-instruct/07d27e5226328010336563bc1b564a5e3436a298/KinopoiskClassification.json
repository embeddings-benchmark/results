{
  "dataset_revision": "5911f26666ac11af46cb9c6849d0dc80a378af24",
  "evaluation_time": 84.22461795806885,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.6716666666666666,
        "f1": 0.6476545663988791,
        "f1_weighted": 0.6476545663988791,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6716666666666666,
        "scores_per_experiment": [
          {
            "accuracy": 0.6746666666666666,
            "f1": 0.64998884298854,
            "f1_weighted": 0.64998884298854
          },
          {
            "accuracy": 0.6273333333333333,
            "f1": 0.5395116792472158,
            "f1_weighted": 0.5395116792472157
          },
          {
            "accuracy": 0.6933333333333334,
            "f1": 0.6851147871880379,
            "f1_weighted": 0.6851147871880379
          },
          {
            "accuracy": 0.6746666666666666,
            "f1": 0.6537567797235678,
            "f1_weighted": 0.6537567797235679
          },
          {
            "accuracy": 0.664,
            "f1": 0.6461389383302979,
            "f1_weighted": 0.6461389383302979
          },
          {
            "accuracy": 0.6786666666666666,
            "f1": 0.6629463945304079,
            "f1_weighted": 0.6629463945304079
          },
          {
            "accuracy": 0.6693333333333333,
            "f1": 0.6561239246851377,
            "f1_weighted": 0.6561239246851377
          },
          {
            "accuracy": 0.6746666666666666,
            "f1": 0.664298738939659,
            "f1_weighted": 0.664298738939659
          },
          {
            "accuracy": 0.678,
            "f1": 0.6493604358600386,
            "f1_weighted": 0.6493604358600387
          },
          {
            "accuracy": 0.682,
            "f1": 0.6693051424958885,
            "f1_weighted": 0.6693051424958885
          }
        ]
      }
    ]
  },
  "task_name": "KinopoiskClassification"
}