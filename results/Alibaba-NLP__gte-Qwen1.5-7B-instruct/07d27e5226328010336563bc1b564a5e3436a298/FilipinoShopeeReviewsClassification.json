{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.36792,
        "f1": 0.347753,
        "f1_weighted": 0.347756,
        "scores_per_experiment": [
          {
            "accuracy": 0.40332,
            "f1": 0.382624,
            "f1_weighted": 0.382605
          },
          {
            "accuracy": 0.35498,
            "f1": 0.326414,
            "f1_weighted": 0.326452
          },
          {
            "accuracy": 0.359863,
            "f1": 0.33397,
            "f1_weighted": 0.333996
          },
          {
            "accuracy": 0.385742,
            "f1": 0.36769,
            "f1_weighted": 0.367641
          },
          {
            "accuracy": 0.359863,
            "f1": 0.336392,
            "f1_weighted": 0.336415
          },
          {
            "accuracy": 0.385254,
            "f1": 0.348206,
            "f1_weighted": 0.348193
          },
          {
            "accuracy": 0.354492,
            "f1": 0.347197,
            "f1_weighted": 0.347188
          },
          {
            "accuracy": 0.374512,
            "f1": 0.363287,
            "f1_weighted": 0.363289
          },
          {
            "accuracy": 0.373535,
            "f1": 0.357659,
            "f1_weighted": 0.35764
          },
          {
            "accuracy": 0.327637,
            "f1": 0.314091,
            "f1_weighted": 0.314136
          }
        ],
        "main_score": 0.36792,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.368262,
        "f1": 0.349274,
        "f1_weighted": 0.349284,
        "scores_per_experiment": [
          {
            "accuracy": 0.395508,
            "f1": 0.376742,
            "f1_weighted": 0.376723
          },
          {
            "accuracy": 0.367188,
            "f1": 0.343084,
            "f1_weighted": 0.343107
          },
          {
            "accuracy": 0.359863,
            "f1": 0.334173,
            "f1_weighted": 0.334197
          },
          {
            "accuracy": 0.375,
            "f1": 0.35865,
            "f1_weighted": 0.358612
          },
          {
            "accuracy": 0.370117,
            "f1": 0.346168,
            "f1_weighted": 0.346216
          },
          {
            "accuracy": 0.382324,
            "f1": 0.345052,
            "f1_weighted": 0.34506
          },
          {
            "accuracy": 0.36377,
            "f1": 0.361106,
            "f1_weighted": 0.361108
          },
          {
            "accuracy": 0.377441,
            "f1": 0.363864,
            "f1_weighted": 0.363878
          },
          {
            "accuracy": 0.36377,
            "f1": 0.349371,
            "f1_weighted": 0.34937
          },
          {
            "accuracy": 0.327637,
            "f1": 0.314534,
            "f1_weighted": 0.314567
          }
        ],
        "main_score": 0.368262,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 50.0818874835968,
  "kg_co2_emissions": 0.012153432870013246
}