{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.85043,
        "f1": 0.737022,
        "f1_weighted": 0.866056,
        "ap": 0.374977,
        "ap_weighted": 0.374977,
        "scores_per_experiment": [
          {
            "accuracy": 0.811856,
            "f1": 0.681304,
            "f1_weighted": 0.834111,
            "ap": 0.29041,
            "ap_weighted": 0.29041
          },
          {
            "accuracy": 0.853093,
            "f1": 0.736111,
            "f1_weighted": 0.867734,
            "ap": 0.366671,
            "ap_weighted": 0.366671
          },
          {
            "accuracy": 0.86512,
            "f1": 0.746073,
            "f1_weighted": 0.876323,
            "ap": 0.376914,
            "ap_weighted": 0.376914
          },
          {
            "accuracy": 0.907216,
            "f1": 0.802275,
            "f1_weighted": 0.910187,
            "ap": 0.47186,
            "ap_weighted": 0.47186
          },
          {
            "accuracy": 0.835911,
            "f1": 0.719958,
            "f1_weighted": 0.854952,
            "ap": 0.348607,
            "ap_weighted": 0.348607
          },
          {
            "accuracy": 0.844502,
            "f1": 0.725173,
            "f1_weighted": 0.860837,
            "ap": 0.350935,
            "ap_weighted": 0.350935
          },
          {
            "accuracy": 0.872852,
            "f1": 0.754609,
            "f1_weighted": 0.882218,
            "ap": 0.388855,
            "ap_weighted": 0.388855
          },
          {
            "accuracy": 0.776632,
            "f1": 0.683256,
            "f1_weighted": 0.812092,
            "ap": 0.33679,
            "ap_weighted": 0.33679
          },
          {
            "accuracy": 0.854811,
            "f1": 0.739197,
            "f1_weighted": 0.869281,
            "ap": 0.371984,
            "ap_weighted": 0.371984
          },
          {
            "accuracy": 0.882302,
            "f1": 0.782259,
            "f1_weighted": 0.892827,
            "ap": 0.446747,
            "ap_weighted": 0.446747
          }
        ],
        "main_score": 0.85043,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 31.406710147857666,
  "kg_co2_emissions": 0.007574307312541318
}