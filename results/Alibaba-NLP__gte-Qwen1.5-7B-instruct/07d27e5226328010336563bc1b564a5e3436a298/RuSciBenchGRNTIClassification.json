{
  "dataset_revision": "673a610d6d3dd91a547a0d57ae1b56f37ebbf6a1",
  "evaluation_time": 175.38222646713257,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.67353515625,
        "f1": 0.6605647875373758,
        "f1_weighted": 0.6606887921446307,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.67353515625,
        "scores_per_experiment": [
          {
            "accuracy": 0.69287109375,
            "f1": 0.6853943231009784,
            "f1_weighted": 0.6855214162196541
          },
          {
            "accuracy": 0.67333984375,
            "f1": 0.6613774754925439,
            "f1_weighted": 0.6614810808494207
          },
          {
            "accuracy": 0.67529296875,
            "f1": 0.6624714325077754,
            "f1_weighted": 0.6626174476178033
          },
          {
            "accuracy": 0.67333984375,
            "f1": 0.6590053333191753,
            "f1_weighted": 0.6591454969159511
          },
          {
            "accuracy": 0.67822265625,
            "f1": 0.6671696203557742,
            "f1_weighted": 0.6672845357084765
          },
          {
            "accuracy": 0.65869140625,
            "f1": 0.6492278612017663,
            "f1_weighted": 0.6492791748778237
          },
          {
            "accuracy": 0.6806640625,
            "f1": 0.6619317114695685,
            "f1_weighted": 0.6621007634082923
          },
          {
            "accuracy": 0.666015625,
            "f1": 0.6511894116309452,
            "f1_weighted": 0.6512824516783355
          },
          {
            "accuracy": 0.68310546875,
            "f1": 0.670518919029475,
            "f1_weighted": 0.6706549028651936
          },
          {
            "accuracy": 0.65380859375,
            "f1": 0.6373617872657549,
            "f1_weighted": 0.6375206513053561
          }
        ]
      }
    ]
  },
  "task_name": "RuSciBenchGRNTIClassification"
}