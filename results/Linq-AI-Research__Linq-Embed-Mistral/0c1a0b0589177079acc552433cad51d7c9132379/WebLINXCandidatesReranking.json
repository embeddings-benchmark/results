{
  "dataset_revision": "ed1c933c2b3617e5700d8a7ebe07f5975969a453",
  "task_name": "WebLINXCandidatesReranking",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "map": 0.158068,
        "mrr": 0.138525,
        "nAUC_map_max": 0.038909,
        "nAUC_map_std": 0.010367,
        "nAUC_map_diff1": 0.107549,
        "nAUC_mrr_max": 0.042822,
        "nAUC_mrr_std": 0.0129,
        "nAUC_mrr_diff1": 0.112341,
        "main_score": 0.138525,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_iid": [
      {
        "map": 0.140427,
        "mrr": 0.122954,
        "nAUC_map_max": 0.114742,
        "nAUC_map_std": 0.123522,
        "nAUC_map_diff1": 0.130009,
        "nAUC_mrr_max": 0.119835,
        "nAUC_mrr_std": 0.107742,
        "nAUC_mrr_diff1": 0.136378,
        "main_score": 0.122954,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_cat": [
      {
        "map": 0.106808,
        "mrr": 0.09093,
        "nAUC_map_max": 0.12924,
        "nAUC_map_std": 0.28792,
        "nAUC_map_diff1": 0.14715,
        "nAUC_mrr_max": 0.119801,
        "nAUC_mrr_std": 0.266574,
        "nAUC_mrr_diff1": 0.137038,
        "main_score": 0.09093,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_geo": [
      {
        "map": 0.130545,
        "mrr": 0.111903,
        "nAUC_map_max": 0.159808,
        "nAUC_map_std": 0.157757,
        "nAUC_map_diff1": 0.192092,
        "nAUC_mrr_max": 0.15623,
        "nAUC_mrr_std": 0.149219,
        "nAUC_mrr_diff1": 0.188038,
        "main_score": 0.111903,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_vis": [
      {
        "map": 0.139388,
        "mrr": 0.120672,
        "nAUC_map_max": 0.158655,
        "nAUC_map_std": 0.169841,
        "nAUC_map_diff1": 0.183005,
        "nAUC_mrr_max": 0.157266,
        "nAUC_mrr_std": 0.155573,
        "nAUC_mrr_diff1": 0.187005,
        "main_score": 0.120672,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ],
    "test_web": [
      {
        "map": 0.107495,
        "mrr": 0.09185,
        "nAUC_map_max": 0.067118,
        "nAUC_map_std": 0.090123,
        "nAUC_map_diff1": 0.220873,
        "nAUC_mrr_max": 0.062486,
        "nAUC_mrr_std": 0.07167,
        "nAUC_mrr_diff1": 0.223559,
        "main_score": 0.09185,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 39003.918290138245,
  "kg_co2_emissions": 3.762134166186393
}