{
  "dataset_revision": "416b34a802308eac30e4192afc0ff99bb8dcc7f2",
  "evaluation_time": 74.08395957946777,
  "kg_co2_emissions": null,
  "mteb_version": "1.19.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.33876953125,
        "f1": 0.4320477109468386,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "lrap": 0.5515333387586783,
        "main_score": 0.33876953125,
        "scores_per_experiment": [
          {
            "accuracy": 0.3203125,
            "f1": 0.3797034278379044,
            "lrap": 0.5184461805555506
          },
          {
            "accuracy": 0.32763671875,
            "f1": 0.4567120053125303,
            "lrap": 0.5623508029513884
          },
          {
            "accuracy": 0.328125,
            "f1": 0.4064684057422513,
            "lrap": 0.5253906249999954
          },
          {
            "accuracy": 0.3427734375,
            "f1": 0.4023477721230191,
            "lrap": 0.5292697482638842
          },
          {
            "accuracy": 0.33154296875,
            "f1": 0.4418000621051738,
            "lrap": 0.5447455512152749
          },
          {
            "accuracy": 0.35498046875,
            "f1": 0.453143723880101,
            "lrap": 0.5694851345486107
          },
          {
            "accuracy": 0.361328125,
            "f1": 0.47656632928185083,
            "lrap": 0.5803765190972222
          },
          {
            "accuracy": 0.33203125,
            "f1": 0.42460658479537217,
            "lrap": 0.5488213433159701
          },
          {
            "accuracy": 0.34814453125,
            "f1": 0.4286615666188703,
            "lrap": 0.5553385416666645
          },
          {
            "accuracy": 0.3408203125,
            "f1": 0.4504672317713125,
            "lrap": 0.5811089409722222
          }
        ]
      }
    ]
  },
  "task_name": "SensitiveTopicsClassification"
}