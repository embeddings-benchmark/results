{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.420361,
        "f1": 0.390694,
        "f1_weighted": 0.39068,
        "scores_per_experiment": [
          {
            "accuracy": 0.429688,
            "f1": 0.404653,
            "f1_weighted": 0.40463
          },
          {
            "accuracy": 0.40625,
            "f1": 0.3726,
            "f1_weighted": 0.372604
          },
          {
            "accuracy": 0.42041,
            "f1": 0.381613,
            "f1_weighted": 0.381639
          },
          {
            "accuracy": 0.433105,
            "f1": 0.411928,
            "f1_weighted": 0.411893
          },
          {
            "accuracy": 0.411621,
            "f1": 0.369399,
            "f1_weighted": 0.369373
          },
          {
            "accuracy": 0.416016,
            "f1": 0.366813,
            "f1_weighted": 0.366827
          },
          {
            "accuracy": 0.419922,
            "f1": 0.398002,
            "f1_weighted": 0.397984
          },
          {
            "accuracy": 0.430664,
            "f1": 0.410922,
            "f1_weighted": 0.410906
          },
          {
            "accuracy": 0.425781,
            "f1": 0.403917,
            "f1_weighted": 0.403877
          },
          {
            "accuracy": 0.410156,
            "f1": 0.387091,
            "f1_weighted": 0.387068
          }
        ],
        "main_score": 0.420361,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.426172,
        "f1": 0.398545,
        "f1_weighted": 0.398534,
        "scores_per_experiment": [
          {
            "accuracy": 0.418457,
            "f1": 0.391469,
            "f1_weighted": 0.391449
          },
          {
            "accuracy": 0.432129,
            "f1": 0.402345,
            "f1_weighted": 0.402341
          },
          {
            "accuracy": 0.419434,
            "f1": 0.38194,
            "f1_weighted": 0.381978
          },
          {
            "accuracy": 0.418457,
            "f1": 0.396155,
            "f1_weighted": 0.396122
          },
          {
            "accuracy": 0.427246,
            "f1": 0.390715,
            "f1_weighted": 0.390696
          },
          {
            "accuracy": 0.431152,
            "f1": 0.384967,
            "f1_weighted": 0.384965
          },
          {
            "accuracy": 0.44873,
            "f1": 0.429845,
            "f1_weighted": 0.429829
          },
          {
            "accuracy": 0.410645,
            "f1": 0.391447,
            "f1_weighted": 0.391434
          },
          {
            "accuracy": 0.456543,
            "f1": 0.437279,
            "f1_weighted": 0.437255
          },
          {
            "accuracy": 0.398926,
            "f1": 0.379289,
            "f1_weighted": 0.379274
          }
        ],
        "main_score": 0.426172,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 45.88421082496643,
  "kg_co2_emissions": 0.0030956425205694816
}