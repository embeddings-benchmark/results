{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.269385,
        "f1": 0.263274,
        "f1_weighted": 0.26327,
        "scores_per_experiment": [
          {
            "accuracy": 0.274902,
            "f1": 0.277371,
            "f1_weighted": 0.277363
          },
          {
            "accuracy": 0.248535,
            "f1": 0.239119,
            "f1_weighted": 0.239138
          },
          {
            "accuracy": 0.240723,
            "f1": 0.238348,
            "f1_weighted": 0.238325
          },
          {
            "accuracy": 0.300781,
            "f1": 0.301037,
            "f1_weighted": 0.301031
          },
          {
            "accuracy": 0.264648,
            "f1": 0.253965,
            "f1_weighted": 0.253924
          },
          {
            "accuracy": 0.275879,
            "f1": 0.265574,
            "f1_weighted": 0.265574
          },
          {
            "accuracy": 0.225586,
            "f1": 0.227426,
            "f1_weighted": 0.227406
          },
          {
            "accuracy": 0.299805,
            "f1": 0.290161,
            "f1_weighted": 0.290178
          },
          {
            "accuracy": 0.309082,
            "f1": 0.297479,
            "f1_weighted": 0.297512
          },
          {
            "accuracy": 0.253906,
            "f1": 0.242261,
            "f1_weighted": 0.242249
          }
        ],
        "main_score": 0.269385,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.266211,
        "f1": 0.260731,
        "f1_weighted": 0.260731,
        "scores_per_experiment": [
          {
            "accuracy": 0.249023,
            "f1": 0.250935,
            "f1_weighted": 0.250939
          },
          {
            "accuracy": 0.265137,
            "f1": 0.257637,
            "f1_weighted": 0.257664
          },
          {
            "accuracy": 0.235352,
            "f1": 0.233496,
            "f1_weighted": 0.233468
          },
          {
            "accuracy": 0.291504,
            "f1": 0.291388,
            "f1_weighted": 0.29139
          },
          {
            "accuracy": 0.25293,
            "f1": 0.245556,
            "f1_weighted": 0.24551
          },
          {
            "accuracy": 0.260742,
            "f1": 0.247861,
            "f1_weighted": 0.247867
          },
          {
            "accuracy": 0.244141,
            "f1": 0.245796,
            "f1_weighted": 0.24579
          },
          {
            "accuracy": 0.303223,
            "f1": 0.297128,
            "f1_weighted": 0.297144
          },
          {
            "accuracy": 0.29248,
            "f1": 0.281561,
            "f1_weighted": 0.281582
          },
          {
            "accuracy": 0.267578,
            "f1": 0.255948,
            "f1_weighted": 0.255953
          }
        ],
        "main_score": 0.266211,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 20.342321157455444,
  "kg_co2_emissions": 0.0009705402312185314
}