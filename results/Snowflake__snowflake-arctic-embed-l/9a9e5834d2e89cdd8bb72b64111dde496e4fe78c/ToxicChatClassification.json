{
  "dataset_revision": "3e0319203c7162b9c9f8015b594441f979c199bc",
  "task_name": "ToxicChatClassification",
  "mteb_version": "1.18.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.82122,
        "f1": 0.688057,
        "f1_weighted": 0.840428,
        "ap": 0.298673,
        "ap_weighted": 0.298673,
        "scores_per_experiment": [
          {
            "accuracy": 0.781787,
            "f1": 0.63377,
            "f1_weighted": 0.80819,
            "ap": 0.230414,
            "ap_weighted": 0.230414
          },
          {
            "accuracy": 0.871134,
            "f1": 0.71781,
            "f1_weighted": 0.873636,
            "ap": 0.318864,
            "ap_weighted": 0.318864
          },
          {
            "accuracy": 0.863402,
            "f1": 0.738176,
            "f1_weighted": 0.873825,
            "ap": 0.361083,
            "ap_weighted": 0.361083
          },
          {
            "accuracy": 0.786942,
            "f1": 0.661831,
            "f1_weighted": 0.815922,
            "ap": 0.275057,
            "ap_weighted": 0.275057
          },
          {
            "accuracy": 0.774055,
            "f1": 0.637143,
            "f1_weighted": 0.804118,
            "ap": 0.241043,
            "ap_weighted": 0.241043
          },
          {
            "accuracy": 0.848797,
            "f1": 0.705466,
            "f1_weighted": 0.859388,
            "ap": 0.307309,
            "ap_weighted": 0.307309
          },
          {
            "accuracy": 0.85567,
            "f1": 0.726461,
            "f1_weighted": 0.867299,
            "ap": 0.342984,
            "ap_weighted": 0.342984
          },
          {
            "accuracy": 0.804124,
            "f1": 0.685964,
            "f1_weighted": 0.830271,
            "ap": 0.307669,
            "ap_weighted": 0.307669
          },
          {
            "accuracy": 0.815292,
            "f1": 0.681145,
            "f1_weighted": 0.83608,
            "ap": 0.287135,
            "ap_weighted": 0.287135
          },
          {
            "accuracy": 0.810997,
            "f1": 0.692807,
            "f1_weighted": 0.835551,
            "ap": 0.315171,
            "ap_weighted": 0.315171
          }
        ],
        "main_score": 0.82122,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 9.145353317260742,
  "kg_co2_emissions": 0.00039863296024633845
}