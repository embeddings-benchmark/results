{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.38.9",
  "scores": {
    "validation": [
      {
        "accuracy": 0.279346,
        "f1": 0.273599,
        "f1_weighted": 0.273589,
        "scores_per_experiment": [
          {
            "accuracy": 0.271973,
            "f1": 0.267152,
            "f1_weighted": 0.267157
          },
          {
            "accuracy": 0.293457,
            "f1": 0.278689,
            "f1_weighted": 0.278624
          },
          {
            "accuracy": 0.245117,
            "f1": 0.24145,
            "f1_weighted": 0.24143
          },
          {
            "accuracy": 0.296387,
            "f1": 0.293698,
            "f1_weighted": 0.293719
          },
          {
            "accuracy": 0.265625,
            "f1": 0.263879,
            "f1_weighted": 0.263846
          },
          {
            "accuracy": 0.271973,
            "f1": 0.264458,
            "f1_weighted": 0.26447
          },
          {
            "accuracy": 0.268066,
            "f1": 0.264798,
            "f1_weighted": 0.264801
          },
          {
            "accuracy": 0.310547,
            "f1": 0.30216,
            "f1_weighted": 0.302134
          },
          {
            "accuracy": 0.288086,
            "f1": 0.280264,
            "f1_weighted": 0.280269
          },
          {
            "accuracy": 0.282227,
            "f1": 0.279441,
            "f1_weighted": 0.279444
          }
        ],
        "main_score": 0.279346,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.284131,
        "f1": 0.277933,
        "f1_weighted": 0.277923,
        "scores_per_experiment": [
          {
            "accuracy": 0.251953,
            "f1": 0.245895,
            "f1_weighted": 0.245897
          },
          {
            "accuracy": 0.306641,
            "f1": 0.293791,
            "f1_weighted": 0.29375
          },
          {
            "accuracy": 0.256348,
            "f1": 0.251455,
            "f1_weighted": 0.25142
          },
          {
            "accuracy": 0.293945,
            "f1": 0.293351,
            "f1_weighted": 0.293346
          },
          {
            "accuracy": 0.291992,
            "f1": 0.28766,
            "f1_weighted": 0.287616
          },
          {
            "accuracy": 0.282227,
            "f1": 0.272861,
            "f1_weighted": 0.27286
          },
          {
            "accuracy": 0.27002,
            "f1": 0.267153,
            "f1_weighted": 0.267169
          },
          {
            "accuracy": 0.298828,
            "f1": 0.289161,
            "f1_weighted": 0.28916
          },
          {
            "accuracy": 0.290039,
            "f1": 0.283585,
            "f1_weighted": 0.283595
          },
          {
            "accuracy": 0.299316,
            "f1": 0.294423,
            "f1_weighted": 0.294419
          }
        ],
        "main_score": 0.284131,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 1.0181076526641846,
  "kg_co2_emissions": null
}