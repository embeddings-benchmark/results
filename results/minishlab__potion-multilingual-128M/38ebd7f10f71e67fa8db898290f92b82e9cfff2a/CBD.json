{
  "dataset_revision": "36ddb419bcffe6a5374c3891957912892916f28d",
  "task_name": "CBD",
  "mteb_version": "2.1.17",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.531,
            "f1": 0.478606,
            "f1_weighted": 0.599592,
            "precision": 0.567516,
            "precision_weighted": 0.840272,
            "recall": 0.644057,
            "recall_weighted": 0.531,
            "ap": 0.182629,
            "ap_weighted": 0.182629
          },
          {
            "accuracy": 0.641,
            "f1": 0.461826,
            "f1_weighted": 0.689132,
            "precision": 0.483633,
            "precision_weighted": 0.756601,
            "recall": 0.47102,
            "recall_weighted": 0.641,
            "ap": 0.128442,
            "ap_weighted": 0.128442
          },
          {
            "accuracy": 0.615,
            "f1": 0.512238,
            "f1_weighted": 0.67612,
            "precision": 0.548606,
            "precision_weighted": 0.8099,
            "recall": 0.601091,
            "recall_weighted": 0.615,
            "ap": 0.167555,
            "ap_weighted": 0.167555
          },
          {
            "accuracy": 0.609,
            "f1": 0.517745,
            "f1_weighted": 0.671305,
            "precision": 0.559628,
            "precision_weighted": 0.821516,
            "recall": 0.626013,
            "recall_weighted": 0.609,
            "ap": 0.178056,
            "ap_weighted": 0.178056
          },
          {
            "accuracy": 0.601,
            "f1": 0.467289,
            "f1_weighted": 0.662651,
            "precision": 0.502351,
            "precision_weighted": 0.769799,
            "recall": 0.504696,
            "recall_weighted": 0.601,
            "ap": 0.135114,
            "ap_weighted": 0.135114
          },
          {
            "accuracy": 0.725,
            "f1": 0.586316,
            "f1_weighted": 0.761647,
            "precision": 0.584401,
            "precision_weighted": 0.825994,
            "recall": 0.648831,
            "recall_weighted": 0.725,
            "ap": 0.199567,
            "ap_weighted": 0.199567
          },
          {
            "accuracy": 0.581,
            "f1": 0.514123,
            "f1_weighted": 0.646074,
            "precision": 0.577348,
            "precision_weighted": 0.844694,
            "recall": 0.666618,
            "recall_weighted": 0.581,
            "ap": 0.195214,
            "ap_weighted": 0.195214
          },
          {
            "accuracy": 0.476,
            "f1": 0.442247,
            "f1_weighted": 0.542683,
            "precision": 0.567742,
            "precision_weighted": 0.847555,
            "recall": 0.637534,
            "recall_weighted": 0.476,
            "ap": 0.178184,
            "ap_weighted": 0.178184
          },
          {
            "accuracy": 0.466,
            "f1": 0.424678,
            "f1_weighted": 0.537542,
            "precision": 0.539244,
            "precision_weighted": 0.812786,
            "recall": 0.581297,
            "recall_weighted": 0.466,
            "ap": 0.157311,
            "ap_weighted": 0.157311
          },
          {
            "accuracy": 0.644,
            "f1": 0.524062,
            "f1_weighted": 0.698952,
            "precision": 0.547999,
            "precision_weighted": 0.806354,
            "recall": 0.595757,
            "recall_weighted": 0.644,
            "ap": 0.16635,
            "ap_weighted": 0.16635
          }
        ],
        "accuracy": 0.5889,
        "f1": 0.492913,
        "f1_weighted": 0.64857,
        "precision": 0.547847,
        "precision_weighted": 0.813547,
        "recall": 0.597691,
        "recall_weighted": 0.5889,
        "ap": 0.168842,
        "ap_weighted": 0.168842,
        "main_score": 0.5889,
        "hf_subset": "default",
        "languages": [
          "pol-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 22.794604778289795,
  "kg_co2_emissions": null
}