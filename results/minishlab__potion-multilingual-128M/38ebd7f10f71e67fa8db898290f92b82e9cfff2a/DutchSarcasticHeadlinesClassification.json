{
  "dataset_revision": "7e520e36394795859583f84f81fcb97de915d05a",
  "task_name": "DutchSarcasticHeadlinesClassification",
  "mteb_version": "2.1.0",
  "scores": {
    "test": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.53997,
            "f1": 0.536544,
            "f1_weighted": 0.54634,
            "precision": 0.549324,
            "precision_weighted": 0.580442,
            "recall": 0.552211,
            "recall_weighted": 0.53997,
            "ap": 0.404573,
            "ap_weighted": 0.404573
          },
          {
            "accuracy": 0.565611,
            "f1": 0.548242,
            "f1_weighted": 0.57002,
            "precision": 0.548668,
            "precision_weighted": 0.577139,
            "recall": 0.55069,
            "recall_weighted": 0.565611,
            "ap": 0.40441,
            "ap_weighted": 0.40441
          },
          {
            "accuracy": 0.575415,
            "f1": 0.567888,
            "f1_weighted": 0.581909,
            "precision": 0.573214,
            "precision_weighted": 0.60311,
            "recall": 0.577898,
            "recall_weighted": 0.575415,
            "ap": 0.420903,
            "ap_weighted": 0.420903
          },
          {
            "accuracy": 0.59276,
            "f1": 0.58659,
            "f1_weighted": 0.599007,
            "precision": 0.592946,
            "precision_weighted": 0.623202,
            "recall": 0.598925,
            "recall_weighted": 0.59276,
            "ap": 0.434985,
            "ap_weighted": 0.434985
          },
          {
            "accuracy": 0.627451,
            "f1": 0.611585,
            "f1_weighted": 0.630885,
            "precision": 0.610761,
            "precision_weighted": 0.63667,
            "recall": 0.61493,
            "recall_weighted": 0.627451,
            "ap": 0.449436,
            "ap_weighted": 0.449436
          },
          {
            "accuracy": 0.595023,
            "f1": 0.582348,
            "f1_weighted": 0.600235,
            "precision": 0.583291,
            "precision_weighted": 0.611412,
            "recall": 0.587717,
            "recall_weighted": 0.595023,
            "ap": 0.428317,
            "ap_weighted": 0.428317
          },
          {
            "accuracy": 0.628205,
            "f1": 0.585638,
            "f1_weighted": 0.61829,
            "precision": 0.594364,
            "precision_weighted": 0.615418,
            "recall": 0.584751,
            "recall_weighted": 0.628205,
            "ap": 0.430789,
            "ap_weighted": 0.430789
          },
          {
            "accuracy": 0.621418,
            "f1": 0.596744,
            "f1_weighted": 0.621267,
            "precision": 0.596822,
            "precision_weighted": 0.62112,
            "recall": 0.596668,
            "recall_weighted": 0.621418,
            "ap": 0.437049,
            "ap_weighted": 0.437049
          },
          {
            "accuracy": 0.52187,
            "f1": 0.519755,
            "f1_weighted": 0.527591,
            "precision": 0.53624,
            "precision_weighted": 0.567469,
            "recall": 0.538077,
            "recall_weighted": 0.52187,
            "ap": 0.396487,
            "ap_weighted": 0.396487
          },
          {
            "accuracy": 0.59276,
            "f1": 0.569853,
            "f1_weighted": 0.594257,
            "precision": 0.569479,
            "precision_weighted": 0.596016,
            "recall": 0.570508,
            "recall_weighted": 0.59276,
            "ap": 0.417617,
            "ap_weighted": 0.417617
          }
        ],
        "accuracy": 0.586048,
        "f1": 0.570519,
        "f1_weighted": 0.58898,
        "precision": 0.575511,
        "precision_weighted": 0.6032,
        "recall": 0.577238,
        "recall_weighted": 0.586048,
        "ap": 0.422457,
        "ap_weighted": 0.422457,
        "main_score": 0.570519,
        "hf_subset": "default",
        "languages": [
          "nld-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 7.273167371749878,
  "kg_co2_emissions": null
}