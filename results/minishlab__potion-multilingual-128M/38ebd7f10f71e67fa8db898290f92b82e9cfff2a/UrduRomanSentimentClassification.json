{
  "dataset_revision": "905c1121c002c4b9adc4ebc5faaf4d6f50d1b1ee",
  "task_name": "UrduRomanSentimentClassification",
  "mteb_version": "2.1.17",
  "scores": {
    "train": [
      {
        "scores_per_experiment": [
          {
            "accuracy": 0.434082,
            "f1": 0.423873,
            "f1_weighted": 0.434086,
            "precision": 0.438584,
            "precision_weighted": 0.463972,
            "recall": 0.436328,
            "recall_weighted": 0.434082,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.409668,
            "f1": 0.389567,
            "f1_weighted": 0.40523,
            "precision": 0.40898,
            "precision_weighted": 0.427976,
            "recall": 0.402248,
            "recall_weighted": 0.409668,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.379883,
            "f1": 0.374838,
            "f1_weighted": 0.382442,
            "precision": 0.379363,
            "precision_weighted": 0.397182,
            "recall": 0.381104,
            "recall_weighted": 0.379883,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.381348,
            "f1": 0.382294,
            "f1_weighted": 0.380441,
            "precision": 0.415498,
            "precision_weighted": 0.441115,
            "recall": 0.404509,
            "recall_weighted": 0.381348,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.369629,
            "f1": 0.3583,
            "f1_weighted": 0.370211,
            "precision": 0.361487,
            "precision_weighted": 0.375511,
            "recall": 0.360173,
            "recall_weighted": 0.369629,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.432617,
            "f1": 0.428488,
            "f1_weighted": 0.433938,
            "precision": 0.437921,
            "precision_weighted": 0.460109,
            "recall": 0.440751,
            "recall_weighted": 0.432617,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.445312,
            "f1": 0.433926,
            "f1_weighted": 0.449435,
            "precision": 0.435323,
            "precision_weighted": 0.457899,
            "recall": 0.436402,
            "recall_weighted": 0.445312,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.366699,
            "f1": 0.363897,
            "f1_weighted": 0.368769,
            "precision": 0.372618,
            "precision_weighted": 0.385648,
            "recall": 0.371588,
            "recall_weighted": 0.366699,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.391113,
            "f1": 0.386776,
            "f1_weighted": 0.394891,
            "precision": 0.392021,
            "precision_weighted": 0.411937,
            "recall": 0.394284,
            "recall_weighted": 0.391113,
            "ap": null,
            "ap_weighted": null
          },
          {
            "accuracy": 0.402832,
            "f1": 0.396029,
            "f1_weighted": 0.407641,
            "precision": 0.402843,
            "precision_weighted": 0.426879,
            "recall": 0.401801,
            "recall_weighted": 0.402832,
            "ap": null,
            "ap_weighted": null
          }
        ],
        "accuracy": 0.401318,
        "f1": 0.393799,
        "f1_weighted": 0.402708,
        "precision": 0.404464,
        "precision_weighted": 0.424823,
        "recall": 0.402919,
        "recall_weighted": 0.401318,
        "ap": NaN,
        "ap_weighted": NaN,
        "main_score": 0.393799,
        "hf_subset": "default",
        "languages": [
          "urd-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 27.924420833587646,
  "kg_co2_emissions": null
}