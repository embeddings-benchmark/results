{
  "dataset_revision": "edfaf9da55d3dd50d43143d90c1ac476895ae6de",
  "task_name": "ToxicConversationsClassification",
  "mteb_version": "1.34.14",
  "scores": {
    "test": [
      {
        "accuracy": 0.66582,
        "f1": 0.498727,
        "f1_weighted": 0.739936,
        "ap": 0.108115,
        "ap_weighted": 0.108115,
        "scores_per_experiment": [
          {
            "accuracy": 0.679688,
            "f1": 0.498961,
            "f1_weighted": 0.752272,
            "ap": 0.101289,
            "ap_weighted": 0.101289
          },
          {
            "accuracy": 0.716309,
            "f1": 0.527312,
            "f1_weighted": 0.778918,
            "ap": 0.114359,
            "ap_weighted": 0.114359
          },
          {
            "accuracy": 0.724121,
            "f1": 0.525514,
            "f1_weighted": 0.783928,
            "ap": 0.109199,
            "ap_weighted": 0.109199
          },
          {
            "accuracy": 0.765137,
            "f1": 0.561848,
            "f1_weighted": 0.813081,
            "ap": 0.130092,
            "ap_weighted": 0.130092
          },
          {
            "accuracy": 0.585449,
            "f1": 0.45533,
            "f1_weighted": 0.679432,
            "ap": 0.099123,
            "ap_weighted": 0.099123
          },
          {
            "accuracy": 0.518555,
            "f1": 0.418932,
            "f1_weighted": 0.621467,
            "ap": 0.094941,
            "ap_weighted": 0.094941
          },
          {
            "accuracy": 0.725586,
            "f1": 0.521237,
            "f1_weighted": 0.784539,
            "ap": 0.104652,
            "ap_weighted": 0.104652
          },
          {
            "accuracy": 0.620117,
            "f1": 0.481114,
            "f1_weighted": 0.707191,
            "ap": 0.110224,
            "ap_weighted": 0.110224
          },
          {
            "accuracy": 0.658691,
            "f1": 0.495413,
            "f1_weighted": 0.737037,
            "ap": 0.106644,
            "ap_weighted": 0.106644
          },
          {
            "accuracy": 0.664551,
            "f1": 0.501614,
            "f1_weighted": 0.741497,
            "ap": 0.110631,
            "ap_weighted": 0.110631
          }
        ],
        "main_score": 0.66582,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 6.723692893981934,
  "kg_co2_emissions": null
}