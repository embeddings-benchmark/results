{
  "dataset_revision": "557bf94ac6177cc442f42d0b09b6e4b76e8f47c9",
  "task_name": "TweetSarcasmClassification",
  "mteb_version": "1.34.14",
  "scores": {
    "test": [
      {
        "accuracy": 0.603081,
        "f1": 0.521199,
        "f1_weighted": 0.649516,
        "ap": 0.206412,
        "ap_weighted": 0.206412,
        "scores_per_experiment": [
          {
            "accuracy": 0.679147,
            "f1": 0.541874,
            "f1_weighted": 0.710642,
            "ap": 0.189438,
            "ap_weighted": 0.189438
          },
          {
            "accuracy": 0.537915,
            "f1": 0.495621,
            "f1_weighted": 0.593914,
            "ap": 0.211753,
            "ap_weighted": 0.211753
          },
          {
            "accuracy": 0.563507,
            "f1": 0.512084,
            "f1_weighted": 0.618684,
            "ap": 0.214511,
            "ap_weighted": 0.214511
          },
          {
            "accuracy": 0.527014,
            "f1": 0.491994,
            "f1_weighted": 0.581758,
            "ap": 0.217593,
            "ap_weighted": 0.217593
          },
          {
            "accuracy": 0.57109,
            "f1": 0.522514,
            "f1_weighted": 0.625008,
            "ap": 0.225252,
            "ap_weighted": 0.225252
          },
          {
            "accuracy": 0.578673,
            "f1": 0.520488,
            "f1_weighted": 0.6329,
            "ap": 0.214513,
            "ap_weighted": 0.214513
          },
          {
            "accuracy": 0.654502,
            "f1": 0.539336,
            "f1_weighted": 0.694346,
            "ap": 0.194723,
            "ap_weighted": 0.194723
          },
          {
            "accuracy": 0.696209,
            "f1": 0.516223,
            "f1_weighted": 0.714808,
            "ap": 0.170594,
            "ap_weighted": 0.170594
          },
          {
            "accuracy": 0.599052,
            "f1": 0.533683,
            "f1_weighted": 0.651182,
            "ap": 0.218168,
            "ap_weighted": 0.218168
          },
          {
            "accuracy": 0.623697,
            "f1": 0.538173,
            "f1_weighted": 0.671921,
            "ap": 0.207579,
            "ap_weighted": 0.207579
          }
        ],
        "main_score": 0.603081,
        "hf_subset": "default",
        "languages": [
          "ara-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 2.9086320400238037,
  "kg_co2_emissions": null
}