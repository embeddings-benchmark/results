{
  "dataset_revision": "14aa5773afa135ba835cc5179bbc4a63657a42ae",
  "task_name": "WisesightSentimentClassification",
  "mteb_version": "1.34.14",
  "scores": {
    "test": [
      {
        "accuracy": 0.279004,
        "f1": 0.233548,
        "f1_weighted": 0.276181,
        "scores_per_experiment": [
          {
            "accuracy": 0.260742,
            "f1": 0.222157,
            "f1_weighted": 0.209149
          },
          {
            "accuracy": 0.283691,
            "f1": 0.256911,
            "f1_weighted": 0.276454
          },
          {
            "accuracy": 0.32959,
            "f1": 0.237919,
            "f1_weighted": 0.363552
          },
          {
            "accuracy": 0.347168,
            "f1": 0.272023,
            "f1_weighted": 0.374104
          },
          {
            "accuracy": 0.272949,
            "f1": 0.213997,
            "f1_weighted": 0.312723
          },
          {
            "accuracy": 0.260742,
            "f1": 0.225368,
            "f1_weighted": 0.291417
          },
          {
            "accuracy": 0.213379,
            "f1": 0.201975,
            "f1_weighted": 0.174918
          },
          {
            "accuracy": 0.273926,
            "f1": 0.228995,
            "f1_weighted": 0.219901
          },
          {
            "accuracy": 0.292969,
            "f1": 0.253429,
            "f1_weighted": 0.278915
          },
          {
            "accuracy": 0.254883,
            "f1": 0.222707,
            "f1_weighted": 0.26068
          }
        ],
        "main_score": 0.233548,
        "hf_subset": "default",
        "languages": [
          "tha-Thai"
        ]
      }
    ]
  },
  "evaluation_time": 3.510394334793091,
  "kg_co2_emissions": null
}