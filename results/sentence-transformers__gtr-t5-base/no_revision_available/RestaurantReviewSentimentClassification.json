{
  "dataset_revision": "d51bf2435d030e0041344f576c5e8d7154828977",
  "task_name": "RestaurantReviewSentimentClassification",
  "mteb_version": "1.34.14",
  "scores": {
    "train": [
      {
        "accuracy": 0.499805,
        "f1": 0.473383,
        "f1_weighted": 0.517018,
        "ap": 0.713142,
        "ap_weighted": 0.713142,
        "scores_per_experiment": [
          {
            "accuracy": 0.42041,
            "f1": 0.418029,
            "f1_weighted": 0.433734,
            "ap": 0.702453,
            "ap_weighted": 0.702453
          },
          {
            "accuracy": 0.486816,
            "f1": 0.463035,
            "f1_weighted": 0.510708,
            "ap": 0.704567,
            "ap_weighted": 0.704567
          },
          {
            "accuracy": 0.519043,
            "f1": 0.494071,
            "f1_weighted": 0.54149,
            "ap": 0.718231,
            "ap_weighted": 0.718231
          },
          {
            "accuracy": 0.406738,
            "f1": 0.406426,
            "f1_weighted": 0.412172,
            "ap": 0.70418,
            "ap_weighted": 0.70418
          },
          {
            "accuracy": 0.499023,
            "f1": 0.460388,
            "f1_weighted": 0.521302,
            "ap": 0.698892,
            "ap_weighted": 0.698892
          },
          {
            "accuracy": 0.537598,
            "f1": 0.467077,
            "f1_weighted": 0.548862,
            "ap": 0.697887,
            "ap_weighted": 0.697887
          },
          {
            "accuracy": 0.54248,
            "f1": 0.511852,
            "f1_weighted": 0.563437,
            "ap": 0.724269,
            "ap_weighted": 0.724269
          },
          {
            "accuracy": 0.57959,
            "f1": 0.539102,
            "f1_weighted": 0.596732,
            "ap": 0.733847,
            "ap_weighted": 0.733847
          },
          {
            "accuracy": 0.541016,
            "f1": 0.511676,
            "f1_weighted": 0.562173,
            "ap": 0.724701,
            "ap_weighted": 0.724701
          },
          {
            "accuracy": 0.465332,
            "f1": 0.462171,
            "f1_weighted": 0.479565,
            "ap": 0.722392,
            "ap_weighted": 0.722392
          }
        ],
        "main_score": 0.499805,
        "hf_subset": "default",
        "languages": [
          "ara-Arab"
        ]
      }
    ]
  },
  "evaluation_time": 5.379749774932861,
  "kg_co2_emissions": null
}