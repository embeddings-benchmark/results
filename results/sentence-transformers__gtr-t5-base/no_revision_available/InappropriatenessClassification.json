{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "task_name": "InappropriatenessClassification",
  "mteb_version": "1.34.14",
  "scores": {
    "test": [
      {
        "accuracy": 0.519482,
        "f1": 0.509718,
        "f1_weighted": 0.509718,
        "ap": 0.510485,
        "ap_weighted": 0.510485,
        "scores_per_experiment": [
          {
            "accuracy": 0.51709,
            "f1": 0.489879,
            "f1_weighted": 0.489879,
            "ap": 0.509088,
            "ap_weighted": 0.509088
          },
          {
            "accuracy": 0.567871,
            "f1": 0.567462,
            "f1_weighted": 0.567462,
            "ap": 0.538275,
            "ap_weighted": 0.538275
          },
          {
            "accuracy": 0.499512,
            "f1": 0.498142,
            "f1_weighted": 0.498142,
            "ap": 0.499756,
            "ap_weighted": 0.499756
          },
          {
            "accuracy": 0.509766,
            "f1": 0.508774,
            "f1_weighted": 0.508774,
            "ap": 0.50497,
            "ap_weighted": 0.50497
          },
          {
            "accuracy": 0.512695,
            "f1": 0.490222,
            "f1_weighted": 0.490222,
            "ap": 0.506461,
            "ap_weighted": 0.506461
          },
          {
            "accuracy": 0.508301,
            "f1": 0.500923,
            "f1_weighted": 0.500923,
            "ap": 0.504206,
            "ap_weighted": 0.504206
          },
          {
            "accuracy": 0.530273,
            "f1": 0.508611,
            "f1_weighted": 0.508611,
            "ap": 0.516717,
            "ap_weighted": 0.516717
          },
          {
            "accuracy": 0.522949,
            "f1": 0.511239,
            "f1_weighted": 0.511239,
            "ap": 0.511877,
            "ap_weighted": 0.511877
          },
          {
            "accuracy": 0.515625,
            "f1": 0.511239,
            "f1_weighted": 0.511239,
            "ap": 0.508018,
            "ap_weighted": 0.508018
          },
          {
            "accuracy": 0.510742,
            "f1": 0.510686,
            "f1_weighted": 0.510686,
            "ap": 0.505484,
            "ap_weighted": 0.505484
          }
        ],
        "main_score": 0.519482,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 5.120187044143677,
  "kg_co2_emissions": null
}